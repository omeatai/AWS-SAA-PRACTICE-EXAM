<details>
  <summary>Question 151</summary>

A company wants to migrate its on-premises data center to AWS. According to the company's compliance requirements, the company can use only the ap-northeast-3 Region. Company administrators are not permitted to connect VPCs to the internet.

Which solutions will meet these requirements? (Choose two.)

- [ ] A. Use AWS Control Tower to implement data residency guardrails to deny internet access and deny access to all AWS Regions except ap-northeast-3.
- [ ] B. Use rules in AWS WAF to prevent internet access. Deny access to all AWS Regions except ap-northeast-3 in the AWS account settings.
- [ ] C. Use AWS Organizations to configure service control policies (SCPS) that prevent VPCs from gaining internet access. Deny access to all AWS Regions except ap-northeast-3.
- [ ] D. Create an outbound rule for the network ACL in each VPC to deny all traffic from 0.0.0.0/0 Create an IAM policy for each user to prevent the use of any AWS Region other than ap-northeast-3.
- [ ] E. Use AWS Config to activate managed rules to detect and alert for internet gateways and to detect and alert for new resources deployed outside of ap-northeast-3.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Use AWS Control Tower to implement data residency guardrails to deny internet access and deny access to all AWS Regions except ap-northeast-3.
- [ ] C. Use AWS Organizations to configure service control policies (SCPS) that prevent VPCs from gaining internet access. Deny access to all AWS Regions except ap-northeast-3.

Why these are the correct answers:

These options provide preventative controls at an organizational level to enforce the specified compliance requirements:

A. Use AWS Control Tower to implement data residency guardrails to deny internet access and deny access to all AWS Regions except ap-northeast-3.
- [ ] AWS Control Tower for Governance: AWS Control Tower provides an easy way to set up and govern a secure, multi-account AWS environment based on best practices. It uses guardrails (which are often implemented using Service Control Policies and AWS Config rules) to enforce policies.
- [ ] Data Residency Guardrails: Control Tower offers specific guardrails for data residency that can be used to deny access to AWS services and operations in unauthorized AWS Regions, thereby ensuring work occurs only in ap-northeast-3.
- [ ] Preventing Internet Access: Control Tower can also deploy guardrails that help prevent VPCs from connecting to the internet, for example, by restricting the creation or use of Internet Gateways.
- [ ] Centralized Enforcement: Control Tower provides a comprehensive, managed way to establish and enforce these types of compliance rules across an organization.

C. Use AWS Organizations to configure service control policies (SCPS) that prevent VPCs from gaining internet access. Deny access to all AWS Regions except ap-northeast-3.
- [ ] AWS Organizations and SCPs: Service Control Policies (SCPs) are a feature of AWS Organizations that offer central control over the maximum available permissions for all accounts in your organization. SCPs can restrict which AWS services, resources, and individual API actions users and roles in member accounts can access.
- [ ] Denying Access to Other Regions: An SCP can be configured to explicitly deny access to all AWS API actions in Regions other than ap-northeast-3. This effectively limits operations to the allowed region.
- [ ] Preventing Internet Access for VPCs: SCPs can also be used to deny specific EC2 actions required to connect a VPC to the internet, such as ec2:CreateInternetGateway, ec2:AttachInternetGateway, or actions that modify route tables to use an Internet Gateway. This ensures administrators cannot connect VPCs to the internet.
- [ ] Preventative Control: SCPs act as preventative guardrails that even administrators in member accounts cannot override (unless they have permissions in the management account to modify the SCP itself).

Why are the other answers wrong?

- [ ] Option B is wrong because: AWS WAF (Web Application Firewall) is used to protect web applications from common web exploits. It does not control general VPC internet connectivity or restrict access to AWS Regions. Denying access to regions in "AWS account settings" is less robust and centrally managed than using SCPs or Control Tower guardrails for organizational compliance.
- [ ] Option D is wrong because:
Network ACLs operate at the subnet level and are stateless. While they can block traffic, managing NACLs across all VPCs to deny all outbound internet traffic (the 0.0.0.0/0 in an outbound rule is not how sources are typically specified for outbound; it would be a destination) can be complex and might inadvertently block necessary internal or AWS service traffic. It's also not as robust as an SCP for preventing administrators from creating internet connectivity.
IAM policies apply to users, groups, and roles, restricting what they can do. While useful, SCPs provide an overarching organizational boundary that applies even to administrators within member accounts.
- [ ] Option E is wrong because: AWS Config is a service for assessing, auditing, and evaluating the configurations of AWS resources. It provides detective controls, meaning it can detect and alert when a non-compliant configuration occurs (like an Internet Gateway being created or resources deployed in a wrong region). However, it does not prevent these actions. The requirement is that administrators "are not permitted" to do these things, which implies a need for preventative controls.

</details>

<details>
  <summary>Question 152</summary>

A company wants to migrate an on-premises data center to AWS. The data center hosts an SFTP server that stores its data on an NFS-based file system. The server holds 200 GB of data that needs to be transferred. The server must be hosted on an Amazon EC2 instance that uses an Amazon Elastic File System (Amazon EFS) file system.

Which combination of steps should a solutions architect take to automate this task? (Choose two.)

- [ ] A. Launch the EC2 instance into the same Availability Zone as the EFS file system.
- [ ] B. Install an AWS DataSync agent in the on-premises data center.
- [ ] C. Create a secondary Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instance for the data.
- [ ] D. Manually use an operating system copy command to push the data to the EC2 instance.
- [ ] E. Use AWS DataSync to create a suitable location configuration for the on-premises SFTP server.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Install an AWS DataSync agent in the on-premises data center.
- [ ] E. Use AWS DataSync to create a suitable location configuration for the on-premises SFTP server.

Why these are the correct answers:

This solution focuses on automating the transfer of 200 GB of data from an on-premises NFS file system (backing an SFTP server) to an Amazon EFS file system in AWS.

B. Install an AWS DataSync agent in the on-premises data center.
- [ ] AWS DataSync Agent: To use AWS DataSync for transferring data from an on-premises location, you need to deploy a DataSync agent (a virtual machine) in your on-premises environment.
- [ ] This agent will access your on-premises NFS file system and manage the data transfer to AWS.

E. Use AWS DataSync to create a suitable location configuration for the on-premises SFTP server.
- [ ] DataSync Locations and Task: Once the agent is deployed and activated, you configure AWS DataSync by creating a source location that points to your on-premises NFS file system (which backs the SFTP server) and a destination location that points to your Amazon EFS file system in AWS.
- [ ] Then, you create a DataSync task to manage the automated transfer of data between these locations.
- [ ] DataSync handles the scheduling, monitoring, data validation, and optimization of the transfer.
- [ ] This combination provides an automated, efficient, and managed way to transfer the data.

Why are the other answers wrong?

- [ ] Option A is wrong because: While an EC2 instance needs to be in a VPC with mount targets for the EFS file system (which exist in specific AZs), EFS itself is a regional service designed to be accessible from any AZ within the region where it's created. This step is related to how the EC2 instance accesses EFS after migration, not directly a step in automating the data transfer from on-premises. The core of the automation is DataSync.
- [ ] Option C is wrong because: The requirement clearly states that the new SFTP server hosted on an EC2 instance must use an Amazon EFS file system, not an EBS volume, for its data. Creating an EBS volume contradicts this requirement.
- [ ] Option D is wrong because: Manually using operating system copy commands (like scp, rsync, or cp over an NFS mount) is not an "automated task" in the context of a managed migration service like DataSync. Manual copies lack the built-in scheduling, monitoring, error handling, data validation, and transfer optimization features that DataSync provides, and they would require more manual intervention, especially for 200 GB of data.

</details>

<details>
  <summary>Question 153</summary>

A company sells ringtones created from clips of popular songs. The files containing the ringtones are stored in Amazon S3 Standard and are at least 128 KB in size. The company has millions of files, but downloads are infrequent for ringtones older than 90 days. The company needs to save money on storage while keeping the most accessed files readily available for its users.

Which action should the company take to meet these requirements MOST cost-effectively?

- [ ] A. Configure S3 Standard-Infrequent Access (S3 Standard-IA) storage for the initial storage tier of the objects.
- [ ] B. Move the files to S3 Intelligent-Tiering and configure it to move objects to a less expensive storage tier after 90 days.
- [ ] C. Configure S3 inventory to manage objects and move them to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days.
- [ ] D. Implement an S3 Lifecycle policy that moves the objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Implement an S3 Lifecycle policy that moves the objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days.

Why this is the correct answer:

This solution directly addresses the defined access patterns and cost-saving requirements:

- [ ] Initial Storage in S3 Standard: New ringtones and those younger than 90 days are frequently accessed ("most accessed files readily available"). S3 Standard is suitable for this, offering high availability and immediate retrieval with no retrieval fees. The problem states files are currently in S3 Standard.
- [ ] Defined Access Pattern Shift: There's a clear shift in access patterns: "downloads are infrequent for ringtones older than 90 days."
- [ ] S3 Lifecycle Policy for Automated Transition: An S3 Lifecycle policy allows you to define rules to automatically transition objects to different storage classes based on their age. Configuring a policy to move objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days automates the cost optimization process.
- [ ] S3 Standard-IA for Infrequent Access with Ready Availability: S3 Standard-IA is designed for data that is accessed less frequently but requires rapid access when needed. It offers a lower storage cost than S3 Standard, making it cost-effective for the ringtones older than 90 days. While there's a per-GB retrieval fee, this is acceptable if downloads are truly infrequent. It keeps the files "readily available."
- [ ] MOST Cost-Effective for This Pattern: For a well-defined access pattern (frequent then infrequent), a direct lifecycle policy to an appropriate infrequent access tier like S3 Standard-IA is a very cost-effective and straightforward approach.

Why are the other answers wrong?

- [ ] Option A is wrong because: Configuring S3 Standard-IA as the initial storage tier for all objects would mean that frequently accessed new ringtones (within the first 90 days) would incur retrieval fees every time they are downloaded. S3 Standard is more cost-effective for the initial period of frequent access.
- [ ] Option B is wrong because: S3 Intelligent-Tiering is primarily designed for data with unknown, changing, or unpredictable access patterns. While it could eventually move data to an infrequent access tier after 90 days if so configured (or if its own monitoring determined it), the access pattern here is relatively predictable (frequent for 90 days, then infrequent). For such a defined pattern, a direct S3 Lifecycle policy (as in option D) is often simpler and can be slightly more cost-effective as it avoids the small per-object monitoring and automation fees associated with S3 Intelligent-Tiering.
- [ ] Option C is wrong because: Amazon S3 Inventory provides flat file lists (CSV, ORC, or Parquet) of your objects and their metadata, which is useful for analysis, auditing, and reporting. S3 Inventory itself does not "manage objects and move them" between storage classes. That functionality is provided by S3 Lifecycle policies.

</details>

<details>
  <summary>Question 154</summary>

A company needs to save the results from a medical trial to an Amazon S3 repository. The repository must allow a few scientists to add new files and must restrict all other users to read-only access. No users can have the ability to modify or delete any files in the repository. The company must keep every file in the repository for a minimum of 1 year after its creation date.

Which solution will meet these requirements?

- [ ] A. Use S3 Object Lock in governance mode with a legal hold of 1 year.
- [ ] B. Use S3 Object Lock in compliance mode with a retention period of 365 days.
- [ ] C. Use an IAM role to restrict all users from deleting or changing objects in the S3 bucket. Use an S3 bucket policy to only allow the IAM role.
- [ ] D. Configure the S3 bucket to invoke an AWS Lambda function every time an object is added. Configure the function to track the hash of the saved object so that modified objects can be marked accordingly.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Use S3 Object Lock in compliance mode with a retention period of 365 days.

Why this is the correct answer:

This solution effectively addresses the requirements for immutability and fixed-term retention:

- [ ] S3 Object Lock for Immutability: The requirement "No users can have the ability to modify or delete any files in the repository" points directly to Amazon S3 Object Lock. S3 Object Lock provides Write-Once-Read-Many (WORM) protection for S3 objects, preventing them from being deleted or overwritten for a fixed amount of time or indefinitely.
- [ ] Compliance Mode for Strict Enforcement: S3 Object Lock offers two retention modes: governance and compliance. Compliance mode is the stricter of the two. When an object version is locked in compliance mode, its retention mode cannot be changed, and its retention period cannot be shortened by any user, including the root user in the AWS account, for the duration of the specified retention period. This ensures the highest level of immutability and meets the "No users can have the ability to modify or delete" requirement.
- [ ] Retention Period of 365 Days: Setting a retention period of 365 days (1 year) for objects when they are uploaded (e.g., via a default bucket setting or by applying it during upload) ensures that "The company must keep every file in the repository for a minimum of 1 year after its creation date."
- [ ] Access Control (Complementary): While S3 Object Lock handles the immutability and retention, separate IAM policies and/or S3 bucket policies would be configured to manage who can add new files (the "few scientists") and who has read-only access ("all other users"). This option focuses on the critical immutability and retention aspects.

Why are the other answers wrong?

- [ ] Option A is wrong because:
S3 Object Lock in governance mode allows users with specific IAM permissions (s3:BypassGovernanceRetention) to override the retention settings or delete the objects. This does not meet the strict requirement that "No users can have the ability to modify or delete any files."
A legal hold provides immutability but has no expiration date; it remains in effect until explicitly removed. While it prevents deletion, a fixed retention period (as in compliance mode) is more directly aligned with the "minimum of 1 year after its creation date" requirement if the goal is a time-bound, unchangeable retention.
- [ ] Option C is wrong because: While IAM roles and S3 bucket policies are essential for controlling access (who can read, who can write), they can typically be modified by users with sufficient administrative privileges (like the root user or account administrators). They do not provide the same level of guaranteed, unchangeable WORM protection that S3 Object Lock in compliance mode offers against all users, including administrators, for a defined period.
- [ ] Option D is wrong because: This describes a detective control, not a preventative one. Tracking hashes with a Lambda function would help identify if an object has been modified after the fact, but it would not prevent the modification or deletion from occurring in the first place. The requirement is that files "cannot be modified or deleted."

</details>

<details>
  <summary>Question 155</summary>

A large media company hosts a web application on AWS. The company wants to start caching confidential media files so that users around the world will have reliable access to the files. The content is stored in Amazon S3 buckets. The company must deliver the content quickly, regardless of where the requests originate geographically.

Which solution will meet these requirements?

- [ ] A. Use AWS DataSync to connect the S3 buckets to the web application.
- [ ] B. Deploy AWS Global Accelerator to connect the S3 buckets to the web application.
- [ ] C. Deploy Amazon CloudFront to connect the S3 buckets to CloudFront edge servers.
- [ ] D. Use Amazon Simple Queue Service (Amazon SQS) to connect the S3 buckets to the web application.

</details>

<details>
  <summary>Answer</summary>

- [ ] C. Deploy Amazon CloudFront to connect the S3 buckets to CloudFront edge servers.

Why this is the correct answer:

- [ ] Amazon CloudFront for Caching and Global Fast Delivery: Amazon CloudFront is a global content delivery network (CDN) service. It is designed to "deliver the content quickly, regardless of where the requests originate geographically" by caching copies of your content (in this case, confidential media files from S3 buckets) at edge locations around the world. When users request the files, they are served from the nearest edge location, which significantly reduces latency.
- [ ] Reliable Access and S3 Origin: CloudFront improves the reliability of access to content. It uses Amazon S3 buckets as an origin to fetch the content when it's not already cached at an edge location.
- [ ] Caching Confidential Media Files: CloudFront supports methods for securely serving private content from S3, such as using Origin Access Identity (OAI) or Origin Access Control (OAC) to restrict direct S3 access and ensure files are only accessible through CloudFront. Signed URLs or signed cookies can also be used with CloudFront to control access to confidential files on a per-user or per-session basis.
- [ ] This solution directly addresses all requirements: caching, reliable access, global fast delivery, and handling confidential content from S3.

Why are the other answers wrong?

- [ ] Option A is wrong because: AWS DataSync is a service for online data transfer, designed to move large amounts of data between on-premises storage and AWS storage, or between different AWS storage services. It is not a content delivery or caching service for providing low-latency access to users around the world.
- [ ] Option B is wrong because: AWS Global Accelerator is a service that improves the availability and performance of your applications with global users by providing static IP addresses and routing traffic over the AWS global network to optimal regional application endpoints (like Load Balancers or EC2 instances). While it helps with application endpoint performance and availability, it is not primarily a content caching service like CloudFront. CloudFront is specifically designed for caching and delivering static/dynamic content from the edge.
- [ ] Option D is wrong because: Amazon Simple Queue Service (Amazon SQS) is a message queuing service used for decoupling and scaling distributed systems by sending, storing, and receiving messages between software components. It has no role in caching or delivering media files to users.


</details>

<details>
  <summary>Question 156</summary>

- [ ] A.  TurnÂ  
A company produces batch data that comes from different databases. The company also produces live stream data from network sensors and application APIs. The company needs to consolidate all the data into one place for business analytics. The company needs to process the incoming data and then stage the data in different Amazon S3 buckets. Teams will later run one-time queries and import the data into a business intelligence tool to show key performance indicators (KPIs).

Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)

- [ ] A. Use Amazon Athena for one-time queries. Use Amazon QuickSight to create dashboards for KPIs.
- [ ] B. Use Amazon Kinesis Data Analytics for one-time queries. Use Amazon QuickSight to create dashboards for KPIs.
- [ ] C. Create custom AWS Lambda functions to move the individual records from the databases to an Amazon Redshift cluster.
- [ ] D. Use an AWS Glue extract, transform, and load (ETL) job to convert the data into JSON format. Load the data into multiple Amazon OpenSearch Service (Amazon Elasticsearch Service) clusters.
- [ ] E. Use blueprints in AWS Lake Formation to identify the data that can be ingested into a data lake. Use AWS Glue to crawl the source, extract the data, and load the data into Amazon S3 in Apache Parquet format.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Use Amazon Athena for one-time queries. Use Amazon QuickSight to create dashboards for KPIs.
- [ ] E. Use blueprints in AWS Lake Formation to identify the data that can be ingested into a data lake. Use AWS Glue to crawl the source, extract the data, and load the data into Amazon S3 in Apache Parquet format.

Why these are the correct answers:

This solution combines services for building a data lake, processing data, and then analyzing and visualizing it, all with an emphasis on minimizing operational overhead.

E. Use blueprints in AWS Lake Formation to identify the data that can be ingested into a data lake. Use AWS Glue to crawl the source, extract the data, and load the data into Amazon S3 in Apache Parquet format.
- [ ] Consolidation and Staging in S3: AWS Lake Formation helps you set up a secure data lake in Amazon S3 quickly. Blueprints can be used to ingest data from various sources (like the "different databases").
- [ ] AWS Glue (an ETL service) can then be used to crawl these sources, extract the data, perform necessary transformations (though this option focuses on extraction and loading), and load it into S3.
- [ ] Apache Parquet Format: Storing data in Apache Parquet format in S3 is highly recommended for analytics.
- [ ] Parquet is a columnar storage format that is optimized for query performance with services like Amazon Athena and can lead to significant cost savings for queries.
- [ ] Least Operational Overhead for Ingestion/Processing: Both Lake Formation and Glue are fully managed services, reducing the operational burden of setting up data ingestion pipelines and preparing data for analytics.

A. Use Amazon Athena for one-time queries. Use Amazon QuickSight to create dashboards for KPIs.
- [ ] Amazon Athena for Ad-Hoc Queries: Once the data is staged in S3 (as per option E), Amazon Athena allows teams to run "one-time queries" directly on the data in S3 using standard SQL.
- [ ] Athena is serverless, so there are no clusters to manage, fitting the "LEAST operational overhead" requirement.
- [ ] Amazon QuickSight for BI and KPIs: Amazon QuickSight is a scalable, serverless business intelligence (BI) service.
- [ ] It can connect to data in S3 (often via Athena) to "import the data into a business intelligence tool to show key performance indicators (KPIs)" through interactive dashboards.
- [ ] This addresses the analysis and visualization requirements with low operational overhead.

Why are the other answers wrong?

- [ ] Option B is wrong because: Amazon Kinesis Data Analytics is designed for real-time processing and analysis of streaming data, not for running "one-time queries" on data already staged in S3 (which is what Athena is for). While the company does have live stream data, Kinesis Data Analytics is an analytics tool for data in motion, not a general query engine for data at rest in S3 for this use case.
- [ ] Option C is wrong because: Creating custom AWS Lambda functions to move data from various databases to an Amazon Redshift cluster would involve significant development and maintenance effort. While Redshift is a data warehouse, the requirement is to stage data in S3 first. For ingesting batch data from databases into S3, AWS Glue (as in option E) or AWS Database Migration Service (DMS) are generally more suitable and offer less operational overhead than custom Lambda functions.
- [ ] Option D is wrong because: While AWS Glue ETL is appropriate for processing, loading data into multiple Amazon OpenSearch Service (Elasticsearch Service) clusters might not be the most cost-effective or operationally simple solution for staging all consolidated data for "one-time queries" and general BI tool import. OpenSearch is optimized for search and log analytics. Storing data in S3 in Parquet format and querying with Athena (as suggested by options E and A) is often more flexible and cost-effective for ad-hoc SQL queries and broader BI consumption. Managing multiple OpenSearch clusters also adds operational overhead.

</details>

<details>
  <summary>Question 157</summary>

A company stores data in an Amazon Aurora PostgreSQL DB cluster. The company must store all the data for 5 years and must delete all the data after 5 years. The company also must indefinitely keep audit logs of actions that are performed within the database. Currently, the company has automated backups configured for Aurora.

Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

- [ ] A. Take a manual snapshot of the DB cluster.
- [ ] B. Create a lifecycle policy for the automated backups.
- [ ] C. Configure automated backup retention for 5 years.
- [ ] D. Configure an Amazon CloudWatch Logs export for the DB cluster.
- [ ] E. Use AWS Backup to take the backups and to keep the backups for 5 years.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Configure an Amazon CloudWatch Logs export for the DB cluster.
- [ ] E. Use AWS Backup to take the backups and to keep the backups for 5 years.

Why these are the correct answers:

This solution addresses both the data retention/deletion requirements and the audit log retention requirement separately using appropriate AWS services.

D. Configure an Amazon CloudWatch Logs export for the DB cluster.
- [ ] Indefinitely Keep Audit Logs: Amazon Aurora PostgreSQL can be configured to publish various logs, including audit logs (if enabled via database parameter group settings), to Amazon CloudWatch Logs. Once the audit logs are in CloudWatch Logs, they can be configured for long-term retention, either within CloudWatch Logs itself (though this can become expensive for indefinite retention of large volumes) or, more commonly, by setting up an export or subscription from CloudWatch Logs to Amazon S3 for durable, cost-effective, and indefinite archival. This meets the requirement to "indefinitely keep audit logs of actions that are performed within the database."

E. Use AWS Backup to take the backups and to keep the backups for 5 years.
- [ ] Data Retention for 5 Years and Deletion: AWS Backup is a fully managed backup service that simplifies and automates data protection across various AWS services, including Amazon Aurora. You can create backup plans in AWS Backup to define how frequently backups are taken and, crucially, their retention period. You can set a retention policy to keep backups for 5 years. AWS Backup also manages the lifecycle of these backups, including their automatic deletion after the 5-year retention period expires. This addresses the requirements to "store all the data for 5 years and must delete all the data after 5 years."
- [ ] Operational Efficiency: Using AWS Backup centralizes backup management and automates the backup and retention lifecycle, reducing operational overhead.

Why are the other answers wrong?

- [ ] Option A is wrong because: Taking a single manual snapshot of the DB cluster is a point-in-time backup. It does not provide an ongoing strategy for backing up all data, retaining it for 5 years, and then ensuring its deletion. Managing a series of manual snapshots for 5 years and their eventual deletion would be operationally burdensome.
- [ ] Option B is wrong because: Amazon RDS automated backups (which Aurora uses) have a maximum retention period of 35 days. You cannot create a lifecycle policy directly on these automated backups to extend their retention to 5 years. For long-term retention beyond 35 days, you need to use manual snapshots or a service like AWS Backup.
- [ ] Option C is wrong because: As stated above, the maximum retention period for standard Amazon RDS automated backups is 35 days. It is not possible to configure automated backup retention directly within RDS/Aurora for 5 years.

</details>

<details>
  <summary>Question 158</summary>

A solutions architect is optimizing a website for an upcoming musical event. Videos of the performances will be streamed in real time and then will be available on demand. The event is expected to attract a global online audience.

Which service will improve the performance of both the real-time and on-demand streaming?

- [ ] A. Amazon CloudFront
- [ ] B. AWS Global Accelerator
- [ ] C. Amazon Route 53
- [ ] D. Amazon S3 Transfer Acceleration

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Amazon CloudFront

Why this is the correct answer:

- [ ] Amazon CloudFront for On-Demand and Real-Time Streaming: Amazon CloudFront is a global content delivery network (CDN) service that is designed to deliver data, videos, applications, and APIs to customers globally with low latency and high transfer speeds.
- [ ] On-Demand Streaming: For on-demand video (e.g., recordings of performances stored in Amazon S3), CloudFront caches the content at its numerous edge locations worldwide. When users request the video, it's served from the edge location closest to them, significantly reducing latency and improving playback performance.
- [ ] Real-Time Streaming: CloudFront can also be used to distribute live video streams. It supports common streaming protocols and can work with live origins (like AWS Elemental MediaStore or other media servers) to deliver real-time streams to a global audience with lower latency by leveraging its edge network.
- [ ] Global Audience and Performance: Given the "global online audience," CloudFront's geographically distributed edge locations are key to providing a consistent, low-latency viewing experience for both types of video content.

Why are the other answers wrong?

- [ ] Option B is wrong because: AWS Global Accelerator improves the availability and performance of your applications by providing static IP addresses and routing traffic over the AWS global network to optimal application endpoints (like Load Balancers or EC2 instances). While it can accelerate traffic to your application origin and is beneficial for certain types of real-time applications (like gaming or some live video contribution), Amazon CloudFront is more specifically designed for caching and delivering video content (both on-demand and live streams) at the edge to a global audience. For video streaming, CloudFront's caching and delivery optimization are typically more impactful.
- [ ] Option C is wrong because: Amazon Route 53 is a scalable Domain Name System (DNS) web service. It translates domain names into IP addresses and can route users to different endpoints based on various routing policies (e.g., latency-based, geoproximity). While Route 53 is essential for directing users to the correct CloudFront distribution or Global Accelerator endpoint, it does not itself cache content or improve the actual streaming performance by bringing content closer to users in the way a CDN does.
- [ ] Option D is wrong because: Amazon S3 Transfer Acceleration is a feature that speeds up file uploads to Amazon S3 over long distances by using CloudFront's edge locations as an entry point to the AWS network. It does not improve the performance of downloading or streaming content from S3 to users globally. For delivering content from S3 to users, a full CloudFront distribution is the appropriate service.

</details>

<details>
  <summary>Question 159</summary>

A company is running a publicly accessible serverless application that uses Amazon API Gateway and AWS Lambda. The application's traffic recently spiked due to fraudulent requests from botnets.

Which steps should a solutions architect take to block requests from unauthorized users? (Choose two.)

- [ ] A. Create a usage plan with an API key that is shared with genuine users only.
- [ ] B. Integrate logic within the Lambda function to ignore the requests from fraudulent IP addresses.
- [ ] C. Implement an AWS WAF rule to target malicious requests and trigger actions to filter them out.
- [ ] D. Convert the existing public API to a private API. Update the DNS records to redirect users to the new API endpoint.
- [ ] E. Create an IAM role for each user attempting to access the API. A user will assume the role when making the API call.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Create a usage plan with an API key that is shared with genuine users only.
- [ ] C. Implement an AWS WAF rule to target malicious requests and trigger actions to filter them out.

Why these are the correct answers:

These two solutions provide effective mechanisms at the API Gateway and edge level to control access and filter malicious traffic:

A. Create a usage plan with an API key that is shared with genuine users only.
- [ ] API Keys for Authentication/Authorization: Amazon API Gateway allows you to create API keys and associate them with usage plans. You can then require clients to send a valid API key in their requests.
- [ ] By distributing API keys only to genuine users or client applications, you can ensure that requests without a valid key (or with a revoked key) are rejected by API Gateway.
- [ ] Throttling and Quotas: Usage plans also allow you to set throttling limits and quotas on a per-API key basis, which can help mitigate abuse from a compromised key or a single misbehaving client.

C. Implement an AWS WAF rule to target malicious requests and trigger actions to filter them out.
- [ ] AWS WAF for Web Exploit and Bot Protection: AWS WAF is a web application firewall that integrates with Amazon API Gateway (and other services like CloudFront and Application Load Balancer).
- [ ] It allows you to create rules to filter web traffic based on various conditions, such as IP addresses, HTTP headers, HTTP body, URI strings, SQL injection, and cross-site scripting.
- [ ] Blocking Botnets: You can use AWS WAF with AWS Managed Rules (like the Bot Control rule set or IP reputation lists) or create custom rules to identify and block traffic originating from known botnets or exhibiting bot-like behavior.
- [ ] This filtering happens before requests hit your API Gateway methods or Lambda functions.
- [ ] Combining these two provides layered security: WAF for broad filtering of malicious traffic and botnets, and API keys/usage plans for more granular access control for legitimate clients.

Why are the other answers wrong?

- [ ] Option B is wrong because: Implementing IP address filtering logic directly within the AWS Lambda function means that the Lambda function is still invoked for every request, including fraudulent ones. This consumes Lambda resources, incurs costs for each invocation, and makes the Lambda function more complex. It's more efficient and cost-effective to block malicious traffic at an earlier stage, like with AWS WAF or API Gateway configurations.
- [ ] Option D is wrong because: Converting a publicly accessible API to a private API makes it accessible only from within your VPC (using VPC endpoints) or via specific private network connections. This would block all public internet traffic, including legitimate users, unless a complex and potentially costly frontend solution is built to bridge public access to the private API. This is not a suitable solution for protecting a "publicly accessible serverless application" from botnets while maintaining public access.
- [ ] Option E is wrong because: Using IAM roles for each individual user to access a public API is generally not a scalable or practical authentication mechanism for a large number of external users. IAM roles are more typically used for granting permissions to AWS services or applications, or for federated enterprise users. For authenticating end-users of a public API, solutions like Amazon Cognito user pools (which can be integrated with API Gateway) or API keys are more appropriate. Managing individual IAM roles for potentially thousands or millions of public users would be an operational nightmare.

</details>

<details>
  <summary>Question 160</summary>

An ecommerce company hosts its analytics application in the AWS Cloud. The application generates about 300 MB of data each month. The data is stored in JSON format. The company is evaluating a disaster recovery solution to back up the data. The data must be accessible in milliseconds if it is needed, and the data must be kept for 30 days.

Which solution meets these requirements MOST cost-effectively?

- [ ] A. Amazon OpenSearch Service (Amazon Elasticsearch Service)
- [ ] B. Amazon S3 Glacier
- [ ] C. Amazon S3 Standard
- [ ] D. Amazon RDS for PostgreSQL

</details>

<details>
  <summary>Answer</summary>

- [ ] C. Amazon S3 Standard

Why this is the correct answer:

- [ ] Amazon S3 Standard for Accessibility and Cost-Effectiveness:
- [ ] Accessible in Milliseconds: Amazon S3 Standard provides low-latency access to data, typically in milliseconds, which meets the requirement for quick accessibility.
- [ ] Suitable for Data Volume and Retention: For a relatively small amount of data (300 MB per month) that needs to be kept for only 30 days and requires fast access, S3 Standard is a very cost-effective storage option. It has no retrieval fees, which is beneficial if the data might be needed.
- [ ] Durability for Backups: S3 Standard is designed for high durability (99.999999999%) and stores data across multiple Availability Zones, making it a reliable choice for storing backups.
- [ ] JSON Format: S3 is well-suited for storing files in any format, including JSON.
- [ ] This solution directly meets the accessibility, retention, and cost-effectiveness requirements for this specific scenario.

Why are the other answers wrong?

- [ ] Option A is wrong because: Amazon OpenSearch Service (formerly Amazon Elasticsearch Service) is primarily a search and analytics engine. While it can store JSON data, using it as a primary backup solution for 300 MB of data per month where the main requirements are millisecond access for 30 days and cost-effectiveness is likely overkill. OpenSearch Service involves running a cluster, which incurs instance costs and can be more expensive than S3 Standard for simple backup storage.
- [ ] Option B is wrong because: Amazon S3 Glacier (referring to S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive) is designed for low-cost, long-term data archiving where retrieval times of minutes to hours (or even longer for Deep Archive) are acceptable. It does not meet the requirement that "data must be accessible in milliseconds if it is needed." (Note: S3 Glacier Instant Retrieval does offer millisecond access from an archive tier, but for data kept only 30 days with potential need for immediate access, S3 Standard is often simpler and its storage cost for this short duration is competitive without involving specific archive tier considerations unless access is truly rare).
- [ ] Option D is wrong because: Amazon RDS for PostgreSQL is a relational database service. Storing 300 MB of JSON backup data in a relational database is generally not the most straightforward or cost-effective approach compared to object storage like S3. While PostgreSQL supports JSON data types, using RDS as a backup target for files in this manner adds unnecessary complexity and cost associated with running a database instance.

</details>

<details>
  <summary>Question 161</summary>

A company has a small Python application that processes JSON documents and outputs the results to an on-premises SQL database. The application runs thousands of times each day. The company wants to move the application to the AWS Cloud. The company needs a highly available solution that maximizes scalability and minimizes operational overhead.

Which solution will meet these requirements?

- [ ] A. Place the JSON documents in an Amazon S3 bucket. Run the Python code on multiple Amazon EC2 instances to process the documents. Store the results in an Amazon Aurora DB cluster.
- [ ] B. Place the JSON documents in an Amazon S3 bucket. Create an AWS Lambda function that runs the Python code to process the documents as they arrive in the S3 bucket. Store the results in an Amazon Aurora DB cluster.
- [ ] C. Place the JSON documents in an Amazon Elastic Block Store (Amazon EBS) volume. Use the EBS Multi-Attach feature to attach the volume to multiple Amazon EC2 instances. Run the Python code on the EC2 instances to process the documents. Store the results on an Amazon RDS DB instance.
- [ ] D. Place the JSON documents in an Amazon Simple Queue Service (Amazon SQS) queue as messages. Deploy the Python code as a container on an Amazon Elastic Container Service (Amazon ECS) cluster that is configured with the Amazon EC2 launch type. Use the container to process the SQS messages. Store the results on an Amazon RDS DB instance.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Place the JSON documents in an Amazon S3 bucket. Create an AWS Lambda function that runs the Python code to process the documents as they arrive in the S3 bucket. Store the results in an Amazon Aurora DB cluster.

Why this is the correct answer:

This solution leverages serverless and managed services to meet all requirements efficiently:

- [ ] Amazon S3 for Storing JSON Documents: Amazon S3 is an excellent choice for storing the input JSON documents. It's highly durable, scalable, and can trigger events.
- [ ] AWS Lambda for Processing: An AWS Lambda function can be written in Python to process the JSON documents. Lambda is serverless, meaning you don't manage any servers. It automatically scales based on the number of incoming events (e.g., new documents arriving in S3), thus maximizing scalability. Since it runs "thousands of times each day" (implying event-driven processing), Lambda is a perfect fit. This also "minimizes operational overhead."
- [ ] S3 Event Trigger: S3 can be configured to automatically trigger the Lambda function whenever a new JSON document is uploaded to the bucket. This creates an event-driven processing pipeline.
- [ ] Amazon Aurora for SQL Database: Amazon Aurora is a fully managed, relational database service compatible with MySQL and PostgreSQL. It provides high availability (e.g., with Multi-AZ configurations or Aurora Replicas), scalability, and significantly reduces operational overhead compared to managing an on-premises SQL database or a database on EC2. Migrating the output to Aurora meets the need for a SQL database in the cloud.
- [ ] This architecture (S3 for input, Lambda for processing, Aurora for output) is highly available, scales seamlessly, and has minimal operational overhead.

Why are the other answers wrong?

- [ ] Option A is wrong because: Running the Python code on multiple Amazon EC2 instances to process the documents involves managing those instances (patching, scaling, operating systems). This incurs more operational overhead than using serverless AWS Lambda. While scalable with Auto Scaling, Lambda offers better operational efficiency for this event-driven task.
- [ ] Option C is wrong because:
Storing JSON documents that need to be processed by multiple instances on a single EBS volume (even with Multi-Attach) is generally less scalable and more complex than using Amazon S3 as the source. EBS Multi-Attach has limitations (e.g., only for io1/io2 volumes, within the same AZ, specific instance types) and requires a file system that supports concurrent access.
It still relies on EC2 instances, leading to higher operational overhead than Lambda.
- [ ] Option D is wrong because:
Placing entire JSON documents directly into SQS messages might be problematic if the documents are large, as SQS messages have a size limit (typically 256 KB). It's more common to store the documents in S3 and send a notification/pointer to SQS.
Using Amazon ECS with the EC2 launch type for processing still involves managing the underlying EC2 instances in the cluster, which has more operational overhead than a fully serverless Lambda approach for this type of document processing task.

</details>

<details>
  <summary>Question 162</summary>

A company wants to use high performance computing (HPC) infrastructure on AWS for financial risk modeling. The company's HPC workloads run on Linux. Each HPC workflow runs on hundreds of Amazon EC2 Spot Instances, is short-lived, and generates thousands of output files that are ultimately stored in persistent storage for analytics and long-term future use. The company seeks a cloud storage solution that permits the copying of on-premises data to long-term persistent storage to make data available for processing by all EC2 instances. The solution should also be a high performance file system that is integrated with persistent storage to read and write datasets and output files.

Which combination of AWS services meets these requirements?

- [ ] A. Amazon FSx for Lustre integrated with Amazon S3
- [ ] B. Amazon FSx for Windows File Server integrated with Amazon S3
- [ ] C. Amazon S3 Glacier integrated with Amazon Elastic Block Store (Amazon EBS)
- [ ] D. Amazon S3 bucket with a VPC endpoint integrated with an Amazon Elastic Block Store (Amazon EBS) General Purpose SSD (gp2) volume

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Amazon FSx for Lustre integrated with Amazon S3

Why this is the correct answer:

This question outlines requirements for a high-performance, scalable file system for HPC workloads, with integration for persistent storage.

- [ ] Amazon FSx for Lustre for High-Performance File System: Amazon FSx for Lustre is specifically designed to provide high-performance, scalable file storage for compute-intensive workloads like HPC, financial risk modeling, machine learning, and media processing. Lustre is a popular open-source parallel file system known for its speed and scalability, making it ideal for workflows running on hundreds of EC2 instances that need fast access to shared datasets and a place to write output files quickly.
- [ ] Integration with Amazon S3 for Persistent Storage: FSx for Lustre seamlessly integrates with Amazon S3. You can link an FSx for Lustre file system to an S3 bucket. This allows:
- [ ] Data in S3 (e.g., on-premises data copied to S3 for long-term storage) to be transparently presented as files in the Lustre file system for processing by the EC2 Spot Instances.
- [ ] Output files generated by the HPC jobs on FSx for Lustre to be easily written back to the linked S3 bucket for "persistent storage for analytics and long-term future use."
- [ ] Shared Access for EC2 Instances: The FSx for Lustre file system can be mounted by hundreds of Linux-based EC2 Spot Instances, providing concurrent access to the data.
- [ ] This combination provides both the high-performance scratch/processing file system (FSx for Lustre) and the durable, long-term persistent storage (S3), with seamless integration between them.

Why are the other answers wrong?

- [ ] Option B is wrong because: Amazon FSx for Windows File Server provides file storage using the SMB protocol and is designed for Windows-based applications. The company's HPC workloads run on Linux, and Lustre (as provided by FSx for Lustre) is a more common and often higher-performing file system for Linux-based HPC clusters.
- [ ] Option C is wrong because: Amazon S3 Glacier is a low-cost archival storage service with retrieval times of minutes to hours. It is not suitable as a high-performance file system for active HPC processing. Amazon EBS provides block storage for individual EC2 instances and is not inherently a shared, distributed file system suitable for hundreds of HPC nodes without additional software and management.
- [ ] Option D is wrong because: While S3 can be used for persistent storage and EBS for instance storage, this option doesn't describe a "high performance file system that is integrated with persistent storage" in the way FSx for Lustre provides a POSIX-compliant, high-throughput, low-latency file system layer on top of S3 for HPC workloads. Accessing S3 directly for all file I/O from hundreds of compute nodes might not provide the same performance as a dedicated Lustre file system.

</details>

<details>
  <summary>Question 163</summary>
 
A company is building a containerized application on premises and decides to move the application to AWS. The application will have thousands of users soon after it is deployed. The company is unsure how to manage the deployment of containers at scale. The company needs to deploy the containerized application in a highly available architecture that minimizes operational overhead.

Which solution will meet these requirements?

- [ ] A. Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Container Service (Amazon ECS) cluster with the AWS Fargate launch type to run the containers. Use target tracking to scale automatically based on demand.
- [ ] B. Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Container Service (Amazon ECS) cluster with the Amazon EC2 launch type to run the containers. Use target tracking to scale automatically based on demand.
- [ ] C. Store container images in a repository that runs on an Amazon EC2 instance. Run the containers on EC2 instances that are spread across multiple Availability Zones. Monitor the average CPU utilization in Amazon CloudWatch. Launch new EC2 instances as needed.
- [ ] D. Create an Amazon EC2 Amazon Machine Image (AMI) that contains the container image. Launch EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon CloudWatch alarm to scale out EC2 instances when the average CPU utilization threshold is breached.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Container Service (Amazon ECS) cluster with the AWS Fargate launch type to run the containers. Use target tracking to scale automatically based on demand.

Why this is the correct answer:

This solution offers a serverless approach to running containers, which directly addresses the requirements for scalability, high availability, and minimized operational overhead.

- [ ] Amazon Elastic Container Registry (Amazon ECR) for Image Storage: ECR is a fully managed Docker container registry that makes it easy to store, manage, share, and deploy your container images and artifacts. This is the standard and recommended place to store container images on AWS.
- [ ] Amazon Elastic Container Service (Amazon ECS) with AWS Fargate Launch Type:
- [ ] ECS is a fully managed container orchestration service that helps you deploy, manage, and scale containerized applications.
- [ ] AWS Fargate is a serverless compute engine for containers that works with ECS. When you use Fargate, you don't need to provision, configure, or manage the underlying EC2 instances (servers). AWS handles all the server management, patching, and scaling of the infrastructure. This directly meets the requirement to "minimize operational overhead" and simplifies managing "deployment of containers at scale."
- [ ] High Availability: ECS services running on Fargate can be configured to distribute tasks (your containers) across multiple Availability Zones, ensuring high availability.
- [ ] Target Tracking Auto Scaling: ECS services can use target tracking auto scaling policies to automatically adjust the number of running tasks based on demand (e.g., CPU utilization, memory utilization, or custom metrics from Amazon CloudWatch). This ensures the application can scale to handle "thousands of users."

Why are the other answers wrong?

- [ ] Option B is wrong because: Using Amazon ECS with the Amazon EC2 launch type means that while ECS manages the container orchestration, the company is still responsible for provisioning, managing, patching, and scaling the underlying EC2 instances that form the ECS cluster. This incurs more operational overhead compared to the serverless Fargate launch type.
- [ ] Option C is wrong because: Storing container images in a self-managed repository on an EC2 instance and manually running containers on EC2 instances (even across multiple AZs with some monitoring) lacks proper container orchestration. This approach has significant operational overhead for managing the repository, the EC2 instances, container lifecycles, and scaling, and it's not an effective way to manage containers "at scale."
- [ ] Option D is wrong because: While creating an AMI with the container image and using an Auto Scaling group can scale EC2 instances, it's not the standard or most efficient way to manage containerized applications. This approach doesn't leverage a container orchestrator like ECS, making tasks like rolling updates, service discovery, and managing multiple containerized services more complex. It also means you are managing the container runtime within the AMI on each EC2 instance.

</details>

<details>
  <summary>Question 164</summary>

A company has two applications: a sender application that sends messages with payloads to be processed and a processing application intended to receive the messages with payloads. The company wants to implement an AWS service to handle messages between the two applications. The sender application can send about 1,000 messages each hour. The messages may take up to 2 days to be processed: If the messages fail to process, they must be retained so that they do not impact the processing of any remaining messages.

Which solution meets these requirements and is the MOST operationally efficient?

- [ ] A. Set up an Amazon EC2 instance running a Redis database. Configure both applications to use the instance. Store, process, and delete the messages, respectively.
- [ ] B. Use an Amazon Kinesis data stream to receive the messages from the sender application. Integrate the processing application with the Kinesis Client Library (KCL).
- [ ] C. Integrate the sender and processor applications with an Amazon Simple Queue Service (Amazon SQS) queue. Configure a dead-letter queue to collect the messages that failed to process.
- [ ] D. Subscribe the processing application to an Amazon Simple Notification Service (Amazon SNS) topic to receive notifications to process. Integrate the sender application to write to the SNS topic.

</details>

<details>
  <summary>Answer</summary>

- [ ] C. Integrate the sender and processor applications with an Amazon Simple Queue Service (Amazon SQS) queue. Configure a dead-letter queue to collect the messages that failed to process.

Why this is the correct answer:

This solution uses Amazon SQS, which is well-suited for decoupling applications and handling asynchronous message processing with features for reliability.

- [ ] Amazon SQS for Decoupled Messaging: Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be always available. This decouples the sender and processing applications.
- [ ] Handles Long Processing Times: SQS messages can be retained in a queue for up to 14 days. The requirement that "messages may take up to 2 days to be processed" is well within this limit. The processing application can retrieve a message and use a visibility timeout to ensure it has enough time to process it before the message becomes visible again in the queue.
- [ ] Dead-Letter Queue (DLQ) for Failed Messages: SQS supports the configuration of a dead-letter queue (DLQ). If the processing application fails to process a message successfully after a configurable number of attempts, SQS can automatically move the undeliverable message to the DLQ. This "retains" the failed messages for later investigation and reprocessing and ensures "they do not impact the processing of any remaining messages" in the main queue.
- [ ] MOST Operationally Efficient: SQS is a fully managed service, requiring minimal operational overhead. Setting up a queue and a DLQ is straightforward. AWS handles the infrastructure, scalability, and availability of the queuing service.

Why are the other answers wrong?

- [ ] Option A is wrong because: Using a self-managed Redis database on an EC2 instance as a message queue introduces significant operational overhead. The company would be responsible for provisioning, managing, patching, and ensuring the availability of the EC2 instance and the Redis software. This is far less operationally efficient than using a managed service like SQS.
- [ ] Option B is wrong because: Amazon Kinesis Data Streams is designed for ingesting and processing large volumes of real-time streaming data (often much higher than 1,000 messages per hour). While it offers durability (data records are typically retained for 24 hours by default, extendable up to 365 days), SQS is generally simpler and more cost-effective for traditional message queuing scenarios with individual message processing and built-in DLQ functionality. Handling retries and failed messages in Kinesis can be more complex for this use case.
- [ ] Option D is wrong because: Amazon Simple Notification Service (SNS) is a publish/subscribe messaging service, primarily used for fanning out messages to multiple subscribers (e.g., sending notifications). It is not designed as a durable queue to hold messages for extended processing times (up to 2 days). If the processing application (subscriber) is unavailable, messages might be lost unless there's a durable endpoint like an SQS queue subscribed to the SNS topic. For direct decoupling between a sender and a single processing application (or a pool of workers for that application) with long processing times and error handling, SQS is more suitable.

</details>

<details>
  <summary>Question 165</summary>

A solutions architect must design a solution that uses Amazon CloudFront with an Amazon S3 origin to store a static website. The company's security policy requires that all website traffic be inspected by AWS WAF.

How should the solutions architect comply with these requirements?

- [ ] A. Configure an S3 bucket policy to accept requests coming from the AWS WAF Amazon Resource Name (ARN) only.
- [ ] B. Configure Amazon CloudFront to forward all incoming requests to AWS WAF before requesting content from the S3 origin.
- [ ] C. Configure a security group that allows Amazon CloudFront IP addresses to access Amazon S3 only. Associate AWS WAF to CloudFront.
- [ ] D. Configure Amazon CloudFront and Amazon S3 to use an origin access identity (OAI) to restrict access to the S3 bucket. Enable AWS WAF on the distribution.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Configure Amazon CloudFront and Amazon S3 to use an origin access identity (OAI) to restrict access to the S3 bucket. Enable AWS WAF on the distribution.

Why this is the correct answer:

This solution addresses both securing the S3 origin and inspecting traffic with AWS WAF in the standard way:

- [ ] Restricting Direct S3 Access with Origin Access Identity (OAI): To ensure that users access the static website content only through Amazon CloudFront and not directly from the S3 bucket URL, you use an Origin Access Identity (OAI). An OAI is a special CloudFront user that you associate with your distribution. You then update the S3 bucket policy to grant read permissions only to this OAI. This prevents public access to the S3 bucket.
- [ ] Enabling AWS WAF on the CloudFront Distribution: AWS WAF (Web Application Firewall) integrates directly with Amazon CloudFront. You create a web ACL (Access Control List) in AWS WAF with your desired rules (e.g., to block common web exploits or filter traffic based on IP addresses, geographic location, etc.) and then associate this web ACL with your CloudFront distribution. This ensures that all website traffic passing through CloudFront is inspected by AWS WAF before it reaches the S3 origin or is served from the CloudFront cache.
- [ ] This combination ensures that the S3 content is private and only accessible via CloudFront, and all traffic coming through CloudFront is inspected by AWS WAF.

Why are the other answers wrong?

- [ ] Option A is wrong because: An S3 bucket policy cannot directly reference an AWS WAF ARN as a principal to grant access. AWS WAF inspects traffic at CloudFront (or other supported services like ALB, API Gateway); it doesn't act as a principal for S3 bucket policies in this manner.
- [ ] Option B is wrong because: Amazon CloudFront does not "forward all incoming requests to AWS WAF" as a separate network hop. Instead, AWS WAF is integrated with CloudFront. When WAF is enabled on a CloudFront distribution, WAF inspects the requests as they arrive at the CloudFront edge locations, before CloudFront serves the content from cache or forwards the request to the origin.
- [ ] Option C is wrong because:
Amazon S3 buckets do not have security groups associated with them. Security groups are used for resources like EC2 instances within a VPC.
Relying on whitelisting CloudFront IP addresses to restrict S3 access is not recommended because these IP ranges can change and are very broad. Using OAI (or the newer Origin Access Control - OAC) is the secure and standard method.
While associating AWS WAF with CloudFront is correct, the S3 access control method described is flawed.

</details>

<details>
  <summary>Question 166</summary>
 
Organizers for a global event want to put daily reports online as static HTML pages. The pages are expected to generate millions of views from users around the world. The files are stored in an Amazon S3 bucket. A solutions architect has been asked to design an efficient and effective solution.

Which action should the solutions architect take to accomplish this?

- [ ] A. Generate presigned URLs for the files.
- [ ] B. Use cross-Region replication to all Regions.
- [ ] C. Use the geoproximity feature of Amazon Route 53.
- [ ] D. Use Amazon CloudFront with the S3 bucket as its origin.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Use Amazon CloudFront with the S3 bucket as its origin.

Why this is the correct answer:

- [ ] Amazon S3 for Storing Static Content: Storing the static HTML pages in an Amazon S3 bucket is a scalable, durable, and cost-effective starting point.
- [ ] Amazon CloudFront for Global Delivery and Performance: The key requirements are handling "millions of views from users around the world" and designing an "efficient and effective solution" (implying low latency and high availability). Amazon CloudFront is a global content delivery network (CDN). By configuring a CloudFront distribution with the S3 bucket as its origin, the static HTML pages will be cached at CloudFront's numerous edge locations worldwide. When users request a page, it will be served from the edge location closest to them, significantly reducing latency and improving the user experience. CloudFront can easily handle millions of requests.
- [ ] Reduced Load on Origin: Caching content at the edge also reduces the number of direct requests to the S3 origin bucket, which can lower costs and further improve performance.
- [ ] This is the standard and most effective architecture for delivering static web content globally with high performance and scalability on AWS.

Why are the other answers wrong?

- [ ] Option A is wrong because: Generating presigned URLs for S3 objects is primarily a mechanism for granting temporary, secure access to private objects in S3. It does not inherently improve performance or scalability for delivering public static content to millions of users globally. Users would still be fetching content directly from the S3 bucket's region, without the benefit of edge caching.
- [ ] Option B is wrong because: While S3 Cross-Region Replication can replicate the S3 bucket content to other AWS Regions, placing data closer to some users, Amazon CloudFront provides a much more extensive network of edge locations (far more numerous and geographically distributed than AWS Regions). Using CloudFront is a more direct and generally more effective way to achieve low-latency global content delivery. Replicating to "all Regions" would also be operationally complex and costly.
- [ ] Option C is wrong because: Amazon Route 53's geoproximity routing (or geolocation/latency-based routing) is a DNS feature that directs users to specific resources (like an S3 bucket endpoint or a load balancer) based on their geographic location or latency. However, if the content is only in a single S3 bucket, Route 53 alone cannot cache the content closer to users. It simply routes them to the origin. CloudFront (Option D) is needed for the actual edge caching and content delivery performance improvement.

</details>

<details>
  <summary>Question 167</summary>

- [ ] A.  TurnÂ  
A company runs a production application on a fleet of Amazon EC2 instances. The application reads the data from an Amazon SQS queue and processes the messages in parallel. The message volume is unpredictable and often has intermittent traffic. This application should continually process messages without any downtime.

Which solution meets these requirements MOST cost-effectively?

- [ ] A. Use Spot Instances exclusively to handle the maximum capacity required.
- [ ] B. Use Reserved Instances exclusively to handle the maximum capacity required.
- [ ] C. Use Reserved Instances for the baseline capacity and use Spot Instances to handle additional capacity.
- [ ] D. Use Reserved Instances for the baseline capacity and use On-Demand Instances to handle additional capacity.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Use Reserved Instances for the baseline capacity and use On-Demand Instances to handle additional capacity.

Why this is the correct answer:

This solution balances cost-effectiveness with the critical requirement of continuous processing without downtime.

- [ ] Reserved Instances (RIs) for Baseline Capacity: Even with unpredictable traffic, there's likely a minimum level of processing capacity that is consistently needed to ensure messages are being processed.
- [ ] Using Reserved Instances for this baseline portion of the workload provides significant cost savings compared to On-Demand pricing for the compute capacity that is always active.
- [ ] On-Demand Instances for Peak/Additional Capacity: For the "unpredictable and often intermittent traffic" that exceeds the baseline, On-Demand Instances provide the flexibility to scale up reliably.
- [ ] Since the application "should continually process messages without any downtime," the guaranteed availability of On-Demand Instances (unlike Spot Instances, which can be interrupted) is crucial for handling these peaks without risking service interruption.
- [ ] MOST Cost-Effective with High Availability: This combination is generally considered a robust and cost-effective strategy.
- [ ] It provides cost savings for the predictable baseline while ensuring that capacity is always available via On-Demand instances to handle unpredictable peaks without the risk of interruptions that could lead to downtime for a critical message processing application.

Why are the other answers wrong?

- [ ] Option A is wrong because: While Spot Instances are the most cost-effective in terms of instance pricing, they can be interrupted by AWS with a two-minute warning. For a production application that "should continually process messages without any downtime," relying exclusively on Spot Instances for all capacity (including what might be considered a baseline) is risky. If a large number of Spot Instances are reclaimed simultaneously, it could lead to a significant disruption in message processing.
- [ ] Option B is wrong because: Purchasing Reserved Instances to handle the maximum potential capacity required for unpredictable and intermittent traffic would lead to substantial underutilization and wasted cost during periods of lower traffic. RIs are a commitment, and paying for peak capacity 24/7 when it's only needed sporadically is not cost-effective.
- [ ] Option C is wrong because: This is a very strong cost optimization strategy and often recommended for fault-tolerant workloads. Using RIs for baseline and Spot Instances for peaks offers great savings. However, the requirement "This application should continually process messages without any downtime" is very stringent. Spot Instances can be interrupted. If such interruptions, even if managed by an Auto Scaling group, could lead to a backlog or a temporary inability to process messages at the required rate, it might be perceived as not meeting the "without any downtime" criteria as strictly as On-Demand instances would for the peak portion.
- [ ] Option D prioritizes the availability of the peak capacity more than the absolute lowest cost for that peak portion. The community vote split in the PDF (D 58%, C 41%) indicates this is a common point of discussion where the interpretation of "without any downtime" is key.

</details>

<details>
  <summary>Question 168</summary>

A security team wants to limit access to specific services or actions in all of the team's AWS accounts. All accounts belong to a large organization in AWS Organizations. The solution must be scalable and there must be a single point where permissions can be maintained.

What should a solutions architect do to accomplish this?

- [ ] A. Create an ACL to provide access to the services or actions.
- [ ] B. Create a security group to allow accounts and attach it to user groups.
- [ ] C. Create cross-account roles in each account to deny access to the services or actions.
- [ ] D. Create a service control policy in the root organizational unit to deny access to the services or actions.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Create a service control policy in the root organizational unit to deny access to the services or actions.

Why this is the correct answer:

- [ ] AWS Organizations and Service Control Policies (SCPs): The question states that all accounts belong to a large organization in AWS Organizations. Service Control Policies (SCPs) are a feature of AWS Organizations designed for centrally managing permissions across multiple AWS accounts. SCPs act as guardrails, allowing you to define the maximum permissions available for IAM users and roles within the member accounts.
- [ ] Scalable and Single Point of Maintenance: SCPs can be attached to the organization root, an Organizational Unit (OU), or individual accounts. By attaching an SCP to the root OU (or a relevant OU containing all the accounts), the policy is enforced consistently across all those accounts. This provides a "scalable" solution where permissions can be "maintained" from a "single point" (the management account where SCPs are defined).
- [ ] Limiting Access with Deny Statements: SCPs can be used to explicitly Deny access to specific AWS services or actions. This is a powerful way to enforce security boundaries and compliance requirements across the organization, ensuring that users and roles in member accounts cannot perform disallowed actions, even if their IAM policies would otherwise permit them.
- [ ] This solution directly meets all the stated requirements by using the appropriate AWS Organizations feature for centralized, preventative permission management.

Why are the other answers wrong?

- [ ] Option A is wrong because:
"ACL" is a general term. If it refers to Network ACLs (NACLs), these are stateless firewalls for controlling network traffic at the subnet level within a VPC and do not manage IAM permissions for AWS services or actions.
If it refers to S3 Access Control Lists (ACLs), these are for managing access to S3 buckets and objects, not for broad control over AWS services and actions across accounts.
Neither type of ACL provides a scalable, central point for maintaining permissions across all services and actions in multiple AWS accounts.
- [ ] Option B is wrong because: Security groups act as stateful firewalls for resources like EC2 instances, controlling inbound and outbound network traffic. They do not manage IAM permissions for AWS services or actions, nor can you "allow accounts" or attach security groups to IAM user groups in the way described to control service access across an organization.
- [ ] Option C is wrong because: Cross-account IAM roles are typically used to grant permissions, allowing users or services in one account to access resources in another account. While IAM policies attached to roles define permissions, creating roles in each account specifically to deny access is not the standard or most efficient method for centrally enforcing restrictions. SCPs (Option D) are designed for this preventative, organization-wide boundary setting. Managing deny logic through individual roles in every account would be operationally complex.

</details>

<details>
  <summary>Question 169</summary>

A company is concerned about the security of its public web application due to recent web attacks. The application uses an Application Load Balancer (ALB). A solutions architect must reduce the risk of DDoS attacks against the application.

What should the solutions architect do to meet this requirement?

- [ ] A. Add an Amazon Inspector agent to the ALB.
- [ ] B. Configure Amazon Macie to prevent attacks.
- [ ] C. Enable AWS Shield Advanced to prevent attacks.
- [ ] D. Configure Amazon GuardDuty to monitor the ALB.

</details>

<details>
  <summary>Answer</summary>

- [ ] C. Enable AWS Shield Advanced to prevent attacks.

Why this is the correct answer:

- [ ] AWS Shield Advanced for DDoS Protection: AWS Shield is a managed Distributed Denial of Service (DDoS) protection service.
- [ ] AWS Shield Advanced provides enhanced protections for applications running on AWS resources such as Application Load Balancers (ALBs), Amazon CloudFront distributions, Amazon Route 53, and EC2 instances.
- [ ] It offers more comprehensive detection and mitigation capabilities against larger and more sophisticated DDoS attacks compared to AWS Shield Standard (which provides default protection).
- [ ] Reduces Risk of DDoS Attacks: By enabling AWS Shield Advanced on the ALB, the company can significantly reduce the risk and impact of DDoS attacks.
- [ ] Shield Advanced provides automatic inline mitigations for many common infrastructure (Layer 3 and 4) attacks and integrates with AWS WAF for application-layer (Layer 7) protection.
- [ ] It also includes 24/7 access to the AWS DDoS Response Team (DRT) for assistance during attacks and cost protection against usage spikes resulting from DDoS attacks.

Why are the other answers wrong?

- [ ] Option A is wrong because: Amazon Inspector is a vulnerability management service that helps improve the security and compliance of applications deployed on Amazon EC2 instances by scanning for software vulnerabilities and unintended network exposure. It does not provide real-time protection against incoming DDoS attacks, nor are agents installed on ALBs (ALBs are managed services).
- [ ] Option B is wrong because: Amazon Macie is a data security and data privacy service that uses machine learning and pattern matching to discover, classify, and protect sensitive data stored in Amazon S3. It is not designed to prevent or mitigate DDoS attacks against web applications or ALBs.
- [ ] Option D is wrong because: Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior within your AWS environment by analyzing various data sources like VPC Flow Logs, AWS CloudTrail event logs, and DNS logs. While GuardDuty might detect indicators of a DDoS attack or compromised resources involved in an attack, it is primarily a detection and alerting service. It does not actively prevent or mitigate DDoS attacks in the way AWS Shield does.

</details>

<details>
  <summary>Question 170</summary>
 
A company's web application is running on Amazon EC2 instances behind an Application Load Balancer. The company recently changed its policy, which now requires the application to be accessed from one specific country only.

Which configuration will meet this requirement?

- [ ] A. Configure the security group for the EC2 instances.
- [ ] B. Configure the security group on the Application Load Balancer.
- [ ] C. Configure AWS WAF on the Application Load Balancer in a VPC.
- [ ] D. Configure the network ACL for the subnet that contains the EC2 instances.

</details>

<details>
  <summary>Answer</summary>

- [ ] C. Configure AWS WAF on the Application Load Balancer in a VPC.

Why this is the correct answer:

- [ ] AWS WAF for Geographic Restriction: AWS WAF (Web Application Firewall) is a service that helps protect your web applications or APIs against common web exploits and bots. A key feature of AWS WAF is the ability to create rules based on the geographic origin of requests (geo match statements).
- [ ] Filtering by Country: You can configure an AWS WAF web ACL with a geo match rule to explicitly allow requests originating only from the "one specific country" and block requests from all other countries.
- [ ] Integration with Application Load Balancer (ALB): AWS WAF can be directly associated with an Application Load Balancer. When associated, WAF inspects all incoming web requests to the ALB and applies the defined rules (including the geo match rule) before traffic is forwarded to the backend EC2 instances.
- [ ] This solution directly and effectively meets the requirement to restrict access to the application to users from a single specific country.

Why are the other answers wrong?

- [ ] Option A is wrong because: Security groups act as stateful firewalls at the instance level. While they can filter traffic based on IP addresses or CIDR ranges, maintaining an accurate and comprehensive list of all IP addresses for an entire country (or conversely, all IP addresses not in that country) within security group rules is impractical, difficult to manage, and would likely exceed rule limits. AWS WAF's geo match feature is designed for this purpose.
- [ ] Option B is wrong because: Similar to option A, security groups associated with an Application Load Balancer are for controlling traffic to and from the load balancer itself based on IP addresses, ports, and protocols. They are not designed for sophisticated geographic-based filtering of application requests.
- [ ] Option D is wrong because: Network ACLs (NACLs) are stateless firewalls that operate at the subnet level. Like security groups, they filter traffic based on IP addresses or CIDR ranges. Attempting to implement country-level blocking using NACLs would face the same difficulties as with security groups regarding the management of IP lists and rule limits. AWS WAF is the more appropriate and effective tool for geographic-based access control.

</details>

<details>
  <summary>Question 171</summary>

A company provides an API to its users that automates inquiries for tax computations based on item prices. The company experiences a larger number of inquiries during the holiday season only that cause slower response times. A solutions architect needs to design a solution that is scalable and elastic.

What should the solutions architect do to accomplish this?

- [ ] A. Provide an API hosted on an Amazon EC2 instance. The EC2 instance performs the required computations when the API request is made.
- [ ] B. Design a REST API using Amazon API Gateway that accepts the item names. API Gateway passes item names to AWS Lambda for tax computations.
- [ ] C. Create an Application Load Balancer that has two Amazon EC2 instances behind it. The EC2 instances will compute the tax on the received item names.
- [ ] D. Design a REST API using Amazon API Gateway that connects with an API hosted on an Amazon EC2 instance. API Gateway accepts and passes the item names to the EC2 instance for tax computations.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Design a REST API using Amazon API Gateway that accepts the item names. API Gateway passes item names to AWS Lambda for tax computations.

Why this is the correct answer:

This solution provides a serverless, highly scalable, and elastic architecture ideal for handling fluctuating loads like holiday season spikes:

- [ ] Amazon API Gateway for Scalable API Frontend: Amazon API Gateway is a fully managed service that allows you to create, publish, maintain, monitor, and secure APIs at any scale. It can handle the "larger number of inquiries" and automatically scales to meet traffic demands.
- [ ] AWS Lambda for Elastic and Scalable Compute: AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers. It executes your code only when needed and scales automatically, from a few requests per day to thousands per second. For the tax computations, Lambda can scale rapidly during the holiday season spikes and scale down to zero when demand is low, ensuring elasticity and cost-effectiveness.
- [ ] Addresses Performance and Scalability: By using API Gateway to receive requests and Lambda to perform the computations, the solution can handle the increased load during peak times, thus preventing "slower response times." The serverless nature means the company doesn't need to pre-provision capacity for peak loads.
- [ ] Minimized Operational Overhead: This is a fully managed, serverless solution, significantly reducing server maintenance and patching efforts.

Why are the other answers wrong?

- [ ] Option A is wrong because: Hosting the API on a single Amazon EC2 instance (or even a manually scaled fleet) would require managing the instance(s), handling scaling manually or via more complex Auto Scaling configurations, and could still lead to slower response times if not scaled adequately for peak loads. This is not as inherently "scalable and elastic" with minimal operational overhead as a serverless solution.
- [ ] Option C is wrong because: While an Application Load Balancer with two EC2 instances provides some load distribution and redundancy, it offers limited elasticity compared to Lambda. If the holiday spike exceeds the capacity of these two instances, performance will still degrade unless a more sophisticated Auto Scaling setup is implemented, which adds complexity. A serverless Lambda backend scales more seamlessly from zero to high demand.
- [ ] Option D is wrong because: Using Amazon API Gateway as a frontend but then passing the requests to an API hosted on an Amazon EC2 instance means the EC2 instance remains the bottleneck for the actual computation. This doesn't fully leverage the scalability and elasticity benefits of serverless compute for the backend processing and still requires managing the EC2 instance. Option B (API Gateway to Lambda) provides a more complete serverless and elastic solution for the compute part.

</details>

<details>
  <summary>Question 172</summary>
Â  
A solutions architect is creating a new Amazon CloudFront distribution for an application. Some of the information submitted by users is sensitive. The application uses HTTPS but needs another layer of security. The sensitive information should be protected throughout the entire application stack, and access to the information should be restricted to certain applications.

Which action should the solutions architect take?

- [ ] A. Configure a CloudFront signed URL.
- [ ] B. Configure a CloudFront signed cookie.
- [ ] C. Configure a CloudFront field-level encryption profile.
- [ ] D. Configure CloudFront and set the Origin Protocol Policy setting to HTTPS Only for the Viewer Protocol Policy.

</details>

<details>
  <summary>Answer</summary>

- [ ] C. Configure a CloudFront field-level encryption profile.

Why this is the correct answer:

- [ ] CloudFront Field-Level Encryption: This feature of Amazon CloudFront allows you to selectively encrypt specific sensitive data fields in an HTTPS POST request at the CloudFront edge location, close to the user. The encryption uses a public key that you provide. The encrypted data is then forwarded to your origin server, and only specific applications at your origin that possess the corresponding private key can decrypt and view this sensitive information.
- [ ] Protection Throughout the Stack: Because the sensitive data is encrypted at the edge and only decrypted by authorized backend applications, it remains protected as it passes through your application stack (e.g., load balancers, web servers, intermediate services) until it reaches the component with the decryption key. This addresses the "protected throughout the entire application stack" requirement.
- [ ] Restricted Access to Certain Applications: By controlling which backend applications have access to the private key needed for decryption, you can restrict access to the sensitive information to only those authorized applications.
- [ ] Another Layer of Security on top of HTTPS: The problem states the application already uses HTTPS (which encrypts the entire request in transit). Field-level encryption adds another layer by encrypting specific sensitive fields within that already encrypted HTTPS request, providing defense in depth.

Why are the other answers wrong?

- [ ] Option A is wrong because: CloudFront signed URLs are used to control access to specific files or content served by CloudFront (e.g., private video files or software downloads). They are used to authorize who can access certain content and for how long, but they do not encrypt sensitive data submitted by users to the application.
- [ ] Option B is wrong because: CloudFront signed cookies function similarly to signed URLs. They are used to control access to multiple restricted files or a broader set of private content, typically for users who have authenticated. Like signed URLs, they do not encrypt sensitive data submitted by users to the application.
- [ ] Option D is wrong because:
Setting the Viewer Protocol Policy to "HTTPS Only" ensures that users can only connect to CloudFront using HTTPS.
Setting the Origin Protocol Policy to "HTTPS Only" ensures that CloudFront connects to your origin server using HTTPS.
While these are essential security practices for ensuring end-to-end encryption of traffic in transit (and the question states "The application uses HTTPS"), they do not provide an additional layer of security for specific sensitive data fields within the already HTTPS-encrypted request. Field-level encryption (Option C) provides that specific additional layer for the data fields themselves.

</details>

<details>
  <summary>Question 173</summary>
 
A gaming company hosts a browser-based application on AWS. The users of the application consume a large number of videos and images that are stored in Amazon S3. This content is the same for all users.

The application has increased in popularity, and millions of users worldwide accessing these media files. The company wants to provide the files to the users while reducing the load on the origin.

Which solution meets these requirements MOST cost-effectively?

- [ ] A. Deploy an AWS Global Accelerator accelerator in front of the web servers.
- [ ] B. Deploy an Amazon CloudFront web distribution in front of the S3 bucket.
- [ ] C. Deploy an Amazon ElastiCache for Redis instance in front of the web servers.
- [ ] D. Deploy an Amazon ElastiCache for Memcached instance in front of the web servers.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Deploy an Amazon CloudFront web distribution in front of the S3 bucket.

Why this is the correct answer:

This scenario perfectly describes the primary use case for a Content Delivery Network (CDN).

- [ ] Amazon S3 for Storing Media Files: Storing the videos and images in Amazon S3 is a scalable and durable solution for the origin content.
- [ ] Amazon CloudFront for Caching and Global Delivery: Amazon CloudFront is AWS's global CDN. By deploying a CloudFront web distribution with the S3 bucket as its origin, the media files (which are "the same for all users," making them highly cacheable) will be cached at CloudFront's numerous edge locations around the world.
- [ ] Reducing Load on Origin: When users request these files, they will be served from the CloudFront edge location closest to them if the content is cached there. This significantly "reduces the load on the origin" S3 bucket because CloudFront handles the bulk of the requests.
- [ ] Improved Performance for Global Users: Serving content from edge locations drastically reduces latency for "millions of users worldwide," leading to faster load times and a better user experience.
- [ ] MOST Cost-Effective: For delivering static content globally at scale, using CloudFront with an S3 origin is generally the most cost-effective solution. Data transfer from S3 to CloudFront is often free or at a reduced cost, and CloudFront's data transfer out pricing, combined with caching, can be more economical than users directly accessing S3 from all over the world.

Why are the other answers wrong?

- [ ] Option A is wrong because: AWS Global Accelerator is designed to improve the performance and availability of applications by routing traffic over the AWS global network to optimal regional application endpoints (like Application Load Balancers or EC2 instances). While it can reduce latency for reaching application endpoints, it is not primarily a content caching service. CloudFront (Option B) is specifically built for caching and delivering static and dynamic content from the edge, which is key for reducing origin load for static media files.
- [ ] Option C is wrong because: Amazon ElastiCache for Redis is an in-memory caching service typically used to cache database query results, session states, or frequently accessed small pieces of data to accelerate application performance. It is not designed for caching and delivering large media files (videos and images) to millions of users worldwide in the way a CDN like CloudFront does.
- [ ] Option D is wrong because: Similar to ElastiCache for Redis, Amazon ElastiCache for Memcached is an in-memory caching service. It is also not suited for the use case of caching and globally distributing large static media files. CloudFront is the appropriate service for edge caching and delivery of such content.

</details>

<details>
  <summary>Question 174</summary>

A company has a multi-tier application that runs six front-end web servers in an Amazon EC2 Auto Scaling group in a single Availability Zone behind an Application Load Balancer (ALB). A solutions architect needs to modify the infrastructure to be highly available without modifying the application.

Which architecture should the solutions architect choose that provides high availability?

- [ ] A. Create an Auto Scaling group that uses three instances across each of two Regions.
- [ ] B. Modify the Auto Scaling group to use three instances across each of two Availability Zones.
- [ ] C. Create an Auto Scaling template that can be used to quickly create more instances in another Region.
- [ ] D. Change the ALB in front of the Amazon EC2 instances in a round-robin configuration to balance traffic to the web tier.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Modify the Auto Scaling group to use three instances across each of two Availability Zones.

Why this is the correct answer:

- [ ] Addressing Single Point of Failure: The current setup has all web servers in a "single Availability Zone." This is a single point of failure; if that Availability Zone (AZ) experiences an outage, the entire web tier becomes unavailable.
- [ ] Multi-AZ for High Availability: To achieve high availability within an AWS Region, the standard practice is to distribute resources across multiple Availability Zones. Modifying the Auto Scaling group to launch instances across at least two Availability Zones (e.g., three instances in each of two AZs, or distributing the existing six instances across two or more AZs) ensures that if one AZ fails, the instances in the other AZ(s) can continue to operate and serve traffic.
- [ ] Application Load Balancer (ALB) Supports Multi-AZ: An Application Load Balancer is designed to distribute traffic to target instances across multiple AZs for which it is configured. This works seamlessly with a Multi-AZ Auto Scaling group.
- [ ] No Application Modification: This change is purely at the infrastructure level (Auto Scaling group configuration, VPC subnet configuration for the ASG) and does not require any modifications to the application code itself, as per the requirement.

Why are the other answers wrong?

- [ ] Option A is wrong because: Deploying instances across multiple AWS Regions is a strategy for disaster recovery or for serving a globally distributed user base with regional endpoints. It introduces significant complexity (e.g., data replication across regions, inter-region latency, more complex routing) and is generally a much larger undertaking than achieving high availability within a single region by using multiple Availability Zones. The first step to high availability is typically Multi-AZ within a region.
- [ ] Option C is wrong because: Creating an Auto Scaling template for another Region is a preparatory step for disaster recovery (i.e., being able to launch resources in another region if the primary region fails). It does not make the current infrastructure in the primary region highly available against an AZ failure within that region.
- [ ] Option D is wrong because: Application Load Balancers already distribute traffic using algorithms like round-robin or least outstanding requests. Simply ensuring or changing to a round-robin configuration does not address the fundamental high availability issue, which is that all instances are currently located in a single Availability Zone. The problem lies in the instance deployment strategy, not the ALB's load distribution method within that single AZ.

</details>

<details>
  <summary>Question 175</summary>

An ecommerce company has an order-processing application that uses Amazon API Gateway and an AWS Lambda function. The application stores data in an Amazon Aurora PostgreSQL database. During a recent sales event, a sudden surge in customer orders occurred. Some customers experienced timeouts, and the application did not process the orders of those customers. A solutions architect determined that the CPU utilization and memory utilization were high on the database because of a large number of open connections. The solutions architect needs to prevent the timeout errors while making the least possible changes to the application.

Which solution will meet these requirements?

- [ ] A. Configure provisioned concurrency for the Lambda function. Modify the database to be a global database in multiple AWS Regions.
- [ ] B. Use Amazon RDS Proxy to create a proxy for the database. Modify the Lambda function to use the RDS Proxy endpoint instead of the database endpoint.
- [ ] C. Create a read replica for the database in a different AWS Region. Use query string parameters in API Gateway to route traffic to the read replica.
- [ ] D. Migrate the data from Aurora PostgreSQL to Amazon DynamoDB by using AWS Database Migration Service (AWS DMS). Modify the Lambda function to use the DynamoDB table.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Use Amazon RDS Proxy to create a proxy for the database. Modify the Lambda function to use the RDS Proxy endpoint instead of the database endpoint.

Why this is the correct answer:

- [ ] Problem: Database Connection Exhaustion: The root cause of the timeouts and high CPU/memory utilization on the database is identified as "a large number of open connections."
- [ ] This is a common issue with serverless applications like AWS Lambda functions connecting to relational databases, as each concurrent - [ ] Lambda execution might try to establish its own database connection, quickly exhausting the database's connection limit during surges.
- [ ] Amazon RDS Proxy for Connection Pooling: Amazon RDS Proxy is a fully managed, highly available database proxy that sits between your application (the Lambda functions) and your Amazon Aurora database.
- [ ] It maintains a pool of established connections to the database and efficiently reuses these connections for application requests.
- [ ] This significantly reduces the number of new connections made directly to the database, especially from frequently invoked, short-lived Lambda functions.
- [ ] Preventing Timeouts and Reducing Load: By managing and reusing connections, RDS Proxy helps prevent the database from being overwhelmed by connection requests during traffic surges, thus reducing CPU/memory utilization related to connection management and preventing timeout errors.
- [ ] Least Possible Changes: Modifying the Lambda function to connect to the RDS Proxy endpoint instead of the direct database endpoint typically involves a minimal change to the database connection string in the Lambda function's configuration or code.
- [ ] This aligns with the requirement of "making the least possible changes to the application."

Why are the other answers wrong?

- [ ] Option A is wrong because:
Configuring provisioned concurrency for the Lambda function helps reduce cold start latency by keeping a certain number of Lambda execution environments warm. It does not solve the problem of each of those (potentially many) warm instances opening a new connection to the database.
Modifying the database to be an Aurora Global Database is a solution for providing low-latency global reads and disaster recovery across regions. It does not address the issue of connection exhaustion on a specific regional database endpoint due to a surge in local application connections.
- [ ] Option C is wrong because: Creating a read replica is for offloading read traffic from the primary database. While this can help with overall database performance if reads are a bottleneck, the problem statement specifically points to "a large number of open connections" causing high CPU and memory, which affects all types of operations, including writes for order processing. Simply routing some traffic to a read replica (especially in a different region, which adds latency) doesn't solve the core connection pooling issue for the primary database that handles writes.
- [ ] Option D is wrong because: Migrating the database from Amazon Aurora PostgreSQL (a relational database) to Amazon DynamoDB (a NoSQL database) is a major architectural change. It would require significant data modeling changes and extensive modifications to the Lambda function's data access logic. This contradicts the requirement of "making the least possible changes to the application."

</details>

<details>
  <summary>Question 176</summary>

An application runs on Amazon EC2 instances in private subnets. The application needs to access an Amazon DynamoDB table.

What is the MOST secure way to access the table while ensuring that the traffic does not leave the AWS network?

- [ ] A. Use a VPC endpoint for DynamoDB.
- [ ] B. Use a NAT gateway in a public subnet.
- [ ] C. Use a NAT instance in a private subnet.
- [ ] D. Use the internet gateway attached to the VPC.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Use a VPC endpoint for DynamoDB.

Why this is the correct answer:

- [ ] VPC Endpoint for DynamoDB (Gateway Endpoint): Amazon DynamoDB supports gateway VPC endpoints. A gateway endpoint allows your EC2 instances in private subnets within your VPC to securely access DynamoDB tables in the same AWS Region without requiring traffic to traverse an Internet Gateway, NAT Gateway, VPN connection, or AWS Direct Connect connection.
- [ ] Traffic Stays Within the AWS Network: When using a VPC endpoint for DynamoDB, traffic between your VPC and DynamoDB remains entirely within the AWS private network. This directly meets the requirement that "traffic does not leave the AWS network."
- [ ] MOST Secure Way: This is considered the most secure method for accessing DynamoDB from within a VPC because it avoids exposing your instances or traffic to the public internet. You can also attach an endpoint policy to the VPC endpoint to further control which DynamoDB tables or actions are accessible from your VPC.
- [ ] No Additional Cost: There are no additional charges for using gateway VPC endpoints.

Why are the other answers wrong?

- [ ] Option B is wrong because: A NAT gateway allows instances in a private subnet to initiate outbound connections to the internet or to public AWS service endpoints (like the public DynamoDB endpoints). While traffic to DynamoDB in the same region via a NAT gateway typically stays within the AWS backbone, it still involves routing through the NAT gateway and accessing public service endpoints. A VPC endpoint provides a more direct and private path and avoids any potential for traffic to inadvertently route externally. It also avoids NAT gateway data processing charges.
- [ ] Option C is wrong because: A NAT instance, similar to a NAT gateway, provides outbound internet connectivity. However, placing a NAT instance in a private subnet is an incorrect configuration, as it would not have a route to the internet gateway. Even if correctly placed in a public subnet, it shares the same drawbacks as a NAT gateway for this specific use case compared to a VPC endpoint and adds the operational overhead of managing an EC2 instance.
- [ ] Option D is wrong because: An internet gateway is used to allow resources with public IP addresses (typically in public subnets) to communicate with the internet. EC2 instances in "private subnets" by definition do not have a direct route to an internet gateway for outbound traffic. If they were to access DynamoDB via an internet gateway, it would imply they are not truly private or are misconfigured, and the traffic would be using public DynamoDB endpoints. This is not the most secure method and doesn't ensure traffic stays off the broader internet path.

</details>

<details>
  <summary>Question 177</summary>
 
An entertainment company is using Amazon DynamoDB to store media metadata. The application is read intensive and experiencing delays. The company does not have staff to handle additional operational overhead and needs to improve the performance efficiency of DynamoDB without reconfiguring the application.

What should a solutions architect recommend to meet this requirement?

- [ ] A. Use Amazon ElastiCache for Redis.
- [ ] B. Use Amazon DynamoDB Accelerator (DAX).
- [ ] C. Replicate data by using DynamoDB global tables.
- [ ] D. Use Amazon ElastiCache for Memcached with Auto Discovery enabled.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Use Amazon DynamoDB Accelerator (DAX).

Why this is the correct answer:

- [ ] Amazon DynamoDB Accelerator (DAX): DAX is a fully managed, highly available, in-memory cache specifically designed for Amazon DynamoDB. It seamlessly integrates with DynamoDB and can significantly reduce read latency from milliseconds to microseconds, even for applications with millions of requests per second.
- [ ] Addresses Read-Intensive Delays: The application is "read intensive and experiencing delays." DAX caches frequently accessed DynamoDB data, serving read requests directly from the cache, which dramatically improves read performance and reduces the load on the underlying DynamoDB table.
- [ ] Without Reconfiguring the Application (Minimal Changes): DAX is API-compatible with DynamoDB. In most cases, applications can use DAX with minimal or no modification by simply updating the DynamoDB client to point to the DAX cluster endpoint. This aligns with the "without reconfiguring the application" (or with very minimal changes) requirement.
- [ ] No Additional Operational Overhead: DAX is a fully managed service. AWS handles the patching, maintenance, scaling, and fault tolerance of the DAX cluster, so the company "does not have staff to handle additional operational overhead."
- [ ] This solution directly targets the read performance issue for DynamoDB with a managed service requiring minimal application changes and low operational overhead.

Why are the other answers wrong?

- [ ] Option A is wrong because: While Amazon ElastiCache for Redis is a powerful in-memory caching service, integrating it with an existing DynamoDB application would typically require significant application code changes to implement caching logic (e.g., check the Redis cache first, then DynamoDB on a cache miss, and update Redis on writes to DynamoDB). This contradicts the requirement of "without reconfiguring the application" (or minimal changes). DAX offers a more transparent caching layer for DynamoDB.
- [ ] Option C is wrong because: DynamoDB global tables provide a multi-region, multi-active database solution, primarily for building globally distributed applications or for disaster recovery. While global tables can improve read latency for users in different geographic regions by providing local replicas, they do not specifically address read delays caused by a read-intensive workload on a single-region DynamoDB table in the same way a dedicated in-memory cache like DAX does. Global tables also add complexity and cost.
- [ ] Option D is wrong because: Similar to ElastiCache for Redis, Amazon ElastiCache for Memcached is a general-purpose in-memory caching service. It would also require application code modifications to integrate caching logic for DynamoDB data, which is not ideal when trying to avoid application reconfiguration. DAX is purpose-built for DynamoDB caching.

</details>

<details>
  <summary>Question 178</summary>

A company's infrastructure consists of Amazon EC2 instances and an Amazon RDS DB instance in a single AWS Region. The company wants to back up its data in a separate Region.

Which solution will meet these requirements with the LEAST operational overhead?

- [ ] A. Use AWS Backup to copy EC2 backups and RDS backups to the separate Region.
- [ ] B. Use Amazon Data Lifecycle Manager (Amazon DLM) to copy EC2 backups and RDS backups to the separate Region.
- [ ] C. Create Amazon Machine Images (AMIs) of the EC2 instances. Copy the AMIs to the separate Region. Create a read replica for the RDS DB instance in the separate Region.
- [ ] D. Create Amazon Elastic Block Store (Amazon EBS) snapshots. Copy the EBS snapshots to the separate Region. Create RDS snapshots. Export the RDS snapshots to Amazon S3. Configure S3 Cross-Region Replication (CRR) to the separate Region.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Use AWS Backup to copy EC2 backups and RDS backups to the separate Region.

Why this is the correct answer:

- [ ] AWS Backup for Centralized Backup Management: AWS Backup is a fully managed backup service that makes it easy to centralize and automate the backup of data across various AWS services, including Amazon EC2 instances (which involves backing up their EBS volumes, often as part of AMIs or snapshots) and Amazon RDS DB instances.
- [ ] Cross-Region Backup Copies: A key feature of AWS Backup is the ability to create backup plans that automatically copy backups to a different AWS Region. This directly meets the requirement to "back up its data in a separate Region."
- [ ] Least Operational Overhead: By using AWS Backup, the company can define backup schedules, retention policies, and cross-region copy rules in a single place. AWS Backup then automates the entire process. This significantly reduces the operational overhead compared to manually creating snapshots, copying them, and managing their lifecycle, or using multiple service-specific tools.
- [ ] This solution provides a unified, automated, and low-overhead way to manage and replicate backups for both EC2 and RDS to another region.

Why are the other answers wrong?

- [ ] Option B is wrong because: Amazon Data Lifecycle Manager (DLM) is primarily used for automating the creation, retention, and deletion of Amazon EBS snapshots and EBS-backed AMIs. While DLM can copy EBS snapshots across regions, it does not natively manage Amazon RDS backups. AWS Backup offers a more comprehensive solution for managing backups across different types of AWS resources, including RDS.
- [ ] Option C is wrong because:
Manually creating AMIs of EC2 instances and copying them to another region is a valid way to back up EC2 instances cross-region, but it's more operationally intensive than using AWS Backup.
Creating an RDS read replica in a separate region is primarily a solution for read scaling or for creating a continuously updated standby for disaster recovery (which can be promoted). While it does replicate data, it's not a "backup" in the traditional sense of a point-in-time snapshot stored for recovery, and it incurs the cost of a running DB instance in the DR region. AWS Backup handles point-in-time RDS backups.
- [ ] Option D is wrong because: This option describes a very manual and fragmented process. It involves manually creating EBS snapshots and copying them, manually creating RDS snapshots, then exporting RDS snapshots to Amazon S3 (which is an extra step and not always the most direct way to manage RDS backups for DR), and then configuring S3 Cross-Region Replication for those exported S3 objects. This has significantly more steps and higher operational overhead compared to the streamlined, automated approach offered by AWS Backup.

</details>

<details>
  <summary>Question 179</summary>

A solutions architect needs to securely store a database user name and password that an application uses to access an Amazon RDS DB instance. The application that accesses the database runs on an Amazon EC2 instance. The solutions architect wants to create a secure parameter in AWS Systems Manager Parameter Store.

What should the solutions architect do to meet this requirement?

- [ ] A. Create an IAM role that has read access to the Parameter Store parameter. Allow Decrypt access to an AWS Key Management Service (AWS KMS) key that is used to encrypt the parameter. Assign this IAM role to the EC2 instance.
- [ ] B. Create an IAM policy that allows read access to the Parameter Store parameter. Allow Decrypt access to an AWS Key Management Service (AWS KMS) key that is used to encrypt the parameter. Assign this IAM policy to the EC2 instance.
- [ ] C. Create an IAM trust relationship between the Parameter Store parameter and the EC2 instance. Specify Amazon RDS as a principal in the trust policy.
- [ ] D. Create an IAM trust relationship between the DB instance and the EC2 instance. Specify Systems Manager as a principal in the trust policy.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Create an IAM role that has read access to the Parameter Store parameter. Allow Decrypt access to an AWS Key Management Service (AWS KMS) key that is used to encrypt the parameter. Assign this IAM role to the EC2 instance.

Why this is the correct answer:

This solution follows AWS best practices for securely providing credentials to an EC2 instance:

- [ ] Secure Storage with Parameter Store: AWS Systems Manager Parameter Store allows you to store configuration data and secrets. For sensitive information like database usernames and passwords, you should create a parameter of type SecureString. SecureString parameters are encrypted using an AWS Key Management Service (AWS KMS) key (either an AWS managed key for Parameter Store or a customer-managed key).
- [ ] IAM Role for EC2 Instance Permissions: The recommended way for an Amazon EC2 instance to securely access other AWS services is by using an IAM role. You create an IAM role and attach policies to it that grant the necessary permissions. This role is then associated with the EC2 instance through an instance profile. The application on the EC2 instance automatically receives temporary security credentials from this role.
- [ ] Required Permissions for the Role: The IAM role attached to the EC2 instance needs the following permissions:
- [ ] Permission to read the specific parameter from Parameter Store (e.g., ssm:GetParameter, ssm:GetParameters on the specific parameter ARN).
- [ ] Permission to decrypt the parameter's value using the AWS KMS key that was used to encrypt it (the kms:Decrypt action on the specific KMS key ARN).
- [ ] Assign Role to EC2 Instance: By assigning this correctly configured IAM role to the EC2 instance, the application running on the instance can securely retrieve and decrypt the database credentials from Parameter Store at runtime without needing to embed credentials in code or configuration files.

Why are the other answers wrong?

- [ ] Option B is wrong because: You cannot directly attach an IAM policy to an Amazon EC2 instance. IAM policies are attached to IAM identities (users, groups, or roles). An IAM role is created, the policy is attached to this role, and then the role is associated with the EC2 instance.
- [ ] Option C is wrong because: IAM trust relationships are configured within an IAM role to define which principals (e.g., an AWS service like EC2, another AWS account, an IAM user) are allowed to assume that role. There is no concept of creating a trust relationship "between the Parameter Store parameter and the EC2 instance." Specifying Amazon RDS as a principal in an EC2 instance's role trust policy for this purpose is incorrect.
- [ ] Option D is wrong because: Similar to option C, this incorrectly describes how IAM trust relationships work. A trust relationship is not established "between the DB instance and the EC2 instance" in this manner for accessing Systems Manager. The EC2 instance needs permissions (via a role) to access Systems Manager Parameter Store.

</details>

<details>
  <summary>Question 180</summary>

A company is designing a cloud communications platform that is driven by APIs. The application is hosted on Amazon EC2 instances behind a Network Load Balancer (NLB). The company uses Amazon API Gateway to provide external users with access to the application through APIs. The company wants to protect the platform against web exploits like SQL injection and also wants to detect and mitigate large, sophisticated DDoS attacks.

Which combination of solutions provides the MOST protection? (Choose two.)

- [ ] A. Use AWS WAF to protect the NLB.
- [ ] B. Use AWS Shield Advanced with the NLB.
- [ ] C. Use AWS WAF to protect Amazon API Gateway.
- [ ] D. Use Amazon GuardDuty with AWS Shield Standard
- [ ] E. Use AWS Shield Standard with Amazon API Gateway.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Use AWS Shield Advanced with the NLB.
- [ ] C. Use AWS WAF to protect Amazon API Gateway.

Why these are the correct answers:

This question requires a layered security approach to protect against different types of attacks at different points in the architecture.

B. Use AWS Shield Advanced with the NLB.
- [ ] DDoS Protection for Infrastructure: The Network Load Balancer (NLB) is fronting the EC2 instances.
- [ ] AWS Shield Advanced provides enhanced protection against large and sophisticated Distributed Denial of Service (DDoS) attacks for AWS resources, including NLBs.
- [ ] This layer helps protect the underlying application infrastructure from volumetric and network-layer DDoS attacks.

C. Use AWS WAF to protect Amazon API Gateway.
- [ ] Web Exploit Protection for APIs: Amazon API Gateway is the entry point for external users accessing the APIs.
- [ ] AWS WAF (Web Application Firewall) integrates with API Gateway to protect against common web exploits such as SQL injection, cross-site scripting (XSS), and other Layer 7 attacks.
- [ ] This addresses the requirement to protect "against web exploits like SQL injection."
- [ ] Combining AWS Shield Advanced on the NLB (for infrastructure DDoS protection) and AWS WAF on API Gateway (for application-layer web exploit protection) provides comprehensive, multi-layered security.

Why are the other answers wrong?

- [ ] Option A is wrong because: AWS WAF is typically associated with Application Load Balancers (ALBs), Amazon API Gateway, or Amazon CloudFront. While a Network Load Balancer deals with network traffic, WAF's capabilities for inspecting HTTP/S requests are best utilized with services that operate at Layer 7. You cannot directly associate AWS WAF with an NLB for HTTP/S inspection in the same way as an ALB or API Gateway.
- [ ] Option D is wrong because:
Amazon GuardDuty is a threat detection service that monitors for malicious activity and unauthorized behavior. It does not actively mitigate DDoS attacks or web exploits.
AWS Shield Standard provides baseline protection against common DDoS attacks for all AWS customers at no additional charge, but for "large, sophisticated DDoS attacks," Shield Advanced (Option B) is required.
- [ ] Option E is wrong because: While AWS Shield Standard offers some DDoS protection, it's not designed for "large, sophisticated DDoS attacks." AWS Shield Advanced is the service for that level of protection. While WAF is needed for web exploits, Shield Standard alone is insufficient for the DDoS requirement stated.
Therefore, Shield Advanced on the NLB and WAF on API Gateway provide the most comprehensive protection for the described scenario.

</details>
















































<details>
  <summary>Question X</summary>

- [ ] A.  TurnÂ  


</details>

<details>
  <summary>Answer</summary>

- [ ] A.  Turn


</details>



