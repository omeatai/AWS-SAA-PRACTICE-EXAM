<details>
  <summary>Question 151</summary>

A company wants to migrate its on-premises data center to AWS. According to the company's compliance requirements, the company can use only the ap-northeast-3 Region. Company administrators are not permitted to connect VPCs to the internet.

Which solutions will meet these requirements? (Choose two.)

- [ ] A. Use AWS Control Tower to implement data residency guardrails to deny internet access and deny access to all AWS Regions except ap-northeast-3.
- [ ] B. Use rules in AWS WAF to prevent internet access. Deny access to all AWS Regions except ap-northeast-3 in the AWS account settings.
- [ ] C. Use AWS Organizations to configure service control policies (SCPS) that prevent VPCs from gaining internet access. Deny access to all AWS Regions except ap-northeast-3.
- [ ] D. Create an outbound rule for the network ACL in each VPC to deny all traffic from 0.0.0.0/0 Create an IAM policy for each user to prevent the use of any AWS Region other than ap-northeast-3.
- [ ] E. Use AWS Config to activate managed rules to detect and alert for internet gateways and to detect and alert for new resources deployed outside of ap-northeast-3.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Use AWS Control Tower to implement data residency guardrails to deny internet access and deny access to all AWS Regions except ap-northeast-3.
- [ ] C. Use AWS Organizations to configure service control policies (SCPS) that prevent VPCs from gaining internet access. Deny access to all AWS Regions except ap-northeast-3.

Why these are the correct answers:

These options provide preventative controls at an organizational level to enforce the specified compliance requirements:

A. Use AWS Control Tower to implement data residency guardrails to deny internet access and deny access to all AWS Regions except ap-northeast-3.
- [ ] AWS Control Tower for Governance: AWS Control Tower provides an easy way to set up and govern a secure, multi-account AWS environment based on best practices. It uses guardrails (which are often implemented using Service Control Policies and AWS Config rules) to enforce policies.
- [ ] Data Residency Guardrails: Control Tower offers specific guardrails for data residency that can be used to deny access to AWS services and operations in unauthorized AWS Regions, thereby ensuring work occurs only in ap-northeast-3.
- [ ] Preventing Internet Access: Control Tower can also deploy guardrails that help prevent VPCs from connecting to the internet, for example, by restricting the creation or use of Internet Gateways.
- [ ] Centralized Enforcement: Control Tower provides a comprehensive, managed way to establish and enforce these types of compliance rules across an organization.

C. Use AWS Organizations to configure service control policies (SCPS) that prevent VPCs from gaining internet access. Deny access to all AWS Regions except ap-northeast-3.
- [ ] AWS Organizations and SCPs: Service Control Policies (SCPs) are a feature of AWS Organizations that offer central control over the maximum available permissions for all accounts in your organization. SCPs can restrict which AWS services, resources, and individual API actions users and roles in member accounts can access.
- [ ] Denying Access to Other Regions: An SCP can be configured to explicitly deny access to all AWS API actions in Regions other than ap-northeast-3. This effectively limits operations to the allowed region.
- [ ] Preventing Internet Access for VPCs: SCPs can also be used to deny specific EC2 actions required to connect a VPC to the internet, such as ec2:CreateInternetGateway, ec2:AttachInternetGateway, or actions that modify route tables to use an Internet Gateway. This ensures administrators cannot connect VPCs to the internet.
- [ ] Preventative Control: SCPs act as preventative guardrails that even administrators in member accounts cannot override (unless they have permissions in the management account to modify the SCP itself).

Why are the other answers wrong?

- [ ] Option B is wrong because: AWS WAF (Web Application Firewall) is used to protect web applications from common web exploits. It does not control general VPC internet connectivity or restrict access to AWS Regions. Denying access to regions in "AWS account settings" is less robust and centrally managed than using SCPs or Control Tower guardrails for organizational compliance.
- [ ] Option D is wrong because:
Network ACLs operate at the subnet level and are stateless. While they can block traffic, managing NACLs across all VPCs to deny all outbound internet traffic (the 0.0.0.0/0 in an outbound rule is not how sources are typically specified for outbound; it would be a destination) can be complex and might inadvertently block necessary internal or AWS service traffic. It's also not as robust as an SCP for preventing administrators from creating internet connectivity.
IAM policies apply to users, groups, and roles, restricting what they can do. While useful, SCPs provide an overarching organizational boundary that applies even to administrators within member accounts.
- [ ] Option E is wrong because: AWS Config is a service for assessing, auditing, and evaluating the configurations of AWS resources. It provides detective controls, meaning it can detect and alert when a non-compliant configuration occurs (like an Internet Gateway being created or resources deployed in a wrong region). However, it does not prevent these actions. The requirement is that administrators "are not permitted" to do these things, which implies a need for preventative controls.

</details>

<details>
  <summary>Question 152</summary>

A company wants to migrate an on-premises data center to AWS. The data center hosts an SFTP server that stores its data on an NFS-based file system. The server holds 200 GB of data that needs to be transferred. The server must be hosted on an Amazon EC2 instance that uses an Amazon Elastic File System (Amazon EFS) file system.

Which combination of steps should a solutions architect take to automate this task? (Choose two.)

- [ ] A. Launch the EC2 instance into the same Availability Zone as the EFS file system.
- [ ] B. Install an AWS DataSync agent in the on-premises data center.
- [ ] C. Create a secondary Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instance for the data.
- [ ] D. Manually use an operating system copy command to push the data to the EC2 instance.
- [ ] E. Use AWS DataSync to create a suitable location configuration for the on-premises SFTP server.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Install an AWS DataSync agent in the on-premises data center.
- [ ] E. Use AWS DataSync to create a suitable location configuration for the on-premises SFTP server.

Why these are the correct answers:

This solution focuses on automating the transfer of 200 GB of data from an on-premises NFS file system (backing an SFTP server) to an Amazon EFS file system in AWS.

B. Install an AWS DataSync agent in the on-premises data center.
- [ ] AWS DataSync Agent: To use AWS DataSync for transferring data from an on-premises location, you need to deploy a DataSync agent (a virtual machine) in your on-premises environment.
- [ ] This agent will access your on-premises NFS file system and manage the data transfer to AWS.

E. Use AWS DataSync to create a suitable location configuration for the on-premises SFTP server.
- [ ] DataSync Locations and Task: Once the agent is deployed and activated, you configure AWS DataSync by creating a source location that points to your on-premises NFS file system (which backs the SFTP server) and a destination location that points to your Amazon EFS file system in AWS.
- [ ] Then, you create a DataSync task to manage the automated transfer of data between these locations.
- [ ] DataSync handles the scheduling, monitoring, data validation, and optimization of the transfer.
- [ ] This combination provides an automated, efficient, and managed way to transfer the data.

Why are the other answers wrong?

- [ ] Option A is wrong because: While an EC2 instance needs to be in a VPC with mount targets for the EFS file system (which exist in specific AZs), EFS itself is a regional service designed to be accessible from any AZ within the region where it's created. This step is related to how the EC2 instance accesses EFS after migration, not directly a step in automating the data transfer from on-premises. The core of the automation is DataSync.
- [ ] Option C is wrong because: The requirement clearly states that the new SFTP server hosted on an EC2 instance must use an Amazon EFS file system, not an EBS volume, for its data. Creating an EBS volume contradicts this requirement.
- [ ] Option D is wrong because: Manually using operating system copy commands (like scp, rsync, or cp over an NFS mount) is not an "automated task" in the context of a managed migration service like DataSync. Manual copies lack the built-in scheduling, monitoring, error handling, data validation, and transfer optimization features that DataSync provides, and they would require more manual intervention, especially for 200 GB of data.

</details>

<details>
  <summary>Question 153</summary>

A company sells ringtones created from clips of popular songs. The files containing the ringtones are stored in Amazon S3 Standard and are at least 128 KB in size. The company has millions of files, but downloads are infrequent for ringtones older than 90 days. The company needs to save money on storage while keeping the most accessed files readily available for its users.

Which action should the company take to meet these requirements MOST cost-effectively?

- [ ] A. Configure S3 Standard-Infrequent Access (S3 Standard-IA) storage for the initial storage tier of the objects.
- [ ] B. Move the files to S3 Intelligent-Tiering and configure it to move objects to a less expensive storage tier after 90 days.
- [ ] C. Configure S3 inventory to manage objects and move them to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days.
- [ ] D. Implement an S3 Lifecycle policy that moves the objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Implement an S3 Lifecycle policy that moves the objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days.

Why this is the correct answer:

This solution directly addresses the defined access patterns and cost-saving requirements:

- [ ] Initial Storage in S3 Standard: New ringtones and those younger than 90 days are frequently accessed ("most accessed files readily available"). S3 Standard is suitable for this, offering high availability and immediate retrieval with no retrieval fees. The problem states files are currently in S3 Standard.
- [ ] Defined Access Pattern Shift: There's a clear shift in access patterns: "downloads are infrequent for ringtones older than 90 days."
- [ ] S3 Lifecycle Policy for Automated Transition: An S3 Lifecycle policy allows you to define rules to automatically transition objects to different storage classes based on their age. Configuring a policy to move objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days automates the cost optimization process.
- [ ] S3 Standard-IA for Infrequent Access with Ready Availability: S3 Standard-IA is designed for data that is accessed less frequently but requires rapid access when needed. It offers a lower storage cost than S3 Standard, making it cost-effective for the ringtones older than 90 days. While there's a per-GB retrieval fee, this is acceptable if downloads are truly infrequent. It keeps the files "readily available."
- [ ] MOST Cost-Effective for This Pattern: For a well-defined access pattern (frequent then infrequent), a direct lifecycle policy to an appropriate infrequent access tier like S3 Standard-IA is a very cost-effective and straightforward approach.

Why are the other answers wrong?

- [ ] Option A is wrong because: Configuring S3 Standard-IA as the initial storage tier for all objects would mean that frequently accessed new ringtones (within the first 90 days) would incur retrieval fees every time they are downloaded. S3 Standard is more cost-effective for the initial period of frequent access.
- [ ] Option B is wrong because: S3 Intelligent-Tiering is primarily designed for data with unknown, changing, or unpredictable access patterns. While it could eventually move data to an infrequent access tier after 90 days if so configured (or if its own monitoring determined it), the access pattern here is relatively predictable (frequent for 90 days, then infrequent). For such a defined pattern, a direct S3 Lifecycle policy (as in option D) is often simpler and can be slightly more cost-effective as it avoids the small per-object monitoring and automation fees associated with S3 Intelligent-Tiering.
- [ ] Option C is wrong because: Amazon S3 Inventory provides flat file lists (CSV, ORC, or Parquet) of your objects and their metadata, which is useful for analysis, auditing, and reporting. S3 Inventory itself does not "manage objects and move them" between storage classes. That functionality is provided by S3 Lifecycle policies.

</details>

<details>
  <summary>Question 154</summary>

A company needs to save the results from a medical trial to an Amazon S3 repository. The repository must allow a few scientists to add new files and must restrict all other users to read-only access. No users can have the ability to modify or delete any files in the repository. The company must keep every file in the repository for a minimum of 1 year after its creation date.

Which solution will meet these requirements?

- [ ] A. Use S3 Object Lock in governance mode with a legal hold of 1 year.
- [ ] B. Use S3 Object Lock in compliance mode with a retention period of 365 days.
- [ ] C. Use an IAM role to restrict all users from deleting or changing objects in the S3 bucket. Use an S3 bucket policy to only allow the IAM role.
- [ ] D. Configure the S3 bucket to invoke an AWS Lambda function every time an object is added. Configure the function to track the hash of the saved object so that modified objects can be marked accordingly.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Use S3 Object Lock in compliance mode with a retention period of 365 days.

Why this is the correct answer:

This solution effectively addresses the requirements for immutability and fixed-term retention:

- [ ] S3 Object Lock for Immutability: The requirement "No users can have the ability to modify or delete any files in the repository" points directly to Amazon S3 Object Lock. S3 Object Lock provides Write-Once-Read-Many (WORM) protection for S3 objects, preventing them from being deleted or overwritten for a fixed amount of time or indefinitely.
- [ ] Compliance Mode for Strict Enforcement: S3 Object Lock offers two retention modes: governance and compliance. Compliance mode is the stricter of the two. When an object version is locked in compliance mode, its retention mode cannot be changed, and its retention period cannot be shortened by any user, including the root user in the AWS account, for the duration of the specified retention period. This ensures the highest level of immutability and meets the "No users can have the ability to modify or delete" requirement.
- [ ] Retention Period of 365 Days: Setting a retention period of 365 days (1 year) for objects when they are uploaded (e.g., via a default bucket setting or by applying it during upload) ensures that "The company must keep every file in the repository for a minimum of 1 year after its creation date."
- [ ] Access Control (Complementary): While S3 Object Lock handles the immutability and retention, separate IAM policies and/or S3 bucket policies would be configured to manage who can add new files (the "few scientists") and who has read-only access ("all other users"). This option focuses on the critical immutability and retention aspects.

Why are the other answers wrong?

- [ ] Option A is wrong because:
S3 Object Lock in governance mode allows users with specific IAM permissions (s3:BypassGovernanceRetention) to override the retention settings or delete the objects. This does not meet the strict requirement that "No users can have the ability to modify or delete any files."
A legal hold provides immutability but has no expiration date; it remains in effect until explicitly removed. While it prevents deletion, a fixed retention period (as in compliance mode) is more directly aligned with the "minimum of 1 year after its creation date" requirement if the goal is a time-bound, unchangeable retention.
- [ ] Option C is wrong because: While IAM roles and S3 bucket policies are essential for controlling access (who can read, who can write), they can typically be modified by users with sufficient administrative privileges (like the root user or account administrators). They do not provide the same level of guaranteed, unchangeable WORM protection that S3 Object Lock in compliance mode offers against all users, including administrators, for a defined period.
- [ ] Option D is wrong because: This describes a detective control, not a preventative one. Tracking hashes with a Lambda function would help identify if an object has been modified after the fact, but it would not prevent the modification or deletion from occurring in the first place. The requirement is that files "cannot be modified or deleted."

</details>

<details>
  <summary>Question 155</summary>

A large media company hosts a web application on AWS. The company wants to start caching confidential media files so that users around the world will have reliable access to the files. The content is stored in Amazon S3 buckets. The company must deliver the content quickly, regardless of where the requests originate geographically.

Which solution will meet these requirements?

- [ ] A. Use AWS DataSync to connect the S3 buckets to the web application.
- [ ] B. Deploy AWS Global Accelerator to connect the S3 buckets to the web application.
- [ ] C. Deploy Amazon CloudFront to connect the S3 buckets to CloudFront edge servers.
- [ ] D. Use Amazon Simple Queue Service (Amazon SQS) to connect the S3 buckets to the web application.

</details>

<details>
  <summary>Answer</summary>

- [ ] C. Deploy Amazon CloudFront to connect the S3 buckets to CloudFront edge servers.

Why this is the correct answer:

- [ ] Amazon CloudFront for Caching and Global Fast Delivery: Amazon CloudFront is a global content delivery network (CDN) service. It is designed to "deliver the content quickly, regardless of where the requests originate geographically" by caching copies of your content (in this case, confidential media files from S3 buckets) at edge locations around the world. When users request the files, they are served from the nearest edge location, which significantly reduces latency.
- [ ] Reliable Access and S3 Origin: CloudFront improves the reliability of access to content. It uses Amazon S3 buckets as an origin to fetch the content when it's not already cached at an edge location.
- [ ] Caching Confidential Media Files: CloudFront supports methods for securely serving private content from S3, such as using Origin Access Identity (OAI) or Origin Access Control (OAC) to restrict direct S3 access and ensure files are only accessible through CloudFront. Signed URLs or signed cookies can also be used with CloudFront to control access to confidential files on a per-user or per-session basis.
- [ ] This solution directly addresses all requirements: caching, reliable access, global fast delivery, and handling confidential content from S3.

Why are the other answers wrong?

- [ ] Option A is wrong because: AWS DataSync is a service for online data transfer, designed to move large amounts of data between on-premises storage and AWS storage, or between different AWS storage services. It is not a content delivery or caching service for providing low-latency access to users around the world.
- [ ] Option B is wrong because: AWS Global Accelerator is a service that improves the availability and performance of your applications with global users by providing static IP addresses and routing traffic over the AWS global network to optimal regional application endpoints (like Load Balancers or EC2 instances). While it helps with application endpoint performance and availability, it is not primarily a content caching service like CloudFront. CloudFront is specifically designed for caching and delivering static/dynamic content from the edge.
- [ ] Option D is wrong because: Amazon Simple Queue Service (Amazon SQS) is a message queuing service used for decoupling and scaling distributed systems by sending, storing, and receiving messages between software components. It has no role in caching or delivering media files to users.


</details>

<details>
  <summary>Question 156</summary>

- [ ] A.  Turn  
A company produces batch data that comes from different databases. The company also produces live stream data from network sensors and application APIs. The company needs to consolidate all the data into one place for business analytics. The company needs to process the incoming data and then stage the data in different Amazon S3 buckets. Teams will later run one-time queries and import the data into a business intelligence tool to show key performance indicators (KPIs).

Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)

- [ ] A. Use Amazon Athena for one-time queries. Use Amazon QuickSight to create dashboards for KPIs.
- [ ] B. Use Amazon Kinesis Data Analytics for one-time queries. Use Amazon QuickSight to create dashboards for KPIs.
- [ ] C. Create custom AWS Lambda functions to move the individual records from the databases to an Amazon Redshift cluster.
- [ ] D. Use an AWS Glue extract, transform, and load (ETL) job to convert the data into JSON format. Load the data into multiple Amazon OpenSearch Service (Amazon Elasticsearch Service) clusters.
- [ ] E. Use blueprints in AWS Lake Formation to identify the data that can be ingested into a data lake. Use AWS Glue to crawl the source, extract the data, and load the data into Amazon S3 in Apache Parquet format.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Use Amazon Athena for one-time queries. Use Amazon QuickSight to create dashboards for KPIs.
- [ ] E. Use blueprints in AWS Lake Formation to identify the data that can be ingested into a data lake. Use AWS Glue to crawl the source, extract the data, and load the data into Amazon S3 in Apache Parquet format.

Why these are the correct answers:

This solution combines services for building a data lake, processing data, and then analyzing and visualizing it, all with an emphasis on minimizing operational overhead.

E. Use blueprints in AWS Lake Formation to identify the data that can be ingested into a data lake. Use AWS Glue to crawl the source, extract the data, and load the data into Amazon S3 in Apache Parquet format.
- [ ] Consolidation and Staging in S3: AWS Lake Formation helps you set up a secure data lake in Amazon S3 quickly. Blueprints can be used to ingest data from various sources (like the "different databases").
- [ ] AWS Glue (an ETL service) can then be used to crawl these sources, extract the data, perform necessary transformations (though this option focuses on extraction and loading), and load it into S3.
- [ ] Apache Parquet Format: Storing data in Apache Parquet format in S3 is highly recommended for analytics.
- [ ] Parquet is a columnar storage format that is optimized for query performance with services like Amazon Athena and can lead to significant cost savings for queries.
- [ ] Least Operational Overhead for Ingestion/Processing: Both Lake Formation and Glue are fully managed services, reducing the operational burden of setting up data ingestion pipelines and preparing data for analytics.

A. Use Amazon Athena for one-time queries. Use Amazon QuickSight to create dashboards for KPIs.
- [ ] Amazon Athena for Ad-Hoc Queries: Once the data is staged in S3 (as per option E), Amazon Athena allows teams to run "one-time queries" directly on the data in S3 using standard SQL.
- [ ] Athena is serverless, so there are no clusters to manage, fitting the "LEAST operational overhead" requirement.
- [ ] Amazon QuickSight for BI and KPIs: Amazon QuickSight is a scalable, serverless business intelligence (BI) service.
- [ ] It can connect to data in S3 (often via Athena) to "import the data into a business intelligence tool to show key performance indicators (KPIs)" through interactive dashboards.
- [ ] This addresses the analysis and visualization requirements with low operational overhead.

Why are the other answers wrong?

- [ ] Option B is wrong because: Amazon Kinesis Data Analytics is designed for real-time processing and analysis of streaming data, not for running "one-time queries" on data already staged in S3 (which is what Athena is for). While the company does have live stream data, Kinesis Data Analytics is an analytics tool for data in motion, not a general query engine for data at rest in S3 for this use case.
- [ ] Option C is wrong because: Creating custom AWS Lambda functions to move data from various databases to an Amazon Redshift cluster would involve significant development and maintenance effort. While Redshift is a data warehouse, the requirement is to stage data in S3 first. For ingesting batch data from databases into S3, AWS Glue (as in option E) or AWS Database Migration Service (DMS) are generally more suitable and offer less operational overhead than custom Lambda functions.
- [ ] Option D is wrong because: While AWS Glue ETL is appropriate for processing, loading data into multiple Amazon OpenSearch Service (Elasticsearch Service) clusters might not be the most cost-effective or operationally simple solution for staging all consolidated data for "one-time queries" and general BI tool import. OpenSearch is optimized for search and log analytics. Storing data in S3 in Parquet format and querying with Athena (as suggested by options E and A) is often more flexible and cost-effective for ad-hoc SQL queries and broader BI consumption. Managing multiple OpenSearch clusters also adds operational overhead.

</details>

<details>
  <summary>Question 157</summary>

A company stores data in an Amazon Aurora PostgreSQL DB cluster. The company must store all the data for 5 years and must delete all the data after 5 years. The company also must indefinitely keep audit logs of actions that are performed within the database. Currently, the company has automated backups configured for Aurora.

Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

- [ ] A. Take a manual snapshot of the DB cluster.
- [ ] B. Create a lifecycle policy for the automated backups.
- [ ] C. Configure automated backup retention for 5 years.
- [ ] D. Configure an Amazon CloudWatch Logs export for the DB cluster.
- [ ] E. Use AWS Backup to take the backups and to keep the backups for 5 years.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Configure an Amazon CloudWatch Logs export for the DB cluster.
- [ ] E. Use AWS Backup to take the backups and to keep the backups for 5 years.

Why these are the correct answers:

This solution addresses both the data retention/deletion requirements and the audit log retention requirement separately using appropriate AWS services.

D. Configure an Amazon CloudWatch Logs export for the DB cluster.
- [ ] Indefinitely Keep Audit Logs: Amazon Aurora PostgreSQL can be configured to publish various logs, including audit logs (if enabled via database parameter group settings), to Amazon CloudWatch Logs. Once the audit logs are in CloudWatch Logs, they can be configured for long-term retention, either within CloudWatch Logs itself (though this can become expensive for indefinite retention of large volumes) or, more commonly, by setting up an export or subscription from CloudWatch Logs to Amazon S3 for durable, cost-effective, and indefinite archival. This meets the requirement to "indefinitely keep audit logs of actions that are performed within the database."

E. Use AWS Backup to take the backups and to keep the backups for 5 years.
- [ ] Data Retention for 5 Years and Deletion: AWS Backup is a fully managed backup service that simplifies and automates data protection across various AWS services, including Amazon Aurora. You can create backup plans in AWS Backup to define how frequently backups are taken and, crucially, their retention period. You can set a retention policy to keep backups for 5 years. AWS Backup also manages the lifecycle of these backups, including their automatic deletion after the 5-year retention period expires. This addresses the requirements to "store all the data for 5 years and must delete all the data after 5 years."
- [ ] Operational Efficiency: Using AWS Backup centralizes backup management and automates the backup and retention lifecycle, reducing operational overhead.

Why are the other answers wrong?

- [ ] Option A is wrong because: Taking a single manual snapshot of the DB cluster is a point-in-time backup. It does not provide an ongoing strategy for backing up all data, retaining it for 5 years, and then ensuring its deletion. Managing a series of manual snapshots for 5 years and their eventual deletion would be operationally burdensome.
- [ ] Option B is wrong because: Amazon RDS automated backups (which Aurora uses) have a maximum retention period of 35 days. You cannot create a lifecycle policy directly on these automated backups to extend their retention to 5 years. For long-term retention beyond 35 days, you need to use manual snapshots or a service like AWS Backup.
- [ ] Option C is wrong because: As stated above, the maximum retention period for standard Amazon RDS automated backups is 35 days. It is not possible to configure automated backup retention directly within RDS/Aurora for 5 years.

</details>



























<details>
  <summary>Question X</summary>

- [ ] A.  Turn  


</details>

<details>
  <summary>Answer</summary>

- [ ] A.  Turn


</details>



