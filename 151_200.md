<details>
  <summary>Question 151</summary>

A company wants to migrate its on-premises data center to AWS. According to the company's compliance requirements, the company can use only the ap-northeast-3 Region. Company administrators are not permitted to connect VPCs to the internet.

Which solutions will meet these requirements? (Choose two.)

- [ ] A. Use AWS Control Tower to implement data residency guardrails to deny internet access and deny access to all AWS Regions except ap-northeast-3.
- [ ] B. Use rules in AWS WAF to prevent internet access. Deny access to all AWS Regions except ap-northeast-3 in the AWS account settings.
- [ ] C. Use AWS Organizations to configure service control policies (SCPS) that prevent VPCs from gaining internet access. Deny access to all AWS Regions except ap-northeast-3.
- [ ] D. Create an outbound rule for the network ACL in each VPC to deny all traffic from 0.0.0.0/0 Create an IAM policy for each user to prevent the use of any AWS Region other than ap-northeast-3.
- [ ] E. Use AWS Config to activate managed rules to detect and alert for internet gateways and to detect and alert for new resources deployed outside of ap-northeast-3.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Use AWS Control Tower to implement data residency guardrails to deny internet access and deny access to all AWS Regions except ap-northeast-3.
- [ ] C. Use AWS Organizations to configure service control policies (SCPS) that prevent VPCs from gaining internet access. Deny access to all AWS Regions except ap-northeast-3.

Why these are the correct answers:

These options provide preventative controls at an organizational level to enforce the specified compliance requirements:

A. Use AWS Control Tower to implement data residency guardrails to deny internet access and deny access to all AWS Regions except ap-northeast-3.
- [ ] AWS Control Tower for Governance: AWS Control Tower provides an easy way to set up and govern a secure, multi-account AWS environment based on best practices. It uses guardrails (which are often implemented using Service Control Policies and AWS Config rules) to enforce policies.
- [ ] Data Residency Guardrails: Control Tower offers specific guardrails for data residency that can be used to deny access to AWS services and operations in unauthorized AWS Regions, thereby ensuring work occurs only in ap-northeast-3.
- [ ] Preventing Internet Access: Control Tower can also deploy guardrails that help prevent VPCs from connecting to the internet, for example, by restricting the creation or use of Internet Gateways.
- [ ] Centralized Enforcement: Control Tower provides a comprehensive, managed way to establish and enforce these types of compliance rules across an organization.

C. Use AWS Organizations to configure service control policies (SCPS) that prevent VPCs from gaining internet access. Deny access to all AWS Regions except ap-northeast-3.
- [ ] AWS Organizations and SCPs: Service Control Policies (SCPs) are a feature of AWS Organizations that offer central control over the maximum available permissions for all accounts in your organization. SCPs can restrict which AWS services, resources, and individual API actions users and roles in member accounts can access.
- [ ] Denying Access to Other Regions: An SCP can be configured to explicitly deny access to all AWS API actions in Regions other than ap-northeast-3. This effectively limits operations to the allowed region.
- [ ] Preventing Internet Access for VPCs: SCPs can also be used to deny specific EC2 actions required to connect a VPC to the internet, such as ec2:CreateInternetGateway, ec2:AttachInternetGateway, or actions that modify route tables to use an Internet Gateway. This ensures administrators cannot connect VPCs to the internet.
- [ ] Preventative Control: SCPs act as preventative guardrails that even administrators in member accounts cannot override (unless they have permissions in the management account to modify the SCP itself).

Why are the other answers wrong?

- [ ] Option B is wrong because: AWS WAF (Web Application Firewall) is used to protect web applications from common web exploits. It does not control general VPC internet connectivity or restrict access to AWS Regions. Denying access to regions in "AWS account settings" is less robust and centrally managed than using SCPs or Control Tower guardrails for organizational compliance.
- [ ] Option D is wrong because:
Network ACLs operate at the subnet level and are stateless. While they can block traffic, managing NACLs across all VPCs to deny all outbound internet traffic (the 0.0.0.0/0 in an outbound rule is not how sources are typically specified for outbound; it would be a destination) can be complex and might inadvertently block necessary internal or AWS service traffic. It's also not as robust as an SCP for preventing administrators from creating internet connectivity.
IAM policies apply to users, groups, and roles, restricting what they can do. While useful, SCPs provide an overarching organizational boundary that applies even to administrators within member accounts.
- [ ] Option E is wrong because: AWS Config is a service for assessing, auditing, and evaluating the configurations of AWS resources. It provides detective controls, meaning it can detect and alert when a non-compliant configuration occurs (like an Internet Gateway being created or resources deployed in a wrong region). However, it does not prevent these actions. The requirement is that administrators "are not permitted" to do these things, which implies a need for preventative controls.

</details>

<details>
  <summary>Question 152</summary>

A company wants to migrate an on-premises data center to AWS. The data center hosts an SFTP server that stores its data on an NFS-based file system. The server holds 200 GB of data that needs to be transferred. The server must be hosted on an Amazon EC2 instance that uses an Amazon Elastic File System (Amazon EFS) file system.

Which combination of steps should a solutions architect take to automate this task? (Choose two.)

- [ ] A. Launch the EC2 instance into the same Availability Zone as the EFS file system.
- [ ] B. Install an AWS DataSync agent in the on-premises data center.
- [ ] C. Create a secondary Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instance for the data.
- [ ] D. Manually use an operating system copy command to push the data to the EC2 instance.
- [ ] E. Use AWS DataSync to create a suitable location configuration for the on-premises SFTP server.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Install an AWS DataSync agent in the on-premises data center.
- [ ] E. Use AWS DataSync to create a suitable location configuration for the on-premises SFTP server.

Why these are the correct answers:

This solution focuses on automating the transfer of 200 GB of data from an on-premises NFS file system (backing an SFTP server) to an Amazon EFS file system in AWS.

B. Install an AWS DataSync agent in the on-premises data center.
- [ ] AWS DataSync Agent: To use AWS DataSync for transferring data from an on-premises location, you need to deploy a DataSync agent (a virtual machine) in your on-premises environment.
- [ ] This agent will access your on-premises NFS file system and manage the data transfer to AWS.

E. Use AWS DataSync to create a suitable location configuration for the on-premises SFTP server.
- [ ] DataSync Locations and Task: Once the agent is deployed and activated, you configure AWS DataSync by creating a source location that points to your on-premises NFS file system (which backs the SFTP server) and a destination location that points to your Amazon EFS file system in AWS.
- [ ] Then, you create a DataSync task to manage the automated transfer of data between these locations.
- [ ] DataSync handles the scheduling, monitoring, data validation, and optimization of the transfer.
- [ ] This combination provides an automated, efficient, and managed way to transfer the data.

Why are the other answers wrong?

- [ ] Option A is wrong because: While an EC2 instance needs to be in a VPC with mount targets for the EFS file system (which exist in specific AZs), EFS itself is a regional service designed to be accessible from any AZ within the region where it's created. This step is related to how the EC2 instance accesses EFS after migration, not directly a step in automating the data transfer from on-premises. The core of the automation is DataSync.
- [ ] Option C is wrong because: The requirement clearly states that the new SFTP server hosted on an EC2 instance must use an Amazon EFS file system, not an EBS volume, for its data. Creating an EBS volume contradicts this requirement.
- [ ] Option D is wrong because: Manually using operating system copy commands (like scp, rsync, or cp over an NFS mount) is not an "automated task" in the context of a managed migration service like DataSync. Manual copies lack the built-in scheduling, monitoring, error handling, data validation, and transfer optimization features that DataSync provides, and they would require more manual intervention, especially for 200 GB of data.

</details>

<details>
  <summary>Question 153</summary>

A company sells ringtones created from clips of popular songs. The files containing the ringtones are stored in Amazon S3 Standard and are at least 128 KB in size. The company has millions of files, but downloads are infrequent for ringtones older than 90 days. The company needs to save money on storage while keeping the most accessed files readily available for its users.

Which action should the company take to meet these requirements MOST cost-effectively?

- [ ] A. Configure S3 Standard-Infrequent Access (S3 Standard-IA) storage for the initial storage tier of the objects.
- [ ] B. Move the files to S3 Intelligent-Tiering and configure it to move objects to a less expensive storage tier after 90 days.
- [ ] C. Configure S3 inventory to manage objects and move them to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days.
- [ ] D. Implement an S3 Lifecycle policy that moves the objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Implement an S3 Lifecycle policy that moves the objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days.

Why this is the correct answer:

This solution directly addresses the defined access patterns and cost-saving requirements:

- [ ] Initial Storage in S3 Standard: New ringtones and those younger than 90 days are frequently accessed ("most accessed files readily available"). S3 Standard is suitable for this, offering high availability and immediate retrieval with no retrieval fees. The problem states files are currently in S3 Standard.
- [ ] Defined Access Pattern Shift: There's a clear shift in access patterns: "downloads are infrequent for ringtones older than 90 days."
- [ ] S3 Lifecycle Policy for Automated Transition: An S3 Lifecycle policy allows you to define rules to automatically transition objects to different storage classes based on their age. Configuring a policy to move objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days automates the cost optimization process.
- [ ] S3 Standard-IA for Infrequent Access with Ready Availability: S3 Standard-IA is designed for data that is accessed less frequently but requires rapid access when needed. It offers a lower storage cost than S3 Standard, making it cost-effective for the ringtones older than 90 days. While there's a per-GB retrieval fee, this is acceptable if downloads are truly infrequent. It keeps the files "readily available."
- [ ] MOST Cost-Effective for This Pattern: For a well-defined access pattern (frequent then infrequent), a direct lifecycle policy to an appropriate infrequent access tier like S3 Standard-IA is a very cost-effective and straightforward approach.

Why are the other answers wrong?

- [ ] Option A is wrong because: Configuring S3 Standard-IA as the initial storage tier for all objects would mean that frequently accessed new ringtones (within the first 90 days) would incur retrieval fees every time they are downloaded. S3 Standard is more cost-effective for the initial period of frequent access.
- [ ] Option B is wrong because: S3 Intelligent-Tiering is primarily designed for data with unknown, changing, or unpredictable access patterns. While it could eventually move data to an infrequent access tier after 90 days if so configured (or if its own monitoring determined it), the access pattern here is relatively predictable (frequent for 90 days, then infrequent). For such a defined pattern, a direct S3 Lifecycle policy (as in option D) is often simpler and can be slightly more cost-effective as it avoids the small per-object monitoring and automation fees associated with S3 Intelligent-Tiering.
- [ ] Option C is wrong because: Amazon S3 Inventory provides flat file lists (CSV, ORC, or Parquet) of your objects and their metadata, which is useful for analysis, auditing, and reporting. S3 Inventory itself does not "manage objects and move them" between storage classes. That functionality is provided by S3 Lifecycle policies.

</details>

<details>
  <summary>Question 154</summary>

A company needs to save the results from a medical trial to an Amazon S3 repository. The repository must allow a few scientists to add new files and must restrict all other users to read-only access. No users can have the ability to modify or delete any files in the repository. The company must keep every file in the repository for a minimum of 1 year after its creation date.

Which solution will meet these requirements?

- [ ] A. Use S3 Object Lock in governance mode with a legal hold of 1 year.
- [ ] B. Use S3 Object Lock in compliance mode with a retention period of 365 days.
- [ ] C. Use an IAM role to restrict all users from deleting or changing objects in the S3 bucket. Use an S3 bucket policy to only allow the IAM role.
- [ ] D. Configure the S3 bucket to invoke an AWS Lambda function every time an object is added. Configure the function to track the hash of the saved object so that modified objects can be marked accordingly.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Use S3 Object Lock in compliance mode with a retention period of 365 days.

Why this is the correct answer:

This solution effectively addresses the requirements for immutability and fixed-term retention:

- [ ] S3 Object Lock for Immutability: The requirement "No users can have the ability to modify or delete any files in the repository" points directly to Amazon S3 Object Lock. S3 Object Lock provides Write-Once-Read-Many (WORM) protection for S3 objects, preventing them from being deleted or overwritten for a fixed amount of time or indefinitely.
- [ ] Compliance Mode for Strict Enforcement: S3 Object Lock offers two retention modes: governance and compliance. Compliance mode is the stricter of the two. When an object version is locked in compliance mode, its retention mode cannot be changed, and its retention period cannot be shortened by any user, including the root user in the AWS account, for the duration of the specified retention period. This ensures the highest level of immutability and meets the "No users can have the ability to modify or delete" requirement.
- [ ] Retention Period of 365 Days: Setting a retention period of 365 days (1 year) for objects when they are uploaded (e.g., via a default bucket setting or by applying it during upload) ensures that "The company must keep every file in the repository for a minimum of 1 year after its creation date."
- [ ] Access Control (Complementary): While S3 Object Lock handles the immutability and retention, separate IAM policies and/or S3 bucket policies would be configured to manage who can add new files (the "few scientists") and who has read-only access ("all other users"). This option focuses on the critical immutability and retention aspects.

Why are the other answers wrong?

- [ ] Option A is wrong because:
S3 Object Lock in governance mode allows users with specific IAM permissions (s3:BypassGovernanceRetention) to override the retention settings or delete the objects. This does not meet the strict requirement that "No users can have the ability to modify or delete any files."
A legal hold provides immutability but has no expiration date; it remains in effect until explicitly removed. While it prevents deletion, a fixed retention period (as in compliance mode) is more directly aligned with the "minimum of 1 year after its creation date" requirement if the goal is a time-bound, unchangeable retention.
- [ ] Option C is wrong because: While IAM roles and S3 bucket policies are essential for controlling access (who can read, who can write), they can typically be modified by users with sufficient administrative privileges (like the root user or account administrators). They do not provide the same level of guaranteed, unchangeable WORM protection that S3 Object Lock in compliance mode offers against all users, including administrators, for a defined period.
- [ ] Option D is wrong because: This describes a detective control, not a preventative one. Tracking hashes with a Lambda function would help identify if an object has been modified after the fact, but it would not prevent the modification or deletion from occurring in the first place. The requirement is that files "cannot be modified or deleted."

</details>

<details>
  <summary>Question 155</summary>

A large media company hosts a web application on AWS. The company wants to start caching confidential media files so that users around the world will have reliable access to the files. The content is stored in Amazon S3 buckets. The company must deliver the content quickly, regardless of where the requests originate geographically.

Which solution will meet these requirements?

- [ ] A. Use AWS DataSync to connect the S3 buckets to the web application.
- [ ] B. Deploy AWS Global Accelerator to connect the S3 buckets to the web application.
- [ ] C. Deploy Amazon CloudFront to connect the S3 buckets to CloudFront edge servers.
- [ ] D. Use Amazon Simple Queue Service (Amazon SQS) to connect the S3 buckets to the web application.

</details>

<details>
  <summary>Answer</summary>

- [ ] C. Deploy Amazon CloudFront to connect the S3 buckets to CloudFront edge servers.

Why this is the correct answer:

- [ ] Amazon CloudFront for Caching and Global Fast Delivery: Amazon CloudFront is a global content delivery network (CDN) service. It is designed to "deliver the content quickly, regardless of where the requests originate geographically" by caching copies of your content (in this case, confidential media files from S3 buckets) at edge locations around the world. When users request the files, they are served from the nearest edge location, which significantly reduces latency.
- [ ] Reliable Access and S3 Origin: CloudFront improves the reliability of access to content. It uses Amazon S3 buckets as an origin to fetch the content when it's not already cached at an edge location.
- [ ] Caching Confidential Media Files: CloudFront supports methods for securely serving private content from S3, such as using Origin Access Identity (OAI) or Origin Access Control (OAC) to restrict direct S3 access and ensure files are only accessible through CloudFront. Signed URLs or signed cookies can also be used with CloudFront to control access to confidential files on a per-user or per-session basis.
- [ ] This solution directly addresses all requirements: caching, reliable access, global fast delivery, and handling confidential content from S3.

Why are the other answers wrong?

- [ ] Option A is wrong because: AWS DataSync is a service for online data transfer, designed to move large amounts of data between on-premises storage and AWS storage, or between different AWS storage services. It is not a content delivery or caching service for providing low-latency access to users around the world.
- [ ] Option B is wrong because: AWS Global Accelerator is a service that improves the availability and performance of your applications with global users by providing static IP addresses and routing traffic over the AWS global network to optimal regional application endpoints (like Load Balancers or EC2 instances). While it helps with application endpoint performance and availability, it is not primarily a content caching service like CloudFront. CloudFront is specifically designed for caching and delivering static/dynamic content from the edge.
- [ ] Option D is wrong because: Amazon Simple Queue Service (Amazon SQS) is a message queuing service used for decoupling and scaling distributed systems by sending, storing, and receiving messages between software components. It has no role in caching or delivering media files to users.


</details>

<details>
  <summary>Question 156</summary>

- [ ] A.  TurnÂ  
A company produces batch data that comes from different databases. The company also produces live stream data from network sensors and application APIs. The company needs to consolidate all the data into one place for business analytics. The company needs to process the incoming data and then stage the data in different Amazon S3 buckets. Teams will later run one-time queries and import the data into a business intelligence tool to show key performance indicators (KPIs).

Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)

- [ ] A. Use Amazon Athena for one-time queries. Use Amazon QuickSight to create dashboards for KPIs.
- [ ] B. Use Amazon Kinesis Data Analytics for one-time queries. Use Amazon QuickSight to create dashboards for KPIs.
- [ ] C. Create custom AWS Lambda functions to move the individual records from the databases to an Amazon Redshift cluster.
- [ ] D. Use an AWS Glue extract, transform, and load (ETL) job to convert the data into JSON format. Load the data into multiple Amazon OpenSearch Service (Amazon Elasticsearch Service) clusters.
- [ ] E. Use blueprints in AWS Lake Formation to identify the data that can be ingested into a data lake. Use AWS Glue to crawl the source, extract the data, and load the data into Amazon S3 in Apache Parquet format.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Use Amazon Athena for one-time queries. Use Amazon QuickSight to create dashboards for KPIs.
- [ ] E. Use blueprints in AWS Lake Formation to identify the data that can be ingested into a data lake. Use AWS Glue to crawl the source, extract the data, and load the data into Amazon S3 in Apache Parquet format.

Why these are the correct answers:

This solution combines services for building a data lake, processing data, and then analyzing and visualizing it, all with an emphasis on minimizing operational overhead.

E. Use blueprints in AWS Lake Formation to identify the data that can be ingested into a data lake. Use AWS Glue to crawl the source, extract the data, and load the data into Amazon S3 in Apache Parquet format.
- [ ] Consolidation and Staging in S3: AWS Lake Formation helps you set up a secure data lake in Amazon S3 quickly. Blueprints can be used to ingest data from various sources (like the "different databases").
- [ ] AWS Glue (an ETL service) can then be used to crawl these sources, extract the data, perform necessary transformations (though this option focuses on extraction and loading), and load it into S3.
- [ ] Apache Parquet Format: Storing data in Apache Parquet format in S3 is highly recommended for analytics.
- [ ] Parquet is a columnar storage format that is optimized for query performance with services like Amazon Athena and can lead to significant cost savings for queries.
- [ ] Least Operational Overhead for Ingestion/Processing: Both Lake Formation and Glue are fully managed services, reducing the operational burden of setting up data ingestion pipelines and preparing data for analytics.

A. Use Amazon Athena for one-time queries. Use Amazon QuickSight to create dashboards for KPIs.
- [ ] Amazon Athena for Ad-Hoc Queries: Once the data is staged in S3 (as per option E), Amazon Athena allows teams to run "one-time queries" directly on the data in S3 using standard SQL.
- [ ] Athena is serverless, so there are no clusters to manage, fitting the "LEAST operational overhead" requirement.
- [ ] Amazon QuickSight for BI and KPIs: Amazon QuickSight is a scalable, serverless business intelligence (BI) service.
- [ ] It can connect to data in S3 (often via Athena) to "import the data into a business intelligence tool to show key performance indicators (KPIs)" through interactive dashboards.
- [ ] This addresses the analysis and visualization requirements with low operational overhead.

Why are the other answers wrong?

- [ ] Option B is wrong because: Amazon Kinesis Data Analytics is designed for real-time processing and analysis of streaming data, not for running "one-time queries" on data already staged in S3 (which is what Athena is for). While the company does have live stream data, Kinesis Data Analytics is an analytics tool for data in motion, not a general query engine for data at rest in S3 for this use case.
- [ ] Option C is wrong because: Creating custom AWS Lambda functions to move data from various databases to an Amazon Redshift cluster would involve significant development and maintenance effort. While Redshift is a data warehouse, the requirement is to stage data in S3 first. For ingesting batch data from databases into S3, AWS Glue (as in option E) or AWS Database Migration Service (DMS) are generally more suitable and offer less operational overhead than custom Lambda functions.
- [ ] Option D is wrong because: While AWS Glue ETL is appropriate for processing, loading data into multiple Amazon OpenSearch Service (Elasticsearch Service) clusters might not be the most cost-effective or operationally simple solution for staging all consolidated data for "one-time queries" and general BI tool import. OpenSearch is optimized for search and log analytics. Storing data in S3 in Parquet format and querying with Athena (as suggested by options E and A) is often more flexible and cost-effective for ad-hoc SQL queries and broader BI consumption. Managing multiple OpenSearch clusters also adds operational overhead.

</details>

<details>
  <summary>Question 157</summary>

A company stores data in an Amazon Aurora PostgreSQL DB cluster. The company must store all the data for 5 years and must delete all the data after 5 years. The company also must indefinitely keep audit logs of actions that are performed within the database. Currently, the company has automated backups configured for Aurora.

Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

- [ ] A. Take a manual snapshot of the DB cluster.
- [ ] B. Create a lifecycle policy for the automated backups.
- [ ] C. Configure automated backup retention for 5 years.
- [ ] D. Configure an Amazon CloudWatch Logs export for the DB cluster.
- [ ] E. Use AWS Backup to take the backups and to keep the backups for 5 years.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Configure an Amazon CloudWatch Logs export for the DB cluster.
- [ ] E. Use AWS Backup to take the backups and to keep the backups for 5 years.

Why these are the correct answers:

This solution addresses both the data retention/deletion requirements and the audit log retention requirement separately using appropriate AWS services.

D. Configure an Amazon CloudWatch Logs export for the DB cluster.
- [ ] Indefinitely Keep Audit Logs: Amazon Aurora PostgreSQL can be configured to publish various logs, including audit logs (if enabled via database parameter group settings), to Amazon CloudWatch Logs. Once the audit logs are in CloudWatch Logs, they can be configured for long-term retention, either within CloudWatch Logs itself (though this can become expensive for indefinite retention of large volumes) or, more commonly, by setting up an export or subscription from CloudWatch Logs to Amazon S3 for durable, cost-effective, and indefinite archival. This meets the requirement to "indefinitely keep audit logs of actions that are performed within the database."

E. Use AWS Backup to take the backups and to keep the backups for 5 years.
- [ ] Data Retention for 5 Years and Deletion: AWS Backup is a fully managed backup service that simplifies and automates data protection across various AWS services, including Amazon Aurora. You can create backup plans in AWS Backup to define how frequently backups are taken and, crucially, their retention period. You can set a retention policy to keep backups for 5 years. AWS Backup also manages the lifecycle of these backups, including their automatic deletion after the 5-year retention period expires. This addresses the requirements to "store all the data for 5 years and must delete all the data after 5 years."
- [ ] Operational Efficiency: Using AWS Backup centralizes backup management and automates the backup and retention lifecycle, reducing operational overhead.

Why are the other answers wrong?

- [ ] Option A is wrong because: Taking a single manual snapshot of the DB cluster is a point-in-time backup. It does not provide an ongoing strategy for backing up all data, retaining it for 5 years, and then ensuring its deletion. Managing a series of manual snapshots for 5 years and their eventual deletion would be operationally burdensome.
- [ ] Option B is wrong because: Amazon RDS automated backups (which Aurora uses) have a maximum retention period of 35 days. You cannot create a lifecycle policy directly on these automated backups to extend their retention to 5 years. For long-term retention beyond 35 days, you need to use manual snapshots or a service like AWS Backup.
- [ ] Option C is wrong because: As stated above, the maximum retention period for standard Amazon RDS automated backups is 35 days. It is not possible to configure automated backup retention directly within RDS/Aurora for 5 years.

</details>

<details>
  <summary>Question 158</summary>

A solutions architect is optimizing a website for an upcoming musical event. Videos of the performances will be streamed in real time and then will be available on demand. The event is expected to attract a global online audience.

Which service will improve the performance of both the real-time and on-demand streaming?

- [ ] A. Amazon CloudFront
- [ ] B. AWS Global Accelerator
- [ ] C. Amazon Route 53
- [ ] D. Amazon S3 Transfer Acceleration

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Amazon CloudFront

Why this is the correct answer:

- [ ] Amazon CloudFront for On-Demand and Real-Time Streaming: Amazon CloudFront is a global content delivery network (CDN) service that is designed to deliver data, videos, applications, and APIs to customers globally with low latency and high transfer speeds.
- [ ] On-Demand Streaming: For on-demand video (e.g., recordings of performances stored in Amazon S3), CloudFront caches the content at its numerous edge locations worldwide. When users request the video, it's served from the edge location closest to them, significantly reducing latency and improving playback performance.
- [ ] Real-Time Streaming: CloudFront can also be used to distribute live video streams. It supports common streaming protocols and can work with live origins (like AWS Elemental MediaStore or other media servers) to deliver real-time streams to a global audience with lower latency by leveraging its edge network.
- [ ] Global Audience and Performance: Given the "global online audience," CloudFront's geographically distributed edge locations are key to providing a consistent, low-latency viewing experience for both types of video content.

Why are the other answers wrong?

- [ ] Option B is wrong because: AWS Global Accelerator improves the availability and performance of your applications by providing static IP addresses and routing traffic over the AWS global network to optimal application endpoints (like Load Balancers or EC2 instances). While it can accelerate traffic to your application origin and is beneficial for certain types of real-time applications (like gaming or some live video contribution), Amazon CloudFront is more specifically designed for caching and delivering video content (both on-demand and live streams) at the edge to a global audience. For video streaming, CloudFront's caching and delivery optimization are typically more impactful.
- [ ] Option C is wrong because: Amazon Route 53 is a scalable Domain Name System (DNS) web service. It translates domain names into IP addresses and can route users to different endpoints based on various routing policies (e.g., latency-based, geoproximity). While Route 53 is essential for directing users to the correct CloudFront distribution or Global Accelerator endpoint, it does not itself cache content or improve the actual streaming performance by bringing content closer to users in the way a CDN does.
- [ ] Option D is wrong because: Amazon S3 Transfer Acceleration is a feature that speeds up file uploads to Amazon S3 over long distances by using CloudFront's edge locations as an entry point to the AWS network. It does not improve the performance of downloading or streaming content from S3 to users globally. For delivering content from S3 to users, a full CloudFront distribution is the appropriate service.

</details>

<details>
  <summary>Question 159</summary>

A company is running a publicly accessible serverless application that uses Amazon API Gateway and AWS Lambda. The application's traffic recently spiked due to fraudulent requests from botnets.

Which steps should a solutions architect take to block requests from unauthorized users? (Choose two.)

- [ ] A. Create a usage plan with an API key that is shared with genuine users only.
- [ ] B. Integrate logic within the Lambda function to ignore the requests from fraudulent IP addresses.
- [ ] C. Implement an AWS WAF rule to target malicious requests and trigger actions to filter them out.
- [ ] D. Convert the existing public API to a private API. Update the DNS records to redirect users to the new API endpoint.
- [ ] E. Create an IAM role for each user attempting to access the API. A user will assume the role when making the API call.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Create a usage plan with an API key that is shared with genuine users only.
- [ ] C. Implement an AWS WAF rule to target malicious requests and trigger actions to filter them out.

Why these are the correct answers:

These two solutions provide effective mechanisms at the API Gateway and edge level to control access and filter malicious traffic:

A. Create a usage plan with an API key that is shared with genuine users only.
- [ ] API Keys for Authentication/Authorization: Amazon API Gateway allows you to create API keys and associate them with usage plans. You can then require clients to send a valid API key in their requests.
- [ ] By distributing API keys only to genuine users or client applications, you can ensure that requests without a valid key (or with a revoked key) are rejected by API Gateway.
- [ ] Throttling and Quotas: Usage plans also allow you to set throttling limits and quotas on a per-API key basis, which can help mitigate abuse from a compromised key or a single misbehaving client.

C. Implement an AWS WAF rule to target malicious requests and trigger actions to filter them out.
- [ ] AWS WAF for Web Exploit and Bot Protection: AWS WAF is a web application firewall that integrates with Amazon API Gateway (and other services like CloudFront and Application Load Balancer).
- [ ] It allows you to create rules to filter web traffic based on various conditions, such as IP addresses, HTTP headers, HTTP body, URI strings, SQL injection, and cross-site scripting.
- [ ] Blocking Botnets: You can use AWS WAF with AWS Managed Rules (like the Bot Control rule set or IP reputation lists) or create custom rules to identify and block traffic originating from known botnets or exhibiting bot-like behavior.
- [ ] This filtering happens before requests hit your API Gateway methods or Lambda functions.
- [ ] Combining these two provides layered security: WAF for broad filtering of malicious traffic and botnets, and API keys/usage plans for more granular access control for legitimate clients.

Why are the other answers wrong?

- [ ] Option B is wrong because: Implementing IP address filtering logic directly within the AWS Lambda function means that the Lambda function is still invoked for every request, including fraudulent ones. This consumes Lambda resources, incurs costs for each invocation, and makes the Lambda function more complex. It's more efficient and cost-effective to block malicious traffic at an earlier stage, like with AWS WAF or API Gateway configurations.
- [ ] Option D is wrong because: Converting a publicly accessible API to a private API makes it accessible only from within your VPC (using VPC endpoints) or via specific private network connections. This would block all public internet traffic, including legitimate users, unless a complex and potentially costly frontend solution is built to bridge public access to the private API. This is not a suitable solution for protecting a "publicly accessible serverless application" from botnets while maintaining public access.
- [ ] Option E is wrong because: Using IAM roles for each individual user to access a public API is generally not a scalable or practical authentication mechanism for a large number of external users. IAM roles are more typically used for granting permissions to AWS services or applications, or for federated enterprise users. For authenticating end-users of a public API, solutions like Amazon Cognito user pools (which can be integrated with API Gateway) or API keys are more appropriate. Managing individual IAM roles for potentially thousands or millions of public users would be an operational nightmare.

</details>

<details>
  <summary>Question 160</summary>

An ecommerce company hosts its analytics application in the AWS Cloud. The application generates about 300 MB of data each month. The data is stored in JSON format. The company is evaluating a disaster recovery solution to back up the data. The data must be accessible in milliseconds if it is needed, and the data must be kept for 30 days.

Which solution meets these requirements MOST cost-effectively?

- [ ] A. Amazon OpenSearch Service (Amazon Elasticsearch Service)
- [ ] B. Amazon S3 Glacier
- [ ] C. Amazon S3 Standard
- [ ] D. Amazon RDS for PostgreSQL

</details>

<details>
  <summary>Answer</summary>

- [ ] C. Amazon S3 Standard

Why this is the correct answer:

- [ ] Amazon S3 Standard for Accessibility and Cost-Effectiveness:
- [ ] Accessible in Milliseconds: Amazon S3 Standard provides low-latency access to data, typically in milliseconds, which meets the requirement for quick accessibility.
- [ ] Suitable for Data Volume and Retention: For a relatively small amount of data (300 MB per month) that needs to be kept for only 30 days and requires fast access, S3 Standard is a very cost-effective storage option. It has no retrieval fees, which is beneficial if the data might be needed.
- [ ] Durability for Backups: S3 Standard is designed for high durability (99.999999999%) and stores data across multiple Availability Zones, making it a reliable choice for storing backups.
- [ ] JSON Format: S3 is well-suited for storing files in any format, including JSON.
- [ ] This solution directly meets the accessibility, retention, and cost-effectiveness requirements for this specific scenario.

Why are the other answers wrong?

- [ ] Option A is wrong because: Amazon OpenSearch Service (formerly Amazon Elasticsearch Service) is primarily a search and analytics engine. While it can store JSON data, using it as a primary backup solution for 300 MB of data per month where the main requirements are millisecond access for 30 days and cost-effectiveness is likely overkill. OpenSearch Service involves running a cluster, which incurs instance costs and can be more expensive than S3 Standard for simple backup storage.
- [ ] Option B is wrong because: Amazon S3 Glacier (referring to S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive) is designed for low-cost, long-term data archiving where retrieval times of minutes to hours (or even longer for Deep Archive) are acceptable. It does not meet the requirement that "data must be accessible in milliseconds if it is needed." (Note: S3 Glacier Instant Retrieval does offer millisecond access from an archive tier, but for data kept only 30 days with potential need for immediate access, S3 Standard is often simpler and its storage cost for this short duration is competitive without involving specific archive tier considerations unless access is truly rare).
- [ ] Option D is wrong because: Amazon RDS for PostgreSQL is a relational database service. Storing 300 MB of JSON backup data in a relational database is generally not the most straightforward or cost-effective approach compared to object storage like S3. While PostgreSQL supports JSON data types, using RDS as a backup target for files in this manner adds unnecessary complexity and cost associated with running a database instance.

</details>

<details>
  <summary>Question 161</summary>

A company has a small Python application that processes JSON documents and outputs the results to an on-premises SQL database. The application runs thousands of times each day. The company wants to move the application to the AWS Cloud. The company needs a highly available solution that maximizes scalability and minimizes operational overhead.

Which solution will meet these requirements?

- [ ] A. Place the JSON documents in an Amazon S3 bucket. Run the Python code on multiple Amazon EC2 instances to process the documents. Store the results in an Amazon Aurora DB cluster.
- [ ] B. Place the JSON documents in an Amazon S3 bucket. Create an AWS Lambda function that runs the Python code to process the documents as they arrive in the S3 bucket. Store the results in an Amazon Aurora DB cluster.
- [ ] C. Place the JSON documents in an Amazon Elastic Block Store (Amazon EBS) volume. Use the EBS Multi-Attach feature to attach the volume to multiple Amazon EC2 instances. Run the Python code on the EC2 instances to process the documents. Store the results on an Amazon RDS DB instance.
- [ ] D. Place the JSON documents in an Amazon Simple Queue Service (Amazon SQS) queue as messages. Deploy the Python code as a container on an Amazon Elastic Container Service (Amazon ECS) cluster that is configured with the Amazon EC2 launch type. Use the container to process the SQS messages. Store the results on an Amazon RDS DB instance.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Place the JSON documents in an Amazon S3 bucket. Create an AWS Lambda function that runs the Python code to process the documents as they arrive in the S3 bucket. Store the results in an Amazon Aurora DB cluster.

Why this is the correct answer:

This solution leverages serverless and managed services to meet all requirements efficiently:

- [ ] Amazon S3 for Storing JSON Documents: Amazon S3 is an excellent choice for storing the input JSON documents. It's highly durable, scalable, and can trigger events.
- [ ] AWS Lambda for Processing: An AWS Lambda function can be written in Python to process the JSON documents. Lambda is serverless, meaning you don't manage any servers. It automatically scales based on the number of incoming events (e.g., new documents arriving in S3), thus maximizing scalability. Since it runs "thousands of times each day" (implying event-driven processing), Lambda is a perfect fit. This also "minimizes operational overhead."
- [ ] S3 Event Trigger: S3 can be configured to automatically trigger the Lambda function whenever a new JSON document is uploaded to the bucket. This creates an event-driven processing pipeline.
- [ ] Amazon Aurora for SQL Database: Amazon Aurora is a fully managed, relational database service compatible with MySQL and PostgreSQL. It provides high availability (e.g., with Multi-AZ configurations or Aurora Replicas), scalability, and significantly reduces operational overhead compared to managing an on-premises SQL database or a database on EC2. Migrating the output to Aurora meets the need for a SQL database in the cloud.
- [ ] This architecture (S3 for input, Lambda for processing, Aurora for output) is highly available, scales seamlessly, and has minimal operational overhead.

Why are the other answers wrong?

- [ ] Option A is wrong because: Running the Python code on multiple Amazon EC2 instances to process the documents involves managing those instances (patching, scaling, operating systems). This incurs more operational overhead than using serverless AWS Lambda. While scalable with Auto Scaling, Lambda offers better operational efficiency for this event-driven task.
- [ ] Option C is wrong because:
Storing JSON documents that need to be processed by multiple instances on a single EBS volume (even with Multi-Attach) is generally less scalable and more complex than using Amazon S3 as the source. EBS Multi-Attach has limitations (e.g., only for io1/io2 volumes, within the same AZ, specific instance types) and requires a file system that supports concurrent access.
It still relies on EC2 instances, leading to higher operational overhead than Lambda.
- [ ] Option D is wrong because:
Placing entire JSON documents directly into SQS messages might be problematic if the documents are large, as SQS messages have a size limit (typically 256 KB). It's more common to store the documents in S3 and send a notification/pointer to SQS.
Using Amazon ECS with the EC2 launch type for processing still involves managing the underlying EC2 instances in the cluster, which has more operational overhead than a fully serverless Lambda approach for this type of document processing task.

</details>

<details>
  <summary>Question 162</summary>

A company wants to use high performance computing (HPC) infrastructure on AWS for financial risk modeling. The company's HPC workloads run on Linux. Each HPC workflow runs on hundreds of Amazon EC2 Spot Instances, is short-lived, and generates thousands of output files that are ultimately stored in persistent storage for analytics and long-term future use. The company seeks a cloud storage solution that permits the copying of on-premises data to long-term persistent storage to make data available for processing by all EC2 instances. The solution should also be a high performance file system that is integrated with persistent storage to read and write datasets and output files.

Which combination of AWS services meets these requirements?

- [ ] A. Amazon FSx for Lustre integrated with Amazon S3
- [ ] B. Amazon FSx for Windows File Server integrated with Amazon S3
- [ ] C. Amazon S3 Glacier integrated with Amazon Elastic Block Store (Amazon EBS)
- [ ] D. Amazon S3 bucket with a VPC endpoint integrated with an Amazon Elastic Block Store (Amazon EBS) General Purpose SSD (gp2) volume

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Amazon FSx for Lustre integrated with Amazon S3

Why this is the correct answer:

This question outlines requirements for a high-performance, scalable file system for HPC workloads, with integration for persistent storage.

- [ ] Amazon FSx for Lustre for High-Performance File System: Amazon FSx for Lustre is specifically designed to provide high-performance, scalable file storage for compute-intensive workloads like HPC, financial risk modeling, machine learning, and media processing. Lustre is a popular open-source parallel file system known for its speed and scalability, making it ideal for workflows running on hundreds of EC2 instances that need fast access to shared datasets and a place to write output files quickly.
- [ ] Integration with Amazon S3 for Persistent Storage: FSx for Lustre seamlessly integrates with Amazon S3. You can link an FSx for Lustre file system to an S3 bucket. This allows:
- [ ] Data in S3 (e.g., on-premises data copied to S3 for long-term storage) to be transparently presented as files in the Lustre file system for processing by the EC2 Spot Instances.
- [ ] Output files generated by the HPC jobs on FSx for Lustre to be easily written back to the linked S3 bucket for "persistent storage for analytics and long-term future use."
- [ ] Shared Access for EC2 Instances: The FSx for Lustre file system can be mounted by hundreds of Linux-based EC2 Spot Instances, providing concurrent access to the data.
- [ ] This combination provides both the high-performance scratch/processing file system (FSx for Lustre) and the durable, long-term persistent storage (S3), with seamless integration between them.

Why are the other answers wrong?

- [ ] Option B is wrong because: Amazon FSx for Windows File Server provides file storage using the SMB protocol and is designed for Windows-based applications. The company's HPC workloads run on Linux, and Lustre (as provided by FSx for Lustre) is a more common and often higher-performing file system for Linux-based HPC clusters.
- [ ] Option C is wrong because: Amazon S3 Glacier is a low-cost archival storage service with retrieval times of minutes to hours. It is not suitable as a high-performance file system for active HPC processing. Amazon EBS provides block storage for individual EC2 instances and is not inherently a shared, distributed file system suitable for hundreds of HPC nodes without additional software and management.
- [ ] Option D is wrong because: While S3 can be used for persistent storage and EBS for instance storage, this option doesn't describe a "high performance file system that is integrated with persistent storage" in the way FSx for Lustre provides a POSIX-compliant, high-throughput, low-latency file system layer on top of S3 for HPC workloads. Accessing S3 directly for all file I/O from hundreds of compute nodes might not provide the same performance as a dedicated Lustre file system.

</details>

<details>
  <summary>Question 163</summary>
 
A company is building a containerized application on premises and decides to move the application to AWS. The application will have thousands of users soon after it is deployed. The company is unsure how to manage the deployment of containers at scale. The company needs to deploy the containerized application in a highly available architecture that minimizes operational overhead.

Which solution will meet these requirements?

- [ ] A. Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Container Service (Amazon ECS) cluster with the AWS Fargate launch type to run the containers. Use target tracking to scale automatically based on demand.
- [ ] B. Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Container Service (Amazon ECS) cluster with the Amazon EC2 launch type to run the containers. Use target tracking to scale automatically based on demand.
- [ ] C. Store container images in a repository that runs on an Amazon EC2 instance. Run the containers on EC2 instances that are spread across multiple Availability Zones. Monitor the average CPU utilization in Amazon CloudWatch. Launch new EC2 instances as needed.
- [ ] D. Create an Amazon EC2 Amazon Machine Image (AMI) that contains the container image. Launch EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon CloudWatch alarm to scale out EC2 instances when the average CPU utilization threshold is breached.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Container Service (Amazon ECS) cluster with the AWS Fargate launch type to run the containers. Use target tracking to scale automatically based on demand.

Why this is the correct answer:

This solution offers a serverless approach to running containers, which directly addresses the requirements for scalability, high availability, and minimized operational overhead.

- [ ] Amazon Elastic Container Registry (Amazon ECR) for Image Storage: ECR is a fully managed Docker container registry that makes it easy to store, manage, share, and deploy your container images and artifacts. This is the standard and recommended place to store container images on AWS.
- [ ] Amazon Elastic Container Service (Amazon ECS) with AWS Fargate Launch Type:
- [ ] ECS is a fully managed container orchestration service that helps you deploy, manage, and scale containerized applications.
- [ ] AWS Fargate is a serverless compute engine for containers that works with ECS. When you use Fargate, you don't need to provision, configure, or manage the underlying EC2 instances (servers). AWS handles all the server management, patching, and scaling of the infrastructure. This directly meets the requirement to "minimize operational overhead" and simplifies managing "deployment of containers at scale."
- [ ] High Availability: ECS services running on Fargate can be configured to distribute tasks (your containers) across multiple Availability Zones, ensuring high availability.
- [ ] Target Tracking Auto Scaling: ECS services can use target tracking auto scaling policies to automatically adjust the number of running tasks based on demand (e.g., CPU utilization, memory utilization, or custom metrics from Amazon CloudWatch). This ensures the application can scale to handle "thousands of users."

Why are the other answers wrong?

- [ ] Option B is wrong because: Using Amazon ECS with the Amazon EC2 launch type means that while ECS manages the container orchestration, the company is still responsible for provisioning, managing, patching, and scaling the underlying EC2 instances that form the ECS cluster. This incurs more operational overhead compared to the serverless Fargate launch type.
- [ ] Option C is wrong because: Storing container images in a self-managed repository on an EC2 instance and manually running containers on EC2 instances (even across multiple AZs with some monitoring) lacks proper container orchestration. This approach has significant operational overhead for managing the repository, the EC2 instances, container lifecycles, and scaling, and it's not an effective way to manage containers "at scale."
- [ ] Option D is wrong because: While creating an AMI with the container image and using an Auto Scaling group can scale EC2 instances, it's not the standard or most efficient way to manage containerized applications. This approach doesn't leverage a container orchestrator like ECS, making tasks like rolling updates, service discovery, and managing multiple containerized services more complex. It also means you are managing the container runtime within the AMI on each EC2 instance.

</details>

<details>
  <summary>Question 164</summary>

- [ ] A.  TurnÂ  


</details>

<details>
  <summary>Answer</summary>

- [ ] A.  Turn


</details>






































<details>
  <summary>Question X</summary>

- [ ] A.  TurnÂ  


</details>

<details>
  <summary>Answer</summary>

- [ ] A.  Turn


</details>



