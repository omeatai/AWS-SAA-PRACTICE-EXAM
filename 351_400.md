<details>
  <summary>Question 351</summary>

A company is moving its data management application to AWS. The company wants to transition to an event-driven architecture. The architecture needs to be more distributed and to use serverless concepts while performing the different aspects of the workflow. The company also wants to minimize operational overhead.

Which solution will meet these requirements?

-   [ ] A. Build out the workflow in AWS Glue. Use AWS Glue to invoke AWS Lambda functions to process the workflow steps.
-   [ ] B. Build out the workflow in AWS Step Functions. Deploy the application on Amazon EC2 instances. Use Step Functions to invoke the workflow steps on the EC2 instances.
-   [ ] C. Build out the workflow in Amazon EventBridge. Use EventBridge to invoke AWS Lambda functions on a schedule to process the workflow steps.
-   [ ] D. Build out the workflow in AWS Step Functions. Use Step Functions to create a state machine. Use the state machine to invoke AWS Lambda functions to process the workflow steps.
</details>
<details>
  <summary>Answer</summary>

-   [ ] D. Build out the workflow in AWS Step Functions. Use Step Functions to create a state machine. Use the state machine to invoke AWS Lambda functions to process the workflow steps.

Why these are the correct answers:

D. Build out the workflow in AWS Step Functions. Use Step Functions to create a state machine. Use the state machine to invoke AWS Lambda functions to process the workflow steps.

-   [ ]   AWS Step Functions allows you to coordinate multiple AWS services into serverless workflows.
-   [ ]   It enables building distributed, event-driven architectures by orchestrating AWS Lambda functions.
-   [ ]   Using a state machine in Step Functions helps manage the workflow logic.
-   [ ]   This solution minimizes operational overhead by using serverless services.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. AWS Glue is primarily for ETL (extract, transform, load) operations, not for orchestrating general application workflows.
-   [ ]   B. Deploying the application on Amazon EC2 instances increases operational overhead and does not align with serverless concepts.
-   [ ]   C. Amazon EventBridge is an event bus service, not a workflow orchestration service like Step Functions; using it on a schedule does not provide the necessary coordination.

Therefore, AWS Step Functions is the most suitable service for building the described event-driven architecture.
</details>
<details>
  <summary>Question 352</summary>

A company is designing the network for an online multi-player game. The game uses the UDP networking protocol and will be deployed in eight AWS Regions. The network architecture needs to minimize latency and packet loss to give end users a high-quality gaming experience.

Which solution will meet these requirements?

-   [ ] A. Setup a transit gateway in each Region. Create inter-Region peering attachments between each transit gateway.
-   [ ] B. Set up AWS Global Accelerator with UDP listeners and endpoint groups in each Region.
-   [ ] C. Set up Amazon CloudFront with UDP turned on. Configure an origin in each Region.
-   [ ] D. Set up a VPC peering mesh between each Region. Turn on UDP for each VPC.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Set up AWS Global Accelerator with UDP listeners and endpoint groups in each Region.

Why these are the correct answers:

B. Set up AWS Global Accelerator with UDP listeners and endpoint groups in each Region.

-   [ ]   AWS Global Accelerator is designed to minimize latency and packet loss for applications, including those using UDP.
-   [ ]   It provides static IP addresses and optimizes network paths to improve performance.
-   [ ]   Using endpoint groups in each Region ensures that traffic is directed to the closest available endpoint.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Transit gateways are for connecting VPCs and on-premises networks, not for optimizing global, low-latency traffic for applications like online games.
-   [ ]   C. Amazon CloudFront is a CDN primarily designed for caching content; while it can accelerate delivery, it's not optimized for real-time UDP traffic like Global Accelerator.
-   [ ]   D. Setting up a full VPC peering mesh is complex and does not provide the same level of global traffic optimization as Global Accelerator.

Therefore, AWS Global Accelerator is the most suitable solution for minimizing latency and packet loss in a global, UDP-based gaming application.
</details>
<details>
  <summary>Question 353</summary>

A company hosts a three-tier web application on Amazon EC2 instances in a single Availability Zone. The web application uses a self-managed MySQL database that is hosted on an EC2 instance to store data in an Amazon Elastic Block Store (Amazon EBS) volume. The MySQL database currently uses a 1 TB Provisioned IOPS SSD (io2) EBS volume. The company expects traffic of 1,000 IOPS for both reads and writes at peak traffic. The company wants to minimize any disruptions, stabilize performance, and reduce costs while retaining the capacity for double the IOPS. The company wants to move the database tier to a fully managed solution that is highly available and fault tolerant. Which solution will meet these requirements MOST cost-effectively?

-   [ ] A. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with an io2 Block Express EBS volume.
-   [ ] B. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with a General Purpose SSD (gp2) EBS volume.
-   [ ] C. Use Amazon S3 Intelligent-Tiering access tiers.
-   [ ] D. Use two large EC2 instances to host the database in active-passive mode.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with a General Purpose SSD (gp2) EBS volume.

Why these are the correct answers:

B. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with a General Purpose SSD (gp2) EBS volume.

-   [ ]   Amazon RDS for MySQL with a Multi-AZ deployment provides high availability and fault tolerance.
-   [ ]   General Purpose SSD (gp2) volumes offer a good balance of performance and cost-effectiveness for many database workloads.
-   [ ]   It is a fully managed solution, reducing operational overhead.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. io2 Block Express volumes are designed for very high IOPS and throughput, which may be more expensive than necessary for the expected 1,000 IOPS.
-   [ ]   C. Amazon S3 is object storage and not suitable for hosting a MySQL database.
-   [ ]   D. Hosting the database on EC2 instances in active-passive mode increases operational overhead and does not provide a fully managed solution.

Therefore, using Amazon RDS for MySQL with a Multi-AZ deployment and gp2 volumes is the most cost-effective and suitable solution.
</details>
<details>
  <summary>Question 354</summary>

A company hosts a serverless application on AWS. The application uses Amazon API Gateway, AWS Lambda, and an Amazon RDS for PostgreSQL database. The company notices an increase in application errors that result from database connection timeouts during times of peak traffic or unpredictable traffic. The company needs a solution that reduces the application failures with the least amount of change to the code. What should a solutions architect do to meet these requirements?

-   [ ] A. Reduce the Lambda concurrency rate.
-   [ ] B. Enable RDS Proxy on the RDS DB instance.
-   [ ] C. Resize the RDS DB instance class to accept more connections.
-   [ ] D. Migrate the database to Amazon DynamoDB with on-demand scaling.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Enable RDS Proxy on the RDS DB instance.

Why these are the correct answers:

B. Enable RDS Proxy on the RDS DB instance.

-   [ ]   Amazon RDS Proxy manages database connections, reducing the impact of connection storms and timeouts.
-   [ ]   It allows serverless applications to scale more efficiently without exhausting database connections.
-   [ ]   Enabling RDS Proxy requires minimal code changes.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Reducing Lambda concurrency can limit the number of function invocations but does not directly address database connection issues.
-   [ ]   C. Resizing the RDS DB instance might increase the number of available connections but is not as efficient as using RDS Proxy for managing connections from serverless functions.
-   [ ]   D. Migrating to Amazon DynamoDB requires significant code changes and is a NoSQL database, which may not be suitable for all application requirements.

Therefore, enabling RDS Proxy is the most suitable solution for addressing database connection timeouts with the least code changes.
</details>
<details>
  <summary>Question 355</summary>

A company is migrating an old application to AWS. The application runs a batch job every hour and is CPU intensive. The batch job takes 15 minutes on average with an on-premises server. The server has 64 virtual CPU (vCPU) and 512 GiB of memory. Which solution will run the batch job within 15 minutes with the LEAST operational overhead?

-   [ ] A. Use AWS Lambda with functional scaling.
-   [ ] B. Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate.
-   [ ] C. Use Amazon Lightsail with AWS Auto Scaling.
-   [ ] D. Use AWS Batch on Amazon EC2.
</details>
<details>
  <summary>Answer</summary>

-   [ ] D. Use AWS Batch on Amazon EC2.

Why these are the correct answers:

D. Use AWS Batch on Amazon EC2.

-   [ ]   AWS Batch is designed for running batch processing workloads efficiently on EC2 instances.
-   [ ]   It can handle CPU-intensive tasks and allows specifying compute resources similar to the on-premises server.
-   [ ]   AWS Batch manages the underlying infrastructure, minimizing operational overhead.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. AWS Lambda has execution time limits and is not designed for long-running, CPU-intensive batch jobs.
-   [ ]   B. Amazon ECS with Fargate is suitable for containerized applications but may not be as efficient for simple batch jobs and introduces containerization overhead.
-   [ ]   C. Amazon Lightsail is for simpler applications and does not provide the necessary control and scalability for demanding batch workloads.

Therefore, AWS Batch on Amazon EC2 is the most appropriate solution for running the CPU-intensive batch job with minimal overhead.
</details>
<details>
  <summary>Question 356</summary>

A company stores its data objects in Amazon S3 Standard storage. A solutions architect has found that 75% of the data is rarely accessed after 30 days. The company needs all the data to remain immediately accessible with the same high availability and resiliency, but the company wants to minimize storage costs. Which storage solution will meet these requirements?

-   [ ] A. Move the data objects to S3 Glacier Deep Archive after 30 days.
-   [ ] B. Move the data objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.
-   [ ] C. Move the data objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.
-   [ ] D. Move the data objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) immediately.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Move the data objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.

Why these are the correct answers:

B. Move the data objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.

-   [ ]   S3 Standard-IA is designed for data that is less frequently accessed but requires rapid retrieval when needed.
-   [ ]   It offers high availability and resiliency while reducing storage costs compared to S3 Standard.
-   [ ]   Using a lifecycle policy to move data after 30 days optimizes costs.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. S3 Glacier Deep Archive is for long-term archival and has retrieval times that are too long for data that needs to be immediately accessible.
-   [ ]   C and D. S3 One Zone-IA reduces availability and durability because data is stored in a single Availability Zone, which does not meet the requirement for high availability and resiliency.

Therefore, S3 Standard-IA with a lifecycle policy is the most suitable solution for cost-effectively storing data that needs to be immediately accessible but is infrequently used.
</details>
<details>
  <summary>Question 357</summary>

A gaming company is moving its public scoreboard from a data center to the AWS Cloud. The company uses Amazon EC2 Windows Server instances behind an Application Load Balancer to host its dynamic application. The company needs a highly available storage solution for the application. The application consists of static files and dynamic server-side code. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

-   [ ] A. Store the static files on Amazon S3. Use Amazon CloudFront to cache objects at the edge.
-   [ ] B. Store the static files on Amazon S3. Use Amazon ElastiCache to cache objects at the edge.
-   [ ] C. Store the server-side code on Amazon Elastic File System (Amazon EFS). Mount the EFS volume on each EC2 instance to share the files.
-   [ ] D. Store the server-side code on Amazon FSx for Windows File Server. Mount the FSx for Windows File Server volume on each EC2 instance to share the files.
-   [ ] E. Store the server-side code on a General Purpose SSD (gp2) Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume on each EC2 instance to share the files.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Store the static files on Amazon S3. Use Amazon CloudFront to cache objects at the edge.
-   [ ] D. Store the server-side code on Amazon FSx for Windows File Server. Mount the FSx for Windows File Server volume on each EC2 instance to share the files.

Why these are the correct answers:

A. Store the static files on Amazon S3. Use Amazon CloudFront to cache objects at the edge.

-   [ ]   Amazon S3 provides highly available and scalable storage for static files.
-   [ ]   Amazon CloudFront, a CDN, caches these files at edge locations, improving performance.

D. Store the server-side code on Amazon FSx for Windows File Server. Mount the FSx for Windows File Server volume on each EC2 instance to share the files.

-   [ ]   Amazon FSx for Windows File Server provides a fully managed, highly available file system that can be accessed by EC2 instances.
-   [ ]   This allows the dynamic server-side code to be shared across multiple instances.

<hr> Why are the other answers wrong? <hr>

-   [ ]   B. Amazon ElastiCache is for caching data to improve application speed, not for storing static files.
-   [ ]   C. Amazon EFS is a scalable file storage for use with EC2 instances, but Amazon FSx for Windows File Server is more suitable for Windows-based applications.
-   [ ]   E. Amazon EBS volumes are block storage and are not designed to be shared across multiple EC2 instances in the same way as a file system.

Therefore, storing static files on S3 with CloudFront and using Amazon FSx for Windows File Server for server-side code is the most appropriate solution.
</details>
<details>
  <summary>Question 358</summary>

A social media company runs its application on Amazon EC2 instances behind an Application Load Balancer (ALB). The ALB is the origin for an Amazon CloudFront distribution. The application has more than a billion images stored in an Amazon S3 bucket and processes thousands of images each second. The company wants to resize the images dynamically and serve appropriate formats to clients. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Install an external image management library on an EC2 instance. Use the image management library to process the images.
-   [ ] B. Create a CloudFront origin request policy. Use the policy to automatically resize images and to serve the appropriate format based on the User-Agent HTTP header in the request.
-   [ ] C. Use a Lambda@Edge function with an external image management library. Associate the Lambda@Edge function with the CloudFront behaviors that serve the images.
-   [ ] D. Create a CloudFront response headers policy. Use the policy to automatically resize images and to serve the appropriate format based on the User-Agent HTTP header in the request.
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. Use a Lambda@Edge function with an external image management library. Associate the Lambda@Edge function with the CloudFront behaviors that serve the images.

Why these are the correct answers:

C. Use a Lambda@Edge function with an external image management library. Associate the Lambda@Edge function with the CloudFront behaviors that serve the images.

-   [ ]   Lambda@Edge allows you to run code at CloudFront edge locations, processing images close to users.
-   [ ]   It enables dynamic image resizing and formatting based on client requests with low latency.
-   [ ]   This solution minimizes operational overhead by leveraging serverless functions at the edge.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Installing an image management library on EC2 instances increases operational overhead and does not provide edge processing.
-   [ ]   B and D. CloudFront origin request or response headers policies cannot perform dynamic image resizing or formatting; they are for modifying headers.

Therefore, Lambda@Edge is the most suitable solution for dynamic image processing with minimal operational overhead.
</details>
<details>
  <summary>Question 359</summary>

A hospital needs to store patient records in an Amazon S3 bucket. The hospital's compliance team must ensure that all protected health information (PHI) is encrypted in transit and at rest. The compliance team must administer the encryption key for data at rest.

Which solution will meet these requirements?

-   [ ] A. Create a public SSL/TLS certificate in AWS Certificate Manager (ACM). Associate the certificate with Amazon S3. Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.
-   [ ] B. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with S3 managed encryption keys (SSE-S3). Assign the compliance team to manage the SSE-S3 keys.
-   [ ] C. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.
-   [ ] D. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Use Amazon Macie to protect the sensitive data that is stored in Amazon S3. Assign the compliance team to manage Macie.
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.

Why these are the correct answers:

C. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.

-   [ ]   The `aws:SecureTransport` condition enforces encryption in transit by requiring HTTPS.
-   [ ]   SSE-KMS encrypts data at rest using AWS KMS, allowing the compliance team to manage the encryption keys.
-   [ ]   This solution meets both encryption in transit and at rest requirements.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Public SSL/TLS certificates are for encrypting data in transit but do not address encryption at rest.
-   [ ]   B. SSE-S3 keys are managed by Amazon, not the compliance team.
-   [ ]   D. Amazon Macie is for discovering and protecting sensitive data, not for encrypting it.

Therefore, enforcing HTTPS and using SSE-KMS with compliance team-managed keys is the correct solution.
</details>
<details>
  <summary>Question 360</summary>

A company uses Amazon API Gateway to run a private gateway with two REST APIs in the same VPC. The BuyStock RESTful web service calls the CheckFunds RESTful web service to ensure that enough funds are available before a stock can be purchased. The company has noticed in the VPC flow logs that the BuyStock RESTful web service calls the CheckFunds RESTful web service over the internet instead of through the VPC. A solutions architect must implement a solution so that the APIs communicate through the VPC. Which solution will meet these requirements with the FEWEST changes to the code?

-   [ ] A. Add an X-API-Key header in the HTTP header for authorization.
-   [ ] B. Use an interface endpoint.
-   [ ] C. Use a gateway endpoint.
-   [ ] D. Add an Amazon Simple Queue Service (Amazon SQS) queue between the two REST APIs.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Use an interface endpoint.

Why these are the correct answers:

B. Use an interface endpoint.

-   [ ]   Interface endpoints use PrivateLink to enable private communication between AWS services and within a VPC without traversing the internet.
-   [ ]   They provide a private IP address within the VPC for accessing the service.
-   [ ]   This solution requires the fewest changes to the code as it primarily involves network configuration.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Adding an X-API-Key header is for authorization, not for routing traffic within the VPC.
-   [ ]   C. Gateway endpoints are for accessing S3 and DynamoDB, not for API Gateway communication within a VPC.
-   [ ]   D. Adding an Amazon SQS queue changes the communication pattern to asynchronous messaging, requiring significant code changes.

Therefore, using an interface endpoint is the most suitable solution for private API communication within a VPC with minimal code changes.
</details>

<details>
  <summary>Question 361</summary>

A company hosts a multiplayer gaming application on AWS. The company wants the application to read data with sub-millisecond latency and run one-time queries on historical data. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Use Amazon RDS for data that is frequently accessed. Run a periodic custom script to export the data to an Amazon S3 bucket.
-   [ ] B. Store the data directly in an Amazon S3 bucket. Implement an S3 Lifecycle policy to move older data to S3 Glacier Deep Archive for long-term storage. Run one-time queries on the data in Amazon S3 by using Amazon Athena.
-   [ ] C. Use Amazon DynamoDB with DynamoDB Accelerator (DAX) for data that is frequently accessed. Export the data to an Amazon S3 bucket by using DynamoDB table export. Run one-time queries on the data in Amazon S3 by using Amazon Athena.
-   [ ] D. Use Amazon DynamoDB for data that is frequently accessed. Turn on streaming to Amazon Kinesis Data Streams. Use Amazon Kinesis Data Firehose to read the data from Kinesis Data Streams. Store the records in an Amazon S3 bucket.
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. Use Amazon DynamoDB with DynamoDB Accelerator (DAX) for data that is frequently accessed. Export the data to an Amazon S3 bucket by using DynamoDB table export. Run one-time queries on the data in Amazon S3 by using Amazon Athena.

Why these are the correct answers:

C. Use Amazon DynamoDB with DynamoDB Accelerator (DAX) for data that is frequently accessed. Export the data to an Amazon S3 bucket by using DynamoDB table export. Run one-time queries on the data in Amazon S3 by using Amazon Athena.

-   [ ]   Amazon DynamoDB provides fast, flexible NoSQL database service for any scale.
-   [ ]   DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement - from milliseconds to microseconds - even at millions of requests per second.
-   [ ]   Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL.
-   [ ]   This solution allows for both sub-millisecond latency for frequently accessed data and efficient querying of historical data with minimal overhead.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. While Amazon RDS is suitable for many applications, it does not offer the sub-millisecond latency of DynamoDB with DAX. Additionally, managing custom scripts for data export adds operational overhead.
-   [ ]   B. Storing all data in Amazon S3 is not optimized for low-latency reads. Athena is suitable for analytics but not for serving frequently accessed data with sub-millisecond latency.
-   [ ]   D. Using Kinesis Data Streams and Firehose adds complexity and is designed for real-time streaming, not for serving data with sub-millisecond latency.

Therefore, using DynamoDB with DAX for frequent access and exporting to S3 for Athena queries is the most appropriate solution.
</details>
<details>
  <summary>Question 362</summary>

A company uses a payment processing system that requires messages for a particular payment ID to be received in the same order that they were sent. Otherwise, the payments might be processed incorrectly.

Which actions should a solutions architect take to meet this requirement? (Choose two.)

-   [ ] A. Write the messages to an Amazon DynamoDB table with the payment ID as the partition key.
-   [ ] B. Write the messages to an Amazon Kinesis data stream with the payment ID as the partition key.
-   [ ] C. Write the messages to an Amazon ElastiCache for Memcached cluster with the payment ID as the key.
-   [ ] D. Write the messages to an Amazon Simple Queue Service (Amazon SQS) queue. Set the message attribute to use the payment ID.
-   [ ] E. Write the messages to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the message group to use the payment ID.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Write the messages to an Amazon Kinesis data stream with the payment ID as the partition key.
-   [ ] E. Write the messages to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the message group to use the payment ID.

Why these are the correct answers:

B. Write the messages to an Amazon Kinesis data stream with the payment ID as the partition key.

-   [ ]   Amazon Kinesis Data Streams can ensure message ordering within each partition key.
-   [ ]   Using the payment ID as the partition key will keep messages for the same payment in order.

E. Write the messages to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the message group to use the payment ID.

-   [ ]   Amazon SQS FIFO (First-In-First-Out) queues also guarantee that messages are processed in the exact order that they are sent.
-   [ ]   Using message groups with the payment ID ensures ordering for each specific payment.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Amazon DynamoDB does not guarantee message ordering.
-   [ ]   C. Amazon ElastiCache for Memcached is a caching service, not a message queuing service, and does not guarantee message ordering.
-   [ ]   D. Standard Amazon SQS queues do not guarantee message ordering.

Therefore, Amazon Kinesis Data Streams and Amazon SQS FIFO queues are the appropriate services to ensure ordered message processing.
</details>
<details>
  <summary>Question 363</summary>

A company is building a game system that needs to send unique events to separate leaderboard, matchmaking, and authentication services concurrently. The company needs an AWS event-driven system that guarantees the order of the events.

Which solution will meet these requirements?

-   [ ] A. Amazon EventBridge event bus
-   [ ] B. Amazon Simple Notification Service (Amazon SNS) FIFO topics
-   [ ] C. Amazon Simple Notification Service (Amazon SNS) standard topics
-   [ ] D. Amazon Simple Queue Service (Amazon SQS) FIFO queues
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Amazon Simple Notification Service (Amazon SNS) FIFO topics

Why these are the correct answers:

B. Amazon Simple Notification Service (Amazon SNS) FIFO topics

-   [ ]   Amazon SNS FIFO (First-In-First-Out) topics ensure that messages are delivered in the order they are sent.
-   [ ]   This is crucial for systems where event order matters, such as updating leaderboards or processing matchmaking events.
-   [ ]   SNS allows for fan-out to multiple services concurrently.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Amazon EventBridge does not guarantee message ordering across different services.
-   [ ]   C. Amazon SNS standard topics do not guarantee message ordering.
-   [ ]   D. Amazon SQS FIFO queues are for queuing messages to be processed by a single service, not for concurrent fan-out to multiple services.

Therefore, Amazon SNS FIFO topics are the most appropriate solution for ordered, concurrent event delivery.
</details>
<details>
  <summary>Question 364</summary>

A hospital is designing a new application that gathers symptoms from patients. The hospital has decided to use Amazon Simple Queue Service (Amazon SQS) and Amazon Simple Notification Service (Amazon SNS) in the architecture. A solutions architect is reviewing the infrastructure design. Data must be encrypted at rest and in transit. Only authorized personnel of the hospital should be able to access the data. Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)

-   [ ] A. Turn on server-side encryption on the SQS components. Update the default key policy to restrict key usage to a set of authorized principals.
-   [ ] B. Turn on server-side encryption on the SNS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals.
-   [ ] C. Turn on encryption on the SNS components. Update the default key policy to restrict key usage to a set of authorized principals. Set a condition in the topic policy to allow only encrypted connections over TLS.
-   [ ] D. Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals. Set a condition in the queue policy to allow only encrypted connections over TLS.
-   [ ] E. Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply an IAM policy to restrict key usage to a set of authorized principals. Set a condition in the queue policy to allow only encrypted connections over TLS.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Turn on server-side encryption on the SNS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals.
-   [ ] D. Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals. Set a condition in the queue policy to allow only encrypted connections over TLS.

Why these are the correct answers:

B. Turn on server-side encryption on the SNS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals.

-   [ ]   Using SSE-KMS encrypts data at rest in SNS topics.
-   [ ]   Applying a key policy restricts access to the KMS key, controlling who can decrypt the data.

D. Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals. Set a condition in the queue policy to allow only encrypted connections over TLS.

-   [ ]   Using SSE-KMS encrypts data at rest in SQS queues.
-   [ ]   Applying a key policy restricts access to the KMS key, controlling who can decrypt the data.
-   [ ]   Setting a condition to allow only encrypted connections over TLS ensures encryption in transit.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. SQS does not directly support server-side encryption without KMS. Updating the default key policy may not provide granular control.
-   [ ]   C. SNS does not directly support setting a condition in the topic policy to allow only encrypted connections over TLS.
-   [ ]   E. IAM policies control AWS resource access but do not control access to KMS keys in the same way as key policies.

Therefore, using SSE-KMS with key policies for both SNS and SQS, and enforcing TLS for SQS, is the most secure and appropriate solution.
</details>
<details>
  <summary>Question 365</summary>

A company has launched an Amazon RDS for MySQL DB instance. Most of the connections to the database come from serverless applications. Application traffic to the database changes significantly at random intervals. At times of high demand, users report that their applications experience database connection rejection errors. Which solution will resolve this issue with the LEAST operational overhead?

-   [ ] A. Create a proxy in RDS Proxy. Configure the users' applications to use the DB instance through RDS Proxy.
-   [ ] B. Deploy Amazon ElastiCache for Memcached between the users' applications and the DB instance.
-   [ ] C. Resize the RDS DB instance class to accept more connections. Configure the users' applications to use the new DB instance.
-   [ ] D. Configure Multi-AZ for the DB instance. Configure the users' applications to switch between the DB instances.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Create a proxy in RDS Proxy. Configure the users' applications to use the DB instance through RDS Proxy.

Why these are the correct answers:

A. Create a proxy in RDS Proxy. Configure the users' applications to use the DB instance through RDS Proxy.

-   [ ]   Amazon RDS Proxy is a fully managed, highly available database proxy for Amazon RDS that makes applications more scalable, more resilient to database failures, and more secure.
-   [ ]   It manages database connections, allowing serverless applications to handle surges in traffic without exhausting database connections.
-   [ ]   This solution requires minimal operational overhead as RDS Proxy is a managed service.

<hr> Why are the other answers wrong? <hr>

-   [ ]   B. Amazon ElastiCache for Memcached is an in-memory caching service, not a database connection management service. It does not address the issue of connection limits.
-   [ ]   C. Resizing the DB instance might increase the number of allowable connections, but it does not efficiently manage connection pooling and scaling like RDS Proxy. It also involves more operational overhead.
-   [ ]   D. Configuring Multi-AZ is for high availability and failover, not for managing database connections from serverless applications.

Therefore, using RDS Proxy is the most suitable and least operationally intensive solution for managing database connections in this scenario.
</details>
<details>
  <summary>Question 366</summary>

A company's web application consists of an Amazon API Gateway API in front of an AWS Lambda function and an Amazon DynamoDB database. The Lambda function handles the business logic, and the DynamoDB table hosts the data. The application uses Amazon Cognito user pools to identify the individual users of the application. A solutions architect needs to update the application so that only users who have a subscription can access premium content. Which solution will meet this requirement with the LEAST operational overhead?

-   [ ] A. Enable API caching and throttling on the API Gateway API.
-   [ ] B. Set up AWS WAF on the API Gateway API. Create a rule to filter users who have a subscription.
-   [ ] C. Apply fine-grained IAM permissions to the premium content in the DynamoDB table.
-   [ ] D. Implement API usage plans and API keys to limit the access of users who do not have a subscription.
</details>
<details>
  <summary>Answer</summary>

-   [ ] D. Implement API usage plans and API keys to limit the access of users who do not have a subscription.

Why these are the correct answers:

D. Implement API usage plans and API keys to limit the access of users who do not have a subscription.

-   [ ]   API Gateway usage plans and API keys allow you to control access to your APIs based on usage and authentication.
-   [ ]   This is a built-in feature of API Gateway and requires the least operational overhead to implement.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. API caching and throttling are for improving performance and preventing abuse, not for controlling access based on user subscriptions.
-   [ ]   B. AWS WAF is for protecting against web exploits, not for authenticating or authorizing users based on subscription status.
-   [ ]   C. Applying fine-grained IAM permissions in DynamoDB is complex and not suitable for controlling API access.

Therefore, API Gateway usage plans and API keys are the most appropriate solution for controlling access to premium content with the least overhead.
</details>
<details>
  <summary>Question 367</summary>

A company is using Amazon Route 53 latency-based routing to route requests to its UDP-based application for users around the world. The application is hosted on redundant servers in the company's on-premises data centers in the United States, Asia, and Europe. The company's compliance requirements state that the application must be hosted on premises. The company wants to improve the performance and availability of the application. What should a solutions architect do to meet these requirements?

-   [ ] A. Configure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-premises endpoints. Create an accelerator by using AWS Global Accelerator, and register the NLBs as its endpoints. Provide access to the application by using a CNAME that points to the accelerator DNS.
-   [ ] B. Configure three Application Load Balancers (ALBs) in the three AWS Regions to address the on-premises endpoints. Create an accelerator by using AWS Global Accelerator, and register the ALBs as its endpoints. Provide access to the application by using a CNAME that points to the accelerator DNS.
-   [ ] C. Configure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-premises endpoints. In Route 53, create a latency-based record that points to the three NLBs, and use it as an origin for an Amazon CloudFront distribution. Provide access to the application by using a CNAME that points to the CloudFront DNS.
-   [ ] D. Configure three Application Load Balancers (ALBs) in the three AWS Regions to address the on-premises endpoints. In Route 53, create a latency-based record that points to the three ALBs, and use it as an origin for an Amazon CloudFront distribution. Provide access to the application by using a CNAME that points to the CloudFront DNS.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Configure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-premises endpoints. Create an accelerator by using AWS Global Accelerator, and register the NLBs as its endpoints. Provide access to the application by using a CNAME that points to the accelerator DNS.

Why these are the correct answers:

A. Configure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-premises endpoints. Create an accelerator by using AWS Global Accelerator, and register the NLBs as its endpoints. Provide access to the application by using a CNAME that points to the accelerator DNS.

-   [ ]   Network Load Balancers (NLBs) are suitable for UDP traffic and can handle high throughput with low latency.
-   [ ]   AWS Global Accelerator optimizes network paths to improve performance and availability for global applications.
-   [ ]   Using a CNAME to point to the accelerator DNS provides a stable entry point.

<hr> Why are the other answers wrong? <hr>

-   [ ]   B and D. Application Load Balancers (ALBs) are designed for HTTP and HTTPS traffic, not UDP.
-   [ ]   C. While CloudFront can improve performance for some applications, it is primarily a CDN and is not optimized for real-time UDP traffic like Global Accelerator.

Therefore, using NLBs with AWS Global Accelerator is the most appropriate solution for improving the performance and availability of a global UDP-based application.
</details>
<details>
  <summary>Question 368</summary>

A solutions architect wants all new users to have specific complexity requirements and mandatory rotation periods for IAM user passwords. What should the solutions architect do to accomplish this?

-   [ ] A. Set an overall password policy for the entire AWS account.
-   [ ] B. Set a password policy for each IAM user in the AWS account.
-   [ ] C. Use third-party vendor software to set password requirements.
-   [ ] D. Attach an Amazon CloudWatch rule to the Create_newuser event to set the password with the appropriate requirements.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Set an overall password policy for the entire AWS account.

Why these are the correct answers:

A. Set an overall password policy for the entire AWS account.

-   [ ]   AWS IAM allows you to set a password policy at the account level.
-   [ ]   This policy applies to all IAM users in the account, ensuring consistent password complexity and rotation requirements.
-   [ ]   This is the most efficient and recommended way to enforce password standards.

<hr> Why are the other answers wrong? <hr>

-   [ ]   B. Setting a password policy for each IAM user is not practical or scalable.
-   [ ]   C. Using third-party software adds complexity and cost.
-   [ ]   D. Using CloudWatch rules is not the standard way to enforce password policies and is overly complex.

Therefore, setting an overall password policy is the correct and most straightforward solution.
</details>
<details>
  <summary>Question 369</summary>

A company has migrated an application to Amazon EC2 Linux instances. One of these EC2 instances runs several 1-hour tasks on a schedule. These tasks were written by different teams and have no common programming language. The company is concerned about performance and scalability while these tasks run on a single instance. A solutions architect needs to implement a solution to resolve these concerns. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Use AWS Batch to run the tasks as jobs. Schedule the jobs by using Amazon EventBridge (Amazon CloudWatch Events).
-   [ ] B. Convert the EC2 instance to a container. Use AWS App Runner to create the container on demand to run the tasks as jobs.
-   [ ] C. Copy the tasks into AWS Lambda functions. Schedule the Lambda functions by using Amazon EventBridge (Amazon CloudWatch Events).
-   [ ] D. Create an Amazon Machine Image (AMI) of the EC2 instance that runs the tasks. Create an Auto Scaling group with the AMI to run multiple copies of the instance.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Use AWS Batch to run the tasks as jobs. Schedule the jobs by using Amazon EventBridge (Amazon CloudWatch Events).

Why these are the correct answers:

A. Use AWS Batch to run the tasks as jobs. Schedule the jobs by using Amazon EventBridge (Amazon CloudWatch Events).

-   [ ]   AWS Batch is designed to run batch computing workloads efficiently on AWS.
-   [ ]   It can handle tasks of varying durations and resource requirements.
-   [ ]   Amazon EventBridge can schedule these batch jobs, automating the process.
-   [ ]   This solution minimizes operational overhead by using managed services.

<hr> Why are the other answers wrong? <hr>

-   [ ]   B. Converting the EC2 instance to a container and using AWS App Runner is more complex and not ideal for simple scheduled tasks.
-   [ ]   C. Lambda functions have execution time limits and might require significant code changes to adapt the tasks.
-   [ ]   D. Creating an AMI and Auto Scaling group is inefficient and costly for running scheduled tasks.

Therefore, AWS Batch with Amazon EventBridge is the most suitable solution for running scheduled tasks with the least overhead.
</details>
<details>
  <summary>Question 370</summary>

A company runs a public three-tier web application in a VPC. The application runs on Amazon EC2 instances across multiple Availability Zones. The EC2 instances that run in private subnets need to communicate with a license server over the internet. The company needs a managed solution that minimizes operational maintenance.

Which solution meets these requirements?

-   [ ] A. Provision a NAT instance in a public subnet. Modify each private subnet's route table with a default route that points to the NAT instance.
-   [ ] B. Provision a NAT instance in a private subnet. Modify each private subnet's route table with a default route that points to the NAT instance.
-   [ ] C. Provision a NAT gateway in a public subnet. Modify each private subnet's route table with a default route that points to the NAT gateway.
-   [ ] D. Provision a NAT gateway in a private subnet. Modify each private subnet's route table with a default route that points to the NAT gateway.
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. Provision a NAT gateway in a public subnet. Modify each private subnet's route table with a default route that points to the NAT gateway.

Why these are the correct answers:

C. Provision a NAT gateway in a public subnet. Modify each private subnet's route table with a default route that points to the NAT gateway.

-   [ ]   NAT gateways allow instances in private subnets to connect to the internet while preventing the internet from initiating connections with the instances.
-   [ ]   NAT gateways are a managed service, reducing operational overhead.
-   [ ]   They must be placed in a public subnet.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A and B. NAT instances require manual management and scaling, increasing operational overhead.
-   [ ]   D. NAT gateways must be in a public subnet, not a private subnet.

Therefore, using a NAT gateway in a public subnet is the most appropriate and managed solution.
</details>

<details>
  <summary>Question 371</summary>

A company is planning to migrate an Oracle database to AWS. The database consists of a single table that contains millions of geographic information systems (GIS) images that are high resolution and are identified by a geographic code. When a natural disaster occurs, tens of thousands of images get updated every few minutes. Each geographic code has a single image or row that is associated with it. The company wants a solution that is highly available and scalable during such events. Which solution meets these requirements MOST cost-effectively?

-   [ ] A. Store the images and geographic codes in a database table. Use Oracle running on an Amazon RDS Multi-AZ DB instance.
-   [ ] B. Store the images in Amazon S3 buckets. Use Amazon DynamoDB with the geographic code as the key and the image S3 URL as the value.
-   [ ] C. Store the images and geographic codes in an Amazon DynamoDB table. Configure DynamoDB Accelerator (DAX) during times of high load.
-   [ ] D. Store the images in Amazon S3 buckets. Store geographic codes and image S3 URLs in a database table. Use Oracle running on an Amazon RDS Multi-AZ DB instance.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Store the images in Amazon S3 buckets. Use Amazon DynamoDB with the geographic code as the key and the image S3 URL as the value.

Why these are the correct answers:

B. Store the images in Amazon S3 buckets. Use Amazon DynamoDB with the geographic code as the key and the image S3 URL as the value.

-   [ ]   Amazon S3 is highly scalable and available for storing large numbers of images.
-   [ ]   Amazon DynamoDB is a NoSQL database that can handle high write throughput and scale dynamically.
-   [ ]   Using the geographic code as the key allows for efficient retrieval and updates of image URLs.
-   [ ]   This solution is cost-effective because S3 and DynamoDB scale independently based on usage.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A and D. Using Amazon RDS for Oracle is more expensive and might not scale as efficiently as DynamoDB for high-frequency updates.
-   [ ]   C. Storing images directly in DynamoDB is not efficient or cost-effective. DynamoDB is designed for smaller data items.

Therefore, storing images in S3 and using DynamoDB to manage the metadata is the most scalable and cost-effective solution.
</details>
<details>
  <summary>Question 372</summary>

A company has an application that collects data from lot sensors on automobiles. The data is streamed and stored in Amazon S3 through Amazon Kinesis Data Firehose. The data produces trillions of S3 objects each year. Each morning, the company uses the data from the previous 30 days to retrain a suite of machine learning (ML) models. Four times each year, the company uses the data from the previous 12 months to perform analysis and train other ML models. The data must be available with minimal delay for up to 1 year. After 1 year, the data must be retained for archival purposes.

Which storage solution meets these requirements MOST cost-effectively?

-   [ ] A. Use the S3 Intelligent-Tiering storage class. Create an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 1 year.
-   [ ] B. Use the S3 Intelligent-Tiering storage class. Configure S3 Intelligent-Tiering to automatically move objects to S3 Glacier Deep Archive after 1 year.
-   [ ] C. Use the S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Create an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 1 year.
-   [ ] D. Use the S3 Standard storage class. Create an S3 Lifecycle policy to transition objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days, and then to S3 Glacier Deep Archive after 1 year.
</details>
<details>
  <summary>Answer</summary>

-   [ ] D. Use the S3 Standard storage class. Create an S3 Lifecycle policy to transition objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days, and then to S3 Glacier Deep Archive after 1 year.

Why these are the correct answers:

D. Use the S3 Standard storage class. Create an S3 Lifecycle policy to transition objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days, and then to S3 Glacier Deep Archive after 1 year.

-   [ ]   S3 Standard is suitable for data that is frequently accessed.
-   [ ]   S3 Standard-IA is cost-effective for data accessed less frequently but still needing quick retrieval.
-   [ ]   S3 Glacier Deep Archive is for long-term archival storage with the lowest costs.
-   [ ]   Using a lifecycle policy automates the transition of data between these storage classes, optimizing costs.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A and B. S3 Intelligent-Tiering is designed for data with unknown or changing access patterns. It is not as cost-effective as using lifecycle policies for known access patterns.
-   [ ]   C. S3 Standard-IA is not ideal for the initial 30 days of frequent access.

Therefore, using S3 Standard with a lifecycle policy to transition to S3 Standard-IA and then Glacier Deep Archive is the most cost-effective solution.
</details>
<details>
  <summary>Question 373</summary>

A company is running several business applications in three separate VPCs within the us-east-1 Region. The applications must be able to communicate between VPCs. The applications also must be able to consistently send hundreds of gigabytes of data each day to a latency-sensitive application that runs in a single on-premises data center. A solutions architect needs to design a network connectivity solution that maximizes cost-effectiveness.

Which solution meets these requirements?

-   [ ] A. Configure three AWS Site-to-Site VPN connections from the data center to AWS. Establish connectivity by configuring one VPN connection for each VPC.
-   [ ] B. Launch a third-party virtual network appliance in each VPC. Establish an IPsec VPN tunnel between the data center and each virtual appliance.
-   [ ] C. Set up three AWS Direct Connect connections from the data center to a Direct Connect gateway in us-east-1. Establish connectivity by configuring each VPC to use one of the Direct Connect connections.
-   [ ] D. Set up one AWS Direct Connect connection from the data center to AWS. Create a transit gateway, and attach each VPC to the transit gateway. Establish connectivity between the Direct Connect connection and the transit gateway.
</details>
<details>
  <summary>Answer</summary>

-   [ ] D. Set up one AWS Direct Connect connection from the data center to AWS. Create a transit gateway, and attach each VPC to the transit gateway. Establish connectivity between the Direct Connect connection and the transit gateway.

Why these are the correct answers:

D. Set up one AWS Direct Connect connection from the data center to AWS. Create a transit gateway, and attach each VPC to the transit gateway. Establish connectivity between the Direct Connect connection and the transit gateway.

-   [ ]   AWS Direct Connect provides a dedicated, high-bandwidth connection between the on-premises data center and AWS, suitable for transferring large amounts of data with low latency.
-   [ ]   AWS Transit Gateway simplifies the interconnection of multiple VPCs and Direct Connect, reducing complexity and cost.
-   [ ]   This solution is cost-effective because it uses a single Direct Connect connection and centralizes connectivity through the transit gateway.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Using multiple Site-to-Site VPN connections is less efficient and scalable for high-volume data transfer and can introduce latency.
-   [ ]   B. Using third-party virtual network appliances adds complexity and management overhead.
-   [ ]   C. Setting up multiple Direct Connect connections is more expensive than using a single connection with a transit gateway.

Therefore, using a single Direct Connect connection with a transit gateway is the most cost-effective and efficient solution.
</details>
<details>
  <summary>Question 374</summary>

An ecommerce company is building a distributed application that involves several serverless functions and AWS services to complete order-processing tasks. These tasks require manual approvals as part of the workflow. A solutions architect needs to design an architecture for the order-processing application. The solution must be able to combine multiple AWS Lambda functions into responsive serverless applications. The solution also must orchestrate data and services that run on Amazon EC2 instances, containers, or on-premises servers. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Use AWS Step Functions to build the application.
-   [ ] B. Integrate all the application components in an AWS Glue job.
-   [ ] C. Use Amazon Simple Queue Service (Amazon SQS) to build the application.
-   [ ] D. Use AWS Lambda functions and Amazon EventBridge events to build the application.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Use AWS Step Functions to build the application.

Why these are the correct answers:

A. Use AWS Step Functions to build the application.

-   [ ]   AWS Step Functions allows you to coordinate multiple AWS services and Lambda functions into serverless workflows.
-   [ ]   It can also orchestrate tasks that involve external systems, such as those on EC2 instances, containers, or on-premises servers.
-   [ ]   Step Functions provides state management, error handling, and visual workflow management, minimizing operational overhead.

<hr> Why are the other answers wrong? <hr>

-   [ ]   B. AWS Glue is designed for ETL (extract, transform, load) operations, not for orchestrating application workflows.
-   [ ]   C. Amazon SQS is a message queuing service and does not provide workflow orchestration capabilities.
-   [ ]   D. Using only Lambda functions and Amazon EventBridge for complex workflows can lead to increased complexity and operational overhead in managing state and coordination.

Therefore, AWS Step Functions is the most appropriate service for building the described order-processing application with the least operational overhead.
</details>
<details>
  <summary>Question 375</summary>

A company has launched an Amazon RDS for MySQL DB instance. Most of the connections to the database come from serverless applications. Application traffic to the database changes significantly at random intervals. At times of high demand, users report that their applications experience database connection rejection errors. Which solution will resolve this issue with the LEAST operational overhead?

-   [ ] A. Create a proxy in RDS Proxy. Configure the users' applications to use the DB instance through RDS Proxy.
-   [ ] B. Deploy Amazon ElastiCache for Memcached between the users' applications and the DB instance.
-   [ ] C. Migrate the DB instance to a different instance class that has higher 1/0 capacity. Configure the users' applications to use the new DB instance.
-   [ ] D. Configure Multi-AZ for the DB instance. Configure the users' applications to switch between the DB instances.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Create a proxy in RDS Proxy. Configure the users' applications to use the DB instance through RDS Proxy.

Why these are the correct answers:

A. Create a proxy in RDS Proxy. Configure the users' applications to use the DB instance through RDS Proxy.

-   [ ]   Amazon RDS Proxy is a fully managed, highly available database proxy service that helps manage database connections.
-   [ ]   It reduces the overhead of opening and closing database connections by pooling and sharing them, which is essential for serverless applications with unpredictable traffic.
-   [ ]   This solution minimizes operational overhead as RDS Proxy is a managed service.

<hr> Why are the other answers wrong? <hr>

-   [ ]   B. Amazon ElastiCache for Memcached is an in-memory caching service and does not address database connection management.
-   [ ]   C. Migrating to a different instance class may increase the number of available connections but does not efficiently manage connection pooling and scaling.
-   [ ]   D. Configuring Multi-AZ is for high availability and failover, not for managing database connections.

Therefore, using RDS Proxy is the most suitable and least operationally intensive solution for managing database connections from serverless applications.
</details>
<details>
  <summary>Question 376</summary>

A company recently deployed a new auditing system to centralize information about operating system versions, patching, and installed software for Amazon EC2 instances. A solutions architect must ensure all instances provisioned through EC2 Auto Scaling groups successfully send reports to the auditing system as soon as they are launched and terminated. Which solution achieves these goals MOST efficiently?

-   [ ] A. Use a scheduled AWS Lambda function and run a script remotely on all EC2 instances to send data to the audit system.
-   [ ] B. Use EC2 Auto Scaling lifecycle hooks to run a custom script to send data to the audit system when instances are launched and terminated.
-   [ ] C. Use an EC2 Auto Scaling launch configuration to run a custom script through user data to send data to the audit system when instances are launched and terminated.
-   [ ] D. Run a custom script on the instance operating system to send data to the audit system. Configure the script to be invoked by the EC2 Auto Scaling group when the instance starts and is terminated.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Use EC2 Auto Scaling lifecycle hooks to run a custom script to send data to the audit system when instances are launched and terminated.

Why these are the correct answers:

B. Use EC2 Auto Scaling lifecycle hooks to run a custom script to send data to the audit system when instances are launched and terminated.

-   [ ]   EC2 Auto Scaling lifecycle hooks allow you to perform custom actions when instances are launched or terminated.
-   [ ]   This ensures that the reporting script runs reliably as part of the scaling process.
-   [ ]   Lifecycle hooks provide a clean and integrated way to manage these actions.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Using a scheduled Lambda function to run scripts remotely adds complexity and may not be as reliable as lifecycle hooks.
-   [ ]   C. User data scripts run only during instance launch and do not handle termination.
-   [ ]   D. Relying on scripts within the instance and configuring them to be invoked by the Auto Scaling group is less reliable and more complex than using lifecycle hooks.

Therefore, lifecycle hooks are the most efficient and reliable solution for this scenario.
</details>
<details>
  <summary>Question 377</summary>

A company is developing a real-time multiplayer game that uses UDP for communications between the client and servers in an Auto Scaling group. Spikes in demand are anticipated during the day, so the game server platform must adapt accordingly. Developers want to store gamer scores and other non-relational data in a database solution that will scale without intervention. Which solution should a solutions architect recommend?

-   [ ] A. Use Amazon Route 53 for traffic distribution and Amazon Aurora Serverless for data storage.
-   [ ] B. Use a Network Load Balancer for traffic distribution and Amazon DynamoDB on-demand for data storage.
-   [ ] C. Use a Network Load Balancer for traffic distribution and Amazon Aurora Global Database for data storage.
-   [ ] D. Use an Application Load Balancer for traffic distribution and Amazon DynamoDB global tables for data storage.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Use a Network Load Balancer for traffic distribution and Amazon DynamoDB on-demand for data storage.

Why these are the correct answers:

B. Use a Network Load Balancer for traffic distribution and Amazon DynamoDB on-demand for data storage.

-   [ ]   Network Load Balancers (NLBs) are designed for UDP traffic and can handle high throughput with low latency.
-   [ ]   Amazon DynamoDB on-demand automatically scales capacity to handle varying workloads, making it suitable for game data.
-   [ ]   This combination provides efficient traffic distribution and a scalable database solution.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Amazon Route 53 is a DNS service and does not distribute traffic like a load balancer. Amazon Aurora Serverless is not ideal for the high write throughput and scalability needs of real-time game data.
-   [ ]   C. Amazon Aurora Global Database is designed for globally distributed applications and is more complex and expensive than necessary for this scenario.
-   [ ]   D. Application Load Balancers (ALBs) are designed for HTTP/HTTPS traffic, not UDP. DynamoDB global tables are for multi-region replication, which is not a primary requirement here.

Therefore, NLBs and DynamoDB on-demand are the most suitable and scalable solution for this gaming application.
</details>
<details>
  <summary>Question 378</summary>

A company hosts a frontend application that uses an Amazon API Gateway API backend that is integrated with AWS Lambda. When the API receives requests, the Lambda function loads many libraries. Then the Lambda function connects to an Amazon RDS database, processes the data, and returns the data to the frontend application. The company wants to ensure that response latency is as low as possible for all its users with the fewest number of changes to the company's operations. Which solution will meet these requirements?

-   [ ] A. Establish a connection between the frontend application and the database to make queries faster by bypassing the API.
-   [ ] B. Configure provisioned concurrency for the Lambda function that handles the requests.
-   [ ] C. Cache the results of the queries in Amazon S3 for faster retrieval of similar datasets.
-   [ ] D. Increase the size of the database to increase the number of connections Lambda can establish at one time.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Configure provisioned concurrency for the Lambda function that handles the requests.

Why these are the correct answers:

B. Configure provisioned concurrency for the Lambda function that handles the requests.

-   [ ]   Provisioned concurrency keeps Lambda functions initialized and ready to respond, reducing cold start latency.
-   [ ]   This improves response time, especially when Lambda functions load many libraries or connect to databases.
-   [ ]   It minimizes operational changes as it's a configuration setting within Lambda.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Establishing a direct connection between the frontend and database is complex and bypasses the API Gateway, which might have security and management implications.
-   [ ]   C. Caching results in S3 adds complexity and is only effective for repeated queries, not for all requests.
-   [ ]   D. Increasing the database size does not directly address Lambda's cold start latency.

Therefore, provisioned concurrency is the most appropriate solution for reducing latency with minimal operational changes.
</details>
<details>
  <summary>Question 379</summary>

A company is migrating its on-premises workload to the AWS Cloud. The company already uses several Amazon EC2 instances and Amazon RDS DB instances. The company wants a solution that automatically starts and stops the EC2 instances and DB instances outside of business hours. The solution must minimize cost and infrastructure maintenance.

Which solution will meet these requirements?

-   [ ] A. Scale the EC2 instances by using elastic resize. Scale the DB instances to zero outside of business hours.
-   [ ] B. Explore AWS Marketplace for partner solutions that will automatically start and stop the EC2 instances and DB instances on a schedule.
-   [ ] C. Launch another EC2 instance. Configure a crontab schedule to run shell scripts that will start and stop the existing EC2 instances and DB instances on a schedule.
-   [ ] D. Create an AWS Lambda function that will start and stop the EC2 instances and DB instances. Configure Amazon EventBridge to invoke the Lambda function on a schedule.
</details>
<details>
  <summary>Answer</summary>

-   [ ] D. Create an AWS Lambda function that will start and stop the EC2 instances and DB instances. Configure Amazon EventBridge to invoke the Lambda function on a schedule.

Why these are the correct answers:

D. Create an AWS Lambda function that will start and stop the EC2 instances and DB instances. Configure Amazon EventBridge to invoke the Lambda function on a schedule.

-   [ ]   AWS Lambda allows you to run code without provisioning or managing servers.
-   [ ]   Amazon EventBridge (formerly CloudWatch Events) can schedule Lambda functions to run at specific times.
-   [ ]   This approach automates the start/stop process, minimizing cost and maintenance.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. EC2 instances cannot be "elastic resized" to zero. RDS instances cannot be scaled to zero.
-   [ ]   B. Relying on partner solutions from AWS Marketplace adds dependency and potential cost.
-   [ ]   C. Launching another EC2 instance to manage start/stop operations increases operational overhead.

Therefore, using Lambda and EventBridge is the most efficient and cost-effective solution.
</details>
<details>
  <summary>Question 380</summary>

A company hosts a three-tier web application that includes a PostgreSQL database. The database stores the metadata from documents. The company searches the metadata for key terms to retrieve documents that the company reviews in a report each month. The documents are stored in Amazon S3. The documents are usually written only once, but they are updated frequently. The reporting process takes a few hours with the use of relational queries. The reporting process must not prevent any document modifications or the addition of new documents. A solutions architect needs to implement a solution to speed up the reporting process. Which solution will meet these requirements with the LEAST amount of change to the application code?

-   [ ] A. Set up a new Amazon DocumentDB (with MongoDB compatibility) cluster that includes a read replica. Scale the read replica to generate the reports.
-   [ ] B. Set up a new Amazon Aurora PostgreSQL DB cluster that includes an Aurora Replica. Issue queries to the Aurora Replica to generate the reports.
-   [ ] C. Set up a new Amazon RDS for PostgreSQL Multi-AZ DB instance. Configure the reporting module to query the secondary RDS node so that the reporting module does not affect the primary node.
-   [ ] D. Set up a new Amazon DynamoDB table to store the documents. Use a fixed write capacity to support new document entries. Automatically scale the read capacity to support the reports.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Set up a new Amazon Aurora PostgreSQL DB cluster that includes an Aurora Replica. Issue queries to the Aurora Replica to generate the reports.

Why these are the correct answers:

B. Set up a new Amazon Aurora PostgreSQL DB cluster that includes an Aurora Replica. Issue queries to the Aurora Replica to generate the reports.

-   [ ]   Amazon Aurora PostgreSQL is compatible with PostgreSQL, minimizing code changes.
-   [ ]   Aurora Replicas provide read scalability and can be used for reporting without impacting the primary database.
-   [ ]   This solution leverages a relational database, which is suitable for the metadata queries.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Amazon DocumentDB is a NoSQL database, requiring significant code changes to adapt from a relational database.
-   [ ]   C. Amazon RDS for PostgreSQL Multi-AZ is for high availability, not for read scaling. Read replicas are more suitable for offloading reporting.
-   [ ]   D. Amazon DynamoDB is a NoSQL database and requires substantial code changes. It is not designed for complex relational queries.

Therefore, using Amazon Aurora PostgreSQL with an Aurora Replica is the most appropriate solution for speeding up reporting with minimal code changes.
</details>

<details>
  <summary>Question 381</summary>

A company hosts a three-tier web application that includes a PostgreSQL database. The database stores the metadata from documents. The company searches the metadata for key terms to retrieve documents that the company reviews in a report each month. The documents are stored in Amazon S3. The documents are usually written only once, but they are updated frequently. The reporting process takes a few hours with the use of relational queries. The reporting process must not prevent any document modifications or the addition of new documents. A solutions architect needs to implement a solution to speed up the reporting process. Which solution will meet these requirements with the LEAST amount of change to the application code?

-   [ ] A. Set up a new Amazon DocumentDB (with MongoDB compatibility) cluster that includes a read replica. Scale the read replica to generate the reports.
-   [ ] B. Set up a new Amazon Aurora PostgreSQL DB cluster that includes an Aurora Replica. Issue queries to the Aurora Replica to generate the reports.
-   [ ] C. Set up a new Amazon RDS for PostgreSQL Multi-AZ DB instance. Configure the reporting module to query the secondary RDS node so that the reporting module does not affect the primary node.
-   [ ] D. Set up a new Amazon DynamoDB table to store the documents. Use a fixed write capacity to support new document entries. Automatically scale the read capacity to support the reports.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Set up a new Amazon Aurora PostgreSQL DB cluster that includes an Aurora Replica. Issue queries to the Aurora Replica to generate the reports.

Why these are the correct answers:

B. Set up a new Amazon Aurora PostgreSQL DB cluster that includes an Aurora Replica. Issue queries to the Aurora Replica to generate the reports.

-   [ ]   Amazon Aurora PostgreSQL is compatible with PostgreSQL, minimizing code changes.
-   [ ]   Aurora Replicas provide read scalability and can be used for reporting without impacting the primary database.
-   [ ]   This solution leverages a relational database, which is suitable for the metadata queries.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Amazon DocumentDB is a NoSQL database, requiring significant code changes to adapt from a relational database.
-   [ ]   C. Amazon RDS for PostgreSQL Multi-AZ is for high availability, not for read scaling. Read replicas are more suitable for offloading reporting.
-   [ ]   D. Amazon DynamoDB is a NoSQL database and requires substantial code changes. It is not designed for complex relational queries.

Therefore, using Amazon Aurora PostgreSQL with an Aurora Replica is the most appropriate solution for speeding up reporting with minimal code changes.
</details>
<details>
  <summary>Question 382</summary>

A company has a three-tier application on AWS that ingests sensor data from its users' devices. The traffic flows through a Network Load Balancer (NLB), then to Amazon EC2 instances for the web tier, and finally to EC2 instances for the application tier. The application tier makes calls to a database.

What should a solutions architect do to improve the security of the data in transit?

-   [ ] A. Configure a TLS listener. Deploy the server certificate on the NLB.
-   [ ] B. Configure AWS Shield Advanced. Enable AWS WAF on the NLB.
-   [ ] C. Change the load balancer to an Application Load Balancer (ALB). Enable AWS WAF on the ALB.
-   [ ] D. Encrypt the Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instances by using AWS Key Management Service (AWS KMS).
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Configure a TLS listener. Deploy the server certificate on the NLB.

Why these are the correct answers:

A. Configure a TLS listener. Deploy the server certificate on the NLB.

-   [ ]   Configuring a TLS listener on the Network Load Balancer (NLB) encrypts data in transit.
-   [ ]   Deploying the server certificate on the NLB ensures that the encryption is handled at the load balancer level.

<hr> Why are the other answers wrong? <hr>

-   [ ]   B. AWS Shield Advanced protects against DDoS attacks, and AWS WAF protects against web exploits, but neither directly encrypts data in transit.
-   [ ]   C. Changing to an Application Load Balancer (ALB) and using AWS WAF does not address the encryption of data in transit in the same way as configuring a TLS listener.
-   [ ]   D. Encrypting EBS volumes encrypts data at rest, not in transit.

Therefore, configuring a TLS listener on the NLB is the most appropriate solution for encrypting data in transit.
</details>
<details>
  <summary>Question 383</summary>

A company is planning to migrate a commercial off-the-shelf application from its on-premises data center to AWS. The software has a software licensing model using sockets and cores with predictable capacity and uptime requirements. The company wants to use its existing licenses, which were purchased earlier this year. Which Amazon EC2 pricing option is the MOST cost-effective?

-   [ ] A. Dedicated Reserved Hosts
-   [ ] B. Dedicated On-Demand Hosts
-   [ ] C. Dedicated Reserved Instances
-   [ ] D. Dedicated On-Demand Instances
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Dedicated Reserved Hosts

Why these are the correct answers:

A. Dedicated Reserved Hosts

-   [ ]   Dedicated Hosts allow you to bring your own software licenses that are tied to specific server hardware.
-   [ ]   Reserved Hosts provide cost savings for long-term usage with predictable capacity and uptime.
-   [ ]   This combination is the most cost-effective for using existing licenses.

<hr> Why are the other answers wrong? <hr>

-   [ ]   B. Dedicated On-Demand Hosts are more expensive for long-term usage.
-   [ ]   C and D. Dedicated Instances do not provide the hardware-level control needed for socket/core-based licensing.

Therefore, Dedicated Reserved Hosts are the most cost-effective option for using existing software licenses.
</details>
<details>
  <summary>Question 384</summary>

A company runs an application on Amazon EC2 Linux instances across multiple Availability Zones. The application needs a storage layer that is highly available and Portable Operating System Interface (POSIX)-compliant. The storage layer must provide maximum data durability and must be shareable across the EC2 instances. The data in the storage layer will be accessed frequently for the first 30 days and will be accessed infrequently after that time. Which solution will meet these requirements MOST cost-effectively?

-   [ ] A. Use the Amazon S3 Standard storage class. Create an S3 Lifecycle policy to move infrequently accessed data to S3 Glacier.
-   [ ] B. Use the Amazon S3 Standard storage class. Create an S3 Lifecycle policy to move infrequently accessed data to S3 Standard-Infrequent Access (S3 Standard-IA).
-   [ ] C. Use the Amazon Elastic File System (Amazon EFS) Standard storage class. Create a lifecycle management policy to move infrequently accessed data to EFS Standard-Infrequent Access (EFS Standard-IA).
-   [ ] D. Use the Amazon Elastic File System (Amazon EFS) One Zone storage class. Create a lifecycle management policy to move infrequently accessed data to EFS One Zone-Infrequent Access (EFS One Zone-IA).
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. Use the Amazon Elastic File System (Amazon EFS) Standard storage class. Create a lifecycle management policy to move infrequently accessed data to EFS Standard-Infrequent Access (EFS Standard-IA).

Why these are the correct answers:

C. Use the Amazon Elastic File System (Amazon EFS) Standard storage class. Create a lifecycle management policy to move infrequently accessed data to EFS Standard-Infrequent Access (EFS Standard-IA).

-   [ ]   Amazon EFS provides a shared file system that is POSIX-compliant and can be accessed by multiple EC2 instances.
-   [ ]   EFS Standard offers high availability and durability.
-   [ ]   Lifecycle policies can move data to EFS Standard-IA for cost savings on infrequently accessed data.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A and B. Amazon S3 is object storage, not file storage, and does not provide POSIX compliance.
-   [ ]   D. EFS One Zone storage class reduces availability and durability, which does not meet the requirements.

Therefore, using EFS Standard with a lifecycle policy is the most appropriate and cost-effective solution.
</details>
<details>
  <summary>Question 385</summary>

A solutions architect is creating a new VPC design. There are two public subnets for the load balancer, two private subnets for web servers, and two private subnets for MySQL. The web servers use only HTTPS. The solutions architect has already created a security group for the load balancer allowing port 443 from 0.0.0.0/0. Company policy requires that each resource has the least access required to still be able to perform its tasks. Which additional configuration strategy should the solutions architect use to meet these requirements?

-   [ ] A. Create a security group for the web servers and allow port 443 from 0.0.0.0/0. Create a security group for the MySQL servers and allow port 3306 from the web servers security group.
-   [ ] B. Create a network ACL for the web servers and allow port 443 from 0.0.0.0/0. Create a network ACL for the MySQL servers and allow port 3306 from the web servers security group.
-   [ ] C. Create a security group for the web servers and allow port 443 from the load balancer. Create a security group for the MySQL servers and allow port 3306 from the web servers security group.
-   [ ] D. Create a network ACL for the web servers and allow port 443 from the load balancer. Create a network ACL for the MySQL servers and allow port 3306 from the web servers security group.
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. Create a security group for the web servers and allow port 443 from the load balancer. Create a security group for the MySQL servers and allow port 3306 from the web servers security group.

Why these are the correct answers:

C. Create a security group for the web servers and allow port 443 from the load balancer. Create a security group for the MySQL servers and allow port 3306 from the web servers security group.

-   [ ]   Security groups act as a virtual firewall for your EC2 instances.
-   [ ]   Allowing port 443 only from the load balancer follows the principle of least privilege.
-   [ ]   Allowing MySQL traffic only from the web servers' security group further restricts access.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Allowing 443 from 0.0.0.0/0 on the web servers opens them to the entire internet, violating least privilege.
-   [ ]   B and D. Network ACLs operate at the subnet level and are not as granular as security groups for instance-level security.

Therefore, using security groups with specific source restrictions is the most secure and appropriate solution.
</details>
<details>
  <summary>Question 386</summary>

An ecommerce company is running a multi-tier application on AWS. The front-end and backend tiers both run on Amazon EC2, and the database runs on Amazon RDS for MySQL. The backend tier communicates with the RDS instance. There are frequent calls to return identical datasets from the database that are causing performance slowdowns. Which action should be taken to improve the performance of the backend?

-   [ ] A. Implement Amazon SNS to store the database calls.
-   [ ] B. Implement Amazon ElastiCache to cache the large datasets.
-   [ ] C. Implement an RDS for MySQL read replica to cache database calls.
-   [ ] D. Implement Amazon Kinesis Data Firehose to stream the calls to the database.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Implement Amazon ElastiCache to cache the large datasets.

Why these are the correct answers:

B. Implement Amazon ElastiCache to cache the large datasets.

-   [ ]   Amazon ElastiCache is a caching service that can store frequently accessed data in memory.
-   [ ]   Caching identical datasets reduces the load on the database and improves response times.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Amazon SNS is a messaging service and is not suitable for caching database results.
-   [ ]   C. RDS read replicas are for offloading read operations but do not provide in-memory caching like ElastiCache.
-   [ ]   D. Amazon Kinesis Data Firehose is for streaming data and is not relevant for caching database results.

Therefore, Amazon ElastiCache is the most appropriate service for improving performance by caching data.
</details>
<details>
  <summary>Question 387</summary>

A new employee has joined a company as a deployment engineer. The deployment engineer will be using AWS CloudFormation templates to create multiple AWS resources. A solutions architect wants the deployment engineer to perform job activities while following the principle of least privilege. Which combination of actions should the solutions architect take to accomplish this goal? (Choose two.)

-   [ ] A. Have the deployment engineer use AWS account root user credentials for performing AWS CloudFormation stack operations.
-   [ ] B. Create a new IAM user for the deployment engineer and add the IAM user to a group that has the PowerUsers IAM policy attached.
-   [ ] C. Create a new IAM user for the deployment engineer and add the IAM user to a group that has the AdministratorAccess IAM policy attached.
-   [ ] D. Create a new IAM user for the deployment engineer and add the IAM user to a group that has an IAM policy that allows AWS CloudFormation actions only.
-   [ ] E. Create an IAM role for the deployment engineer to explicitly define the permissions specific to the AWS CloudFormation stack and launch stacks using that IAM role.
</details>
<details>
  <summary>Answer</summary>

-   [ ] D. Create a new IAM user for the deployment engineer and add the IAM user to a group that has an IAM policy that allows AWS CloudFormation actions only.
-   [ ] E. Create an IAM role for the deployment engineer to explicitly define the permissions specific to the AWS CloudFormation stack and launch stacks using that IAM role.

Why these are the correct answers:

D. Create a new IAM user for the deployment engineer and add the IAM user to a group that has an IAM policy that allows AWS CloudFormation actions only.

-   [ ]   Creating an IAM user and assigning specific CloudFormation permissions follows the principle of least privilege.
-   [ ]   This limits the user's access to only the necessary actions.

E. Create an IAM role for the deployment engineer to explicitly define the permissions specific to the AWS CloudFormation stack and launch stacks using that IAM role.

-   [ ]   Using an IAM role provides temporary credentials and fine-grained control over permissions for CloudFormation operations.
-   [ ]   This is a secure and recommended practice.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Using root user credentials grants full access to all AWS resources, violating least privilege.
-   [ ]   B. The PowerUsers IAM policy grants broad access, which is not aligned with least privilege.
-   [ ]   C. The AdministratorAccess IAM policy grants full access, violating least privilege.

Therefore, creating a dedicated IAM user/group with specific CloudFormation permissions and using an IAM role are the most secure and appropriate solutions.
</details>
<details>
  <summary>Question 388</summary>

A company is deploying a two-tier web application in a VPC. The web tier is using an Amazon EC2 Auto Scaling group with public subnets that span multiple Availability Zones. The database tier consists of an Amazon RDS for MySQL DB instance in separate private subnets. The web tier requires access to the database to retrieve product information.

The web application is not working as intended. The web application reports that it cannot connect to the database. The database is confirmed to be up and running. All configurations for the network ACLs, security groups, and route tables are still in their default states. What should a solutions architect recommend to fix the application?

-   [ ] A. Add an explicit rule to the private subnet's network ACL to allow traffic from the web tier's EC2 instances.
-   [ ] B. Add a route in the VPC route table to allow traffic between the web tier's EC2 instances and the database tier.
-   [ ] C. Deploy the web tier's EC2 instances and the database tier's RDS instance into two separate VPCs, and configure VPC peering.
-   [ ] D. Add an inbound rule to the security group of the database tier's RDS instance to allow traffic from the web tier's security group.
</details>
<details>
  <summary>Answer</summary>

-   [ ] D. Add an inbound rule to the security group of the database tier's RDS instance to allow traffic from the web tier's security group.

Why these are the correct answers:

D. Add an inbound rule to the security group of the database tier's RDS instance to allow traffic from the web tier's security group.

-   [ ]   Security groups control inbound and outbound traffic for EC2 instances and RDS instances.
-   [ ]   The default state of security groups is to deny all inbound traffic.
-   [ ]   Adding an inbound rule allows the web tier to connect to the database.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Network ACLs are subnet-level firewalls and are not the primary reason for connection issues if security groups are not configured.
-   [ ]   B. Route tables control traffic routing between subnets, which is not the issue here.
-   [ ]   C. VPC peering is for connecting different VPCs, not for enabling communication within the same VPC.

Therefore, configuring the security group is the correct solution.
</details>
<details>
  <summary>Question 389</summary>

A company has a large dataset for its online advertising business stored in an Amazon RDS for MySQL DB instance in a single Availability Zone. The company wants business reporting queries to run without impacting the write operations to the production DB instance. Which solution meets these requirements?

-   [ ] A. Deploy RDS read replicas to process the business reporting queries.
-   [ ] B. Scale out the DB instance horizontally by placing it behind an Elastic Load Balancer.
-   [ ] C. Scale up the DB instance to a larger instance type to handle write operations and queries.
-   [ ] D. Deploy the DB instance in multiple Availability Zones to process the business reporting queries.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Deploy RDS read replicas to process the business reporting queries.

Why these are the correct answers:

A. Deploy RDS read replicas to process the business reporting queries.

-   [ ]   RDS read replicas allow you to create one or more read-only copies of your primary database instance.
-   [ ]   Reporting queries can be directed to the read replicas, offloading the primary instance.

<hr> Why are the other answers wrong? <hr>

-   [ ]   B. Elastic Load Balancers distribute traffic across EC2 instances, not database instances.
-   [ ]   C. Scaling up the DB instance increases its size but does not separate read and write workloads.
-   [ ]   D. Multi-AZ deployments are for high availability and failover, not for read scaling.

Therefore, using RDS read replicas is the correct solution for offloading reporting queries.
</details>
<details>
  <summary>Question 390</summary>

A company hosts a three-tier ecommerce application on a fleet of Amazon EC2 instances. The instances run in an Auto Scaling group behind an Application Load Balancer (ALB). All ecommerce data is stored in an Amazon RDS for MariaDB Multi-AZ DB instance. The company wants to optimize customer session management during transactions. The application must store session data durably. Which solutions will meet these requirements? (Choose two.)

-   [ ] A. Turn on the sticky sessions feature (session affinity) on the ALB.
-   [ ] B. Use an Amazon DynamoDB table to store customer session information.
-   [ ] C. Deploy an Amazon Cognito user pool to manage user session information.
-   [ ] D. Deploy an Amazon ElastiCache for Redis cluster to store customer session information.
-   [ ] E. Use AWS Systems Manager Application Manager in the application to manage user session information.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Turn on the sticky sessions feature (session affinity) on the ALB.
-   [ ] D. Deploy an Amazon ElastiCache for Redis cluster to store customer session information.

Why these are the correct answers:

A. Turn on the sticky sessions feature (session affinity) on the ALB.

-   [ ]   Sticky sessions ensure that a user's requests are consistently routed to the same EC2 instance.
-   [ ]   This can simplify session management but does not provide durable storage.

D. Deploy an Amazon ElastiCache for Redis cluster to store customer session information.

-   [ ]   ElastiCache provides a fast, in-memory data store for session data.
-   [ ]   Redis offers durability through persistence options.

<hr> Why are the other answers wrong? <hr>

-   [ ]   B. DynamoDB is a NoSQL database and might be overkill for simple session storage.
-   [ ]   C. Cognito is for user authentication and management, not session data storage.
-   [ ]   E. Systems Manager Application Manager is for application management, not session management.

Therefore, sticky sessions and ElastiCache are the most appropriate solutions for session management with durability.
</details>

<details>
  <summary>Question 391</summary>

A company needs a backup strategy for its three-tier stateless web application. The web application runs on Amazon EC2 instances in an Auto Scaling group with a dynamic scaling policy that is configured to respond to scaling events. The database tier runs on Amazon RDS for PostgreSQL. The web application does not require temporary local storage on the EC2 instances. The company's recovery point objective (RPO) is 2 hours.

The backup strategy must maximize scalability and optimize resource utilization for this environment. Which solution will meet these requirements?

-   [ ] A. Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances and database every 2 hours to meet the RPO.
-   [ ] B. Configure a snapshot lifecycle policy to take Amazon Elastic Block Store (Amazon EBS) snapshots. Enable automated backups in Amazon RDS to meet the RPO.
-   [ ] C. Retain the latest Amazon Machine Images (AMIs) of the web and application tiers. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO.
-   [ ] D. Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances every 2 hours. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO.
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. Retain the latest Amazon Machine Images (AMIs) of the web and application tiers. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO.

Why these are the correct answers:

C. Retain the latest Amazon Machine Images (AMIs) of the web and application tiers. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO.

-   [ ]   For stateless web applications, AMIs are sufficient for recovery because the instances do not store persistent data.
-   [ ]   Amazon RDS automated backups with point-in-time recovery allow for granular recovery within the RPO.
-   [ ]   This solution maximizes scalability and optimizes resource utilization by focusing on RDS backups and AMI management.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A, B, and D. Taking EBS snapshots of EC2 instances is unnecessary for stateless applications and inefficient.

Therefore, using AMIs for the web tier and RDS automated backups with point-in-time recovery is the most appropriate solution.
</details>
<details>
  <summary>Question 392</summary>

A company wants to deploy a new public web application on AWS. The application includes a web server tier that uses Amazon EC2 instances. The application also includes a database tier that uses an Amazon RDS for MySQL DB instance. The application must be secure and accessible for global customers that have dynamic IP addresses. How should a solutions architect configure the security groups to meet these requirements?

-   [ ] A. Configure the security group for the web servers to allow inbound traffic on port 443 from 0.0.0.0/0. Configure the security group for the DB instance to allow inbound traffic on port 3306 from the security group of the web servers.
-   [ ] B. Configure the security group for the web servers to allow inbound traffic on port 443 from the IP addresses of the customers. Configure the security group for the DB instance to allow inbound traffic on port 3306 from the security group of the web servers.
-   [ ] C. Configure the security group for the web servers to allow inbound traffic on port 443 from the IP addresses of the customers. Configure the security group for the DB instance to allow inbound traffic on port 3306 from the IP addresses of the customers.
-   [ ] D. Configure the security group for the web servers to allow inbound traffic on port 443 from 0.0.0.0/0. Configure the security group for the DB instance to allow inbound traffic on port 3306 from 0.0.0.0/0.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Configure the security group for the web servers to allow inbound traffic on port 443 from 0.0.0.0/0. Configure the security group for the DB instance to allow inbound traffic on port 3306 from the security group of the web servers.

Why these are the correct answers:

A. Configure the security group for the web servers to allow inbound traffic on port 443 from 0.0.0.0/0. Configure the security group for the DB instance to allow inbound traffic on port 3306 from the security group of the web servers.

-   [ ]   Allowing inbound traffic on port 443 from 0.0.0.0/0 (any IP address) to the web servers enables global customer access.
-   [ ]   Restricting inbound traffic on port 3306 to the DB instance from the web servers' security group enhances security.

<hr> Why are the other answers wrong? <hr>

-   [ ]   B and C. Configuring the web servers' security group to allow traffic only from customer IP addresses is not feasible due to dynamic IP addresses.
-   [ ]   D. Allowing inbound traffic on port 3306 from 0.0.0.0/0 to the DB instance is a security risk.

Therefore, Option A provides the necessary security while allowing global access.
</details>
<details>
  <summary>Question 393</summary>

A payment processing company records all voice communication with its customers and stores the audio files in an Amazon S3 bucket. The company needs to capture the text from the audio files. The company must remove from the text any personally identifiable information (PII) that belongs to customers. What should a solutions architect do to meet these requirements?

-   [ ] A. Process the audio files by using Amazon Kinesis Video Streams. Use an AWS Lambda function to scan for known PII patterns.
-   [ ] B. When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start an Amazon Textract task to analyze the call recordings.
-   [ ] C. Configure an Amazon Transcribe transcription job with PII redaction turned on. When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start the transcription job. Store the output in a separate S3 bucket.
-   [ ] D. Create an Amazon Connect contact flow that ingests the audio files with transcription turned on. Embed an AWS Lambda function to scan for known PII patterns. Use Amazon EventBridge to start the contact flow when an audio file is uploaded to the S3 bucket.
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. Configure an Amazon Transcribe transcription job with PII redaction turned on. When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start the transcription job. Store the output in a separate S3 bucket.

Why these are the correct answers:

C. Configure an Amazon Transcribe transcription job with PII redaction turned on. When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start the transcription job. Store the output in a separate S3 bucket.

-   [ ]   Amazon Transcribe can automatically transcribe audio files and redact PII.
-   [ ]   Using a Lambda function to trigger the transcription job automates the process.
-   [ ]   Storing the output in a separate S3 bucket keeps the original audio and transcribed text separate.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Kinesis Video Streams is for real-time video and audio streaming, not for batch transcription. Using Lambda to scan for PII is less efficient than Transcribe's built-in redaction.
-   [ ]   B. Amazon Textract is for analyzing text and documents, not for transcribing audio.
-   [ ]   D. Amazon Connect is for contact centers, and using EventBridge to start a contact flow for transcription is overly complex.

Therefore, Amazon Transcribe with PII redaction is the most appropriate and efficient solution.
</details>
<details>
  <summary>Question 394</summary>

A company is running a multi-tier ecommerce web application in the AWS Cloud. The application runs on Amazon EC2 instances with an Amazon RDS for MySQL Multi-AZ DB instance. Amazon RDS is configured with the latest generation DB instance with 2,000 GB of storage in a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume. The database performance affects the application during periods of high demand. A database administrator analyzes the logs in Amazon CloudWatch Logs and discovers that the application performance always degrades when the number of read and write IOPS is higher than 20,000. What should a solutions architect do to improve the application performance?

-   [ ] A. Replace the volume with a magnetic volume.
-   [ ] B. Increase the number of IOPS on the gp3 volume.
-   [ ] C. Replace the volume with a Provisioned IOPS SSD (io2) volume.
-   [ ] D. Replace the 2,000 GB gp3 volume with two 1,000 GB gp3 volumes.
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. Replace the volume with a Provisioned IOPS SSD (io2) volume.

Why these are the correct answers:

C. Replace the volume with a Provisioned IOPS SSD (io2) volume.

-   [ ]   Provisioned IOPS SSD (io2) volumes are designed for high IOPS workloads, providing consistent and predictable performance.
-   [ ]   If the application consistently requires more than 20,000 IOPS, io2 volumes are a better choice than gp3.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Magnetic volumes are the slowest and least performant, unsuitable for high-demand applications.
-   [ ]   B. While you can increase IOPS on gp3 volumes, io2 volumes are specifically designed for high-performance database workloads and offer better consistency.
-   [ ]   D. Replacing a large volume with smaller ones does not directly address the IOPS limitation.

Therefore, using io2 volumes is the most appropriate solution to improve performance under high IOPS demands.
</details>
<details>
  <summary>Question 395</summary>

An IAM user made several configuration changes to AWS resources in their company's account during a production deployment last week. A solutions architect learned that a couple of security group rules are not configured as desired. The solutions architect wants to confirm which IAM user was responsible for making changes. Which service should the solutions architect use to find the desired information?

-   [ ] A. Amazon GuardDuty
-   [ ] B. Amazon Inspector
-   [ ] C. AWS CloudTrail
-   [ ] D. AWS Config
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. AWS CloudTrail

Why these are the correct answers:

C. AWS CloudTrail

-   [ ]   AWS CloudTrail records API calls made within an AWS account.
-   [ ]   It provides a detailed log of who made which API calls and when, allowing you to track configuration changes.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Amazon GuardDuty detects malicious activity, not configuration changes.
-   [ ]   B. Amazon Inspector assesses EC2 instances for vulnerabilities, not user actions.
-   [ ]   D. AWS Config tracks resource configurations but does not provide a history of API calls by users.

Therefore, AWS CloudTrail is the appropriate service for auditing user actions.
</details>
<details>
  <summary>Question 396</summary>

A company has implemented a self-managed DNS service on AWS. The solution consists of the following:

* Amazon EC2 instances in different AWS Regions
* Endpoints of a standard accelerator in AWS Global Accelerator

The company wants to protect the solution against DDoS attacks. What should a solutions architect do to meet this requirement?

-   [ ] A. Subscribe to AWS Shield Advanced. Add the accelerator as a resource to protect.
-   [ ] B. Subscribe to AWS Shield Advanced. Add the EC2 instances as resources to protect.
-   [ ] C. Create an AWS WAF web ACL that includes a rate-based rule. Associate the web ACL with the accelerator.
-   [ ] D. Create an AWS WAF web ACL that includes a rate-based rule. Associate the web ACL with the EC2 instances.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Subscribe to AWS Shield Advanced. Add the accelerator as a resource to protect.

Why these are the correct answers:

A. Subscribe to AWS Shield Advanced. Add the accelerator as a resource to protect.

-   [ ]   AWS Shield Advanced provides enhanced DDoS protection for resources like Global Accelerator.
-   [ ]   Protecting the accelerator protects the underlying EC2 instances.

<hr> Why are the other answers wrong? <hr>

-   [ ]   B. Protecting individual EC2 instances is less efficient than protecting the entry point (Global Accelerator).
-   [ ]   C and D. AWS WAF is for protecting web applications from web exploits, not general DDoS protection for network services.

Therefore, AWS Shield Advanced with Global Accelerator protection is the most appropriate solution.
</details>
<details>
  <summary>Question 397</summary>

An ecommerce company needs to run a scheduled daily job to aggregate and filter sales records for analytics. The company stores the sales records in an Amazon S3 bucket. Each object can be up to 10 GB in size. Based on the number of sales events, the job can take up to an hour to complete. The CPU and memory usage of the job are constant and are known in advance. A solutions architect needs to minimize the amount of operational effort that is needed for the job to run. Which solution meets these requirements?

-   [ ] A. Create an AWS Lambda function that has an Amazon EventBridge notification. Schedule the EventBridge event to run once a day.
-   [ ] B. Create an AWS Lambda function. Create an Amazon API Gateway HTTP API, and integrate the API with the function. Create an Amazon EventBridge scheduled event that calls the API and invokes the function.
-   [ ] C. Create an Amazon Elastic Container Service (Amazon ECS) cluster with an AWS Fargate launch type. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job.
-   [ ] D. Create an Amazon Elastic Container Service (Amazon ECS) cluster with an Amazon EC2 launch type and an Auto Scaling group with at least one EC2 instance. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job.
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. Create an Amazon Elastic Container Service (Amazon ECS) cluster with an AWS Fargate launch type. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job.

Why these are the correct answers:

C. Create an Amazon Elastic Container Service (Amazon ECS) cluster with an AWS Fargate launch type. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job.

-   [ ]   Amazon ECS with Fargate allows you to run containers without managing the underlying infrastructure.
-   [ ]   Amazon EventBridge can schedule the ECS task to run daily.
-   [ ]   This solution is suitable for long-running jobs with known resource requirements and minimizes operational overhead.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A and B. Lambda functions have execution time limits and are not suitable for a 1-hour job.
-   [ ]   D. Using EC2 launch type and Auto Scaling group adds operational overhead compared to Fargate.

Therefore, ECS with Fargate and EventBridge is the most appropriate solution.
</details>
<details>
  <summary>Question 398</summary>

A company needs to transfer 600 TB of data from its on-premises network-attached storage (NAS) system to the AWS Cloud. The data transfer must be complete within 2 weeks. The data is sensitive and must be encrypted in transit. The company's internet connection can support an upload speed of 100 Mbps. Which solution meets these requirements MOST cost-effectively?

-   [ ] A. Use Amazon S3 multi-part upload functionality to transfer the files over HTTPS.
-   [ ] B. Create a VPN connection between the on-premises NAS system and the nearest AWS Region. Transfer the data over the VPN connection.
-   [ ] C. Use the AWS Snow Family console to order several AWS Snowball Edge Storage Optimized devices. Use the devices to transfer the data to Amazon S3.
-   [ ] D. Set up a 10 Gbps AWS Direct Connect connection between the company location and the nearest AWS Region. Transfer the data over a VPN connection into the Region to store the data in Amazon S3.
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. Use the AWS Snow Family console to order several AWS Snowball Edge Storage Optimized devices. Use the devices to transfer the data to Amazon S3.

Why these are the correct answers:

C. Use the AWS Snow Family console to order several AWS Snowball Edge Storage Optimized devices. Use the devices to transfer the data to Amazon S3.

-   [ ]   AWS Snowball Edge devices are designed for transferring large amounts of data.
-   [ ]   They are secure and can transfer data faster than over a limited internet connection.
-   [ ]   This is the most cost-effective way to transfer 600 TB within 2 weeks, given the bandwidth constraint.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A and B. Transferring 600 TB over a 100 Mbps connection within 2 weeks is not feasible.
-   [ ]   D. Setting up a 10 Gbps Direct Connect connection is expensive and may not be necessary for a one-time migration.

Therefore, AWS Snowball Edge is the most cost-effective and efficient solution.
</details>
<details>
  <summary>Question 399</summary>

A financial company hosts a web application on AWS. The application uses an Amazon API Gateway Regional API endpoint to give users the ability to retrieve current stock prices. The company's security team has noticed an increase in the number of API requests. The security team is concerned that HTTP flood attacks might take the application offline. A solutions architect must design a solution to protect the application from this type of attack. Which solution meets these requirements with the LEAST operational overhead?

-   [ ] A. Create an Amazon CloudFront distribution in front of the API Gateway Regional API endpoint with a maximum TTL of 24 hours.
-   [ ] B. Create a Regional AWS WAF web ACL with a rate-based rule. Associate the web ACL with the API Gateway stage.
-   [ ] C. Use Amazon CloudWatch metrics to monitor the Count metric and alert the security team when the predefined rate is reached.
-   [ ] D. Create an Amazon CloudFront distribution with Lambda@Edge in front of the API Gateway Regional API endpoint. Create an AWS Lambda function to block requests from IP addresses that exceed the predefined rate.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Create a Regional AWS WAF web ACL with a rate-based rule. Associate the web ACL with the API Gateway stage.

Why these are the correct answers:

B. Create a Regional AWS WAF web ACL with a rate-based rule. Associate the web ACL with the API Gateway stage.

-   [ ]   AWS WAF (Web Application Firewall) can protect API Gateway endpoints from HTTP flood attacks.
-   [ ]   Rate-based rules in WAF allow you to limit the number of requests from a single IP address.
-   [ ]   This solution is managed and has the least operational overhead.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. CloudFront is for content delivery and caching, not for protecting against HTTP flood attacks.
-   [ ]   C. CloudWatch metrics and alerts only notify about the attack but do not prevent it.
-   [ ]   D. Using CloudFront with Lambda@Edge adds complexity and operational overhead.

Therefore, AWS WAF with a rate-based rule is the most appropriate and least operationally intensive solution.
</details>

<details>
  <summary>Question 400</summary>

A meteorological startup company has a custom web application to sell weather data to its users online. The company uses Amazon DynamoDB to store its data and wants to build a new service that sends an alert to the managers of four internal teams every time a new weather event is recorded. The company does not want this new service to affect the performance of the current application. What should a solutions architect do to meet these requirements with the LEAST amount of operational overhead?

-   [ ] A. Use DynamoDB transactions to write new event data to the table. Configure the transactions to notify internal teams.
-   [ ] B. Have the current application publish a message to four Amazon Simple Notification Service (Amazon SNS) topics. Have each team subscribe to one topic.
-   [ ] C. Enable Amazon DynamoDB Streams on the table. Use triggers to write to a single Amazon Simple Notification Service (Amazon SNS) topic to which the teams can subscribe.
-   [ ] D. Add a custom attribute to each record to flag new items. Write a cron job that scans the table every minute for items that are new and notifies an Amazon Simple Queue Service (Amazon SQS) queue to which the teams can subscribe.
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. Enable Amazon DynamoDB Streams on the table. Use triggers to write to a single Amazon Simple Notification Service (Amazon SNS) topic to which the teams can subscribe.

Why these are the correct answers:

C. Enable Amazon DynamoDB Streams on the table. Use triggers to write to a single Amazon Simple Notification Service (Amazon SNS) topic to which the teams can subscribe.

-   [ ]   DynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table and can trigger other AWS services.
-   [ ]   Using a trigger (e.g., AWS Lambda) to process the stream and publish to a single SNS topic simplifies the notification process.
-   [ ]   This approach minimizes the impact on the current application's performance and reduces operational overhead.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. DynamoDB transactions are for ensuring atomicity, consistency, isolation, and durability (ACID) properties during write operations, not for notifications.
-   [ ]   B. Having the application publish to four SNS topics increases complexity and potential performance impact on the application.
-   [ ]   D. Adding a custom attribute and using a cron job to scan the table is inefficient, adds overhead, and can impact performance.

Therefore, DynamoDB Streams with SNS is the most suitable solution for efficient and low-overhead notifications.
</details>



















