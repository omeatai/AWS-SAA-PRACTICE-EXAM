<details>
  <summary>Question 351</summary>

A company is moving its data management application to AWS. The company wants to transition to an event-driven architecture. The architecture needs to be more distributed and to use serverless concepts while performing the different aspects of the workflow. The company also wants to minimize operational overhead.

Which solution will meet these requirements?

-   [ ] A. Build out the workflow in AWS Glue. Use AWS Glue to invoke AWS Lambda functions to process the workflow steps.
-   [ ] B. Build out the workflow in AWS Step Functions. Deploy the application on Amazon EC2 instances. Use Step Functions to invoke the workflow steps on the EC2 instances.
-   [ ] C. Build out the workflow in Amazon EventBridge. Use EventBridge to invoke AWS Lambda functions on a schedule to process the workflow steps.
-   [ ] D. Build out the workflow in AWS Step Functions. Use Step Functions to create a state machine. Use the state machine to invoke AWS Lambda functions to process the workflow steps.
</details>
<details>
  <summary>Answer</summary>

-   [ ] D. Build out the workflow in AWS Step Functions. Use Step Functions to create a state machine. Use the state machine to invoke AWS Lambda functions to process the workflow steps.

Why these are the correct answers:

D. Build out the workflow in AWS Step Functions. Use Step Functions to create a state machine. Use the state machine to invoke AWS Lambda functions to process the workflow steps.

-   [ ]   AWS Step Functions allows you to coordinate multiple AWS services into serverless workflows.
-   [ ]   It enables building distributed, event-driven architectures by orchestrating AWS Lambda functions.
-   [ ]   Using a state machine in Step Functions helps manage the workflow logic.
-   [ ]   This solution minimizes operational overhead by using serverless services.

Why are the other answers wrong?

-   [ ]   A. AWS Glue is primarily for ETL (extract, transform, load) operations, not for orchestrating general application workflows.
-   [ ]   B. Deploying the application on Amazon EC2 instances increases operational overhead and does not align with serverless concepts.
-   [ ]   C. Amazon EventBridge is an event bus service, not a workflow orchestration service like Step Functions; using it on a schedule does not provide the necessary coordination.

Therefore, AWS Step Functions is the most suitable service for building the described event-driven architecture.
</details>
<details>
  <summary>Question 352</summary>

A company is designing the network for an online multi-player game. The game uses the UDP networking protocol and will be deployed in eight AWS Regions. The network architecture needs to minimize latency and packet loss to give end users a high-quality gaming experience.

Which solution will meet these requirements?

-   [ ] A. Setup a transit gateway in each Region. Create inter-Region peering attachments between each transit gateway.
-   [ ] B. Set up AWS Global Accelerator with UDP listeners and endpoint groups in each Region.
-   [ ] C. Set up Amazon CloudFront with UDP turned on. Configure an origin in each Region.
-   [ ] D. Set up a VPC peering mesh between each Region. Turn on UDP for each VPC.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Set up AWS Global Accelerator with UDP listeners and endpoint groups in each Region.

Why these are the correct answers:

B. Set up AWS Global Accelerator with UDP listeners and endpoint groups in each Region.

-   [ ]   AWS Global Accelerator is designed to minimize latency and packet loss for applications, including those using UDP.
-   [ ]   It provides static IP addresses and optimizes network paths to improve performance.
-   [ ]   Using endpoint groups in each Region ensures that traffic is directed to the closest available endpoint.

Why are the other answers wrong?

-   [ ]   A. Transit gateways are for connecting VPCs and on-premises networks, not for optimizing global, low-latency traffic for applications like online games.
-   [ ]   C. Amazon CloudFront is a CDN primarily designed for caching content; while it can accelerate delivery, it's not optimized for real-time UDP traffic like Global Accelerator.
-   [ ]   D. Setting up a full VPC peering mesh is complex and does not provide the same level of global traffic optimization as Global Accelerator.

Therefore, AWS Global Accelerator is the most suitable solution for minimizing latency and packet loss in a global, UDP-based gaming application.
</details>
<details>
  <summary>Question 353</summary>

A company hosts a three-tier web application on Amazon EC2 instances in a single Availability Zone. The web application uses a self-managed MySQL database that is hosted on an EC2 instance to store data in an Amazon Elastic Block Store (Amazon EBS) volume. The MySQL database currently uses a 1 TB Provisioned IOPS SSD (io2) EBS volume. The company expects traffic of 1,000 IOPS for both reads and writes at peak traffic. The company wants to minimize any disruptions, stabilize performance, and reduce costs while retaining the capacity for double the IOPS. The company wants to move the database tier to a fully managed solution that is highly available and fault tolerant. Which solution will meet these requirements MOST cost-effectively?

-   [ ] A. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with an io2 Block Express EBS volume.
-   [ ] B. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with a General Purpose SSD (gp2) EBS volume.
-   [ ] C. Use Amazon S3 Intelligent-Tiering access tiers.
-   [ ] D. Use two large EC2 instances to host the database in active-passive mode.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with a General Purpose SSD (gp2) EBS volume.

Why these are the correct answers:

B. Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with a General Purpose SSD (gp2) EBS volume.

-   [ ]   Amazon RDS for MySQL with a Multi-AZ deployment provides high availability and fault tolerance.
-   [ ]   General Purpose SSD (gp2) volumes offer a good balance of performance and cost-effectiveness for many database workloads.
-   [ ]   It is a fully managed solution, reducing operational overhead.

Why are the other answers wrong?

-   [ ]   A. io2 Block Express volumes are designed for very high IOPS and throughput, which may be more expensive than necessary for the expected 1,000 IOPS.
-   [ ]   C. Amazon S3 is object storage and not suitable for hosting a MySQL database.
-   [ ]   D. Hosting the database on EC2 instances in active-passive mode increases operational overhead and does not provide a fully managed solution.

Therefore, using Amazon RDS for MySQL with a Multi-AZ deployment and gp2 volumes is the most cost-effective and suitable solution.
</details>
<details>
  <summary>Question 354</summary>

A company hosts a serverless application on AWS. The application uses Amazon API Gateway, AWS Lambda, and an Amazon RDS for PostgreSQL database. The company notices an increase in application errors that result from database connection timeouts during times of peak traffic or unpredictable traffic. The company needs a solution that reduces the application failures with the least amount of change to the code. What should a solutions architect do to meet these requirements?

-   [ ] A. Reduce the Lambda concurrency rate.
-   [ ] B. Enable RDS Proxy on the RDS DB instance.
-   [ ] C. Resize the RDS DB instance class to accept more connections.
-   [ ] D. Migrate the database to Amazon DynamoDB with on-demand scaling.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Enable RDS Proxy on the RDS DB instance.

Why these are the correct answers:

B. Enable RDS Proxy on the RDS DB instance.

-   [ ]   Amazon RDS Proxy manages database connections, reducing the impact of connection storms and timeouts.
-   [ ]   It allows serverless applications to scale more efficiently without exhausting database connections.
-   [ ]   Enabling RDS Proxy requires minimal code changes.

Why are the other answers wrong?

-   [ ]   A. Reducing Lambda concurrency can limit the number of function invocations but does not directly address database connection issues.
-   [ ]   C. Resizing the RDS DB instance might increase the number of available connections but is not as efficient as using RDS Proxy for managing connections from serverless functions.
-   [ ]   D. Migrating to Amazon DynamoDB requires significant code changes and is a NoSQL database, which may not be suitable for all application requirements.

Therefore, enabling RDS Proxy is the most suitable solution for addressing database connection timeouts with the least code changes.
</details>
<details>
  <summary>Question 355</summary>

A company is migrating an old application to AWS. The application runs a batch job every hour and is CPU intensive. The batch job takes 15 minutes on average with an on-premises server. The server has 64 virtual CPU (vCPU) and 512 GiB of memory. Which solution will run the batch job within 15 minutes with the LEAST operational overhead?

-   [ ] A. Use AWS Lambda with functional scaling.
-   [ ] B. Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate.
-   [ ] C. Use Amazon Lightsail with AWS Auto Scaling.
-   [ ] D. Use AWS Batch on Amazon EC2.
</details>
<details>
  <summary>Answer</summary>

-   [ ] D. Use AWS Batch on Amazon EC2.

Why these are the correct answers:

D. Use AWS Batch on Amazon EC2.

-   [ ]   AWS Batch is designed for running batch processing workloads efficiently on EC2 instances.
-   [ ]   It can handle CPU-intensive tasks and allows specifying compute resources similar to the on-premises server.
-   [ ]   AWS Batch manages the underlying infrastructure, minimizing operational overhead.

Why are the other answers wrong?

-   [ ]   A. AWS Lambda has execution time limits and is not designed for long-running, CPU-intensive batch jobs.
-   [ ]   B. Amazon ECS with Fargate is suitable for containerized applications but may not be as efficient for simple batch jobs and introduces containerization overhead.
-   [ ]   C. Amazon Lightsail is for simpler applications and does not provide the necessary control and scalability for demanding batch workloads.

Therefore, AWS Batch on Amazon EC2 is the most appropriate solution for running the CPU-intensive batch job with minimal overhead.
</details>
<details>
  <summary>Question 356</summary>

A company stores its data objects in Amazon S3 Standard storage. A solutions architect has found that 75% of the data is rarely accessed after 30 days. The company needs all the data to remain immediately accessible with the same high availability and resiliency, but the company wants to minimize storage costs. Which storage solution will meet these requirements?

-   [ ] A. Move the data objects to S3 Glacier Deep Archive after 30 days.
-   [ ] B. Move the data objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.
-   [ ] C. Move the data objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.
-   [ ] D. Move the data objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) immediately.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Move the data objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.

Why these are the correct answers:

B. Move the data objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.

-   [ ]   S3 Standard-IA is designed for data that is less frequently accessed but requires rapid retrieval when needed.
-   [ ]   It offers high availability and resiliency while reducing storage costs compared to S3 Standard.
-   [ ]   Using a lifecycle policy to move data after 30 days optimizes costs.

Why are the other answers wrong?

-   [ ]   A. S3 Glacier Deep Archive is for long-term archival and has retrieval times that are too long for data that needs to be immediately accessible.
-   [ ]   C and D. S3 One Zone-IA reduces availability and durability because data is stored in a single Availability Zone, which does not meet the requirement for high availability and resiliency.

Therefore, S3 Standard-IA with a lifecycle policy is the most suitable solution for cost-effectively storing data that needs to be immediately accessible but is infrequently used.
</details>
<details>
  <summary>Question 357</summary>

A gaming company is moving its public scoreboard from a data center to the AWS Cloud. The company uses Amazon EC2 Windows Server instances behind an Application Load Balancer to host its dynamic application. The company needs a highly available storage solution for the application. The application consists of static files and dynamic server-side code. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

-   [ ] A. Store the static files on Amazon S3. Use Amazon CloudFront to cache objects at the edge.
-   [ ] B. Store the static files on Amazon S3. Use Amazon ElastiCache to cache objects at the edge.
-   [ ] C. Store the server-side code on Amazon Elastic File System (Amazon EFS). Mount the EFS volume on each EC2 instance to share the files.
-   [ ] D. Store the server-side code on Amazon FSx for Windows File Server. Mount the FSx for Windows File Server volume on each EC2 instance to share the files.
-   [ ] E. Store the server-side code on a General Purpose SSD (gp2) Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume on each EC2 instance to share the files.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Store the static files on Amazon S3. Use Amazon CloudFront to cache objects at the edge.
-   [ ] D. Store the server-side code on Amazon FSx for Windows File Server. Mount the FSx for Windows File Server volume on each EC2 instance to share the files.

Why these are the correct answers:

A. Store the static files on Amazon S3. Use Amazon CloudFront to cache objects at the edge.

-   [ ]   Amazon S3 provides highly available and scalable storage for static files.
-   [ ]   Amazon CloudFront, a CDN, caches these files at edge locations, improving performance.

D. Store the server-side code on Amazon FSx for Windows File Server. Mount the FSx for Windows File Server volume on each EC2 instance to share the files.

-   [ ]   Amazon FSx for Windows File Server provides a fully managed, highly available file system that can be accessed by EC2 instances.
-   [ ]   This allows the dynamic server-side code to be shared across multiple instances.

Why are the other answers wrong?

-   [ ]   B. Amazon ElastiCache is for caching data to improve application speed, not for storing static files.
-   [ ]   C. Amazon EFS is a scalable file storage for use with EC2 instances, but Amazon FSx for Windows File Server is more suitable for Windows-based applications.
-   [ ]   E. Amazon EBS volumes are block storage and are not designed to be shared across multiple EC2 instances in the same way as a file system.

Therefore, storing static files on S3 with CloudFront and using Amazon FSx for Windows File Server for server-side code is the most appropriate solution.
</details>
<details>
  <summary>Question 358</summary>

A social media company runs its application on Amazon EC2 instances behind an Application Load Balancer (ALB). The ALB is the origin for an Amazon CloudFront distribution. The application has more than a billion images stored in an Amazon S3 bucket and processes thousands of images each second. The company wants to resize the images dynamically and serve appropriate formats to clients. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Install an external image management library on an EC2 instance. Use the image management library to process the images.
-   [ ] B. Create a CloudFront origin request policy. Use the policy to automatically resize images and to serve the appropriate format based on the User-Agent HTTP header in the request.
-   [ ] C. Use a Lambda@Edge function with an external image management library. Associate the Lambda@Edge function with the CloudFront behaviors that serve the images.
-   [ ] D. Create a CloudFront response headers policy. Use the policy to automatically resize images and to serve the appropriate format based on the User-Agent HTTP header in the request.
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. Use a Lambda@Edge function with an external image management library. Associate the Lambda@Edge function with the CloudFront behaviors that serve the images.

Why these are the correct answers:

C. Use a Lambda@Edge function with an external image management library. Associate the Lambda@Edge function with the CloudFront behaviors that serve the images.

-   [ ]   Lambda@Edge allows you to run code at CloudFront edge locations, processing images close to users.
-   [ ]   It enables dynamic image resizing and formatting based on client requests with low latency.
-   [ ]   This solution minimizes operational overhead by leveraging serverless functions at the edge.

Why are the other answers wrong?

-   [ ]   A. Installing an image management library on EC2 instances increases operational overhead and does not provide edge processing.
-   [ ]   B and D. CloudFront origin request or response headers policies cannot perform dynamic image resizing or formatting; they are for modifying headers.

Therefore, Lambda@Edge is the most suitable solution for dynamic image processing with minimal operational overhead.
</details>
<details>
  <summary>Question 359</summary>

A hospital needs to store patient records in an Amazon S3 bucket. The hospital's compliance team must ensure that all protected health information (PHI) is encrypted in transit and at rest. The compliance team must administer the encryption key for data at rest.

Which solution will meet these requirements?

-   [ ] A. Create a public SSL/TLS certificate in AWS Certificate Manager (ACM). Associate the certificate with Amazon S3. Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.
-   [ ] B. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with S3 managed encryption keys (SSE-S3). Assign the compliance team to manage the SSE-S3 keys.
-   [ ] C. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.
-   [ ] D. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Use Amazon Macie to protect the sensitive data that is stored in Amazon S3. Assign the compliance team to manage Macie.
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.

Why these are the correct answers:

C. Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.

-   [ ]   The `aws:SecureTransport` condition enforces encryption in transit by requiring HTTPS.
-   [ ]   SSE-KMS encrypts data at rest using AWS KMS, allowing the compliance team to manage the encryption keys.
-   [ ]   This solution meets both encryption in transit and at rest requirements.

Why are the other answers wrong?

-   [ ]   A. Public SSL/TLS certificates are for encrypting data in transit but do not address encryption at rest.
-   [ ]   B. SSE-S3 keys are managed by Amazon, not the compliance team.
-   [ ]   D. Amazon Macie is for discovering and protecting sensitive data, not for encrypting it.

Therefore, enforcing HTTPS and using SSE-KMS with compliance team-managed keys is the correct solution.
</details>
<details>
  <summary>Question 360</summary>

A company uses Amazon API Gateway to run a private gateway with two REST APIs in the same VPC. The BuyStock RESTful web service calls the CheckFunds RESTful web service to ensure that enough funds are available before a stock can be purchased. The company has noticed in the VPC flow logs that the BuyStock RESTful web service calls the CheckFunds RESTful web service over the internet instead of through the VPC. A solutions architect must implement a solution so that the APIs communicate through the VPC. Which solution will meet these requirements with the FEWEST changes to the code?

-   [ ] A. Add an X-API-Key header in the HTTP header for authorization.
-   [ ] B. Use an interface endpoint.
-   [ ] C. Use a gateway endpoint.
-   [ ] D. Add an Amazon Simple Queue Service (Amazon SQS) queue between the two REST APIs.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Use an interface endpoint.

Why these are the correct answers:

B. Use an interface endpoint.

-   [ ]   Interface endpoints use PrivateLink to enable private communication between AWS services and within a VPC without traversing the internet.
-   [ ]   They provide a private IP address within the VPC for accessing the service.
-   [ ]   This solution requires the fewest changes to the code as it primarily involves network configuration.

Why are the other answers wrong?

-   [ ]   A. Adding an X-API-Key header is for authorization, not for routing traffic within the VPC.
-   [ ]   C. Gateway endpoints are for accessing S3 and DynamoDB, not for API Gateway communication within a VPC.
-   [ ]   D. Adding an Amazon SQS queue changes the communication pattern to asynchronous messaging, requiring significant code changes.

Therefore, using an interface endpoint is the most suitable solution for private API communication within a VPC with minimal code changes.
</details>



























