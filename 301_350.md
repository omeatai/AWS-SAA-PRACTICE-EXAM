<details>
  <summary>Question 301</summary>

A university research laboratory needs to migrate 30 TB of data from an on-premises Windows file server to Amazon FSx for Windows File Server. The laboratory has a 1 Gbps network link that many other departments in the university share. The laboratory wants to implement a data migration service that will maximize the performance of the data transfer. However, the laboratory needs to be able to control the amount of bandwidth that the service uses to minimize the impact on other departments. The data migration must take place within the next 5 days.

Which AWS solution will meet these requirements?

- [ ] A. AWS Snowcone
- [ ] B. Amazon FSx File Gateway
- [ ] C. AWS DataSync
- [ ] D. AWS Transfer Family
</details>

<details>
  <summary>Answer</summary>

- [ ] C. AWS DataSync

Why these are the correct answers:

C. AWS DataSync

- [ ] AWS DataSync is designed to efficiently and securely transfer large amounts of data between on-premises storage and AWS storage services.
- [ ] It can control the amount of bandwidth used during the transfer, which is crucial for minimizing the impact on other departments sharing the network link.
- [ ] DataSync can handle the migration of large datasets like 30TB and can be configured to operate within the given time frame.

Why are the other answers wrong?

- [ ] A. AWS Snowcone is a physical, rugged, and secure edge computing and data transfer device. While it's used for data transfer, it doesn't provide the bandwidth control needed and involves physical shipping, which may not meet the 5-day timeframe.
- [ ] B. Amazon FSx File Gateway provides low-latency access to FSx for Windows File Server file shares from on-premises, but it's not the primary service for the initial migration of large datasets.
- [ ] D. AWS Transfer Family is a managed service that supports file transfers into and out of Amazon S3 or Amazon FSx for Windows File Server using protocols like SFTP, FTPS, and FTP. It doesn’t offer the same level of bandwidth control and optimization for large-scale data migration as DataSync.

Therefore, AWS DataSync is the most suitable solution for this scenario.
</details>
<details>
  <summary>Question 302</summary>

A company wants to create a mobile app that allows users to stream slow-motion video clips on their mobile devices. Currently, the app captures video clips and uploads the video clips in raw format into an Amazon S3 bucket. The app retrieves these video clips directly from the S3 bucket. However, the videos are large in their raw format. Users are experiencing issues with buffering and playback on mobile devices. The company wants to implement solutions to maximize the performance and scalability of the app while minimizing operational overhead. Which combination of solutions will meet these requirements? (Choose two.)

- [ ] A. Deploy Amazon CloudFront for content delivery and caching.
- [ ] B. Use AWS DataSync to replicate the video files across AW'S Regions in other S3 buckets.
- [ ] C. Use Amazon Elastic Transcoder to convert the video files to more appropriate formats.
- [ ] D. Deploy an Auto Sealing group of Amazon EC2 instances in Local Zones for content delivery and caching.
- [ ] E. Deploy an Auto Scaling group of Amazon EC2 instances to convert the video files to more appropriate formats.
</details>

<details>
  <summary>Answer</summary>

- [ ] A. Deploy Amazon CloudFront for content delivery and caching.
- [ ] C. Use Amazon Elastic Transcoder to convert the video files to more appropriate formats.

Why these are the correct answers:

A. Deploy Amazon CloudFront for content delivery and caching.

- [ ] Amazon CloudFront is a content delivery network (CDN) that can cache video content closer to users, reducing latency and improving streaming performance.
- [ ] It enhances scalability by distributing the load across multiple edge locations.

C. Use Amazon Elastic Transcoder to convert the video files to more appropriate formats.

- [ ] Amazon Elastic Transcoder converts video files from their original format to formats that are more suitable for streaming on mobile devices, reducing file size and buffering.
- [ ] It minimizes operational overhead by providing a managed service for media transcoding.

Why are the other answers wrong?

- [ ] B. AWS DataSync is used for data transfer, not for optimizing video delivery or format conversion. Replicating files across regions doesn’t address the buffering and playback issues.
- [ ] D. Deploying an Auto Scaling group of Amazon EC2 instances in Local Zones for content delivery and caching is overly complex and expensive compared to using CloudFront, which is specifically designed for this purpose.
- [ ] E. Deploying an Auto Scaling group of EC2 instances to convert video files is a viable option but introduces more operational overhead than using Amazon Elastic Transcoder, a managed service that handles transcoding automatically.

Therefore, Amazon CloudFront and Amazon Elastic Transcoder are the most appropriate solutions.
</details>
<details>
  <summary>Question 303</summary>

A company is launching a new application deployed on an Amazon Elastic Container Service (Amazon ECS) cluster and is using the Fargate launch type for ECS tasks. The company is monitoring CPU and memory usage because it is expecting high traffic to the application upon its launch. However, the company wants to reduce costs when utilization decreases. What should a solutions architect recommend?

- [ ] A. Use Amazon EC2 Auto Scaling to scale at certain periods based on previous traffic patterns.
- [ ] B. Use an AWS Lambda function to scale Amazon ECS based on metric breaches that trigger an Amazon CloudWatch alarm.
- [ ] C. Use Amazon EC2 Auto Scaling with simple scaling policies to scale when ECS metric breaches trigger an Amazon CloudWatch alarm.
- [ ] D. Use AWS Application Auto Scaling with target tracking policies to scale when ECS metric breaches trigger an Amazon CloudWatch alarm.
</details>
<details>
  <summary>Answer</summary>

- [ ] D. Use AWS Application Auto Scaling with target tracking policies to scale when ECS metric breaches trigger an Amazon CloudWatch alarm.

Why these are the correct answers:

D. Use AWS Application Auto Scaling with target tracking policies to scale when ECS metric breaches trigger an Amazon CloudWatch alarm.

- [ ] AWS Application Auto Scaling is designed to scale resources like ECS tasks.
- [ ] Target tracking policies allow you to set a target value for a metric (e.g., CPU utilization), and Auto Scaling automatically adjusts the number of tasks to maintain that target.
- [ ] This approach is more responsive and cost-effective than scaling at fixed periods or using Lambda functions for scaling.

Why are the other answers wrong?

- [ ] A. Amazon EC2 Auto Scaling is for scaling EC2 instances, not ECS tasks using Fargate.
- [ ] B. Using an AWS Lambda function to scale ECS adds complexity and overhead compared to using Application Auto Scaling directly.
- [ ] C. While EC2 Auto Scaling can use CloudWatch alarms, it's not the right service for scaling Fargate tasks; Application Auto Scaling is more suitable and provides target tracking.

Therefore, AWS Application Auto Scaling with target tracking policies is the recommended solution.
</details>
<details>
  <summary>Question 304</summary>

A company recently created a disaster recovery site in a different AWS Region. The company needs to transfer large amounts of data back and forth between NFS file systems in the two Regions on a periodic basis. Which solution will meet these requirements with the LEAST operational overhead?

- [ ] A. Use AWS DataSync.
- [ ] B. Use AWS Snowball devices.
- [ ] C. Set up an SFTP server on Amazon EC2.
- [ ] D. Use AWS Database Migration Service (AWS DMS).
</details>
<details>
  <summary>Answer</summary>

- [ ] A. Use AWS DataSync.

Why these are the correct answers:

A. Use AWS DataSync.

- [ ] AWS DataSync is designed to automate and accelerate the transfer of data between on-premises storage and AWS storage services, or between AWS storage services.
- [ ] It minimizes operational overhead with its automation features.
- [ ] DataSync is efficient for transferring large amounts of data and can handle periodic transfers.

Why are the other answers wrong?

- [ ] B. AWS Snowball devices are physical devices and involve manual shipping, which increases operational overhead and is not suitable for periodic transfers.
- [ ] C. Setting up an SFTP server on Amazon EC2 requires manual management of the server, security, and transfers, leading to higher operational overhead.
- [ ] D. AWS Database Migration Service (AWS DMS) is designed for database migrations, not for general file system transfers.

Therefore, AWS DataSync is the most appropriate solution with the least operational overhead.
</details>
<details>
  <summary>Question 305</summary>

A company is designing a shared storage solution for a gaming application that is hosted in the AWS Cloud. The company needs the ability to use SMB clients to access data. The solution must be fully managed. Which AWS solution meets these requirements?

- [ ] A. Create an AWS DataSync task that shares the data as a mountable file system. Mount the file system to the application server.
- [ ] B. Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the instance. Connect the application server to the file share.
- [ ] C. Create an Amazon FSx for Windows File Server file system. Attach the file system to the origin server. Connect the application server to the file system.
- [ ] D. Create an Amazon S3 bucket. Assign an IAM role to the application to grant access to the S3 bucket. Mount the S3 bucket to the application server.
</details>
<details>
  <summary>Answer</summary>

- [ ] C. Create an Amazon FSx for Windows File Server file system. Attach the file system to the origin server. Connect the application server to the file system.

Why these are the correct answers:

C. Create an Amazon FSx for Windows File Server file system. Attach the file system to the origin server. Connect the application server to the file system.

- [ ] Amazon FSx for Windows File Server provides a fully managed file system that supports the SMB protocol.
- [ ] It eliminates the operational overhead of managing file servers.
- [ ] This solution allows SMB clients to access the data as required.

Why are the other answers wrong?

- [ ] A. AWS DataSync is for data transfer, not for providing a shared file system.
- [ ] B. Creating an EC2 Windows instance and configuring a file share requires manual management of the server, which is not a fully managed solution.
- [ ] D. Amazon S3 is object storage and does not natively support SMB file shares. Mounting an S3 bucket as a file system is not straightforward or performant for applications needing SMB access.

Therefore, Amazon FSx for Windows File Server is the best solution for a fully managed, SMB-compatible shared storage.
</details>
<details>
  <summary>Question 306</summary>

A company wants to run an in-memory database for a latency-sensitive application that runs on Amazon EC2 instances. The application processes more than 100,000 transactions each minute and requires high network throughput. A solutions architect needs to provide a cost-effective network design that minimizes data transfer charges. Which solution meets these requirements?

- [ ] A. Launch all EC2 instances in the same Availability Zone within the same AWS Region. Specify a placement group with cluster strategy when launching EC2 instances.
- [ ] B. Launch all EC2 instances in different Availability Zones within the same AWS Region. Specify a placement group with partition strategy when launching EC2 instances.
- [ ] C. Deploy an Auto Scaling group to launch EC2 instances in different Availability Zones based on a network utilization target.
- [ ] D. Deploy an Auto Scaling group with a step scaling policy to launch EC2 instances in different Availability Zones.
</details>
<details>
  <summary>Answer</summary>

- [ ] A. Launch all EC2 instances in the same Availability Zone within the same AWS Region. Specify a placement group with cluster strategy when launching EC2 instances.

Why these are the correct answers:

A. Launch all EC2 instances in the same Availability Zone within the same AWS Region. Specify a placement group with cluster strategy when launching EC2 instances.

- [ ] Launching EC2 instances in the same Availability Zone minimizes latency and maximizes network throughput.
- [ ] A cluster placement group ensures instances are physically close together, further reducing latency and increasing throughput.
- [ ] This configuration is cost-effective as it reduces data transfer charges between Availability Zones.

Why are the other answers wrong?

- [ ] B. Spreading instances across different Availability Zones increases latency and data transfer costs. A partition placement group is for large distributed applications, not for latency-sensitive in-memory databases.
- [ ] C. Auto Scaling across different Availability Zones adds latency and costs. Scaling based on network utilization doesn't address the need for minimizing latency for in-memory databases.
- [ ] D. Step scaling across Availability Zones also introduces latency and costs, and it's not optimized for the low-latency requirements of an in-memory database.

Therefore, placing instances in the same Availability Zone with a cluster placement group is the best solution.
</details>
<details>
  <summary>Question 307</summary>

A company that primarily runs its application servers on premises has decided to migrate to AWS. The company wants to minimize its need to scale its Internet Small Computer Systems Interface (iSCSI) storage on premises. The company wants only its recently accessed data to remain stored locally. Which AWS solution should the company use to meet these requirements?

- [ ] A. Amazon S3 File Gateway
- [ ] B. AWS Storage Gateway Tape Gateway
- [ ] C. AWS Storage Gateway Volume Gateway stored volumes
- [ ] D. AWS Storage Gateway Volume Gateway cached volumes
</details>
<details>
  <summary>Answer</summary>

- [ ] D. AWS Storage Gateway Volume Gateway cached volumes

Why these are the correct answers:

D. AWS Storage Gateway Volume Gateway cached volumes

- [ ] Volume Gateway in cached mode stores frequently accessed data locally while asynchronously backing up all data to AWS.
- [ ] This minimizes the need to scale on-premises iSCSI storage because only active data is kept locally.
- [ ] It meets the requirement of keeping only recently accessed data stored locally.

Why are the other answers wrong?

- [ ] A. Amazon S3 File Gateway provides a file interface to S3, not iSCSI, and doesn't cache frequently accessed data locally in the same way as Volume Gateway.
- [ ] B. AWS Storage Gateway Tape Gateway is for backup and archival, not for providing low-latency access to recently accessed data.
- [ ] C. AWS Storage Gateway Volume Gateway stored volumes store the entire dataset locally and back it up to AWS, which does not minimize the need for on-premises storage scaling.

Therefore, Volume Gateway cached volumes is the most suitable solution.
</details>
<details>
  <summary>Question 308</summary>

A company has multiple AWS accounts that use consolidated billing. The company runs several active high performance Amazon RDS for Oracle On-Demand DB instances for 90 days. The company's finance team has access to AWS Trusted Advisor in the consolidated billing account and all other AWS accounts. The finance team needs to use the appropriate AWS account to access the Trusted Advisor check recommendations for RDS. The finance team must review the appropriate Trusted Advisor check to reduce RDS costs. Which combination of steps should the finance team take to meet these requirements? (Choose two.)

- [ ] A. Use the Trusted Advisor recommendations from the account where the RDS instances are running.
- [ ] B. Use the Trusted Advisor recommendations from the consolidated billing account to see all RDS instance checks at the same time.
- [ ] C. Review the Trusted Advisor check for Amazon RDS Reserved Instance Optimization.
- [ ] D. Review the Trusted Advisor check for Amazon RDS Idle DB Instances.
- [ ] E. Review the Trusted Advisor check for Amazon Redshift Reserved Node Optimization.
</details>
<details>
  <summary>Answer</summary>

- [ ] B. Use the Trusted Advisor recommendations from the consolidated billing account to see all RDS instance checks at the same time.
- [ ] D. Review the Trusted Advisor check for Amazon RDS Idle DB Instances.

Why these are the correct answers:

B. Use the Trusted Advisor recommendations from the consolidated billing account to see all RDS instance checks at the same time.

- [ ] Consolidated billing accounts provide a centralized view of Trusted Advisor checks across all linked accounts.
- [ ] This allows the finance team to efficiently review RDS costs for all accounts in one place.

D. Review the Trusted Advisor check for Amazon RDS Idle DB Instances.

- [ ] The Idle DB Instances check identifies RDS instances that are not being used, which can lead to cost savings by stopping or deleting them.

Why are the other answers wrong?

- [ ] A. While checking recommendations in individual accounts is possible, it's less efficient than using the consolidated view.
- [ ] C. Reserved Instance Optimization is relevant for long-term cost savings but not for identifying immediate cost reductions from unused instances.
- [ ] E. Redshift Reserved Node Optimization is specific to Amazon Redshift, not Amazon RDS.

Therefore, using the consolidated billing account and checking for idle DB instances are the most appropriate steps.
</details>

<details>
  <summary>Question 309</summary>

A solutions architect needs to optimize storage costs. The solutions architect must identify any Amazon S3 buckets that are no longer being accessed or are rarely accessed. Which solution will accomplish this goal with the LEAST operational overhead?

- [ ] A. Analyze bucket access patterns by using the S3 Storage Lens dashboard for advanced activity metrics.
- [ ] B. Analyze bucket access patterns by using the S3 dashboard in the AWS Management Console.
- [ ] C. Turn on the Amazon CloudWatch BucketSizeBytes metric for buckets. Analyze bucket access patterns by using the metrics data with Amazon Athena.
- [ ] D. Turn on AWS CloudTrail for S3 object monitoring. Analyze bucket access patterns by using CloudTrail logs that are integrated with Amazon CloudWatch Logs.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Analyze bucket access patterns by using the S3 Storage Lens dashboard for advanced activity metrics.

Why these are the correct answers:

A. Analyze bucket access patterns by using the S3 Storage Lens dashboard for advanced activity metrics.

-   [ ] S3 Storage Lens provides a single view of object storage across your organization with advanced metrics and visualizations.
-   [ ] It offers insights into storage usage and activity trends, making it easy to identify inactive or rarely accessed buckets.
-   [ ] This solution has the least operational overhead as it's a managed service designed for this purpose.

Why are the other answers wrong?

-   [ ] B. The S3 dashboard in the AWS Management Console provides basic storage metrics but lacks the advanced analytics and visualization capabilities of S3 Storage Lens.
-   [ ] C. Using CloudWatch metrics and Amazon Athena requires setting up and managing metric collection and writing queries, increasing operational overhead.
-   [ ] D. Using AWS CloudTrail for S3 object monitoring generates detailed logs but requires significant effort to analyze and extract access patterns, leading to high operational overhead.

Therefore, S3 Storage Lens is the most efficient solution for identifying rarely accessed S3 buckets.
</details>
<details>
  <summary>Question 310</summary>

A company sells datasets to customers who do research in artificial intelligence and machine learning (AI/ML). The datasets are large, formatted files that are stored in an Amazon S3 bucket in the us-east-1 Region. The company hosts a web application that the customers use to purchase access to a given dataset. The web application is deployed on multiple Amazon EC2 instances behind an Application Load Balancer. After a purchase is made, customers receive an S3 signed URL that allows access to the files. The customers are distributed across North America and Europe. The company wants to reduce the cost that is associated with data transfers and wants to maintain or improve performance. What should a solutions architect do to meet these requirements?

-   [ ] A. Configure S3 Transfer Acceleration on the existing S3 bucket. Direct customer requests to the S3 Transfer Acceleration endpoint. Continue to use S3 signed URLs for access control.
-   [ ] B. Deploy an Amazon CloudFront distribution with the existing S3 bucket as the origin. Direct customer requests to the CloudFront URL. Switch to CloudFront signed URLs for access control.
-   [ ] C. Set up a second S3 bucket in the eu-central-1 Region with S3 Cross-Region Replication between the buckets. Direct customer requests to the closest Region. Continue to use S3 signed URLs for access control.
-   [ ] D. Modify the web application to enable streaming of the datasets to end users. Configure the web application to read the data from the existing S3 bucket. Implement access control directly in the application.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Deploy an Amazon CloudFront distribution with the existing S3 bucket as the origin. Direct customer requests to the CloudFront URL. Switch to CloudFront signed URLs for access control.

Why these are the correct answers:

B. Deploy an Amazon CloudFront distribution with the existing S3 bucket as the origin. Direct customer requests to the CloudFront URL. Switch to CloudFront signed URLs for access control.

-   [ ] Amazon CloudFront is a content delivery network (CDN) that caches data closer to users, reducing latency and data transfer costs.
-   [ ] Using CloudFront signed URLs provides secure access to the datasets.
-   [ ] This solution improves performance for users in both North America and Europe by delivering the data from edge locations.

Why are the other answers wrong?

-   [ ] A. S3 Transfer Acceleration speeds up transfers over long distances but does not cache data like CloudFront, so it's less effective for reducing costs and improving performance for repeated access.
-   [ ] C. Cross-Region Replication increases storage costs and complexity. While it reduces latency for users in one region, it doesn’t provide the global caching benefits of CloudFront.
-   [ ] D. Modifying the web application for streaming and implementing access control within the application adds significant complexity and operational overhead.

Therefore, Amazon CloudFront is the most efficient solution for reducing costs and improving performance.
</details>
<details>
  <summary>Question 311</summary>

A company is using AWS to design a web application that will process insurance quotes. Users will request quotes from the application. Quotes must be separated by quote type, must be responded to within 24 hours, and must not get lost. The solution must maximize operational efficiency and must minimize maintenance. Which solution meets these requirements?

-   [ ] A. Create multiple Amazon Kinesis data streams based on the quote type. Configure the web application to send messages to the proper data stream. Configure each backend group of application servers to use the Kinesis Client Library (KCL) to pool messages from its own data stream.
-   [ ] B. Create an AWS Lambda function and an Amazon Simple Notification Service (Amazon SNS) topic for each quote type. Subscribe the Lambda function to its associated SNS topic. Configure the application to publish requests for quotes to the appropriate SNS topic.
-   [ ] C. Create a single Amazon Simple Notification Service (Amazon SNS) topic. Subscribe Amazon Simple Queue Service (Amazon SQS) queues to the SNS topic. Configure SNS message filtering to publish messages to the proper SQS queue based on the quote type. Configure each backend application server to use its own SQS queue.
-   [ ] D. Create multiple Amazon Kinesis Data Firehose delivery streams based on the quote type to deliver data streams to an Amazon OpenSearch Service cluster. Configure the application to send messages to the proper delivery stream. Configure each backend group of application servers to search for the messages from OpenSearch Service and process them accordingly.
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. Create a single Amazon Simple Notification Service (Amazon SNS) topic. Subscribe Amazon Simple Queue Service (Amazon SQS) queues to the SNS topic. Configure SNS message filtering to publish messages to the proper SQS queue based on the quote type. Configure each backend application server to use its own SQS queue.

Why these are the correct answers:

C. Create a single Amazon Simple Notification Service (Amazon SNS) topic. Subscribe Amazon Simple Queue Service (Amazon SQS) queues to the SNS topic. Configure SNS message filtering to publish messages to the proper SQS queue based on the quote type. Configure each backend application server to use its own SQS queue.

-   [ ] Amazon SNS allows for decoupled message publishing, and Amazon SQS ensures that messages are not lost.
-   [ ] SNS message filtering enables routing messages to the appropriate SQS queue based on quote type.
-   [ ] This solution maximizes operational efficiency by using managed services and minimizes maintenance.

Why are the other answers wrong?

-   [ ] A. Using Kinesis data streams is more complex and designed for real-time streaming, not for asynchronous processing of requests.
-   [ ] B. Using multiple SNS topics and Lambda functions increases complexity and maintenance overhead.
-   [ ] D. Kinesis Data Firehose and Amazon OpenSearch Service are for data analytics and search, not for reliable message queuing and processing.

Therefore, using SNS with SQS and message filtering is the most suitable solution.
</details>
<details>
  <summary>Question 312</summary>

A company has an application that runs on several Amazon EC2 instances. Each EC2 instance has multiple Amazon Elastic Block Store (Amazon EBS) data volumes attached to it. The application's EC2 instance configuration and data need to be backed up nightly. The application also needs to be recoverable in a different AWS Region. Which solution will meet these requirements in the MOST operationally efficient way?

-   [ ] A. Write an AWS Lambda function that schedules nightly snapshots of the application's EBS volumes and copies the snapshots to a different Region.
-   [ ] B. Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to another Region. Add the application's EC2 instances as resources.
-   [ ] C. Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to another Region. Add the application's EBS volumes as resources.
-   [ ] D. Write an AWS Lambda function that schedules nightly snapshots of the application's EBS volumes and copies the snapshots to a different Availability Zone.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to another Region. Add the application's EC2 instances as resources.

Why these are the correct answers:

B. Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to another Region. Add the application's EC2 instances as resources.

-   [ ] AWS Backup centralizes backup management and automates backups across AWS services.
-   [ ] Backing up EC2 instances includes all attached EBS volumes.
-   [ ] Copying backups to another Region supports disaster recovery.
-   [ ] This solution is the most operationally efficient as it minimizes the need for custom scripting.

Why are the other answers wrong?

-   [ ] A. Writing a Lambda function for backups increases operational overhead compared to using AWS Backup.
-   [ ] C. While backing up EBS volumes is an option, backing up the EC2 instances simplifies the process and ensures the entire instance configuration is included.
-   [ ] D. Backing up to a different Availability Zone does not provide disaster recovery to a different AWS Region.

Therefore, using AWS Backup to back up the EC2 instances and copying the backups to another Region is the most efficient solution.
</details>
<details>
  <summary>Question 313</summary>

A company is building a mobile app on AWS. The company wants to expand its reach to millions of users. The company needs to build a platform so that authorized users can watch the company's content on their mobile devices. What should a solutions architect recommend to meet these requirements?

-   [ ] A. Publish content to a public Amazon S3 bucket. Use AWS Key Management Service (AWS KMS) keys to stream content.
-   [ ] B. Set up IPsec VPN between the mobile app and the AWS environment to stream content.
-   [ ] C. Use Amazon CloudFront. Provide signed URLs to stream content.
-   [ ] D. Set up AWS Client VPN between the mobile app and the AWS environment to stream content.
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. Use Amazon CloudFront. Provide signed URLs to stream content.

Why these are the correct answers:

C. Use Amazon CloudFront. Provide signed URLs to stream content.

-   [ ] Amazon CloudFront is a content delivery network (CDN) that efficiently distributes content to a large number of users with low latency.
-   [ ] Signed URLs provide secure access to the content.
-   [ ] This solution is scalable and suitable for streaming content to millions of users.

Why are the other answers wrong?

-   [ ] A. Using a public S3 bucket is not secure, and AWS KMS is for encryption, not for streaming content.
-   [ ] B. Setting up an IPsec VPN is complex, not scalable for millions of users, and adds latency.
-   [ ] D. AWS Client VPN is for secure access to AWS resources, not for content delivery to end-users.

Therefore, using Amazon CloudFront with signed URLs is the most appropriate solution.
</details>
<details>
  <summary>Question 314</summary>

A company has an on-premises MySQL database used by the global sales team with infrequent access patterns. The sales team requires the database to have minimal downtime. A database administrator wants to migrate this database to AWS without selecting a particular instance type in anticipation of more users in the future. Which service should a solutions architect recommend?

-   [ ] A. Amazon Aurora MySQL
-   [ ] B. Amazon Aurora Serverless for MySQL
-   [ ] C. Amazon Redshift Spectrum
-   [ ] D. Amazon RDS for MySQL
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Amazon Aurora Serverless for MySQL

Why these are the correct answers:

B. Amazon Aurora Serverless for MySQL

-   [ ] Amazon Aurora Serverless automatically starts up, shuts down, and scales capacity based on application needs, which is ideal for infrequent access patterns.
-   [ ] It provides minimal downtime and eliminates the need to select a specific instance type.
-   [ ] This service is cost-effective because you only pay for the database capacity you use.

Why are the other answers wrong?

-   [ ] A. Amazon Aurora MySQL requires provisioning and managing database instances, which is not optimal for infrequent access.
-   [ ] C. Amazon Redshift Spectrum is for querying data in S3, not for hosting a MySQL database.
-   [ ] D. Amazon RDS for MySQL requires selecting and managing database instances, which is not suitable for the requirement of not selecting an instance type.

Therefore, Amazon Aurora Serverless for MySQL is the most appropriate service.
</details>
<details>
  <summary>Question 315</summary>

A company experienced a breach that affected several applications in its on-premises data center. The attacker took advantage of vulnerabilities in the custom applications that were running on the servers. The company is now migrating its applications to run on Amazon EC2 instances. The company wants to implement a solution that actively scans for vulnerabilities on the EC2 instances and sends a report that details the findings. Which solution will meet these requirements?

-   [ ] A. Deploy AWS Shield to scan the EC2 instances for vulnerabilities. Create an AWS Lambda function to log any findings to AWS CloudTrail.
-   [ ] B. Deploy Amazon Macie and AWS Lambda functions to scan the EC2 instances for vulnerabilities. Log any findings to AWS CloudTrail.
-   [ ] C. Turn on Amazon GuardDuty. Deploy the GuardDuty agents to the EC2 instances. Configure an AWS Lambda function to automate the generation and distribution of reports that detail the findings.
-   [ ] D. Turn on Amazon Inspector. Deploy the Amazon Inspector agent to the EC2 instances. Configure an AWS Lambda function to automate the generation and distribution of reports that detail the findings.
</details>
<details>
  <summary>Answer</summary>

-   [ ] D. Turn on Amazon Inspector. Deploy the Amazon Inspector agent to the EC2 instances. Configure an AWS Lambda function to automate the generation and distribution of reports that detail the findings.

Why these are the correct answers:

D. Turn on Amazon Inspector. Deploy the Amazon Inspector agent to the EC2 instances. Configure an AWS Lambda function to automate the generation and distribution of reports that detail the findings.

-   [ ] Amazon Inspector is designed to analyze the security of EC2 instances and identify vulnerabilities.
-   [ ] Deploying the Inspector agent on the EC2 instances allows for in-depth scanning.
-   [ ] Using a Lambda function to automate report generation and distribution meets the requirement for detailed findings reports.

Why are the other answers wrong?

-   [ ] A. AWS Shield protects against DDoS attacks, not application vulnerabilities. CloudTrail logs API calls, not vulnerabilities within EC2 instances.
-   [ ] B. Amazon Macie discovers and protects sensitive data, not application vulnerabilities.
-   [ ] C. Amazon GuardDuty detects malicious activity and unauthorized behavior, not application vulnerabilities.

Therefore, Amazon Inspector is the most appropriate solution for scanning EC2 instances for vulnerabilities.
</details>
<details>
  <summary>Question 316</summary>

A company uses an Amazon EC2 instance to run a script to poll for and process messages in an Amazon Simple Queue Service (Amazon SQS) queue. The company wants to reduce operational costs while maintaining its ability to process a growing number of messages that are added to the queue. What should a solutions architect recommend to meet these requirements?

-   [ ] A. Increase the size of the EC2 instance to process messages faster.
-   [ ] B. Use Amazon EventBridge to turn off the EC2 instance when the instance is underutilized.
-   [ ] C. Migrate the script on the EC2 instance to an AWS Lambda function with the appropriate runtime.
-   [ ] D. Use AWS Systems Manager Run Command to run the script on demand.
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. Migrate the script on the EC2 instance to an AWS Lambda function with the appropriate runtime.

Why these are the correct answers:

C. Migrate the script on the EC2 instance to an AWS Lambda function with the appropriate runtime.

-   [ ] AWS Lambda allows you to run code without provisioning or managing servers, reducing operational costs.
-   [ ] Lambda scales automatically with the number of messages in the SQS queue, efficiently handling a growing workload.
-   [ ] This approach eliminates the need to pay for a continuously running EC2 instance.

Why are the other answers wrong?

-   [ ] A. Increasing the size of the EC2 instance increases costs, not reduces them.
-   [ ] B. Using Amazon EventBridge to turn off the EC2 instance may save costs during idle times but doesn’t optimize processing efficiency.
-   [ ] D. AWS Systems Manager Run Command is for managing EC2 instances, not for cost-effectively processing SQS messages.

Therefore, migrating the script to an AWS Lambda function is the most cost-effective solution.
</details>

<details>
  <summary>Question 317</summary>

A company uses a legacy application to produce data in CSV format. The legacy application stores the output data in Amazon S3. The company is deploying a new commercial off-the-shelf (COTS) application that can perform complex SQL queries to analyze data that is stored in Amazon Redshift and Amazon S3 only. However, the COTS application cannot process the .csv files that the legacy application produces. The company cannot update the legacy application to produce data in another format. The company needs to implement a solution so that the COTS application can use the data that the legacy application produces. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Create an AWS Glue extract, transform, and load (ETL) job that runs on a schedule. Configure the ETL job to process the .csv files and store the processed data in Amazon Redshift.
-   [ ] B. Develop a Python script that runs on Amazon EC2 instances to convert the .csv files to .sql files. Invoke the Python script on a cron schedule to store the output files in Amazon S3.
-   [ ] C. Create an AWS Lambda function and an Amazon DynamoDB table. Use an S3 event to invoke the Lambda function. Configure the Lambda function to perform an extract, transform, and load (ETL) job to process the .csv files and store the processed data in the DynamoDB table.
-   [ ] D. Use Amazon EventBridge to launch an Amazon EMR cluster on a weekly schedule. Configure the EMR cluster to perform an extract, transform, and load (ETL) job to process the .csv files and store the processed data in an Amazon Redshift table.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Create an AWS Glue extract, transform, and load (ETL) job that runs on a schedule. Configure the ETL job to process the .csv files and store the processed data in Amazon Redshift.

Why these are the correct answers:

A. Create an AWS Glue extract, transform, and load (ETL) job that runs on a schedule. Configure the ETL job to process the .csv files and store the processed data in Amazon Redshift.

-   [ ] AWS Glue is a fully managed ETL service that simplifies the process of preparing and loading data for analytics.
-   [ ] It can natively process CSV files and transform the data to be compatible with Amazon Redshift.
-   [ ] Scheduling the Glue job automates the data transformation process with minimal operational overhead.

Why are the other answers wrong?

-   [ ] B. Developing and managing Python scripts on EC2 instances requires more operational overhead than using AWS Glue, a managed service.
-   [ ] C. Using AWS Lambda and Amazon DynamoDB is not suitable for complex SQL queries and is not as efficient for ETL tasks as AWS Glue.
-   [ ] D. Amazon EMR is designed for big data processing and is overkill for simple CSV transformations, leading to higher costs and operational overhead.

Therefore, AWS Glue provides the most efficient and least operationally intensive solution.
</details>
<details>
  <summary>Question 318</summary>

A company recently migrated its entire IT environment to the AWS Cloud. The company discovers that users are provisioning oversized Amazon EC2 instances and modifying security group rules without using the appropriate change control process. A solutions architect must devise a strategy to track and audit these inventory and configuration changes. Which actions should the solutions architect take to meet these requirements? (Choose two.)

-   [ ] A. Enable AWS CloudTrail and use it for auditing.
-   [ ] B. Use data lifecycle policies for the Amazon EC2 instances.
-   [ ] C. Enable AWS Trusted Advisor and reference the security dashboard.
-   [ ] D. Enable AWS Config and create rules for auditing and compliance purposes.
-   [ ] E. Restore previous resource configurations with an AWS CloudFormation template.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Enable AWS CloudTrail and use it for auditing.
-   [ ] D. Enable AWS Config and create rules for auditing and compliance purposes.

Why these are the correct answers:

A. Enable AWS CloudTrail and use it for auditing.

-   [ ] AWS CloudTrail records API calls made within an AWS account, providing a detailed audit trail of actions taken by users.
-   [ ] This helps in tracking who provisioned resources and modified security group rules.

D. Enable AWS Config and create rules for auditing and compliance purposes.

-   [ ] AWS Config monitors resource configurations and can detect deviations from desired settings.
-   [ ] It allows for the creation of rules to audit changes and ensure compliance with change control processes.

Why are the other answers wrong?

-   [ ] B. Data lifecycle policies manage the lifecycle of objects in S3, not EC2 instance configurations or security group rules.
-   [ ] C. AWS Trusted Advisor provides recommendations on cost optimization, performance, and security but does not track configuration changes in the same way as AWS Config.
-   [ ] E. CloudFormation templates are for infrastructure as code and not for auditing changes made outside of CloudFormation.

Therefore, AWS CloudTrail and AWS Config are the most appropriate services for tracking and auditing changes.
</details>
<details>
  <summary>Question 319</summary>

A company has hundreds of Amazon EC2 Linux-based instances in the AWS Cloud. Systems administrators have used shared SSH keys to manage the instances. After a recent audit, the company's security team is mandating the removal of all shared keys. A solutions architect must design a solution that provides secure access to the EC2 instances. Which solution will meet this requirement with the LEAST amount of administrative overhead?

-   [ ] A. Use AWS Systems Manager Session Manager to connect to the EC2 instances.
-   [ ] B. Use AWS Security Token Service (AWS STS) to generate one-time SSH keys on demand.
-   [ ] C. Allow shared SSH access to a set of bastion instances. Configure all other instances to allow only SSH access from the bastion instances.
-   [ ] D. Use an Amazon Cognito custom authorizer to authenticate users. Invoke an AWS Lambda function to generate a temporary SSH key.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Use AWS Systems Manager Session Manager to connect to the EC2 instances.

Why these are the correct answers:

A. Use AWS Systems Manager Session Manager to connect to the EC2 instances.

-   [ ] AWS Systems Manager Session Manager allows you to manage EC2 instances through a browser-based shell or the AWS CLI, without the need for SSH keys.
-   [ ] It provides secure access with detailed logging and reduces administrative overhead by eliminating the need for key management.

Why are the other answers wrong?

-   [ ] B. Generating one-time SSH keys using AWS STS adds complexity and overhead in key management.
-   [ ] C. Using bastion instances still involves managing SSH access to those instances and adds network complexity.
-   [ ] D. Using Amazon Cognito and Lambda to generate temporary SSH keys is overly complex and adds significant overhead.

Therefore, AWS Systems Manager Session Manager is the simplest and most efficient solution.
</details>
<details>
  <summary>Question 320</summary>

A company is using a fleet of Amazon EC2 instances to ingest data from on-premises data sources. The data is in JSON format and ingestion rates can be as high as 1 GB/s. When an EC2 instance is rebooted, the data in-flight is lost. The company's data science team wants to query ingested data in near-real time. Which solution provides near-real-time data querying that is scalable with minimal data loss?

-   [ ] A. Publish data to Amazon Kinesis Data Streams, Use Kinesis Data Analytics to query the data.
-   [ ] B. Publish data to Amazon Kinesis Data Firehose with Amazon Redshift as the destination. Use Amazon Redshift to query the data.
-   [ ] C. Store ingested data in an EC2 instance store. Publish data to Amazon Kinesis Data Firehose with Amazon S3 as the destination. Use Amazon Athena to query the data.
-   [ ] D. Store ingested data in an Amazon Elastic Block Store (Amazon EBS) volume. Publish data to Amazon ElastiCache for Redis. Subscribe to the Redis channel to query the data.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Publish data to Amazon Kinesis Data Streams, Use Kinesis Data Analytics to query the data.

Why these are the correct answers:

A. Publish data to Amazon Kinesis Data Streams, Use Kinesis Data Analytics to query the data.

-   [ ] Amazon Kinesis Data Streams can handle high-volume, real-time data ingestion.
-   [ ] Kinesis Data Analytics allows for querying and processing streaming data in real-time with minimal data loss.
-   [ ] This combination provides scalability and near-real-time querying capabilities.

Why are the other answers wrong?

-   [ ] B. Kinesis Data Firehose is for delivering streaming data to destinations like Amazon Redshift, not for real-time querying. Redshift is not designed for near-real-time queries on ingested data.
-   [ ] C. EC2 instance store is ephemeral and data is lost on reboot. Kinesis Data Firehose with S3 and Athena is for batch processing and querying data at rest, not near-real-time.
-   [ ] D. EBS volumes are persistent but do not provide real-time querying capabilities. Amazon ElastiCache for Redis is an in-memory data store, not designed for ingesting and querying large volumes of streaming data.

Therefore, Kinesis Data Streams with Kinesis Data Analytics is the most suitable solution.
</details>

<details>
  <summary>Question 321</summary>

What should a solutions architect do to ensure that all objects uploaded to an Amazon S3 bucket are encrypted?

-   [ ] A. Update the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set.
-   [ ] B. Update the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set to private.
-   [ ] C. Update the bucket policy to deny if the PutObject does not have an aws:Secure Transport header set to true.
-   [ ] D. Update the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption header set.
</details>
<details>
  <summary>Answer</summary>

-   [ ] D. Update the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption header set.

Why these are the correct answers:

D. Update the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption header set.

-   [ ] The `x-amz-server-side-encryption` header is used to request server-side encryption when uploading objects to S3.
-   [ ] Denying uploads that do not include this header ensures that all objects are encrypted.
-   [ ] This is the most direct way to enforce encryption at the bucket level.

Why are the other answers wrong?

-   [ ] A and B. The `s3:x-amz-acl` header is related to access control lists (ACLs), not encryption.
-   [ ] C. The `aws:SecureTransport` header is related to requiring secure transport (HTTPS), not server-side encryption of objects.

Therefore, enforcing the `x-amz-server-side-encryption` header is the correct method to ensure all objects are encrypted.
</details>
<details>
  <summary>Question 322</summary>

A solutions architect is designing a multi-tier application for a company. The application's users upload images from a mobile device. The application generates a thumbnail of each image and returns a message to the user to confirm that the image was uploaded successfully. The thumbnail generation can take up to 60 seconds, but the company wants to provide a faster response time to its users to notify them that the original image was received. The solutions architect must design the application to asynchronously dispatch requests to the different application tiers. What should the solutions architect do to meet these requirements?

-   [ ] A. Write a custom AWS Lambda function to generate the thumbnail and alert the user. Use the image upload process as an event source to invoke the Lambda function.
-   [ ] B. Create an AWS Step Functions workflow. Configure Step Functions to handle the orchestration between the application tiers and alert the user when thumbnail generation is complete.
-   [ ] C. Create an Amazon Simple Queue Service (Amazon SQS) message queue. As images are uploaded, place a message on the SQS queue for thumbnail generation. Alert the user through an application message that the image was received.
-   [ ] D. Create Amazon Simple Notification Service (Amazon SNS) notification topics and subscriptions. Use one subscription with the application to generate the thumbnail after the image upload is complete. Use a second subscription to message the user's mobile app by way of a push notification after thumbnail generation is complete.
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. Create an Amazon Simple Queue Service (Amazon SQS) message queue. As images are uploaded, place a message on the SQS queue for thumbnail generation. Alert the user through an application message that the image was received.

Why these are the correct answers:

C. Create an Amazon Simple Queue Service (Amazon SQS) message queue. As images are uploaded, place a message on the SQS queue for thumbnail generation. Alert the user through an application message that the image was received.

-   [ ] Amazon SQS allows for decoupling the image upload process from the thumbnail generation process.
-   [ ] The application can quickly acknowledge the image upload to the user and then asynchronously process the thumbnail generation.
-   [ ] This approach provides a faster response time to the user while ensuring the thumbnail is generated.

Why are the other answers wrong?

-   [ ] A. Using Lambda directly coupled with the upload process does not provide asynchronous processing and may still delay the response to the user.
-   [ ] B. AWS Step Functions is more suited for complex workflows, not just simple asynchronous processing of image uploads.
-   [ ] D. Amazon SNS is for pub/sub messaging, not for queuing tasks. It doesn't guarantee message processing like SQS.

Therefore, Amazon SQS is the most suitable service for asynchronous processing in this scenario.
</details>
<details>
  <summary>Question 323</summary>

A company's facility has badge readers at every entrance throughout the building. When badges are scanned, the readers send a message over HTTPS to indicate who attempted to access that particular entrance. A solutions architect must design a system to process these messages from the sensors. The solution must be highly available, and the results must be made available for the company's security team to analyze. Which system architecture should the solutions architect recommend?

-   [ ] A. Launch an Amazon EC2 instance to serve as the HTTPS endpoint and to process the messages. Configure the EC2 instance to save the results to an Amazon S3 bucket.
-   [ ] B. Create an HTTPS endpoint in Amazon API Gateway. Configure the API Gateway endpoint to invoke an AWS Lambda function to process the messages and save the results to an Amazon DynamoDB table.
-   [ ] C. Use Amazon Route 53 to direct incoming sensor messages to an AWS Lambda function. Configure the Lambda function to process the messages and save the results to an Amazon DynamoDB table.
-   [ ] D. Create a gateway VPC endpoint for Amazon S3. Configure a Site-to-Site VPN connection from the facility network to the VPC so that sensor data can be written directly to an S3 bucket by way of the VPC endpoint.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Create an HTTPS endpoint in Amazon API Gateway. Configure the API Gateway endpoint to invoke an AWS Lambda function to process the messages and save the results to an Amazon DynamoDB table.

Why these are the correct answers:

B. Create an HTTPS endpoint in Amazon API Gateway. Configure the API Gateway endpoint to invoke an AWS Lambda function to process the messages and save the results to an Amazon DynamoDB table.

-   [ ] Amazon API Gateway provides a scalable and secure HTTPS endpoint.
-   [ ] AWS Lambda allows for serverless processing of the messages.
-   [ ] Amazon DynamoDB is a highly available and scalable NoSQL database to store the results.
-   [ ] This architecture is highly available and suitable for real-time processing and analysis.

Why are the other answers wrong?

-   [ ] A. Using a single EC2 instance introduces a single point of failure and is not as scalable or highly available as API Gateway and Lambda.
-   [ ] C. Amazon Route 53 is a DNS service and cannot directly receive and process HTTPS requests.
-   [ ] D. Using a VPC endpoint and Site-to-Site VPN is overly complex for this scenario and does not provide the processing capabilities needed.

Therefore, API Gateway, Lambda, and DynamoDB provide the best architecture for this use case.
</details>
<details>
  <summary>Question 324</summary>

A company wants to implement a disaster recovery plan for its primary on-premises file storage volume. The file storage volume is mounted from an Internet Small Computer Systems Interface (iSCSI) device on a local storage server. The file storage volume holds hundreds of terabytes (TB) of data. The company wants to ensure that end users retain immediate access to all file types from the on-premises systems without experiencing latency. Which solution will meet these requirements with the LEAST amount of change to the company's existing infrastructure?

-   [ ] A. Provision an Amazon S3 File Gateway as a virtual machine (VM) that is hosted on premises. Set the local cache to 10 TB. Modify existing applications to access the files through the NFS protocol. To recover from a disaster, provision an Amazon EC2 instance and mount the S3 bucket that contains the files.
-   [ ] B. Provision an AWS Storage Gateway tape gateway. Use a data backup solution to back up all existing data to a virtual tape library. Configure the data backup solution to run nightly after the initial backup is complete. To recover from a disaster, provision an Amazon EC2 instance and restore the data to an Amazon Elastic Block Store (Amazon EBS) volume from the volumes in the virtual tape library.
-   [ ] C. Provision an AWS Storage Gateway Volume Gateway cached volume. Set the local cache to 10 TB. Mount the Volume Gateway cached volume to the existing file server by using iSCSI, and copy all files to the storage volume. Configure scheduled snapshots of the storage volume. To recover from a disaster, restore a snapshot to an Amazon Elastic Block Store (Amazon EBS) volume and attach the EBS volume to an Amazon EC2 instance.
-   [ ] D. Provision an AWS Storage Gateway Volume Gateway stored volume with the same amount of disk space as the existing file storage volume. Mount the Volume Gateway stored volume to the existing file server by using iSCSI, and copy all files to the storage volume. Configure scheduled snapshots of the storage volume. To recover from a disaster, restore a snapshot to an Amazon Elastic Block Store (Amazon EBS) volume and attach the EBS volume to an Amazon EC2 instance.
</details>
<details>
  <summary>Answer</summary>

-   [ ] D. Provision an AWS Storage Gateway Volume Gateway stored volume with the same amount of disk space as the existing file storage volume. Mount the Volume Gateway stored volume to the existing file server by using iSCSI, and copy all files to the storage volume. Configure scheduled snapshots of the storage volume. To recover from a disaster, restore a snapshot to an Amazon Elastic Block Store (Amazon EBS) volume and attach the EBS volume to an Amazon EC2 instance.

Why these are the correct answers:

D. Provision an AWS Storage Gateway Volume Gateway stored volume with the same amount of disk space as the existing file storage volume. Mount the Volume Gateway stored volume to the existing file server by using iSCSI, and copy all files to the storage volume. Configure scheduled snapshots of the storage volume. To recover from a disaster, restore a snapshot to an Amazon Elastic Block Store (Amazon EBS) volume and attach the EBS volume to an Amazon EC2 instance.

-   [ ] Volume Gateway stored volumes store the entire dataset locally and provide low-latency access, meeting the requirement for immediate access without latency.
-   [ ] Using iSCSI allows for minimal changes to the existing infrastructure.
-   [ ] Snapshots provide a mechanism for disaster recovery by allowing restoration to an EBS volume in AWS.

Why are the other answers wrong?

-   [ ] A. S3 File Gateway uses NFS, requiring changes to the application, and the local cache may not be sufficient for immediate access to all files.
-   [ ] B. Tape Gateway is for backup and archival, not for providing immediate access to files. Restoration from tape is time-consuming.
-   [ ] C. Volume Gateway cached volumes only store a subset of data locally, which may not provide immediate access to all files.

Therefore, Volume Gateway stored volumes provide the best solution for immediate access and minimal changes.
</details>
<details>
  <summary>Question 325</summary>

A company is hosting a web application from an Amazon S3 bucket. The application uses Amazon Cognito as an identity provider to authenticate users and return a JSON Web Token (JWT) that provides access to protected resources that are stored in another S3 bucket. Upon deployment of the application, users report errors and are unable to access the protected content. A solutions architect must resolve this issue by providing proper permissions so that users can access the protected content. Which solution meets these requirements?

-   [ ] A. Update the Amazon Cognito identity pool to assume the proper IAM role for access to the protected content.
-   [ ] B. Update the S3 ACL to allow the application to access the protected content.
-   [ ] C. Redeploy the application to Amazon S3 to prevent eventually consistent reads in the S3 bucket from affecting the ability of users to access the protected content.
-   [ ] D. Update the Amazon Cognito pool to use custom attribute mappings within the identity pool and grant users the proper permissions to access the protected content.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Update the Amazon Cognito identity pool to assume the proper IAM role for access to the protected content.

Why these are the correct answers:

A. Update the Amazon Cognito identity pool to assume the proper IAM role for access to the protected content.

-   [ ] Amazon Cognito identity pools provide AWS credentials to users so they can access AWS resources.
-   [ ] IAM roles define the permissions for these users.
-   [ ] Updating the identity pool to assume the correct IAM role grants users the necessary permissions to access the protected S3 content.

Why are the other answers wrong?

-   [ ] B. S3 ACLs are an older access control mechanism and are not as flexible or secure as IAM roles. They are not the recommended way to grant access to S3 resources in this scenario.
-   [ ] C. Redeploying the application to S3 does not address the permission issue. Eventually consistent reads are a characteristic of S3 but are not the cause of the access problem.
-   [ ] D. Custom attribute mappings in Cognito are for passing user attributes, not for granting AWS resource access permissions.

Therefore, updating the Cognito identity pool with the correct IAM role is the appropriate solution.
</details>
<details>
  <summary>Question 326</summary>

An image hosting company uploads its large assets to Amazon S3 Standard buckets. The company uses multipart upload in parallel by using S3 APIs and overwrites if the same object is uploaded again. For the first 30 days after upload, the objects will be accessed frequently. The objects will be used less frequently after 30 days, but the access patterns for each object will be inconsistent. The company must optimize its S3 storage costs while maintaining high availability and resiliency of stored assets. Which combination of actions should a solutions architect recommend to meet these requirements? (Choose two.)

-   [ ] A. Move assets to S3 Intelligent-Tiering after 30 days.
-   [ ] B. Configure an S3 Lifecycle policy to clean up incomplete multipart uploads.
-   [ ] C. Configure an S3 Lifecycle policy to clean up expired object delete markers.
-   [ ] D. Move assets to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.
-   [ ] E. Move assets to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Move assets to S3 Intelligent-Tiering after 30 days.
-   [ ] B. Configure an S3 Lifecycle policy to clean up incomplete multipart uploads.

Why these are the correct answers:

A. Move assets to S3 Intelligent-Tiering after 30 days.

-   [ ] S3 Intelligent-Tiering automatically optimizes storage costs by moving data to the most cost-effective tier based on access patterns.
-   [ ] It's suitable for data with inconsistent access patterns, ensuring cost savings while maintaining performance.

B. Configure an S3 Lifecycle policy to clean up incomplete multipart uploads.

-   [ ] Multipart uploads that are not completed can consume storage.
-   [ ] Cleaning up these incomplete uploads helps to optimize storage costs.

Why are the other answers wrong?

-   [ ] C. Expired object delete markers are automatically cleaned up by S3 and do not require a lifecycle policy.
-   [ ] D. S3 Standard-IA is suitable for less frequently accessed data but does not automatically optimize for changing access patterns like S3 Intelligent-Tiering.
-   [ ] E. S3 One Zone-IA is cheaper but reduces availability and resiliency, which is not suitable for this company's requirements.

Therefore, S3 Intelligent-Tiering and cleaning up incomplete multipart uploads are the best choices.
</details>
<details>
  <summary>Question 327</summary>

A solutions architect must secure a VPC network that hosts Amazon EC2 instances. The EC2 instances contain highly sensitive data and run in a private subnet. According to company policy, the EC2 instances that run in the VPC can access only approved third-party software repositories on the internet for software product updates that use the third party's URL. Other internet traffic must be blocked. Which solution meets these requirements?

-   [ ] A. Update the route table for the private subnet to route the outbound traffic to an AWS Network Firewall firewall. Configure domain list rule groups.
-   [ ] B. Set up an AWS WAF web ACL. Create a custom set of rules that filter traffic requests based on source and destination IP address range sets.
-   [ ] C. Implement strict inbound security group rules. Configure an outbound rule that allows traffic only to the authorized software repositories on the internet by specifying the URLs.
-   [ ] D. Configure an Application Load Balancer (ALB) in front of the EC2 instances. Direct all outbound traffic to the ALB. Use a URL-based rule listener in the ALB's target group for outbound access to the internet.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Update the route table for the private subnet to route the outbound traffic to an AWS Network Firewall firewall. Configure domain list rule groups.

Why these are the correct answers:

A. Update the route table for the private subnet to route the outbound traffic to an AWS Network Firewall firewall. Configure domain list rule groups.

-   [ ] AWS Network Firewall provides stateful inspection and centralized network traffic filtering.
-   [ ] Domain list rule groups allow you to filter traffic based on fully qualified domain names (FQDNs), meeting the requirement to allow traffic only to specific software repositories.
-   [ ] This solution provides robust security and control over outbound traffic.

Why are the other answers wrong?

-   [ ] B. AWS WAF is for protecting web applications from web exploits, not for controlling outbound traffic from EC2 instances based on URLs.
-   [ ] C. Security groups are stateful but operate at the instance level and do not provide URL-based filtering for outbound traffic.
-   [ ] D. Application Load Balancers are for distributing incoming application traffic and do not provide outbound traffic filtering based on URLs.

Therefore, AWS Network Firewall with domain list rule groups is the most appropriate solution.
</details>
<details>
  <summary>Question 328</summary>

A company is hosting a three-tier ecommerce application in the AWS Cloud. The company hosts the website on Amazon S3 and integrates the website with an API that handles sales requests. The company hosts the API on three Amazon EC2 instances behind an Application Load Balancer (ALB). The API consists of static and dynamic front-end content along with backend workers that process sales requests asynchronously. The company is expecting a significant and sudden increase in the number of sales requests during events for the launch of new products. What should a solutions architect recommend to ensure that all the requests are processed successfully?

-   [ ] A. Add an Amazon CloudFront distribution for the dynamic content. Increase the number of EC2 instances to handle the increase in traffic.
-   [ ] B. Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto Scaling group to launch new instances based on network traffic.
-   [ ] C. Add an Amazon CloudFront distribution for the dynamic content. Add an Amazon ElastiCache instance in front of the ALB to reduce traffic for the API to handle.
-   [ ] D. Add an Amazon CloudFront distribution for the static content. Add an Amazon Simple Queue Service (Amazon SQS) queue to receive requests from the website for later processing by the EC2 instances.
</details>
<details>
  <summary>Answer</summary>

-   [ ] D. Add an Amazon CloudFront distribution for the static content. Add an Amazon Simple Queue Service (Amazon SQS) queue to receive requests from the website for later processing by the EC2 instances.

Why these are the correct answers:

D. Add an Amazon CloudFront distribution for the static content. Add an Amazon Simple Queue Service (Amazon SQS) queue to receive requests from the website for later processing by the EC2 instances.

-   [ ] Amazon CloudFront improves performance by caching static content closer to users, reducing the load on the API.
-   [ ] Amazon SQS decouples the website from the API, allowing the API to process requests asynchronously and handle sudden increases in traffic.
-   [ ] This ensures that requests are queued and processed successfully without overwhelming the API.

Why are the other answers wrong?

-   [ ] A. Caching dynamic content is more complex and may not be as effective as caching static content. Simply increasing EC2 instances may not handle sudden spikes efficiently.
-   [ ] B. Auto Scaling helps but queuing requests is more effective for handling sudden spikes without losing data.
-   [ ] C. ElastiCache is for caching frequently accessed data, not for queuing requests. Caching dynamic content may not be the primary solution.

Therefore, using CloudFront for static content and SQS for queuing is the most effective solution.
</details>
<details>
  <summary>Question 329</summary>

A security audit reveals that Amazon EC2 instances are not being patched regularly. A solutions architect needs to provide a solution that will run regular security scans across a large fleet of EC2 instances. The solution should also patch the EC2 instances on a regular schedule and provide a report of each instance's patch status. Which solution will meet these requirements?

-   [ ] A. Set up Amazon Macie to scan the EC2 instances for software vulnerabilities. Set up a cron job on each EC2 instance to patch the instance on a regular schedule.
-   [ ] B. Turn on Amazon GuardDuty in the account. Configure GuardDuty to scan the EC2 instances for software vulnerabilities. Set up AWS Systems Manager Session Manager to patch the EC2 instances on a regular schedule.
-   [ ] C. Set up Amazon Detective to scan the EC2 instances for software vulnerabilities. Set up an Amazon EventBridge scheduled rule to patch the EC2 instances on a regular schedule.
-   [ ] D. Turn on Amazon Inspector in the account. Configure Amazon Inspector to scan the EC2 instances for software vulnerabilities. Set up AWS Systems Manager Patch Manager to patch the EC2 instances on a regular schedule.
</details>
<details>
  <summary>Answer</summary>

-   [ ] D. Turn on Amazon Inspector in the account. Configure Amazon Inspector to scan the EC2 instances for software vulnerabilities. Set up AWS Systems Manager Patch Manager to patch the EC2 instances on a regular schedule.

Why these are the correct answers:

D. Turn on Amazon Inspector in the account. Configure Amazon Inspector to scan the EC2 instances for software vulnerabilities. Set up AWS Systems Manager Patch Manager to patch the EC2 instances on a regular schedule.

-   [ ] Amazon Inspector is designed to assess EC2 instances for vulnerabilities and security best practices.
-   [ ] AWS Systems Manager Patch Manager automates the process of patching EC2 instances and provides reporting on patch compliance.
-   [ ] This combination provides both vulnerability scanning and automated patching with reporting.

Why are the other answers wrong?

-   [ ] A. Amazon Macie discovers and protects sensitive data, not software vulnerabilities. Cron jobs require manual management and reporting.
-   [ ] B. Amazon GuardDuty detects malicious activity, not software vulnerabilities. Session Manager is for interactive sessions, not automated patching.
-   [ ] C. Amazon Detective analyzes security findings to identify root causes, not for vulnerability scanning or patching. EventBridge is for event-driven tasks, not patching.

Therefore, Amazon Inspector and Systems Manager Patch Manager are the most appropriate services.
</details>
<details>
  <summary>Question 330</summary>

A company is planning to store data on Amazon RDS DB instances. The company must encrypt the data at rest. What should a solutions architect do to meet this requirement?

-   [ ] A. Create a key in AWS Key Management Service (AWS KMS). Enable encryption for the DB instances.
-   [ ] B. Create an encryption key. Store the key in AWS Secrets Manager. Use the key to encrypt the DB instances.
-   [ ] C. Generate a certificate in AWS Certificate Manager (ACM). Enable SSL/TLS on the DB instances by using the certificate.
-   [ ] D. Generate a certificate in AWS Identity and Access Management (IAM). Enable SSL/TLS on the DB instances by using the certificate.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Create a key in AWS Key Management Service (AWS KMS). Enable encryption for the DB instances.

Why these are the correct answers:

A. Create a key in AWS Key Management Service (AWS KMS). Enable encryption for the DB instances.

-   [ ] AWS KMS is the service for managing encryption keys.
-   [ ] RDS supports encryption at rest using KMS keys.
-   [ ] This is the standard and recommended way to encrypt data at rest in RDS.

Why are the other answers wrong?

-   [ ] B. AWS Secrets Manager is for storing secrets, not for encrypting RDS instances.
-   [ ] C and D. AWS Certificate Manager and IAM certificates are for SSL/TLS, which encrypts data in transit, not at rest.

Therefore, using AWS KMS to encrypt RDS instances is the correct solution.
</details>

<details>
  <summary>Question 331</summary>

A company must migrate 20 TB of data from a data center to the AWS Cloud within 30 days. The company's network bandwidth is limited to 15 Mbps and cannot exceed 70% utilization. What should a solutions architect do to meet these requirements?

-   [ ] A. Use AWS Snowball.
-   [ ] B. Use AWS DataSync.
-   [ ] C. Use a secure VPN connection.
-   [ ] D. Use Amazon S3 Transfer Acceleration.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Use AWS Snowball.

Why these are the correct answers:

A. Use AWS Snowball.

-   [ ] AWS Snowball is a physical data transport solution that uses devices to transfer large amounts of data into and out of AWS. [cite: 326, 327, 328]
-   [ ] Given the limited network bandwidth and the large amount of data, Snowball is more efficient than transferring data over the network. [cite: 326, 327, 328]
-   [ ] It can meet the 30-day migration timeframe, avoiding network congestion. [cite: 326, 327, 328]

Why are the other answers wrong?

-   [ ] B. AWS DataSync is a data transfer service, but transferring 20 TB over a 15 Mbps connection within 30 days is impractical. [cite: 326, 327, 328, 40, 41]
-   [ ] C. A secure VPN connection is also limited by the network bandwidth and is too slow for transferring such a large dataset in the given timeframe. [cite: 326, 327, 328]
-   [ ] D. Amazon S3 Transfer Acceleration optimizes network transfers to S3 but is still constrained by the available bandwidth, making it unsuitable for this large migration. [cite: 94, 95, 96, 97]

Therefore, AWS Snowball is the most appropriate solution for this scenario.
</details>
<details>
  <summary>Question 332</summary>

A company needs to provide its employees with secure access to confidential and sensitive files. The company wants to ensure that the files can be accessed only by authorized users. The files must be downloaded securely to the employees' devices. The files are stored in an on-premises Windows file server. However, due to an increase in remote usage, the file server is running out of capacity. Which solution will meet these requirements?

-   [ ] A. Migrate the file server to an Amazon EC2 instance in a public subnet. Configure the security group to limit inbound traffic to the employees' IP addresses.
-   [ ] B. Migrate the files to an Amazon FSx for Windows File Server file system. Integrate the Amazon FSx file system with the on-premises Active Directory. Configure AWS Client VPN.
-   [ ] C. Migrate the files to Amazon S3, and create a private VPC endpoint. Create a signed URL to allow download.
-   [ ] D. Migrate the files to Amazon S3, and create a public VPC endpoint. Allow employees to sign on with AWS IAM Identity Center (AWS Single Sign-On).
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Migrate the files to an Amazon FSx for Windows File Server file system. Integrate the Amazon FSx file system with the on-premises Active Directory. Configure AWS Client VPN.

Why these are the correct answers:

B. Migrate the files to an Amazon FSx for Windows File Server file system. Integrate the Amazon FSx file system with the on-premises Active Directory. Configure AWS Client VPN.

-   [ ] Amazon FSx for Windows File Server provides a fully managed Windows file server in the AWS Cloud. [cite: 336, 337, 48, 49]
-   [ ] Integrating with on-premises Active Directory ensures that only authorized users can access the files. [cite: 336, 337, 355, 356]
-   [ ] AWS Client VPN provides secure access to the file system, allowing employees to download files securely. [cite: 336, 337]
-   [ ] This solution addresses the capacity issue and meets the security requirements. [cite: 331, 332, 333, 334]

Why are the other answers wrong?

-   [ ] A. Migrating the file server to an EC2 instance in a public subnet exposes it to the internet and managing security groups is less secure than using a VPN. [cite: 334, 335, 336, 337]
-   [ ] C. Using S3 with a private VPC endpoint requires changes to how employees access and download files, and signed URLs add complexity. [cite: 338, 339, 340, 39, 40]
-   [ ] D. A public VPC endpoint is less secure, and relying solely on IAM Identity Center does not provide the same level of secure file access as a VPN. [cite: 338, 339, 340]

Therefore, using Amazon FSx for Windows File Server with Active Directory and AWS Client VPN is the most suitable solution.
</details>
<details>
  <summary>Question 333</summary>

A company's application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. On the first day of every month at midnight, the application becomes much slower when the month-end financial calculation batch runs. This causes the CPU utilization of the EC2 instances to immediately peak to 100%, which disrupts the application. What should a solutions architect recommend to ensure the application is able to handle the workload and avoid downtime?

-   [ ] A. Configure an Amazon CloudFront distribution in front of the ALB.
-   [ ] B. Configure an EC2 Auto Scaling simple scaling policy based on CPU utilization.
-   [ ] C. Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule.
-   [ ] D. Configure Amazon ElastiCache to remove some of the workload from the EC2 instances.
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule.

Why these are the correct answers:

C. Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule.

-   [ ] Scheduled scaling allows the Auto Scaling group to increase capacity in anticipation of the known monthly peak load. [cite: 343, 344, 345, 346, 347, 348, 349, 350, 351]
-   [ ] This ensures that additional EC2 instances are available before the CPU utilization spikes, preventing downtime. [cite: 343, 344, 345, 346, 347, 348, 349, 350, 351]

Why are the other answers wrong?

-   [ ] A. CloudFront is for caching and content delivery, not for scaling EC2 instances to handle increased CPU load. [cite: 297, 298, 299, 300, 301, 302, 303, 304, 347, 348, 349, 350, 351]
-   [ ] B. Simple scaling policies react to CPU utilization, which means instances are added *after* the CPU spikes and the application slows down. [cite: 343, 344, 345, 346, 347, 348, 349, 350, 351]
-   [ ] D. ElastiCache improves performance by caching data, but it does not directly address the need to scale EC2 instances for increased CPU load during batch processing. [cite: 396, 397, 398, 399, 400, 401, 350, 351]

Therefore, a scheduled scaling policy is the most effective solution to handle predictable monthly spikes.
</details>
<details>
  <summary>Question 334</summary>

A company wants to give a customer the ability to use on-premises Microsoft Active Directory to download files that are stored in Amazon S3. The customer's application uses an SFTP client to download the files. Which solution will meet these requirements with the LEAST operational overhead and no changes to the customer's application?

-   [ ] A. Set up AWS Transfer Family with SFTP for Amazon S3. Configure integrated Active Directory authentication.
-   [ ] B. Set up AWS Database Migration Service (AWS DMS) to synchronize the on-premises client with Amazon S3. Configure integrated Active Directory authentication.
-   [ ] C. Set up AWS DataSync to synchronize between the on-premises location and the S3 location by using AWS IAM Identity Center (AWS Single Sign-On).
-   [ ] D. Set up a Windows Amazon EC2 instance with SFTP to connect the on-premises client with Amazon S3. Integrate AWS Identity and Access Management (IAM).
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Set up AWS Transfer Family with SFTP for Amazon S3. Configure integrated Active Directory authentication.

Why these are the correct answers:

A. Set up AWS Transfer Family with SFTP for Amazon S3. Configure integrated Active Directory authentication.

-   [ ] AWS Transfer Family provides a fully managed SFTP service that allows users to securely transfer files into and out of Amazon S3. [cite: 355, 356]
-   [ ] It supports integration with on-premises Active Directory, enabling users to authenticate with their existing credentials. [cite: 355, 356, 336, 337]
-   [ ] This solution requires minimal operational overhead and no changes to the customer's application. [cite: 352, 353, 354, 355, 356]

Why are the other answers wrong?

-   [ ] B. AWS DMS is for database migration, not for file transfers, and is not suitable for SFTP access. [cite: 42, 356, 357]
-   [ ] C. AWS DataSync is for large-scale data transfers and does not directly provide SFTP access with Active Directory integration. [cite: 40, 41, 356, 357]
-   [ ] D. Setting up an EC2 instance with SFTP requires more operational overhead and managing authentication with IAM is more complex than using AWS Transfer Family. [cite: 46, 47, 48, 49, 358, 359]

Therefore, AWS Transfer Family is the most appropriate solution for secure SFTP access with Active Directory integration.
</details>
<details>
  <summary>Question 335</summary>

A company is experiencing sudden increases in demand. The company needs to provision large Amazon EC2 instances from an Amazon Machine Image (AMI). The instances will run in an Auto Scaling group. The company needs a solution that provides minimum initialization latency to meet the demand. Which solution meets these requirements?

-   [ ] A. Use the aws ec2 register-image command to create an AMI from a snapshot. Use AWS Step Functions to replace the AMI in the Auto Scaling group.
-   [ ] B. Enable Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot. Provision an AMI by using the snapshot. Replace the AMI in the Auto Scaling group with the new AMI.
-   [ ] C. Enable AMI creation and define lifecycle rules in Amazon Data Lifecycle Manager (Amazon DLM). Create an AWS Lambda function that modifies the AMI in the Auto Scaling group.
-   [ ] D. Use Amazon EventBridge to invoke AWS Backup lifecycle policies that provision AMIs. Configure Auto Scaling group capacity limits as an event source in EventBridge.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Enable Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot. Provision an AMI by using the snapshot. Replace the AMI in the Auto Scaling group with the new AMI.

Why these are the correct answers:

B. Enable Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot. Provision an AMI by using the snapshot. Replace the AMI in the Auto Scaling group with the new AMI.

-   [ ] EBS fast snapshot restore allows volumes created from snapshots to be fully initialized at creation, minimizing latency. [cite: 365, 366, 367]
-   [ ] Using this snapshot to create an AMI ensures that instances launched from that AMI also benefit from fast initialization. [cite: 365, 366, 367]
-   [ ] Replacing the AMI in the Auto Scaling group ensures that new instances are launched quickly to meet demand. [cite: 360, 361, 362, 363, 364, 365, 366, 367]

Why are the other answers wrong?

-   [ ] A. Using the `aws ec2 register-image` command and Step Functions does not provide the fast initialization of EBS volumes. [cite: 363, 364, 365, 366, 367]
-   [ ] C. Amazon DLM manages EBS snapshots and AMIs but does not provide the fast snapshot restore capability. Using a Lambda function adds complexity. [cite: 367, 368, 369, 370]
-   [ ] D. AWS Backup is for data backup and restore, not for optimizing instance initialization latency. EventBridge is for event-driven tasks, not for fast instance provisioning. [cite: 125, 126, 127, 369, 370]

Therefore, EBS fast snapshot restore is the most effective solution for minimizing initialization latency.
</details>
<details>
  <summary>Question 336</summary>

A company hosts a multi-tier web application that uses an Amazon Aurora MySQL DB cluster for storage. The application tier is hosted on Amazon EC2 instances. The company's IT security guidelines mandate that the database credentials be encrypted and rotated every 14 days. What should a solutions architect do to meet this requirement with the LEAST operational effort?

-   [ ] A. Create a new AWS Key Management Service (AWS KMS) encryption key. Use AWS Secrets Manager to create a new secret that uses the KMS key with the appropriate credentials. Associate the secret with the Aurora DB cluster. Configure a custom rotation period of 14 days.
-   [ ] B. Create two parameters in AWS Systems Manager Parameter Store: one for the user name as a string parameter and one that uses the SecureString type for the password. Select AWS Key Management Service (AWS KMS) encryption for the password parameter, and load these parameters in the application tier. Implement an AWS Lambda function that rotates the password every 14 days.
-   [ ] C. Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system in all EC2 instances of the application tier. Restrict the access to the file on the file system so that the application can read the file and that only super users can modify the file. Implement an AWS Lambda function that rotates the key in Aurora every 14 days and writes new credentials into the file.
-   [ ] D. Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon S3 bucket that the application uses to load the credentials. Download the file to the application regularly to ensure that the correct credentials are used. Implement an AWS Lambda function that rotates the Aurora credentials every 14 days and uploads these credentials to the file in the S3 bucket.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Create a new AWS Key Management Service (AWS KMS) encryption key. Use AWS Secrets Manager to create a new secret that uses the KMS key with the appropriate credentials. Associate the secret with the Aurora DB cluster. Configure a custom rotation period of 14 days.

Why these are the correct answers:

A. Create a new AWS Key Management Service (AWS KMS) encryption key. Use AWS Secrets Manager to create a new secret that uses the KMS key with the appropriate credentials. Associate the secret with the Aurora DB cluster. Configure a custom rotation period of 14 days.

-   [ ] AWS Secrets Manager is designed to manage database credentials securely. [cite: 418, 419, 420, 421, 422, 423, 374, 375, 376]
-   [ ] It integrates with AWS KMS for encryption and supports automatic rotation of credentials. [cite: 418, 419, 420, 421, 422, 423, 374, 375, 376]
-   [ ] Configuring a 14-day rotation period meets the requirement with minimal operational effort. [cite: 371, 372, 373, 374, 375, 376, 383, 384, 385, 386]

Why are the other answers wrong?

-   [ ] B. Parameter Store is for storing configuration data, not specifically for rotating database credentials. Implementing a Lambda function adds operational overhead. [cite: 377, 378, 379, 380, 381, 382, 383]
-   [ ] C. Storing credentials in EFS and using a Lambda function to rotate them is more complex and adds operational overhead compared to Secrets Manager. [cite: 377, 378, 379, 380, 381, 382, 383]
-   [ ] D. Storing credentials in S3 and downloading them regularly is less secure and adds more operational overhead than using Secrets Manager. [cite: 383, 384, 385, 386, 387]

Therefore, AWS Secrets Manager is the most efficient and secure solution for rotating database credentials.
</details>
<details>
  <summary>Question 337</summary>

A company has deployed a web application on AWS. The company hosts the backend database on Amazon RDS for MySQL with a primary DB instance and five read replicas to support scaling needs. The read replicas must lag no more than 1 second behind the primary DB instance. The database routinely runs scheduled stored procedures. As traffic on the website increases, the replicas experience additional lag during periods of peak load. A solutions architect must reduce the replication lag as much as possible. The solutions architect must minimize changes to the application code and must minimize ongoing operational overhead. Which solution will meet these requirements?

-   [ ] A. Migrate the database to Amazon Aurora MySQL. Replace the read replicas with Aurora Replicas, and configure Aurora Auto Scaling. Replace the stored procedures with Aurora MySQL native functions.
-   [ ] B. Deploy an Amazon ElastiCache for Redis cluster in front of the database. Modify the application to check the cache before the application queries the database. Replace the stored procedures with AWS Lambda functions.
-   [ ] C. Migrate the database to a MySQL database that runs on Amazon EC2 instances. Choose large, compute optimized EC2 instances for all replica nodes. Maintain the stored procedures on the EC2 instances.
-   [ ] D. Migrate the database to Amazon DynamoDB. Provision a large number of read capacity units (RCUs) to support the required throughput, and configure on-demand capacity scaling. Replace the stored procedures with DynamoDB streams.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Migrate the database to Amazon Aurora MySQL. Replace the read replicas with Aurora Replicas, and configure Aurora Auto Scaling. Replace the stored procedures with Aurora MySQL native functions.

Why these are the correct answers:

A. Migrate the database to Amazon Aurora MySQL. Replace the read replicas with Aurora Replicas, and configure Aurora Auto Scaling. Replace the stored procedures with Aurora MySQL native functions.

-   [ ] Amazon Aurora MySQL is designed to minimize replication lag. [cite: 394, 395, 396]
-   [ ] Aurora Replicas provide low replication lag compared to standard MySQL read replicas. [cite: 394, 395, 396]
-   [ ] Aurora Auto Scaling automatically adjusts database capacity to handle increased load. [cite: 394, 395, 396]
-   [ ] Replacing stored procedures with Aurora MySQL native functions can improve performance. [cite: 394, 395, 396]
-   [ ] This solution minimizes application code changes and operational overhead. [cite: 388, 389, 390, 391, 392, 393, 394, 395, 396]

Why are the other answers wrong?

-   [ ] B. ElastiCache reduces database load but does not directly address replication lag. Modifying the application to use the cache adds complexity. [cite: 396, 397, 398, 399, 400, 401]
-   [ ] C. Migrating to MySQL on EC2 instances increases operational overhead and does not guarantee reduced replication lag. [cite: 396, 397, 398, 399, 400, 401]
-   [ ] D. DynamoDB is a NoSQL database and requires significant application code changes. It is not suitable for replacing a MySQL database. [cite: 396, 397, 398, 399, 400, 401]

Therefore, migrating to Amazon Aurora MySQL is the most appropriate solution.
</details>
<details>
  <summary>Question 338</summary>

A solutions architect must create a disaster recovery (DR) plan for a high-volume software as a service (SaaS) platform. All data for the platform is stored in an Amazon Aurora MySQL DB cluster. The DR plan must replicate data to a secondary AWS Region. Which solution will meet these requirements MOST cost-effectively?

-   [ ] A. Use MySQL binary log replication to an Aurora cluster in the secondary Region. Provision one DB instance for the Aurora cluster in the secondary Region.
-   [ ] B. Set up an Aurora global database for the DB cluster. When setup is complete, remove the DB instance from the secondary Region.
-   [ ] C. Use AWS Database Migration Service (AWS DMS) to continuously replicate data to an Aurora cluster in the secondary Region. Remove the DB instance from the secondary Region.
-   [ ] D. Set up an Aurora global database for the DB cluster. Specify a minimum of one DB instance in the secondary Region.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Set up an Aurora global database for the DB cluster. When setup is complete, remove the DB instance from the secondary Region.

Why these are the correct answers:

B. Set up an Aurora global database for the DB cluster. When setup is complete, remove the DB instance from the secondary Region.

-   [ ] Aurora Global Database is designed for disaster recovery and replicates data with low latency to a secondary Region. [cite: 407, 408, 409, 410, 411, 412]
-   [ ] By removing the DB instance in the secondary Region after setup, you minimize costs while still having the replication in place. [cite: 407, 408, 409, 410, 411, 412]
-   [ ] This approach is cost-effective because you only incur the cost of storage and data transfer until a failover is needed. [cite: 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412]

Why are the other answers wrong?

-   [ ] A. MySQL binary log replication is complex to set up and manage, and provisioning a DB instance incurs ongoing costs. [cite: 404, 405, 406, 407, 408, 409, 410, 411, 412]
-   [ ] C. AWS DMS is for database migration, not efficient for continuous replication in a DR scenario, and also incurs costs for the replication instance. [cite: 409, 410, 411, 412]
-   [ ] D. Keeping a DB instance running in the secondary Region increases costs, making it less cost-effective than removing it after setup. [cite: 409, 410, 411, 412]

Therefore, using Aurora Global Database and removing the DB instance is the most cost-effective DR solution.
</details>
<details>
  <summary>Question 339</summary>

A company has a custom application with embedded credentials that retrieves information from an Amazon RDS MySQL DB instance. Management says the application must be made more secure with the least amount of programming effort. What should a solutions architect do to meet these requirements?

-   [ ] A. Use AWS Key Management Service (AWS KMS) to create keys. Configure the application to load the database credentials from AWS KMS. Enable automatic key rotation.
-   [ ] B. Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Create an AWS Lambda function that rotates the credentials in Secret Manager.
-   [ ] C. Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Set up a credentials rotation schedule for the application user in the RDS for MySQL database using Secrets Manager.
-   [ ] D. Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Systems Manager Parameter Store. Configure the application to load the database credentials from Parameter Store. Set up a credentials rotation schedule for the application user in the RDS for MySQL database using Parameter Store.
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Set up a credentials rotation schedule for the application user in the RDS for MySQL database using Secrets Manager.

Why these are the correct answers:

C. Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Set up a credentials rotation schedule for the application user in the RDS for MySQL database using Secrets Manager.

-   [ ] AWS Secrets Manager is designed to store and manage database credentials securely. [cite: 418, 419, 420, 421, 422, 423]
-   [ ] It allows for automatic rotation of credentials, enhancing security. [cite: 418, 419, 420, 421, 422, 423]
-   [ ] This solution minimizes programming effort by using Secrets Manager's built-in rotation capabilities. [cite: 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423]

Why are the other answers wrong?

-   [ ] A. Using AWS KMS to store database credentials requires more programming effort to retrieve and manage the credentials. [cite: 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423]
-   [ ] B. Using a Lambda function to rotate credentials adds complexity and operational overhead. [cite: 418, 419, 420, 421, 422, 423]
-   [ ] D. AWS Systems Manager Parameter Store is for configuration data, not for secure storage and rotation of database credentials. [cite: 423, 424, 425, 426, 427]

Therefore, AWS Secrets Manager is the most suitable solution for secure credential management and rotation.
</details>
<details>
  <summary>Question 340</summary>

A media company hosts its website on AWS. The website application's architecture includes a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB) and a database that is hosted on Amazon Aurora. The company's cybersecurity team reports that the application is vulnerable to SQL injection. How should the company resolve this issue?

-   [ ] A. Use AWS WAF in front of the ALB. Associate the appropriate web ACLs with AWS WAF.
-   [ ] B. Create an ALB listener rule to reply to SQL injections with a fixed response.
-   [ ] C. Subscribe to AWS Shield Advanced to block all SQL injection attempts automatically.
-   [ ] D. Set up Amazon Inspector to block all SQL injection attempts automatically.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Use AWS WAF in front of the ALB. Associate the appropriate web ACLs with AWS WAF.

Why these are the correct answers:

A. Use AWS WAF in front of the ALB. Associate the appropriate web ACLs with AWS WAF.

-   [ ] AWS WAF (Web Application Firewall) is designed to protect web applications from common web exploits, including SQL injection. [cite: 430, 431, 432, 285, 286, 287, 288, 289]
-   [ ] Web ACLs (Web Access Control Lists) contain rules that filter malicious web requests. [cite: 430, 431, 432, 285, 286, 287, 288, 289]
-   [ ] Placing WAF in front of the ALB protects the EC2 instances and the Aurora database. [cite: 428, 429, 430, 431, 432]

Why are the other answers wrong?

-   [ ] B. ALB listener rules are for routing traffic, not for filtering SQL injection attacks. [cite: 430, 431, 432]
-   [ ] C. AWS Shield Advanced protects against DDoS attacks, not SQL injection. [cite: 146, 147, 433]
-   [ ] D. Amazon Inspector scans for vulnerabilities but does not actively block attacks. [cite: 309, 310, 311, 312, 313, 314, 315, 316, 434]

Therefore, AWS WAF is the most appropriate service to mitigate SQL injection attacks.
</details>


































