<details>
  <summary>Question 301</summary>

A university research laboratory needs to migrate 30 TB of data from an on-premises Windows file server to Amazon FSx for Windows File Server. The laboratory has a 1 Gbps network link that many other departments in the university share. The laboratory wants to implement a data migration service that will maximize the performance of the data transfer. However, the laboratory needs to be able to control the amount of bandwidth that the service uses to minimize the impact on other departments. The data migration must take place within the next 5 days.

Which AWS solution will meet these requirements?

- [ ] A. AWS Snowcone
- [ ] B. Amazon FSx File Gateway
- [ ] C. AWS DataSync
- [ ] D. AWS Transfer Family
</details>

<details>
  <summary>Answer</summary>

- [ ] C. AWS DataSync

Why these are the correct answers:

C. AWS DataSync

- [ ] AWS DataSync is designed to efficiently and securely transfer large amounts of data between on-premises storage and AWS storage services.
- [ ] It can control the amount of bandwidth used during the transfer, which is crucial for minimizing the impact on other departments sharing the network link.
- [ ] DataSync can handle the migration of large datasets like 30TB and can be configured to operate within the given time frame.

Why are the other answers wrong?

- [ ] A. AWS Snowcone is a physical, rugged, and secure edge computing and data transfer device. While it's used for data transfer, it doesn't provide the bandwidth control needed and involves physical shipping, which may not meet the 5-day timeframe.
- [ ] B. Amazon FSx File Gateway provides low-latency access to FSx for Windows File Server file shares from on-premises, but it's not the primary service for the initial migration of large datasets.
- [ ] D. AWS Transfer Family is a managed service that supports file transfers into and out of Amazon S3 or Amazon FSx for Windows File Server using protocols like SFTP, FTPS, and FTP. It doesn’t offer the same level of bandwidth control and optimization for large-scale data migration as DataSync.

Therefore, AWS DataSync is the most suitable solution for this scenario.
</details>
<details>
  <summary>Question 302</summary>

A company wants to create a mobile app that allows users to stream slow-motion video clips on their mobile devices. Currently, the app captures video clips and uploads the video clips in raw format into an Amazon S3 bucket. The app retrieves these video clips directly from the S3 bucket. However, the videos are large in their raw format. Users are experiencing issues with buffering and playback on mobile devices. The company wants to implement solutions to maximize the performance and scalability of the app while minimizing operational overhead. Which combination of solutions will meet these requirements? (Choose two.)

- [ ] A. Deploy Amazon CloudFront for content delivery and caching.
- [ ] B. Use AWS DataSync to replicate the video files across AW'S Regions in other S3 buckets.
- [ ] C. Use Amazon Elastic Transcoder to convert the video files to more appropriate formats.
- [ ] D. Deploy an Auto Sealing group of Amazon EC2 instances in Local Zones for content delivery and caching.
- [ ] E. Deploy an Auto Scaling group of Amazon EC2 instances to convert the video files to more appropriate formats.
</details>

<details>
  <summary>Answer</summary>

- [ ] A. Deploy Amazon CloudFront for content delivery and caching.
- [ ] C. Use Amazon Elastic Transcoder to convert the video files to more appropriate formats.

Why these are the correct answers:

A. Deploy Amazon CloudFront for content delivery and caching.

- [ ] Amazon CloudFront is a content delivery network (CDN) that can cache video content closer to users, reducing latency and improving streaming performance.
- [ ] It enhances scalability by distributing the load across multiple edge locations.

C. Use Amazon Elastic Transcoder to convert the video files to more appropriate formats.

- [ ] Amazon Elastic Transcoder converts video files from their original format to formats that are more suitable for streaming on mobile devices, reducing file size and buffering.
- [ ] It minimizes operational overhead by providing a managed service for media transcoding.

Why are the other answers wrong?

- [ ] B. AWS DataSync is used for data transfer, not for optimizing video delivery or format conversion. Replicating files across regions doesn’t address the buffering and playback issues.
- [ ] D. Deploying an Auto Scaling group of Amazon EC2 instances in Local Zones for content delivery and caching is overly complex and expensive compared to using CloudFront, which is specifically designed for this purpose.
- [ ] E. Deploying an Auto Scaling group of EC2 instances to convert video files is a viable option but introduces more operational overhead than using Amazon Elastic Transcoder, a managed service that handles transcoding automatically.

Therefore, Amazon CloudFront and Amazon Elastic Transcoder are the most appropriate solutions.
</details>
<details>
  <summary>Question 303</summary>

A company is launching a new application deployed on an Amazon Elastic Container Service (Amazon ECS) cluster and is using the Fargate launch type for ECS tasks. The company is monitoring CPU and memory usage because it is expecting high traffic to the application upon its launch. However, the company wants to reduce costs when utilization decreases. What should a solutions architect recommend?

- [ ] A. Use Amazon EC2 Auto Scaling to scale at certain periods based on previous traffic patterns.
- [ ] B. Use an AWS Lambda function to scale Amazon ECS based on metric breaches that trigger an Amazon CloudWatch alarm.
- [ ] C. Use Amazon EC2 Auto Scaling with simple scaling policies to scale when ECS metric breaches trigger an Amazon CloudWatch alarm.
- [ ] D. Use AWS Application Auto Scaling with target tracking policies to scale when ECS metric breaches trigger an Amazon CloudWatch alarm.
</details>
<details>
  <summary>Answer</summary>

- [ ] D. Use AWS Application Auto Scaling with target tracking policies to scale when ECS metric breaches trigger an Amazon CloudWatch alarm.

Why these are the correct answers:

D. Use AWS Application Auto Scaling with target tracking policies to scale when ECS metric breaches trigger an Amazon CloudWatch alarm.

- [ ] AWS Application Auto Scaling is designed to scale resources like ECS tasks.
- [ ] Target tracking policies allow you to set a target value for a metric (e.g., CPU utilization), and Auto Scaling automatically adjusts the number of tasks to maintain that target.
- [ ] This approach is more responsive and cost-effective than scaling at fixed periods or using Lambda functions for scaling.

Why are the other answers wrong?

- [ ] A. Amazon EC2 Auto Scaling is for scaling EC2 instances, not ECS tasks using Fargate.
- [ ] B. Using an AWS Lambda function to scale ECS adds complexity and overhead compared to using Application Auto Scaling directly.
- [ ] C. While EC2 Auto Scaling can use CloudWatch alarms, it's not the right service for scaling Fargate tasks; Application Auto Scaling is more suitable and provides target tracking.

Therefore, AWS Application Auto Scaling with target tracking policies is the recommended solution.
</details>
<details>
  <summary>Question 304</summary>

A company recently created a disaster recovery site in a different AWS Region. The company needs to transfer large amounts of data back and forth between NFS file systems in the two Regions on a periodic basis. Which solution will meet these requirements with the LEAST operational overhead?

- [ ] A. Use AWS DataSync.
- [ ] B. Use AWS Snowball devices.
- [ ] C. Set up an SFTP server on Amazon EC2.
- [ ] D. Use AWS Database Migration Service (AWS DMS).
</details>
<details>
  <summary>Answer</summary>

- [ ] A. Use AWS DataSync.

Why these are the correct answers:

A. Use AWS DataSync.

- [ ] AWS DataSync is designed to automate and accelerate the transfer of data between on-premises storage and AWS storage services, or between AWS storage services.
- [ ] It minimizes operational overhead with its automation features.
- [ ] DataSync is efficient for transferring large amounts of data and can handle periodic transfers.

Why are the other answers wrong?

- [ ] B. AWS Snowball devices are physical devices and involve manual shipping, which increases operational overhead and is not suitable for periodic transfers.
- [ ] C. Setting up an SFTP server on Amazon EC2 requires manual management of the server, security, and transfers, leading to higher operational overhead.
- [ ] D. AWS Database Migration Service (AWS DMS) is designed for database migrations, not for general file system transfers.

Therefore, AWS DataSync is the most appropriate solution with the least operational overhead.
</details>
<details>
  <summary>Question 305</summary>

A company is designing a shared storage solution for a gaming application that is hosted in the AWS Cloud. The company needs the ability to use SMB clients to access data. The solution must be fully managed. Which AWS solution meets these requirements?

- [ ] A. Create an AWS DataSync task that shares the data as a mountable file system. Mount the file system to the application server.
- [ ] B. Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the instance. Connect the application server to the file share.
- [ ] C. Create an Amazon FSx for Windows File Server file system. Attach the file system to the origin server. Connect the application server to the file system.
- [ ] D. Create an Amazon S3 bucket. Assign an IAM role to the application to grant access to the S3 bucket. Mount the S3 bucket to the application server.
</details>
<details>
  <summary>Answer</summary>

- [ ] C. Create an Amazon FSx for Windows File Server file system. Attach the file system to the origin server. Connect the application server to the file system.

Why these are the correct answers:

C. Create an Amazon FSx for Windows File Server file system. Attach the file system to the origin server. Connect the application server to the file system.

- [ ] Amazon FSx for Windows File Server provides a fully managed file system that supports the SMB protocol.
- [ ] It eliminates the operational overhead of managing file servers.
- [ ] This solution allows SMB clients to access the data as required.

Why are the other answers wrong?

- [ ] A. AWS DataSync is for data transfer, not for providing a shared file system.
- [ ] B. Creating an EC2 Windows instance and configuring a file share requires manual management of the server, which is not a fully managed solution.
- [ ] D. Amazon S3 is object storage and does not natively support SMB file shares. Mounting an S3 bucket as a file system is not straightforward or performant for applications needing SMB access.

Therefore, Amazon FSx for Windows File Server is the best solution for a fully managed, SMB-compatible shared storage.
</details>
<details>
  <summary>Question 306</summary>

A company wants to run an in-memory database for a latency-sensitive application that runs on Amazon EC2 instances. The application processes more than 100,000 transactions each minute and requires high network throughput. A solutions architect needs to provide a cost-effective network design that minimizes data transfer charges. Which solution meets these requirements?

- [ ] A. Launch all EC2 instances in the same Availability Zone within the same AWS Region. Specify a placement group with cluster strategy when launching EC2 instances.
- [ ] B. Launch all EC2 instances in different Availability Zones within the same AWS Region. Specify a placement group with partition strategy when launching EC2 instances.
- [ ] C. Deploy an Auto Scaling group to launch EC2 instances in different Availability Zones based on a network utilization target.
- [ ] D. Deploy an Auto Scaling group with a step scaling policy to launch EC2 instances in different Availability Zones.
</details>
<details>
  <summary>Answer</summary>

- [ ] A. Launch all EC2 instances in the same Availability Zone within the same AWS Region. Specify a placement group with cluster strategy when launching EC2 instances.

Why these are the correct answers:

A. Launch all EC2 instances in the same Availability Zone within the same AWS Region. Specify a placement group with cluster strategy when launching EC2 instances.

- [ ] Launching EC2 instances in the same Availability Zone minimizes latency and maximizes network throughput.
- [ ] A cluster placement group ensures instances are physically close together, further reducing latency and increasing throughput.
- [ ] This configuration is cost-effective as it reduces data transfer charges between Availability Zones.

Why are the other answers wrong?

- [ ] B. Spreading instances across different Availability Zones increases latency and data transfer costs. A partition placement group is for large distributed applications, not for latency-sensitive in-memory databases.
- [ ] C. Auto Scaling across different Availability Zones adds latency and costs. Scaling based on network utilization doesn't address the need for minimizing latency for in-memory databases.
- [ ] D. Step scaling across Availability Zones also introduces latency and costs, and it's not optimized for the low-latency requirements of an in-memory database.

Therefore, placing instances in the same Availability Zone with a cluster placement group is the best solution.
</details>
<details>
  <summary>Question 307</summary>

A company that primarily runs its application servers on premises has decided to migrate to AWS. The company wants to minimize its need to scale its Internet Small Computer Systems Interface (iSCSI) storage on premises. The company wants only its recently accessed data to remain stored locally. Which AWS solution should the company use to meet these requirements?

- [ ] A. Amazon S3 File Gateway
- [ ] B. AWS Storage Gateway Tape Gateway
- [ ] C. AWS Storage Gateway Volume Gateway stored volumes
- [ ] D. AWS Storage Gateway Volume Gateway cached volumes
</details>
<details>
  <summary>Answer</summary>

- [ ] D. AWS Storage Gateway Volume Gateway cached volumes

Why these are the correct answers:

D. AWS Storage Gateway Volume Gateway cached volumes

- [ ] Volume Gateway in cached mode stores frequently accessed data locally while asynchronously backing up all data to AWS.
- [ ] This minimizes the need to scale on-premises iSCSI storage because only active data is kept locally.
- [ ] It meets the requirement of keeping only recently accessed data stored locally.

Why are the other answers wrong?

- [ ] A. Amazon S3 File Gateway provides a file interface to S3, not iSCSI, and doesn't cache frequently accessed data locally in the same way as Volume Gateway.
- [ ] B. AWS Storage Gateway Tape Gateway is for backup and archival, not for providing low-latency access to recently accessed data.
- [ ] C. AWS Storage Gateway Volume Gateway stored volumes store the entire dataset locally and back it up to AWS, which does not minimize the need for on-premises storage scaling.

Therefore, Volume Gateway cached volumes is the most suitable solution.
</details>
<details>
  <summary>Question 308</summary>

A company has multiple AWS accounts that use consolidated billing. The company runs several active high performance Amazon RDS for Oracle On-Demand DB instances for 90 days. The company's finance team has access to AWS Trusted Advisor in the consolidated billing account and all other AWS accounts. The finance team needs to use the appropriate AWS account to access the Trusted Advisor check recommendations for RDS. The finance team must review the appropriate Trusted Advisor check to reduce RDS costs. Which combination of steps should the finance team take to meet these requirements? (Choose two.)

- [ ] A. Use the Trusted Advisor recommendations from the account where the RDS instances are running.
- [ ] B. Use the Trusted Advisor recommendations from the consolidated billing account to see all RDS instance checks at the same time.
- [ ] C. Review the Trusted Advisor check for Amazon RDS Reserved Instance Optimization.
- [ ] D. Review the Trusted Advisor check for Amazon RDS Idle DB Instances.
- [ ] E. Review the Trusted Advisor check for Amazon Redshift Reserved Node Optimization.
</details>
<details>
  <summary>Answer</summary>

- [ ] B. Use the Trusted Advisor recommendations from the consolidated billing account to see all RDS instance checks at the same time.
- [ ] D. Review the Trusted Advisor check for Amazon RDS Idle DB Instances.

Why these are the correct answers:

B. Use the Trusted Advisor recommendations from the consolidated billing account to see all RDS instance checks at the same time.

- [ ] Consolidated billing accounts provide a centralized view of Trusted Advisor checks across all linked accounts.
- [ ] This allows the finance team to efficiently review RDS costs for all accounts in one place.

D. Review the Trusted Advisor check for Amazon RDS Idle DB Instances.

- [ ] The Idle DB Instances check identifies RDS instances that are not being used, which can lead to cost savings by stopping or deleting them.

Why are the other answers wrong?

- [ ] A. While checking recommendations in individual accounts is possible, it's less efficient than using the consolidated view.
- [ ] C. Reserved Instance Optimization is relevant for long-term cost savings but not for identifying immediate cost reductions from unused instances.
- [ ] E. Redshift Reserved Node Optimization is specific to Amazon Redshift, not Amazon RDS.

Therefore, using the consolidated billing account and checking for idle DB instances are the most appropriate steps.
</details>










































