# AWS-SAA-PRACTICE-EXAM Questions 101-110

<details>
  <summary>Question 101</summary>
 
A solutions architect is designing a VPC with public and private subnets. The VPC and subnets use IPv4 CIDR blocks. There is one public subnet and one private subnet in each of three Availability Zones (AZs) for high availability. An internet gateway is used to provide internet access for the public subnets. The private subnets require access to the internet to allow Amazon EC2 instances to download software updates.

What should the solutions architect do to enable Internet access for the private subnets?

- [ ] A. Create three NAT gateways, one for each public subnet in each AZ. Create a private route table for each AZ that forwards non-VPC traffic to the NAT gateway in its AZ.
- [ ] B. Create three NAT instances, one for each private subnet in each AZ. Create a private route table for each AZ that forwards non-VPC traffic to the NAT instance in its AZ.
- [ ] C. Create a second internet gateway on one of the private subnets. Update the route table for the private subnets that forward non-VPC traffic to the private internet gateway.
- [ ] D. Create an egress-only internet gateway on one of the public subnets. Update the route table for the private subnets that forward non-VPC traffic to the egress-only Internet gateway.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Create three NAT gateways, one for each public subnet in each AZ. Create a private route table for each AZ that forwards non-VPC traffic to the NAT gateway in it

Why this is the correct answer:

- [ ] NAT Gateways for Private Subnet Outbound Access: Amazon EC2 instances in private subnets need a way to access the internet for tasks like downloading software updates without being directly exposed to incoming traffic from the internet. NAT (Network Address Translation) gateways are designed for this purpose. They allow instances in private subnets to initiate outbound IPv4 traffic to the internet but prevent unsolicited inbound connections.
- [ ] High Availability Across Availability Zones: The architecture has three Availability Zones (AZs) for high availability. To ensure that internet access for private subnets remains available even if one AZ has an issue, it's a best practice to deploy a NAT gateway in a public subnet in each AZ.
- [ ] Zonal Routing for Resilience and Cost: Each private subnet (or the group of private subnets within a specific AZ) should have its route table configured to direct internet-bound traffic (0.0.0.0/0) to the NAT gateway located in its own AZ. This avoids cross-AZ data transfer costs for NAT gateway traffic and ensures that the failure of a NAT gateway in one AZ does not impact the internet connectivity of private subnets in other AZs.

<hr> Why are the other answers wrong? <hr>

- [ ] Option B is wrong because: While NAT instances can also provide NAT functionality, NAT gateways are AWS-managed, offering higher availability, greater bandwidth, and less administrative effort (no need to patch or manage the instance OS) compared to self-managed NAT instances. For a new design emphasizing high availability, NAT gateways are preferred. Also, placing NAT instances in private subnets is incorrect; they need to be in a public subnet with a route to the internet gateway.
- [ ] Option C is wrong because: A VPC can have only one Internet Gateway (IGW) attached to it. It's not possible to create a "second internet gateway." Furthermore, Internet Gateways are for providing direct, two-way internet access, typically for public subnets, not for providing outbound-only access for private subnets.
- [ ] Option D is wrong because: An Egress-Only Internet Gateway is used specifically for IPv6 traffic. It allows instances in your VPC to initiate outbound connections over IPv6 to the internet while preventing the internet from initiating IPv6 connections to your instances. The question specifies that the VPC and subnets use "IPv4 CIDR blocks," and the common need for software updates implies IPv4 internet access. An egress-only internet gateway does not provide IPv4 outbound connectivity.

</details>  

<details>
  <summary>Question 102</summary>

A company wants to migrate an on-premises data center to AWS. The data center hosts an SFTP server that stores its data on an NFS-based file system. The server holds 200 GB of data that needs to be transferred. The server must be hosted on an Amazon EC2 instance that uses an Amazon Elastic File System (Amazon EFS) file system.

Which combination of steps should a solutions architect take to automate this task? (Choose two.)

- [ ] A. Launch the EC2 instance into the same Availability Zone as the EFS file system.
- [ ] B. Install an AWS DataSync agent in the on-premises data center.
- [ ] C. Create a secondary Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instance for the data.
- [ ] D. Manually use an operating system copy command to push the data to the EC2 instance.
- [ ] E. Use AWS DataSync to create a suitable location configuration for the on-premises SFTP server.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Install an AWS DataSync agent in the on-premises data center.
- [ ] E. Use AWS DataSync to create a suitable location configuration for the on-premises SFTP server.

Why these are the correct answers:

This solution focuses on automating the transfer of 200 GB of data from an on-premises NFS file system (backing an SFTP server) to an Amazon EFS file system in AWS.

B. Install an AWS DataSync agent in the on-premises data center.
- [ ] AWS DataSync Agent: To use AWS DataSync for transferring data from an on-premises location, you need to deploy a DataSync agent (a virtual machine) in your on-premises environment.
- [ ] This agent will access your on-premises NFS file system and manage the data transfer to AWS.

E. Use AWS DataSync to create a suitable location configuration for the on-premises SFTP server.
- [ ] DataSync Locations and Task: Once the agent is deployed and activated, you configure AWS DataSync by creating a source location that points to your on-premises NFS file system (which backs the SFTP server) and a destination location that points to your Amazon EFS file system in AWS.
- [ ] Then, you create a DataSync task to manage the automated transfer of data between these locations. DataSync handles the scheduling, monitoring, data validation, and optimization of the transfer.
- [ ] This combination provides an automated, efficient, and managed way to transfer the data.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: While an EC2 instance needs to be in a VPC with mount targets for the EFS file system (which exist in specific AZs), EFS itself is a regional service designed to be accessible from any AZ within the region where it's created. This step is related to how the EC2 instance accesses EFS after migration, not directly a step in automating the data transfer from on-premises. The core of the automation is DataSync.
- [ ] Option C is wrong because: The requirement clearly states that the new SFTP server hosted on an EC2 instance must use an Amazon EFS file system, not an EBS volume, for its data. Creating an EBS volume contradicts this requirement.
- [ ] Option D is wrong because: Manually using operating system copy commands (like scp, rsync, or cp over an NFS mount) is not an "automated task" in the context of a managed migration service like DataSync. Manual copies lack the built-in scheduling, monitoring, error handling, data validation, and transfer optimization features that DataSync provides, and they would require more manual intervention, especially for 200 GB of data.

</details>

<details>
  <summary>Question 103</summary>

A company has an AWS Glue extract, transform, and load (ETL) job that runs every day at the same time. The job processes XML data that is in an Amazon S3 bucket. New data is added to the S3 bucket every day. A solutions architect notices that AWS Glue is processing all the data during each run.

What should the solutions architect do to prevent AWS Glue from reprocessing old data?

- [ ] A. Edit the job to use job bookmarks.
- [ ] B. Edit the job to delete data after the data is processed.
- [ ] C. Edit the job by setting the NumberOfWorkers field to 1.
- [ ] D. Use a FindMatches machine learning (ML) transform.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Edit the job to use job bookmarks.

Why this is the correct answer:

- [ ] AWS Glue Job Bookmarks: AWS Glue job bookmarks are a feature that allows AWS Glue to keep track of data that has already been processed during previous runs of an ETL job. When job bookmarks are enabled and used correctly, on subsequent runs, the Glue job will only process new data that has arrived in the Amazon S3 bucket (or other source) since the last successful run.
- [ ] Preventing Reprocessing of Old Data: This directly addresses the problem described: "AWS Glue is processing all the data during each run." By enabling job bookmarks, the job will skip the old, already processed data and only focus on the new XML data added daily.
- [ ] Suitable for Incremental Processing: This is the standard mechanism in AWS Glue for achieving incremental processing of data from sources like S3 where new files are added over time.

<hr> Why are the other answers wrong? <hr>

- [ ] Option B is wrong because: Editing the job to delete data after it is processed might not be a desirable or safe approach. The source data in S3 might be needed for other purposes, auditing, re-running the ETL job in case of issues, or historical record-keeping. Deleting source data is a destructive action and not the primary method for preventing reprocessing.
- [ ] Option C is wrong because: The NumberOfWorkers field in an AWS Glue job configuration relates to the number of Data Processing Units (DPUs) allocated to the job, which affects its processing capacity and parallelism. Setting this field to 1 would reduce the job's performance and make it run slower, but it would not, by itself, prevent the job from reprocessing old data if job bookmarks are not enabled.
- [ ] Option D is wrong because: The FindMatches ML transform in AWS Glue is used for identifying duplicate or matching records within a dataset (e.g., for data deduplication or record linkage). It is not a feature designed to prevent an ETL job from reprocessing input files that have already been processed in previous runs.

</details>

<details>
  <summary>Question 104</summary>
 
A solutions architect must design a highly available infrastructure for a website. The website is powered by Windows web servers that run on Amazon EC2 instances. The solutions architect must implement a solution that can mitigate a large-scale DDoS attack that originates from thousands of IP addresses. Downtime is not acceptable for the website.

Which actions should the solutions architect take to protect the website from such an attack? (Choose two.)

- [ ] A. Use AWS Shield Advanced to stop the DDoS attack.
- [ ] B. Configure Amazon GuardDuty to automatically block the attackers.
- [ ] C. Configure the website to use Amazon CloudFront for both static and dynamic content.
- [ ] D. Use an AWS Lambda function to automatically add attacker IP addresses to VPC network ACLS.
- [ ] E. Use EC2 Spot Instances in an Auto Scaling group with a target tracking scaling policy that is set to 80% CPU utilization.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Use AWS Shield Advanced to stop the DDoS attack.
- [ ] C. Configure the website to use Amazon CloudFront for both static and dynamic content.

Why these are the correct answers:

This solution combines a CDN with advanced DDoS protection services:

A. Use AWS Shield Advanced to stop the DDoS attack.
- [ ] Enhanced DDoS Protection: AWS Shield Advanced provides significantly more robust protection against larger and more sophisticated Distributed Denial of Service (DDoS) attacks compared to the default AWS Shield Standard.
- [ ] It offers detection and mitigation against volumetric network layer attacks, state-exhaustion attacks, and application layer attacks (when used with AWS WAF).
- [ ] Access to DDoS Response Team (DRT): Shield Advanced customers get 24/7 access to the AWS DDoS Response Team (DRT) for assistance during an attack, which is crucial when "Downtime is not acceptable."
- [ ] Cost Protection: It also provides cost protection against usage spikes on protected resources like CloudFront, ELB, and EC2 that might result from a DDoS attack.

C. Configure the website to use Amazon CloudFront for both static and dynamic content.
- [ ] Distributed Attack Absorption: Amazon CloudFront is a global content delivery network (CDN) with a large network of edge locations.
- [ ] This distributed infrastructure can absorb many common infrastructure-level DDoS attacks (like SYN floods or UDP reflection attacks) at the edge, preventing them from reaching your origin servers.
- [ ] Reduced Attack Surface: By serving content from the edge, CloudFront reduces the direct exposure of your origin EC2 instances.
- [ ] Integration with WAF and Shield: CloudFront integrates seamlessly with AWS WAF (for application-layer filtering) and AWS Shield, providing a layered security approach at the edge.
- [ ] This combination is highly effective in mitigating DDoS attacks.

<hr> Why are the other answers wrong? <hr>

- [ ] Option B is wrong because: Amazon GuardDuty is a threat detection service that monitors for malicious activity and unauthorized behavior within your AWS environment. While it can help detect indicators of a DDoS attack or compromised resources, GuardDuty itself does not automatically block attackers or provide DDoS mitigation. It provides findings that can then be used to trigger other responses, but it's not a primary mitigation tool.
- [ ] Option D is wrong because: While it's possible to build a custom solution using AWS Lambda to dynamically update VPC Network ACLs (NACLs) based on detected malicious IP addresses, this approach has limitations for "large-scale DDoS attacks originating from thousands of IP addresses." NACLs have rule limits, and managing dynamic updates at this scale can be complex, slow to react, and operationally burdensome compared to dedicated DDoS mitigation services like AWS Shield and AWS WAF.
- [ ] Option E is wrong because: EC2 Spot Instances can be terminated by AWS with a two-minute notice if AWS needs the capacity back. Relying on Spot Instances for a critical website where "Downtime is not acceptable" is risky, especially during a DDoS attack which could cause unpredictable load and instance behavior. While Auto Scaling helps with legitimate traffic, Spot Instances are not a DDoS mitigation strategy.

</details>

<details>
  <summary>Question 105</summary>

A company is preparing to deploy a new serverless workload. A solutions architect must use the principle of least privilege to configure permissions that will be used to run an AWS Lambda function. An Amazon EventBridge (Amazon CloudWatch Events) rule will invoke the function.

Which solution meets these requirements?

- [ ] A. Add an execution role to the function with lambda:InvokeFunction as the action and * as the principal.
- [ ] B. Add an execution role to the function with lambda:InvokeFunction as the action and Service: https://www.google.com/url?sa=E&amp;source=gmail&amp;q=lambda.amazonaws.com as the principal.
- [ ] C. Add a resource-based policy to the function with lambda:* as the action and Service: https://www.google.com/url?sa=E&amp;source=gmail&amp;q=events.amazonaws.com as the principal.
- [ ] D. Add a resource-based policy to the function with lambda:InvokeFunction as the action and Service: https://www.google.com/url?sa=E&amp;source=gmail&amp;q=events.amazonaws.com as the principal.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Add a resource-based policy to the function with lambda:InvokeFunction as the action and Service: https://www.google.com/url?sa=E&amp;source=gmail&amp;q=events.amazonaws.com as the principal.

Why this is the correct answer:

- [ ] Resource-Based Policy for Lambda Invocation: To allow an AWS service like Amazon EventBridge to invoke an AWS Lambda function, you need to grant permission to the EventBridge service principal in the Lambda function's resource-based policy (also known as the function policy).
- [ ] Principle of Least Privilege - Action: The specific action that EventBridge needs to invoke the Lambda function is lambda:InvokeFunction. Granting only this action, rather than a wildcard like lambda:*, adheres to the principle of least privilege.
- [ ] Principle of Least Privilege - Principal: The principal that needs this permission is the EventBridge service itself, which is specified as Service: events.amazonaws.com.
- [ ] This configuration specifically allows only the EventBridge service to invoke this particular Lambda function, which is the most secure and least privileged setup for this scenario.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: It describes configuring an IAM execution role, which defines what the Lambda function can do (its permissions to access other AWS services), not who can invoke the Lambda function. The Principal in an execution role's trust policy should be Service: lambda.amazonaws.com (allowing the Lambda service to assume the role), not * (which is too permissive). lambda:InvokeFunction is an action granted to an invoker, not typically an action performed by the function itself through its execution role in this context.
- [ ] Option B is wrong because: This also incorrectly focuses on the execution role for granting invocation permission. The principal Service: lambda.amazonaws.com is correct for the trust policy of an execution role, but this doesn't grant EventBridge permission to invoke the function.
- [ ] Option C is wrong because: While using a resource-based policy for the Lambda function and specifying Service: events.amazonaws.com as the principal are correct steps, granting the lambda:* action is too permissive. This would allow EventBridge to perform all possible Lambda actions on the function (e.g., delete the function, update its configuration), which violates the principle of least privilege. Only lambda:InvokeFunction is required.

</details>

<details>
  <summary>Question 106</summary>

A company is preparing to store confidential data in Amazon S3. For compliance reasons, the data must be encrypted at rest. Encryption key usage must be logged for auditing purposes. Keys must be rotated every year.

Which solution meets these requirements and is the MOST operationally efficient?

- [ ] A. Server-side encryption with customer-provided keys (SSE-C)
- [ ] B. Server-side encryption with Amazon S3 managed keys (SSE-S3)
- [ ] C. Server-side encryption with AWS KMS keys (SSE-KMS) with manual rotation
- [ ] D. Server-side encryption with AWS KMS keys (SSE-KMS) with automatic rotation

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Server-side encryption with AWS KMS keys (SSE-KMS) with automatic rotation

Why this is the correct answer:

- [ ] Server-Side Encryption with AWS KMS keys (SSE-KMS): This option encrypts data at rest in Amazon S3 using encryption keys managed in AWS Key Management Service (KMS). This provides strong encryption and allows for centralized key management.
- [ ] Logging of Key Usage: AWS KMS is integrated with AWS CloudTrail. All API calls made to KMS, including when S3 uses a KMS key for encryption or decryption (as with SSE-KMS), are logged in CloudTrail. This provides an audit trail of key usage, meeting the requirement for logging.
- [ ] Automatic Key Rotation: AWS KMS supports automatic annual rotation of the key material for customer master keys (CMKs) that are customer managed (you create them in KMS, and AWS rotates the backing key material). This meets the requirement that "Keys must be rotated every year" with minimal manual intervention.
- [ ] MOST Operationally Efficient: Using SSE-KMS with automatic key rotation is highly operationally efficient. AWS manages the encryption process, the secure storage of the keys, the automatic rotation of key material, and the logging of key usage through CloudTrail. This significantly reduces the operational burden on the company compared to managing keys and rotation manually.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: With Server-Side Encryption with Customer-Provided Keys (SSE-C), the company is responsible for managing the encryption keys themselves. This includes key generation, secure storage, rotation, and providing the key with every request to S3. This involves significant operational overhead and complexity, and AWS does not manage or log these customer-provided keys in KMS/CloudTrail in the same way.
- [ ] Option B is wrong because: Server-Side Encryption with Amazon S3 Managed Keys (SSE-S3) uses encryption keys that are fully managed by Amazon S3. While this is very simple and encrypts data at rest, it provides less control and less detailed audit information about key usage compared to SSE-KMS. The rotation of these S3-managed keys is handled by AWS, but the customer has no direct control or visibility into this process, and the auditing of key usage for compliance might not be as robust as with SSE-KMS.
- [ ] Option C is wrong because: While SSE-KMS is the correct encryption method, relying on "manual rotation" of KMS keys introduces operational overhead. The administrator would need to remember to rotate the keys annually and perform the rotation steps. Automatic rotation (as in option D) is more operationally efficient and less prone to human error for meeting the annual rotation requirement.

</details>

<details>
  <summary>Question 107</summary>

A bicycle sharing company is developing a multi-tier architecture to track the location of its bicycles during peak operating hours. The company wants to use these data points in its existing analytics platform. A solutions architect must determine the most viable multi-tier option to support this architecture. The data points must be accessible from the REST API.

Which action meets these requirements for storing and retrieving location data?

- [ ] A. Use Amazon Athena with Amazon S3.
- [ ] B. Use Amazon API Gateway with AWS Lambda.
- [ ] C. Use Amazon QuickSight with Amazon Redshift.
- [ ] D. Use Amazon API Gateway with Amazon Kinesis Data Analytics.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Use Amazon API Gateway with AWS Lambda.

Why this is the correct answer:

This question focuses on the components needed for storing and retrieving location data that must be accessible via a REST API, and then subsequently used in an analytics platform.

- [ ] Amazon API Gateway for REST API: Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. This directly addresses the requirement that "The data points must be accessible from the REST API." API Gateway would serve as the frontend for these API calls.
- [ ] AWS Lambda for Backend Logic (Storing and Retrieving): AWS Lambda functions can be integrated with API Gateway to provide the backend logic. When API Gateway receives a request (e.g., to store a new location or retrieve a location), it can invoke a Lambda function. This Lambda function would then contain the code to interact with a suitable database or storage service (like Amazon DynamoDB, which is often used for such use cases due to its scalability and low latency, though the specific database isn't named in this option) to perform the actual "storing and retrieving location data."
- [ ] Multi-Tier Architecture and Viability: This combination forms a scalable, serverless backend, fitting a multi-tier architecture. The data, once stored by this mechanism, can then be fed into the "existing analytics platform."

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: Amazon Athena is a serverless query service primarily used for analyzing data in Amazon S3 using standard SQL. While S3 could store location data and Athena could analyze it (fitting the "analytics platform" part), Athena is not designed for real-time transactional storing and retrieving of individual data points via a low-latency REST API, especially for tracking active bicycle locations.
- [ ] Option C is wrong because: Amazon QuickSight is a business intelligence (BI) service used for creating visualizations and dashboards. Amazon Redshift is a data warehousing service. Both are components of an analytics platform but are not the primary tools for building a REST API to store and retrieve individual, real-time location data points for a tracking system.
- [ ] Option D is wrong because: While Amazon API Gateway is correct for the REST API frontend, Amazon Kinesis Data Analytics is designed for real-time processing and analysis of streaming data. It's not a storage service from which individual data points would typically be retrieved via an API in the context of a simple store/retrieve pattern. Data would flow through Kinesis Data Analytics for analysis, not be stored in it for API-based retrieval of specific records.

</details>

<details>
  <summary>Question 108</summary>

A company has an automobile sales website that stores its listings in a database on Amazon RDS. When an automobile is sold, the listing needs to be removed from the website and the data must be sent to multiple target systems.

Which design should a solutions architect recommend?

- [ ] A. Create an AWS Lambda function triggered when the database on Amazon RDS is updated to send the information to an Amazon Simple Queue Service (Amazon SQS) queue for the targets to consume.
- [ ] B. Create an AWS Lambda function triggered when the database on Amazon RDS is updated to send the information to an Amazon Simple Queue Service (Amazon SQS) FIFO queue for the targets to consume.
- [ ] C. Subscribe to an RDS event notification and send an Amazon Simple Queue Service (Amazon SQS) queue fanned out to multiple Amazon Simple Notification Service (Amazon SNS) topics. Use AWS Lambda functions to update the targets.
- [ ] D. Subscribe to an RDS event notification and send an Amazon Simple Notification Service (Amazon SNS) topic fanned out to multiple Amazon Simple Queue Service (Amazon SQS) queues. Use AWS Lambda functions to update the targets.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Create an AWS Lambda function triggered when the database on Amazon RDS is updated to send the information to an Amazon Simple Queue Service (Amazon SQS) queue for the targets to consume.

Why this is the correct answer:

This question describes a scenario where an event (an automobile sale, resulting in a database update) needs to trigger multiple downstream actions: removing the listing from the website and sending data to several target systems.

- [ ] Triggering Mechanism (Interpreted): The phrase "triggered when the database on Amazon RDS is updated" likely implies that the application logic that records the sale in the RDS database will also initiate the trigger for an AWS Lambda function. (Direct RDS triggers for data manipulation events to Lambda are not a standard out-of-the-box feature; it's usually application-driven or via more complex Change Data Capture setups).
- [ ] AWS Lambda for Orchestration: The triggered Lambda function can perform the immediate necessary actions. This includes:
- [ ] Logic to remove the listing from the website (e.g., by updating a cache, calling a website API, or modifying another data store that the website reads from).
- [ ] Sending the relevant sales data as a message to an Amazon SQS queue.
- [ ] Amazon SQS for Decoupling and Multiple Consumers: The SQS queue acts as a durable and reliable buffer. "Multiple target systems" can then have their own consumer applications (which could also be other Lambda functions or EC2-based applications) that independently poll this SQS queue to receive and process the sales data. This decouples the initial sales event processing from the individual target systems, allowing them to process data at their own pace and providing resilience.
- [ ] This approach provides a decoupled architecture where the initial Lambda handles primary post-sale actions and SQS ensures reliable delivery of sales information to various downstream systems.

<hr> Why are the other answers wrong? <hr>

- [ ] Option B is wrong because: While using an SQS queue is good, specifying an SQS FIFO (First-In, First-Out) queue might be overly restrictive if strict ordering of sales data processing by all target systems is not explicitly required. Standard SQS queues offer higher throughput. If ordering is critical for all consumers, FIFO would be considered, but the question doesn't stress this as a primary requirement over the general mechanism. The core solution in A (Lambda + SQS) is sound.
- [ ] Option C is wrong because:
RDS event notifications typically relate to operational events of the DB instance (e.g., backups, failovers, parameter group changes), not to data modification events within tables like a sale being recorded.
The fan-out pattern is described incorrectly: SQS fanning out to multiple SNS topics is not the standard way to distribute a single message to multiple distinct processing queues. The typical pattern is SNS fanning out to multiple SQS queues.
- [ ] Option D is wrong because: Similar to option C, relying on "RDS event notification" to capture a business event like a sale (which is a data change) is generally not how RDS event notifications work. While the subsequent pattern of SNS fanning out to multiple SQS queues for different target systems (each consumed by Lambda) is a valid and robust fan-out architecture, the initial trigger mechanism via RDS operational event notifications is likely incorrect for this use case. Option A's implied application-driven Lambda trigger is more plausible for acting on a data update.

Given the options, Option A provides a viable (with interpretation of the trigger) and straightforward approach for handling the post-sale tasks using Lambda and SQS for distribution to multiple systems.

</details>

<details>
  <summary>Question 109</summary>

A company needs to store data in Amazon S3 and must prevent the data from being changed. The company wants new objects that are uploaded to Amazon S3 to remain unchangeable for a nonspecific amount of time until the company decides to modify the objects. Only specific users in the company's AWS account can have the ability to delete the objects.

What should a solutions architect do to meet these requirements?

- [ ] A. Create an S3 Glacier vault. Apply a write-once, read-many (WORM) vault lock policy to the objects.
- [ ] B. Create an S3 bucket with S3 Object Lock enabled. Enable versioning. Set a retention period of 100 years. Use governance mode as the S3 bucket's default retention mode for new objects.
- [ ] C. Create an S3 bucket. Use AWS CloudTrail to track any S3 API events that modify the objects. Upon notification, restore the modified objects from any backup versions that the company has.
- [ ] D. Create an S3 bucket with S3 Object Lock enabled. Enable versioning. Add a legal hold to the objects. Add the s3:PutObjectLegalHold permission to the IAM policies of users who need to delete the objects.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Create an S3 bucket with S3 Object Lock enabled. Enable versioning. Add a legal hold to the objects. Add the s3:PutObjectLegalHold permission to the IAM policies of users who need to delete the objects.

Why this is the correct answer:

- [ ] S3 Object Lock for Immutability: The requirement is to "prevent the data from being changed." S3 Object Lock provides Write-Once-Read-Many (WORM) capabilities to protect objects from being deleted or overwritten.
- [ ] Versioning Prerequisite: S3 Object Lock requires S3 Versioning to be enabled on the bucket.
- [ ] Legal Hold for Indefinite, Controllable Retention: The company wants objects to remain unchangeable "for a nonspecific amount of time until the company decides to modify the objects." A legal hold, which is a feature of S3 Object Lock, provides this exact functionality. A legal hold prevents an object version from being overwritten or deleted, but unlike a retention period, a legal hold does not have an expiration date. It remains in effect until explicitly removed.
- [ ] Controlled Deletion/Modification via Legal Hold Removal: "Only specific users... can have the ability to delete the objects." To delete an object under a legal hold, the legal hold must first be removed. The permission s3:PutObjectLegalHold allows users to place and remove legal holds on objects. By granting this permission only to the specific users, the company controls who can effectively make an object deletable (by first removing the legal hold).
- [ ] This solution meets all requirements: data is unchangeable, the unchangeable period is indefinite until a decision is made, and specific users control the ability to lift this protection.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: While S3 Glacier Vault Lock can enforce WORM policies, it applies to S3 Glacier vaults, which are for archival storage. The question implies uploads to standard "Amazon S3" buckets and doesn't necessarily dictate archival from the outset. S3 Object Lock is a feature for standard S3 buckets.
- [ ] Option B is wrong because: Setting a fixed retention period (e.g., 100 years) with S3 Object Lock does not meet the requirement for the objects to remain unchangeable for a "nonspecific amount of time until the company decides to modify the objects." A legal hold provides this indefinite protection that can be explicitly removed when needed. Governance mode, while allowing overrides with special permissions, is tied to a specific retention period.
- [ ] Option C is wrong because: Using AWS CloudTrail to track modifications and then restoring from backups is a reactive approach (detect and recover), not a preventative one. The requirement is to "prevent the data from being changed" in the first place.

</details>

<details>
  <summary>Question 110</summary>

A social media company allows users to upload images to its website. The website runs on Amazon EC2 instances. During upload requests, the website resizes the images to a standard size and stores the resized images in Amazon S3. Users are experiencing slow upload requests to the website. The company needs to reduce coupling within the application and improve website performance. A solutions architect must design the most operationally efficient process for image uploads.

Which combination of actions should the solutions architect take to meet these requirements? (Choose two.)

- [ ] A. Configure the application to upload images to S3 Glacier.
- [ ] B. Configure the web server to upload the original images to Amazon S3.
- [ ] C. Configure the application to upload images directly from each user's browser to Amazon S3 through the use of a presigned URL
- [ ] D. Configure S3 Event Notifications to invoke an AWS Lambda function when an image is uploaded. Use the function to resize the image.
- [ ] E. Create an Amazon EventBridge (Amazon CloudWatch Events) rule that invokes an AWS Lambda function on a schedule to resize uploaded images.

</details>

<details>
  <summary>Answer</summary>

- [ ] C. Configure the application to upload images directly from each user's browser to Amazon S3 through the use of a presigned URL
- [ ] D. Configure S3 Event Notifications to invoke an AWS Lambda function when an image is uploaded. Use the function to resize the image.

Why these are the correct answers:

This solution aims to offload work from the EC2 instances and process images asynchronously, improving website performance and operational efficiency.

C. Configure the application to upload images directly from each user's browser to Amazon S3 through the use of a presigned URL
- [ ] Improved Website Performance: Currently, the EC2 instances handle the image uploads, which contributes to slow requests.
- [ ] By allowing users' browsers to upload images directly to an Amazon S3 bucket using a presigned URL, the web servers (EC2 instances) are bypassed for the actual file transfer.
- [ ] This significantly reduces the load on the web servers, freeing them up to handle other requests and improving the perceived performance for the user during the upload process.
- [ ] Reduced Coupling: This decouples the web servers from the responsibility of handling the raw image uploads.

D. Configure S3 Event Notifications to invoke an AWS Lambda function when an image is uploaded. Use the function to resize the image.
- [ ] Asynchronous Image Resizing: Instead of the EC2 instances resizing images synchronously during the upload request (which causes slowness), this approach processes images asynchronously.
- [ ] When an original image is uploaded to S3 (as per option C), an S3 event notification can automatically trigger an AWS Lambda function.
- [ ] Serverless and Scalable Resizing: The Lambda function can then perform the image resizing.
- [ ] Lambda is serverless, scales automatically with the number of uploads, and is operationally efficient as you don't manage servers for this processing.
- [ ] The resized image can then be stored back in S3. This decouples the resizing logic from the web application.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: Amazon S3 Glacier is a low-cost storage class designed for data archiving where retrieval times of minutes to hours are acceptable. It is not suitable for storing images that need to be processed (resized) and potentially accessed shortly after upload.
- [ ] Option B is wrong because: While having the web server upload the original images to S3 is a step towards using S3, it doesn't fundamentally solve the problem of "slow upload requests" if the web server is still the intermediary handling the full upload from the user's browser and potentially doing synchronous resizing. Option C, direct browser upload to S3, is more effective at offloading the web server.
- [ ] Option E is wrong because: Using a scheduled EventBridge rule to trigger a Lambda function for image resizing means that images would not be processed immediately after upload. Instead, they would be processed in batches when the schedule runs. This could lead to delays in users seeing their resized images and doesn't fit the typical expectation of quick processing after an upload. S3 event notifications (as in option D) provide near real-time, event-driven processing.

</details>

# AWS-SAA-PRACTICE-EXAM Questions 111-120

<details>
  <summary>Question 111</summary>

A company recently migrated a message processing system to AWS. The system receives messages into an ActiveMQ queue running on an Amazon EC2 instance. Messages are processed by a consumer application running on Amazon EC2. The consumer application processes the messages and writes results to a MySQL database running on Amazon EC2. The company wants this application to be highly available with low operational complexity.

Which architecture offers the HIGHEST availability?

- [ ] A. Add a second ActiveMQ server to another Availability Zone. Add an additional consumer EC2 instance in another Availability Zone. Replicate the MySQL database to another Availability Zone.
- [ ] B. Use Amazon MQ with active/standby brokers configured across two Availability Zones. Add an additional consumer EC2 instance in another Availability Zone. Replicate the MySQL database to another Availability Zone.
- [ ] C. Use Amazon MQ with active/standby brokers configured across two Availability Zones. Add an additional consumer EC2 instance in another Availability Zone. Use Amazon RDS for MySQL with Multi-AZ enabled.
- [ ] D. Use Amazon MQ with active/standby brokers configured across two Availability Zones. Add an Auto Scaling group for the consumer EC2 instances across two Availability Zones. Use Amazon RDS for MySQL with Multi-AZ enabled.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Use Amazon MQ with active/standby brokers configured across two Availability Zones. Add an Auto Scaling group for the consumer EC2 instances across two Availability Zones. Use Amazon RDS for MySQL with Multi-AZ enabled.

Why this is the correct answer:

This solution leverages managed services and Auto Scaling for the highest availability and reduced operational complexity:

- [ ] Amazon MQ for Message Broker: Migrating the self-managed ActiveMQ on EC2 to Amazon MQ (which supports ActiveMQ as an engine) provides a managed message broker service. Configuring Amazon MQ with active/standby brokers across two - [ ] Availability Zones ensures high availability for the message queue itself, with automatic failover. This significantly reduces operational complexity compared to managing ActiveMQ on EC2.
- [ ] Auto Scaling Group for Consumer EC2 Instances: Placing the consumer EC2 instances into an Auto Scaling group that spans across two Availability Zones ensures that the message processing tier is both highly available and scalable. If an instance or an AZ fails, the Auto Scaling group can launch new instances in healthy AZs to maintain processing capacity.
- [ ] Amazon RDS for MySQL with Multi-AZ: Migrating the self-managed MySQL database on EC2 to Amazon RDS for MySQL with the Multi-AZ deployment option provides a highly available and durable database. RDS Multi-AZ automatically creates and maintains a synchronous standby replica in a different AZ. In the event of a primary database failure or AZ outage, RDS automatically fails over to the standby replica, minimizing downtime. This is far less operationally complex than self-managing database replication and failover.
- [ ] This combination provides high availability across all critical components (message queue, consumer application, database) using managed services where possible, thus reducing operational burden.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: While it attempts to introduce redundancy, it relies on self-managing the ActiveMQ servers and self-managing MySQL database replication across Availability Zones. This involves significant operational overhead for setup, maintenance, patching, and ensuring reliable failover compared to using managed AWS services.
- [ ] Option B is wrong because: It correctly suggests using Amazon MQ for the message broker. However, for the consumer application, simply adding "an additional consumer EC2 instance in another Availability Zone" does not provide the same level of automated scaling, health checking, and instance replacement as an Auto Scaling group. More critically, "Replicate the MySQL database to another Availability Zone" implies a self-managed replication for MySQL on EC2, which is less available and more operationally complex than using Amazon RDS Multi-AZ.
- [ ] Option C is wrong because: This option is better as it includes Amazon MQ and Amazon RDS for MySQL with Multi-AZ, which are good choices. However, for the consumer EC2 instances, only "Add an additional consumer EC2 instance in another Availability Zone" is mentioned. An Auto Scaling group (as in option D) provides superior availability and scalability for the consumer tier by automatically managing the desired number of instances and replacing unhealthy ones across AZs.

</details>
  
<details>
  <summary>Question 112</summary>

A company hosts a containerized web application on a fleet of on-premises servers that process incoming requests. The number of requests is growing quickly. The on-premises servers cannot handle the increased number of requests. The company wants to move the application to AWS with minimum code changes and minimum development effort.

Which solution will meet these requirements with the LEAST operational overhead?

- [ ] A. Use AWS Fargate on Amazon Elastic Container Service (Amazon ECS) to run the containerized web application with Service Auto Scaling. Use an Application Load Balancer to distribute the incoming requests.
- [ ] B. Use two Amazon EC2 instances to host the containerized web application. Use an Application Load Balancer to distribute the incoming requests.
- [ ] C. Use AWS Lambda with a new code that uses one of the supported languages. Create multiple Lambda functions to support the load. Use Amazon API Gateway as an entry point to the Lambda functions.
- [ ] D. Use a high performance computing (HPC) solution such as AWS ParallelCluster to establish an HPC cluster that can process the incoming requests at the appropriate scale.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Use AWS Fargate on Amazon Elastic Container Service (Amazon ECS) to run the containerized web application with Service Auto Scaling. Use an Application Load Balancer to distribute the incoming requests.

Why this is the correct answer:

- [ ] Leveraging Existing Containerization: The application is already containerized. Amazon Elastic Container Service (ECS) is a fully managed container orchestration service that makes it easy to run, stop, and manage Docker containers on a cluster. This aligns with "minimum code changes."   
- [ ] AWS Fargate for Serverless Compute: AWS Fargate is a serverless compute engine for containers that works with Amazon ECS. When using Fargate, you do not need to provision, configure, or manage the underlying EC2 instances. AWS handles the server management. This directly addresses the requirement for the "LEAST operational overhead" and simplifies scaling.
- [ ] Service Auto Scaling: Amazon ECS services running on Fargate can be configured with Service Auto Scaling to automatically adjust the number of running container tasks based on demand (e.g., CPU or memory utilization, or custom metrics). This handles the "growing quickly" number of requests.
- [ ] Application Load Balancer (ALB): An ALB is well-suited to distribute incoming HTTP/HTTPS traffic across the container tasks managed by ECS on Fargate, providing a scalable and highly available frontend.
- [ ] Minimum Development Effort: Since the application is already containerized, deploying it to ECS on Fargate generally requires minimal development effort beyond defining the task definitions and service configurations.

<hr> Why are the other answers wrong? <hr>

- [ ] Option B is wrong because: Simply using "two Amazon EC2 instances" to host the application does not offer a scalable solution for a rapidly growing number of requests and involves higher operational overhead for managing the instances, container runtime, and scaling compared to Fargate. It's not designed to handle quick growth efficiently.
- [ ] Option C is wrong because: Migrating a containerized web application to AWS Lambda would likely require significant code changes and refactoring to fit Lambda's event-driven, stateless execution model. This contradicts the "minimum code changes and minimum development effort" requirement. While Lambda is serverless, it's a different paradigm than running existing containers.
- [ ] Option D is wrong because: AWS ParallelCluster is designed for orchestrating and managing High-Performance Computing (HPC) clusters, typically for batch processing, scientific research, or complex simulations. It is not an appropriate or cost-effective solution for hosting a general "containerized web application" that processes incoming user requests.

</details>

<details>
  <summary>Question 113</summary>

A company uses 50 TB of data for reporting. The company wants to move this data from on premises to AWS. A custom application in the company's data center runs a weekly data transformation job. The company plans to pause the application until the data transfer is complete and needs to begin the transfer process as soon as possible. The data center does not have any available network bandwidth for additional workloads. A solutions architect must transfer the data and must configure the transformation job to continue to run in the AWS Cloud.

Which solution will meet these requirements with the LEAST operational overhead?

- [ ] A. Use AWS DataSync to move the data. Create a custom transformation job by using AWS Glue.
- [ ] B. Order an AWS Snowcone device to move the data. Deploy the transformation application to the device.
- [ ] C. Order an AWS Snowball Edge Storage Optimized device. Copy the data to the device. Create a custom transformation job by using AWS Glue.
- [ ] D. Order an AWS Snowball Edge Storage Optimized device that includes Amazon EC2 compute. Copy the data to the device. Create a new EC2 instance on AWS to run the transformation application.

</details>

<details>
  <summary>Answer</summary>

- [ ] C. Order an AWS Snowball Edge Storage Optimized device. Copy the data to the device. Create a custom transformation job by using AWS Glue.

Why this is the correct answer:

This solution addresses the key constraints: large data volume (50 TB), no available network bandwidth for transfer, and the need to run transformations in AWS with minimal operational overhead.

- [ ] AWS Snowball Edge Storage Optimized for Data Transfer: Given that the "data center does not have any available network bandwidth for additional workloads," an online transfer method is not feasible. For 50 TB of data, an AWS Snowball Edge Storage Optimized device is a suitable offline data transfer solution. The company can order the device, copy the 50 TB of data to it on-premises, and then ship it to AWS for ingestion into Amazon S3. This allows the transfer to "begin as soon as possible" without impacting existing network bandwidth.
- [ ] AWS Glue for Data Transformation in the Cloud: Once the data is in Amazon S3, AWS Glue can be used to perform the "weekly data transformation job." AWS Glue is a fully managed ETL (extract, transform, and load) service that is serverless. This means there are no servers to manage, and it aligns with the requirement for the "LEAST operational overhead" for running the transformation job in the AWS Cloud.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: AWS DataSync is an online data transfer service. The problem statement explicitly says, "The data center does not have any available network bandwidth for additional workloads," making DataSync unsuitable for the initial large data transfer.
- [ ] Option B is wrong because: An AWS Snowcone device has a limited storage capacity (typically up to 8 TB for HDD or 14 TB for SSD). For 50 TB of data, multiple Snowcone devices would be required, which is less efficient than a single Snowball Edge device. While Snowcone supports edge compute, deploying and running the transformation application directly on the device is not the most straightforward way to have the job "continue to run in the AWS Cloud" long-term with least operational overhead. AWS Glue is better for cloud-based ETL.
- [ ] Option D is wrong because: While using an AWS Snowball Edge Storage Optimized device for the transfer is correct, setting up and managing a new EC2 instance on AWS to run the existing transformation application involves more operational overhead (patching, OS management, scaling) compared to using a serverless ETL service like AWS Glue. The requirement is for the "LEAST operational overhead" for the transformation job in the cloud.

</details>

<details>
  <summary>Question 114</summary>

A company has created an image analysis application in which users can upload photos and add photo frames to their images. The users upload images and metadata to indicate which photo frames they want to add to their images. The application uses a single Amazon EC2 instance and Amazon DynamoDB to store the metadata. The application is becoming more popular, and the number of users is increasing. The company expects the number of concurrent users to vary significantly depending on the time of day and day of week. The company must ensure that the application can scale to meet the needs of the growing user base.

Which solution meets these requirements?

- [ ] A. Use AWS Lambda to process the photos. Store the photos and metadata in DynamoDB.
- [ ] B. Use Amazon Kinesis Data Firehose to process the photos and to store the photos and metadata.
- [ ] C. Use AWS Lambda to process the photos. Store the photos in Amazon S3. Retain DynamoDB to store the metadata.
- [ ] D. Increase the number of EC2 instances to three. Use Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volumes to store the photos and metadata.

</details>

<details>
  <summary>Answer</summary>

- [ ] C. Use AWS Lambda to process the photos. Store the photos in Amazon S3. Retain DynamoDB to store the metadata.

Why this is the correct answer:

This solution leverages serverless and managed services for scalability and efficiency:

- [ ] AWS Lambda for Photo Processing: AWS Lambda is a serverless compute service that can execute code in response to events (e.g., an image upload). It automatically scales based on the number of incoming requests, making it ideal for handling the "varying significantly" number of concurrent users and the growing user base. Using Lambda to process photos (add frames) offloads this compute-intensive task from a fixed EC2 instance.
- [ ] Amazon S3 for Storing Photos: Amazon S3 is the most appropriate service for storing image files. It is highly scalable, durable, and cost-effective for storing large amounts of user-generated content like photos.
- [ ] Retain DynamoDB for Metadata: The question states the application already uses Amazon DynamoDB to store metadata, and DynamoDB is excellent for this purpose due to its scalability, low latency, and flexible schema. Retaining DynamoDB for metadata is a sound choice.
- [ ] Scalability: This architecture (Lambda for processing, S3 for photo storage, DynamoDB for metadata) is inherently scalable. Each component can scale independently to meet demand.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: While using AWS Lambda for processing and DynamoDB for metadata is good, storing entire photo files (which can be several megabytes or more) directly in Amazon DynamoDB is generally not recommended or cost-effective. DynamoDB has an item size limit (400 KB), and storing large binary objects is better suited for Amazon S3.
- [ ] Option B is wrong because: Amazon Kinesis Data Firehose is a service for loading streaming data into data lakes, data stores, and analytics services. It is not designed for interactive image processing tasks like adding photo frames based on user requests, nor is it a primary storage solution for individual image files and their metadata in the context of a user-facing application.
- [ ] Option D is wrong because: Simply increasing the number of EC2 instances to a fixed number (three) is not a solution that "can scale to meet the needs of the growing user base" dynamically or handle significantly varying demand cost-effectively. It can lead to either under-provisioning (poor performance during peaks) or over-provisioning (wasted cost during lulls). Storing photos and metadata on EBS volumes attached to EC2 instances is also less scalable and more operationally intensive than using S3 and DynamoDB.

</details>

<details>
  <summary>Question 115</summary>

A medical records company is hosting an application on Amazon EC2 instances. The application processes customer data files that are stored on Amazon S3. The EC2 instances are hosted in public subnets. The EC2 instances access Amazon S3 over the internet, but they do not require any other network access. A new requirement mandates that the network traffic for file transfers take a private route and not be sent over the internet.

Which change to the network architecture should a solutions architect recommend to meet this requirement?

- [ ] A. Create a NAT gateway. Configure the route table for the public subnets to send traffic to Amazon S3 through the NAT gateway.
- [ ] B. Configure the security group for the EC2 instances to restrict outbound traffic so that only traffic to the S3 prefix list is permitted.
- [ ] C. Move the EC2 instances to private subnets. Create a VPC endpoint for Amazon S3, and link the endpoint to the route table for the private subnets.
- [ ] D. Remove the internet gateway from the VPC. Set up an AWS Direct Connect connection, and route traffic to Amazon S3 over the Direct Connect connection.

</details>

<details>
  <summary>Answer</summary>

- [ ] C. Move the EC2 instances to private subnets. Create a VPC endpoint for Amazon S3, and link the endpoint to the route table for the private subnets.

Why this is the correct answer:

This solution addresses both security best practices and the specific requirement for private S3 access:

- [ ] Move EC2 Instances to Private Subnets: Hosting application instances, especially those processing sensitive data like medical records, in public subnets is generally not recommended if they do not need direct inbound internet access. Moving them to private subnets enhances their security posture by removing direct internet exposure.
- [ ] VPC Endpoint for Amazon S3 (Gateway Endpoint): A VPC gateway endpoint for S3 allows instances within your VPC to communicate with Amazon S3 without traffic traversing the public internet. Traffic routes privately within the AWS network. This directly meets the requirement that "network traffic for file transfers take a private route and not be sent over the internet."
- [ ] Link Endpoint to Route Table: The gateway endpoint is associated with the route tables of the private subnets where the EC2 instances reside. This ensures that traffic destined for S3 from these instances is routed through the private endpoint.
- [ ] No Other Network Access Needed: The problem states the EC2 instances "do not require any other network access" (implying general internet access beyond S3 is not needed). This makes the private subnet with a VPC endpoint for S3 an ideal and secure configuration.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: A NAT gateway is used to enable instances in private subnets to initiate outbound connections to the internet. If the instances are already in public subnets, they would typically use an Internet Gateway for internet access. Routing traffic from public subnets through a NAT gateway to access S3 is an incorrect use of NAT gateways and doesn't ensure a private route in the intended way; a VPC endpoint is the proper solution.
- [ ] Option B is wrong because: Security groups act as firewalls at the instance level. Restricting outbound traffic to the S3 prefix list is a good security measure to limit what the EC2 instances can connect to. However, it does not change the network path of the traffic. If the instances are in public subnets and an Internet Gateway is present, traffic to S3 public endpoints might still traverse the internet or AWS public network space. This does not guarantee a "private route."
- [ ] Option D is wrong because:
Removing the internet gateway from the VPC would cut off all direct internet access for resources that might legitimately need it (though the question states these EC2s don't need other internet access).
AWS Direct Connect is a service for establishing a dedicated private network connection from an on-premises environment to AWS. It is not the primary mechanism for ensuring EC2 instances within a VPC access S3 privately. A VPC endpoint is the direct solution for intra-AWS private connectivity to S3 from a VPC.

</details>

<details>
  <summary>Question 116</summary>

A company uses a popular content management system (CMS) for its corporate website. However, the required patching and maintenance are burdensome. The company is redesigning its website and wants a new solution. The website will be updated four times a year and does not need to have any dynamic content available. The solution must provide high scalability and enhanced security.

Which combination of changes will meet these requirements with the LEAST operational overhead? (Choose two.)

- [ ] A. Configure Amazon CloudFront in front of the website to use HTTPS functionality.
- [ ] B. Deploy an AWS WAF web ACL in front of the website to provide HTTPS functionality.
- [ ] C. Create and deploy an AWS Lambda function to manage and serve the website content.
- [ ] D. Create the new website and an Amazon S3 bucket. Deploy the website on the S3 bucket with static website hosting enabled.
- [ ] E. Create the new website. Deploy the website by using an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Configure Amazon CloudFront in front of the website to use HTTPS functionality.
- [ ] D. Create the new website and an Amazon S3 bucket. Deploy the website on the S3 bucket with static website hosting enabled.

Why these are the correct answers:

This solution leverages serverless and managed services for a static website, which is ideal for low operational overhead, high scalability, and enhanced security.

D. Create the new website and an Amazon S3 bucket. Deploy the website on the S3 bucket with static website hosting enabled.
- [ ] Static Content Hosting on S3: The requirement states the website "does not need to have any dynamic content available" and is updated infrequently (four times a year). Amazon S3 is an excellent choice for hosting static websites (HTML, CSS, JavaScript, images). It is highly durable, scalable, and very cost-effective.
- [ ] Least Operational Overhead: S3 static website hosting is serverless, meaning there are no servers to manage, patch, or scale, thus minimizing operational overhead.

A. Configure Amazon CloudFront in front of the website to use HTTPS functionality.
- [ ] HTTPS and Enhanced Security: Amazon CloudFront is a global content delivery network (CDN). You can configure it with an SSL/TLS certificate (e.g., a free one from AWS Certificate Manager) to serve your website content over HTTPS, which enhances security. CloudFront also helps protect against common DDoS attacks by absorbing traffic at its distributed edge locations.
- [ ] High Scalability and Performance: CloudFront caches your static content from the S3 origin at edge locations around the world, closer to your users. This provides high scalability to handle traffic spikes and significantly improves website loading performance by reducing latency.
- [ ] Integration: CloudFront integrates seamlessly with S3 as an origin.

<hr> Why are the other answers wrong? <hr>

- [ ] Option B is wrong because: AWS WAF (Web Application Firewall) is a service that helps protect web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources. While WAF is important for security and often used with CloudFront or an Application Load Balancer, it does not directly "provide HTTPS functionality." HTTPS termination is handled by services like CloudFront or ALBs using SSL/TLS certificates.   
- [ ] Option C is wrong because: Using AWS Lambda to manage and serve the content of a purely static website is an overly complex and less efficient solution compared to S3 static website hosting. While Lambda can be used to serve dynamic content or as part of a serverless backend, for hosting static files, S3 is more straightforward, cost-effective, and operationally simpler.
- [ ] Option E is wrong because: Deploying a static website using an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer is a solution designed for dynamic applications or those requiring server-side compute. For a website with no dynamic content, this approach incurs unnecessary costs (for EC2 instances, ALB) and significantly higher operational overhead (managing instances, patching, OS updates) compared to the S3 and CloudFront serverless model.

</details>

<details>
  <summary>Question 117</summary>

A company stores its application logs in an Amazon CloudWatch Logs log group. A new policy requires the company to store all application logs in Amazon OpenSearch Service (Amazon Elasticsearch Service) in near-real time.

Which solution will meet this requirement with the LEAST operational overhead?

- [ ] A. Configure a CloudWatch Logs subscription to stream the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service).
- [ ] B. Create an AWS Lambda function. Use the log group to invoke the function to write the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service).
- [ ] C. Create an Amazon Kinesis Data Firehose delivery stream. Configure the log group as the delivery stream's sources. Configure Amazon OpenSearch Service (Amazon Elasticsearch Service) as the delivery stream's destination.
- [ ] D. Install and configure Amazon Kinesis Agent on each application server to deliver the logs to Amazon Kinesis Data Streams. Configure Kinesis Data Streams to deliver the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service).

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Configure a CloudWatch Logs subscription to stream the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service).

Why this is the correct answer:

- [ ] CloudWatch Logs Subscriptions: Amazon CloudWatch Logs allows you to create subscription filters. These filters can match log events from a log group and stream them in near real-time to various AWS destinations, including Amazon Kinesis Data Streams, AWS Lambda, and directly to an Amazon OpenSearch Service (formerly Amazon Elasticsearch Service) domain.
- [ ] Direct Streaming to OpenSearch Service: Configuring a CloudWatch Logs subscription to stream directly to an Amazon OpenSearch Service domain is a built-in, fully managed capability. This is the most straightforward and direct method for this specific requirement.
- [ ] Near-Real Time: Subscription filters process and deliver data in near-real time.
- [ ] Least Operational Overhead: This approach requires minimal setup and no ongoing management of intermediary services like custom Lambda functions or Kinesis Data Firehose delivery streams (if their specific features aren't needed). AWS manages the data streaming pipeline from CloudWatch Logs to OpenSearch Service.

<hr> Why are the other answers wrong? <hr>

- [ ] Option B is wrong because: While you can configure a CloudWatch Logs subscription to trigger an AWS Lambda function, which then writes the logs to OpenSearch Service, this introduces custom code that needs to be developed, deployed, monitored, and maintained. A direct subscription (as in Option A) avoids this custom Lambda logic, resulting in less operational overhead.
- [ ] Option C is wrong because: This proposes a path: CloudWatch Logs -> Amazon Kinesis Data Firehose -> Amazon OpenSearch Service. While this is a valid and robust way to get data into OpenSearch Service (and Firehose offers features like data transformation, batching, compression, and retries), it involves an additional service (Kinesis Data Firehose) to configure and manage. If the only requirement is to stream logs from CloudWatch Logs to OpenSearch Service without needing the specific transformation or buffering capabilities of Firehose, a direct subscription (Option A) is simpler and has less operational overhead.
- [ ] Option D is wrong because: This solution suggests changing the log collection mechanism by installing and configuring the Amazon Kinesis Agent on each application server to send logs to Amazon Kinesis Data Streams, which then deliver to OpenSearch Service. The problem states the logs are already being collected in CloudWatch Logs. This option introduces unnecessary complexity and operational overhead by requiring agent installation and management on application servers and doesn't leverage the existing CloudWatch Logs setup.

</details>

<details>
  <summary>Question 118</summary>

A company is building a web-based application running on Amazon EC2 instances in multiple Availability Zones. The web application will provide access to a repository of text documents totaling about 900 TB in size. The company anticipates that the web application will experience periods of high demand. A solutions architect must ensure that the storage component for the text documents can scale to meet the demand of the application at all times. The company is concerned about the overall cost of the solution.

Which storage solution meets these requirements MOST cost-effectively?

- [ ] A. Amazon Elastic Block Store (Amazon EBS)
- [ ] B. Amazon Elastic File System (Amazon EFS)
- [ ] C. Amazon OpenSearch Service (Amazon Elasticsearch Service)
- [ ] D. Amazon S3

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Amazon S3

Why this is the correct answer:

- [ ] Amazon S3 for Scalability and Cost-Effectiveness: Amazon S3 (Simple Storage Service) is designed for storing and retrieving massive amounts of data (like 900 TB) with high durability and availability. It scales automatically to handle storage needs and request loads, which is crucial for an application experiencing periods of high demand. Critically, S3 is generally the MOST cost-effective AWS storage option for large object storage like text documents, especially when considering the scale of 900 TB.
- [ ] Accessibility for Web Applications: The text documents stored in S3 can be easily accessed by the web application running on EC2 instances, either directly via the S3 API or by serving them through services like Amazon CloudFront for optimized delivery.
- [ ] Meeting Demand: S3's inherent scalability ensures it can handle the fluctuating demand of the application.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: Amazon Elastic Block Store (Amazon EBS) provides block-level storage volumes for use with EC2 instances. While EBS offers various performance tiers, storing and managing 900 TB of data directly on EBS volumes across multiple EC2 instances would be significantly more expensive and operationally complex than using S3. EBS volumes are also zonal, so sharing this amount of data across a fleet of instances in multiple Availability Zones would require additional mechanisms if a shared repository is needed, adding complexity and cost. S3 is better suited as a central repository for this volume of data.
- [ ] Option B is wrong because: Amazon Elastic File System (Amazon EFS) provides scalable, shared file storage that can be accessed by multiple EC2 instances. While EFS can scale to petabytes, its cost per GB is typically higher than Amazon S3. For storing a large repository of 900 TB of text documents where cost-effectiveness is a major concern, S3 is usually the more economical choice. EFS is often preferred when applications require a POSIX-compliant file system interface.
- [ ] Option C is wrong because: Amazon OpenSearch Service (Amazon Elasticsearch Service) is a managed service for search, analytics, and visualization of data. While it could be used to index the content of the text documents to make them searchable, OpenSearch Service itself is not designed or cost-effective as a primary storage solution for 900 TB of raw text documents. It would typically be used in conjunction with a primary storage service like S3, where the documents are stored, and OpenSearch Service stores the search index.

</details>

<details>
  <summary>Question 119</summary>

A global company is using Amazon API Gateway to design REST APIs for its loyalty club users in the us-east-1 Region and the ap-southeast-2 Region. A solutions architect must design a solution to protect these API Gateway managed REST APIs across multiple accounts from SQL injection and cross-site scripting attacks.

Which solution will meet these requirements with the LEAST amount of administrative effort?

- [ ] A. Set up AWS WAF in both Regions. Associate Regional web ACLs with an API stage.
- [ ] B. Set up AWS Firewall Manager in both Regions. Centrally configure AWS WAF rules.
- [ ] C. Set up AWS Shield in bath Regions. Associate Regional web ACLs with an API stage.
- [ ] D. Set up AWS Shield in one of the Regions. Associate Regional web ACLs with an API stage.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Set up AWS Firewall Manager in both Regions. Centrally configure AWS WAF rules.

Why this is the correct answer:

- [ ] AWS WAF for Application Layer Protection: AWS WAF (Web Application Firewall) is the appropriate service to protect web applications and APIs (like those managed by Amazon API Gateway) from common web exploits such as SQL injection and cross-site scripting (XSS) attacks.
- [ ] AWS Firewall Manager for Centralized Management: The company has APIs in multiple regions (us-east-1, ap-southeast-2) and potentially "across multiple accounts." AWS Firewall Manager is a security management service that allows you to centrally configure and manage firewall rules (including AWS WAF rules) across your AWS accounts and applications within an AWS Organization. This means you can define a common security policy (e.g., WAF rules for SQL injection and XSS protection) and automatically apply it to all your API Gateway instances in different regions and accounts.
- [ ] Least Amount of Administrative Effort: Using Firewall Manager to centrally deploy and manage AWS WAF rules significantly reduces the administrative effort compared to manually configuring and maintaining WAF web ACLs in each region and for each account individually. It ensures consistency and simplifies policy updates.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: While setting up AWS WAF in both regions and associating web ACLs with the API stages is necessary for protection, doing this manually in each region (and potentially each account if "across multiple accounts" implies different AWS accounts) does not represent the "LEAST amount of administrative effort." AWS Firewall Manager (Option B) is designed for centralized management, which reduces this effort.
- [ ] Option C is wrong because: AWS Shield is a managed Distributed Denial of Service (DDoS) protection service. While AWS Shield Advanced works in conjunction with AWS WAF for comprehensive protection, Shield itself primarily focuses on mitigating DDoS attacks, not specifically SQL injection or cross-site scripting. AWS WAF is the service tailored for protecting against these application-layer attacks.
- [ ] Option D is wrong because: Similar to option C, AWS Shield is for DDoS protection. Furthermore, protections like WAF rules need to be applied in each region where the API Gateway endpoints exist, not just one region, to be effective for all APIs.


</details>

<details>
  <summary>Question 120</summary>
 
A company has implemented a self-managed DNS solution on three Amazon EC2 instances behind a Network Load Balancer (NLB) in the us-west-2 Region. Most of the company's users are located in the United States and Europe. The company wants to improve the performance and availability of the solution. The company launches and configures three EC2 instances in the eu-west-1 Region and adds the EC2 instances as targets for a new NLB.

Which solution can the company use to route traffic to all the EC2 instances?

- [ ] A. Create an Amazon Route 53 geolocation routing policy to route requests to one of the two NLBs. Create an Amazon CloudFront distribution. Use the Route 53 record as the distribution's origin.
- [ ] B. Create a standard accelerator in AWS Global Accelerator. Create endpoint groups in us-west-2 and eu-west-1. Add the two NLBs as endpoints for the endpoint groups.
- [ ] C. Attach Elastic IP addresses to the six EC2 instances. Create an Amazon Route 53 geolocation routing policy to route requests to one of the six EC2 instances. Create an Amazon CloudFront distribution. Use the Route 53 record as the distribution's origin.
- [ ] D. Replace the two NLBs with two Application Load Balancers (ALBs). Create an Amazon Route 53 latency routing policy to route requests to one of the two ALBs. Create an Amazon CloudFront distribution. Use the Route 53 record as the distribution's origin.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Create a standard accelerator in AWS Global Accelerator. Create endpoint groups in us-west-2 and eu-west-1. Add the two NLBs as endpoints for the endpoint groups.

Why this is the correct answer:

- [ ] AWS Global Accelerator for Global Traffic Management: AWS Global Accelerator is a service that improves the availability and performance of your applications with local or global users. It provides static IP addresses that act as a fixed entry point to your application endpoints in one or more AWS Regions. Global Accelerator directs user traffic to the optimal endpoint based on factors like user location, endpoint health, and configured weights.   
- [ ] Suitable for Non-HTTP (DNS) Traffic: DNS primarily uses UDP (and sometimes TCP) on port 53. Global Accelerator supports both TCP and UDP traffic, making it suitable for routing DNS requests.
- [ ] Improved Performance and Availability: By creating endpoint groups in us-west-2 and eu-west-1 and adding the respective NLBs as endpoints, Global Accelerator will route users to the nearest healthy regional deployment of the DNS service. This reduces latency and improves availability through health checks and automatic failover.
- [ ] Uses Existing NLBs: This solution leverages the existing Network Load Balancers, which are appropriate for handling DNS traffic to the EC2 instances.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: Amazon CloudFront is a content delivery network (CDN) designed primarily for caching and delivering web content (HTTP/HTTPS). It is not suitable for distributing or routing DNS query traffic. While Amazon Route 53 geolocation routing can direct users to the NLB in the geographically closest region, Global Accelerator offers additional benefits like routing traffic over the optimized AWS global network and providing static anycast IP addresses.
- [ ] Option C is wrong because: Attaching Elastic IP addresses to individual EC2 instances and using Route 53 to route directly to these instances bypasses the Network Load Balancers. This would eliminate the load balancing and health checking benefits provided by the NLBs, reducing availability and scalability. Also, CloudFront is not suitable for DNS traffic.
- [ ] Option D is wrong because:
Application Load Balancers (ALBs) operate at Layer 7 and are designed for HTTP/HTTPS traffic. They are not suitable for handling DNS traffic, which primarily uses UDP/TCP on port 53. Network Load Balancers are the correct choice for this protocol.

Again, CloudFront is not appropriate for DNS traffic distribution.

</details>

# AWS-SAA-PRACTICE-EXAM Questions 121-130

<details>
  <summary>Question 121</summary>

A company is running an online transaction processing (OLTP) workload on AWS. This workload uses an unencrypted Amazon RDS DB instance in a Multi-AZ deployment. Daily database snapshots are taken from this instance.

What should a solutions architect do to ensure the database and snapshots are always encrypted moving forward?

- [ ] A. Encrypt a copy of the latest DB snapshot. Replace existing DB instance by restoring the encrypted snapshot.
- [ ] B. Create a new encrypted Amazon Elastic Block Store (Amazon EBS) volume and copy the snapshots to it. Enable encryption on the DB instance.
- [ ] C. Copy the snapshots and enable encryption using AWS Key Management Service (AWS KMS) Restore encrypted snapshot to an existing DB instance.
- [ ] D. Copy the snapshots to an Amazon S3 bucket that is encrypted using server-side encryption with AWS Key Management Service (AWS KMS) managed keys (SSE-KMS).

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Encrypt a copy of the latest DB snapshot. Replace existing DB instance by restoring the encrypted snapshot.

Why this is the correct answer:

- [ ] Process for Encrypting an Unencrypted RDS Instance: You cannot directly encrypt an existing Amazon RDS DB instance that was originally created unencrypted. The standard procedure to achieve encryption for such an instance involves using snapshots:
- [ ] Take a final snapshot of the existing unencrypted DB instance.
- [ ] Copy this snapshot. During the copy process, you can specify an AWS Key Management Service (AWS KMS) key to encrypt the copied snapshot.
- [ ] Restore a new DB instance from this newly created encrypted snapshot. The new DB instance will have its underlying storage encrypted with the specified KMS key.
- [ ] Once the new encrypted DB instance is running and tested, update your application to connect to the new instance, and then you can delete the old unencrypted DB instance.
- [ ] Encryption of Future Snapshots: When an RDS DB instance is encrypted (because it was restored from an encrypted snapshot), all subsequent automated and manual snapshots taken from this instance will also be encrypted using the same KMS key by default. This ensures that snapshots are "always encrypted moving forward."
- [ ] This approach ensures both the live database and all its future snapshots are encrypted.

<hr> Why are the other answers wrong? <hr>

- [ ] Option B is wrong because: Amazon RDS manages its own underlying EBS storage. You do not directly create EBS volumes and copy RDS snapshots to them to achieve RDS encryption. Furthermore, you cannot simply "Enable encryption on the DB instance" if it was initially created as unencrypted. The snapshot copy-and-restore method is required.
- [ ] Option C is wrong because: While copying snapshots and enabling encryption during the copy is correct, you cannot "Restore encrypted snapshot to an existing DB instance." An RDS snapshot restore operation always creates a new DB instance. You then migrate your application to use this new, encrypted instance.
- [ ] Option D is wrong because: While RDS snapshots are stored in Amazon S3 managed by AWS, copying these snapshots to your own S3 bucket and encrypting that bucket does not encrypt the actual RDS DB instance or ensure that its future automated RDS snapshots are encrypted. The encryption needs to be applied at the RDS level during the instance creation (from an encrypted snapshot).

</details>  

<details>
  <summary>Question 122</summary>

- [ ] A. Use multi-factor authentication (MFA) to protect the encryption keys.ations.

What should a solutions architect do to reduce the operational burden?

- [ ] A. Use multi-factor authentication (MFA) to protect the encryption keys.
- [ ] B. Use AWS Key Management Service (AWS KMS) to protect the encryption keys.
- [ ] C. Use AWS Certificate Manager (ACM) to create, store, and assign the encryption keys.
- [ ] D. Use an IAM policy to limit the scope of users who have access permissions to protect the encryption keys.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Use AWS Key Management Service (AWS KMS) to protect the encryption keys.

Why this is the correct answer:

- [ ] AWS Key Management Service (AWS KMS): AWS KMS is a managed service specifically designed for creating, managing, and controlling cryptographic keys. It provides a highly available and durable infrastructure for key storage and performs encryption and decryption operations.
- [ ] Reduces Operational Burden: Because KMS is a fully managed service, AWS handles the operational aspects of the key management infrastructure, such as hardware provisioning, patching, maintenance, availability, and scalability. This significantly "reduces the operational burden" on the company compared to building and maintaining an on-premises or custom key management solution. Developers can easily integrate with KMS using the AWS SDK to encrypt and decrypt data in their applications.
- [ ] Scalable Infrastructure: KMS is designed to scale to meet the needs of various applications and developers requiring encryption capabilities.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: Multi-factor authentication (MFA) is an important security measure for protecting access to AWS accounts and sensitive operations (including the management of KMS keys via IAM users or roles). However, MFA itself is an authentication mechanism, not a "key management infrastructure" or a service that protects the encryption keys at rest or manages their lifecycle.
- [ ] Option C is wrong because: AWS Certificate Manager (ACM) is a service for provisioning, managing, and deploying public and private SSL/TLS certificates. Certificates are used for authenticating identities and securing network communications (e.g., HTTPS). ACM is not primarily designed for managing symmetric or asymmetric encryption keys that developers would use for general data encryption within applications. AWS KMS is the appropriate service for data encryption keys.
- [ ] Option D is wrong because: Using IAM (Identity and Access Management) policies is crucial for controlling who has access to encryption keys (e.g., keys managed by AWS KMS) and what actions they can perform. This is a vital part of securing a key management infrastructure. However, IAM policies are an access control mechanism, not the key management infrastructure itself. You still need a service like KMS to create, store, and manage the actual encryption keys.

</details>

<details>
  <summary>Question 123</summary>

A company has a dynamic web application hosted on two Amazon EC2 instances. The company has its own SSL certificate, which is on each instance to perform SSL termination. There has been an increase in traffic recently, and the operations team determined that SSL encryption and decryption is causing the compute capacity of the web servers to reach their maximum limit.

What should a solutions architect do to increase the application's performance?

- [ ] A. Create a new SSL certificate using AWS Certificate Manager (ACM). Install the ACM certificate on each instance.
- [ ] B. Create an Amazon S3 bucket Migrate the SSL certificate to the S3 bucket. Configure the EC2 instances to reference the bucket for SSL termination.
- [ ] C. Create another EC2 instance as a proxy server. Migrate the SSL certificate to the new instance and configure it to direct connections to the existing EC2 instances.
- [ ] D. Import the SSL certificate into AWS Certificate Manager (ACM). Create an Application Load Balancer with an HTTPS listener that uses the SSL certificate from ACM.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Import the SSL certificate into AWS Certificate Manager (ACM). Create an Application Load Balancer with an HTTPS listener that uses the SSL certificate from ACM.

Why this is the correct answer:

- [ ] Offloading SSL Termination to Application Load Balancer (ALB): The core issue is that SSL encryption/decryption is consuming excessive CPU resources on the EC2 web servers, impacting performance. An - [ ] Application Load Balancer can terminate SSL/TLS connections. This means the ALB handles the computationally intensive SSL handshake and encryption/decryption processes, freeing up the EC2 instances' CPU cycles to focus on application logic. This directly addresses the performance bottleneck.
- [ ] Using Existing SSL Certificate with ACM: The company has its "own SSL certificate." AWS Certificate Manager (ACM) allows you to import third-party SSL/TLS certificates. Once imported into ACM, this certificate can be easily deployed on the ALB.
- [ ] Improved Performance and Scalability: By offloading SSL termination to the ALB, the EC2 instances have more capacity to serve application requests, leading to increased performance. The ALB also provides load balancing across the EC2 instances, which helps with scalability and availability.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: Simply creating a new SSL certificate with ACM and installing it on the EC2 instances does not solve the problem. SSL termination would still occur on the EC2 instances, and they would continue to experience high CPU load due to SSL processing. The key is to offload this processing from the instances.
- [ ] Option B is wrong because: Storing an SSL certificate in an Amazon S3 bucket and configuring EC2 instances to reference it for SSL termination is not a standard, secure, or efficient way to handle SSL. This approach doesn't address the SSL processing load on the EC2 instances and introduces complexities in managing certificate access.
- [ ] Option C is wrong because: While creating a dedicated EC2 instance to act as a proxy server (e.g., running Nginx or HAProxy) for SSL termination could offload the SSL processing from the web servers, this solution involves managing an additional EC2 instance (patching, scaling, high availability). Using a managed service like an Application Load Balancer (as in Option D) achieves the same SSL offloading benefits with significantly less operational overhead.

</details>

<details>
  <summary>Question 124</summary>
 
A company has a highly dynamic batch processing job that uses many Amazon EC2 instances to complete it. The job is stateless in nature, can be started and stopped at any given time with no negative impact, and typically takes upwards of 60 minutes total to complete. The company has asked a solutions architect to design a scalable and cost-effective solution that meets the requirements of the job.

What should the solutions architect recommend?

- [ ] A. Implement EC2 Spot Instances.
- [ ] B. Purchase EC2 Reserved Instances.
- [ ] C. Implement EC2 On-Demand Instances.
- [ ] D. Implement the processing on AWS Lambda.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Implement EC2 Spot Instances.

Why this is the correct answer:

- [ ] EC2 Spot Instances for Cost-Effectiveness: Amazon EC2 Spot Instances allow you to bid on unused EC2 capacity and can provide savings of up to 90% compared to On-Demand prices. For batch processing jobs that are fault-tolerant and flexible in their execution time, Spot Instances are an extremely cost-effective option.
- [ ] Suitable for Stateless and Interruptible Workloads: The problem states the job is "stateless in nature, can be started and stopped at any given time with no negative impact." This makes it an ideal workload for Spot Instances because if a Spot Instance is interrupted (reclaimed by AWS with a two-minute warning), the stateless nature of the job means it can be resumed or restarted on another instance without significant data loss or corruption.
- [ ] Scalable: Spot Instances can be requested in large numbers to provide the necessary compute capacity for the "many Amazon EC2 instances" needed to complete the job.
- [ ] Handles Job Duration: The job duration of "upwards of 60 minutes" is generally well within the typical runtimes achievable with Spot Instances, especially if the application is designed to handle potential interruptions gracefully (e.g., by checkpointing or processing data in smaller, independent chunks).

<hr> Why are the other answers wrong? <hr>

- [ ] Option B is wrong because: EC2 Reserved Instances (RIs) provide a discount in exchange for a 1-year or 3-year commitment to use EC2. RIs are best suited for continuous, predictable workloads. For a "highly dynamic batch processing job" that might not run 24/7 or whose instance requirements might change, RIs could lead to underutilized committed capacity and may not be as cost-effective as Spot Instances, especially given the interruptible nature of the job.
- [ ] Option C is wrong because: EC2 On-Demand Instances provide compute capacity with no long-term commitment, billed by the second (for Linux). While they offer flexibility, On-Demand pricing is significantly higher than Spot Instance pricing. Since the workload is stateless and interruptible, the cost savings offered by Spot Instances make them a more compelling choice for cost-effectiveness.
- [ ] Option D is wrong because: AWS Lambda functions currently have a maximum execution duration of 15 minutes (900 seconds). The batch processing job "typically takes upwards of 60 minutes total to complete." While a large job could be broken down into many smaller tasks that run on Lambda and are orchestrated (e.g., by AWS Step Functions), if individual components of the batch job themselves are long-running, Lambda might not be the most direct or suitable compute option. For a job already designed to use "many Amazon EC2 instances," leveraging EC2 Spot Instances is a more direct fit for the described processing duration.

</details>

<details>
  <summary>Question 125</summary>

A company runs its two-tier ecommerce website on AWS. The web tier consists of a load balancer that sends traffic to Amazon EC2 instances. The database tier uses an Amazon RDS DB instance. The EC2 instances and the RDS DB instance should not be exposed to the public internet. The EC2 instances require internet access to complete payment processing of orders through a third-party web service. The application must be highly available.

Which combination of configuration options will meet these requirements? (Choose two.)

- [ ] A. Use an Auto Scaling group to launch the EC2 instances in private subnets. Deploy an RDS Multi-AZ DB instance in private subnets.
- [ ] B. Configure a VPC with two private subnets and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the private subnets.
- [ ] C. Use an Auto Scaling group to launch the EC2 instances in public subnets across two Availability Zones. Deploy an RDS Multi-AZ DB instance in private subnets.
- [ ] D. Configure a VPC with one public subnet, one private subnet, and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the public subnet.
- [ ] E. Configure a VPC with two public subnets, two private subnets, and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the public subnets.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Use an Auto Scaling group to launch the EC2 instances in private subnets. Deploy an RDS Multi-AZ DB instance in private subnets.
- [ ] E. Configure a VPC with two public subnets, two private subnets, and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the public subnets.

Why these are the correct answers:

This question asks for a combination of configurations to build a highly available and secure two-tier web application.

A. Use an Auto Scaling group to launch the EC2 instances in private subnets. Deploy an RDS Multi-AZ DB instance in private subnets.
- [ ] Security for EC2 and RDS: Placing both the EC2 instances (application/web tier) and the RDS DB instance in private subnets ensures they are "not exposed to the public internet," meeting a key security requirement.
- [ ] High Availability for EC2: Using an Auto Scaling group to launch EC2 instances allows the application tier to scale based on demand and maintain availability by replacing unhealthy instances. For high availability, this Auto Scaling group should be configured to span multiple Availability Zones.
- [ ] High Availability for RDS: Deploying the RDS DB instance as a Multi-AZ deployment ensures database high availability through a synchronous standby replica in a different AZ with automatic failover.

E. Configure a VPC with two public subnets, two private subnets, and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the public subnets.
- [ ] VPC Structure for High Availability: A VPC configured with public and private subnets across at least two Availability Zones is fundamental for a highly available architecture. "Two public subnets, two private subnets" typically implies one of each per AZ for a two-AZ setup.
- [ ] Application Load Balancer (ALB) in Public Subnets: The load balancer (which would be an ALB for an HTTP/S website) needs to be accessible from the internet to receive user traffic. Placing it in the public subnets achieves this. An ALB itself is highly available as it distributes traffic across multiple AZs.
- [ ] NAT Gateways for Outbound Internet Access: The EC2 instances in private subnets "require internet access to complete payment processing." NAT gateways, deployed in public subnets (one in each AZ for high availability), allow instances in private subnets to initiate outbound connections to the internet while preventing inbound connections from the internet. Route tables for the private subnets would be configured to route internet-bound traffic through these NAT gateways.

Combining these two options (A and E) creates a complete, secure, and highly available architecture:

- [ ] A well-structured VPC with public and private subnets across multiple AZs.
- [ ] A public-facing ALB in the public subnets.
- [ ] EC2 instances for the application tier in private subnets, managed by an Auto Scaling group spanning multiple AZs.
- [ ] An RDS Multi-AZ database in private subnets.
- [ ] Highly available NAT gateways in public subnets for outbound internet access from the EC2 instances.

<hr> Why are the other answers wrong? <hr>

- [ ] Option B is wrong because: Deploying an Application Load Balancer in private subnets would make it an internal load balancer, which is not suitable for a public-facing ecommerce website that needs to receive traffic from the internet.
- [ ] Option C is wrong because: Placing the EC2 instances (application/web tier) in public subnets would expose them directly to the internet, which contradicts the requirement that "The EC2 instances... should not be exposed to the public internet."
- [ ] Option D (the first "D" in the PDF) is wrong because: While it correctly places the ALB in a public subnet and mentions two NAT gateways across two AZs (good for HA), stating "one public subnet, one private subnet" in total for a VPC spanning two AZs is an insufficient network design for high availability. For a robust two-AZ setup, you'd typically have at least one public and one private subnet in each of the two Availability Zones.

</details>

<details>
  <summary>Question 126</summary>

A solutions architect needs to implement a solution to reduce a company's storage costs. All the company's data is in the Amazon S3 Standard storage class. The company must keep all data for at least 25 years. Data from the most recent 2 years must be highly available and immediately retrievable.

Which solution will meet these requirements?

- [ ] A. Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive immediately.
- [ ] B. Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 2 years.
- [ ] C. Use S3 Intelligent-Tiering. Activate the archiving option to ensure that data is archived in S3 Glacier Deep Archive.
- [ ] D. Set up an S3 Lifecycle policy to transition objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) immediately and to S3 Glacier Deep Archive after 2 years.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 2 years.

Why this is the correct answer:

This solution balances the need for immediate access to recent data with cost-effective long-term archival:

- [ ] Initial Storage (Implicitly S3 Standard): The data is currently in S3 Standard. For the "most recent 2 years," this data needs to be "highly available and immediately retrievable." S3 Standard meets these criteria.
- [ ] Transition to S3 Glacier Deep Archive after 2 Years: After 2 years, the data can be moved to a very low-cost archival storage class. Amazon S3 Glacier Deep Archive is AWS's lowest-cost storage class and is designed for long-term retention (like 25 years) of data that is rarely, if ever, accessed and for which retrieval times of several hours are acceptable. An S3 Lifecycle policy can be configured to automatically transition objects from their current storage class (e.g., S3 Standard or S3 Standard-IA if it was tiered there first) to S3 Glacier Deep Archive after they reach 2 years of age.
- [ ] Cost Reduction: This approach significantly reduces long-term storage costs by moving the bulk of the data (older than 2 years) to the most economical archive tier.
- [ ] Meets Retention and Accessibility Requirements: It ensures data is kept for 25 years, recent data (within 2 years) remains immediately accessible, and older data is archived cost-effectively.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: Transitioning objects to S3 Glacier Deep Archive "immediately" would make the data from the "most recent 2 years" not immediately retrievable (retrieval takes hours) and not highly available in the sense of instant access. This violates a key requirement.
- [ ] Option C is wrong because: S3 Intelligent-Tiering is designed for data with unknown or changing access patterns. While it can be configured to automatically archive data to S3 Glacier Deep Archive, the access pattern here is somewhat defined (immediately accessible for 2 years, then archive). A direct S3 Lifecycle policy (as in option B) to transition to S3 Glacier Deep Archive after a fixed 2-year period is a more straightforward and potentially more cost-effective approach for this predictable archival need. S3 Intelligent-Tiering incurs small per-object monitoring fees for its automatic tiering capabilities.
- [ ] Option D is wrong because: Transitioning objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) "immediately" means that data, including the most recent 2 years' data, would be stored in a single Availability Zone. This makes it less durable and not "highly available" in the event of an AZ failure, which is risky for data that must be kept for 25 years. For the initial 2 years when data needs to be highly available, a multi-AZ storage class like S3 Standard or S3 Standard-IA would be more appropriate.

</details>

<details>
  <summary>Question 127</summary>

A media company is evaluating the possibility of moving its systems to the AWS Cloud. The company needs at least 10 TB of storage with the maximum possible I/O performance for video processing, 300 TB of very durable storage for storing media content, and 900 TB of storage to meet requirements for archival media that is not in use anymore.

Which set of services should a solutions architect recommend to meet these requirements?

- [ ] A. Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage
- [ ] B. Amazon EBS for maximum performance, Amazon EFS for durable data storage, and Amazon S3 Glacier for archival storage
- [ ] C. Amazon EC2 instance store for maximum performance, Amazon EFS for durable data storage, and Amazon S3 for archival storage
- [ ] D. Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage

Why this is the correct answer:

This question asks for a set of storage services to meet three distinct requirements: maximum I/O performance for processing, durable storage for primary content, and archival storage.

- [ ] Amazon EC2 Instance Store for Maximum Performance (10 TB for video processing): EC2 instance stores provide temporary block-level storage that is physically attached to the host EC2 instance. NVMe-based instance stores offer very high IOPS and extremely low latency, making them ideal for workloads requiring the "maximum possible I/O performance," such as scratch space for video processing. While instance store is ephemeral (data is lost when the instance stops or terminates), it's suitable for temporary high-performance processing, with the source and destination data residing in durable storage.
- [ ] Amazon S3 for Durable Data Storage (300 TB for media content): Amazon S3 is designed for high durability (99.999999999%), availability, and scalability. It is an excellent choice for storing 300 TB of primary media content that needs to be "very durable." S3 is also cost-effective for storing large amounts of data.
- [ ] Amazon S3 Glacier for Archival Storage (900 TB for archival media): Amazon S3 Glacier (and its variants like S3 Glacier Deep Archive) provides secure, durable, and extremely low-cost storage for data archiving and long-term backup. It is perfect for "archival media that is not in use anymore," where retrieval times of minutes to hours are acceptable.
- [ ] This combination correctly matches each storage requirement with the most appropriate AWS service.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: While high-performance Amazon EBS volumes (like io2 Block Express) can provide excellent I/O, EC2 instance stores (especially NVMe) generally offer the absolute "maximum possible I/O performance" due to their direct attachment and lack of network overhead, which might be preferred for intensive video processing scratch space. The rest of the option (S3 and S3 Glacier) is appropriate. The choice between EBS and instance store for "maximum performance" often hinges on whether persistence of that high-performance tier itself is required (EBS) or if it's purely for temporary, high-speed scratch space (instance store).
- [ ] Option B is wrong because: Using Amazon EFS for 300 TB of "durable data storage" for media content is generally more expensive per GB than Amazon S3. While EFS is durable and provides file system access, S3 is typically more cost-effective and better suited for storing large object-based media content unless a shared file system interface is strictly required for that primary content.
- [ ] Option C is wrong because:
Using Amazon EFS for 300 TB of durable media content storage has the same cost-effectiveness concern as in option B when compared to S3.
Using "Amazon S3 for archival storage" is too generic. While S3 has archive tiers (Glacier, Glacier Deep Archive), explicitly mentioning S3 Glacier (as in options A, B, and D) is more precise for the "archival media" requirement.

</details>

<details>
  <summary>Question 128</summary>

A company wants to run applications in containers in the AWS Cloud. These applications are stateless and can tolerate disruptions within the underlying infrastructure. The company needs a solution that minimizes cost and operational overhead.

What should a solutions architect do to meet these requirements?

- [ ] A. Use Spot Instances in an Amazon EC2 Auto Scaling group to run the application containers.
- [ ] B. Use Spot Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group.
- [ ] C. Use On-Demand Instances in an Amazon EC2 Auto Scaling group to run the application containers.
- [ ] D. Use On-Demand Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Use Spot Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group.

Why this is the correct answer:

- [ ] Container Orchestration with Amazon EKS: Amazon EKS is a managed Kubernetes service that makes it easier to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane. This helps manage containerized applications.   
- [ ] Cost Minimization with Spot Instances: The applications are "stateless and can tolerate disruptions." This makes them ideal candidates for Amazon EC2 Spot Instances, which offer significant cost savings (up to 90% off On-Demand prices) by allowing you to use spare EC2 capacity. Since the applications can handle interruptions, using Spot Instances directly addresses the "minimizes cost" requirement.
- [ ] EKS Managed Node Groups with Spot: EKS managed node groups automate the provisioning and lifecycle management of EC2 instances (nodes) for your EKS cluster. You can configure these managed node groups to use Spot Instances. This combines the orchestration benefits of Kubernetes with the cost savings of Spot Instances.
- [ ] Reduced Operational Overhead: Using EKS with managed node groups (especially compared to self-managing Kubernetes on EC2) reduces the operational overhead associated with the Kubernetes control plane and worker node management. While there's still some overhead compared to a fully serverless solution like Fargate (not an option here for EKS Spot directly), it's less than managing everything manually.
- [ ] This solution effectively balances running containerized applications with cost minimization and a reduction in operational overhead for the underlying compute infrastructure.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: While using Spot Instances in an EC2 Auto Scaling group is cost-effective for disruption-tolerant workloads, it doesn't provide a container orchestration platform like Kubernetes (EKS) or ECS. If the company specifically wants to run applications "in containers" with the benefits of an orchestrator (like service discovery, deployment strategies, etc.), simply using EC2 Auto Scaling groups with Spot might require more manual effort to manage the container lifecycle.
- [ ] Option C is wrong because: Using On-Demand Instances is significantly more expensive than Spot Instances. Since the applications are stateless and can tolerate disruptions, there's a missed opportunity for cost savings if On-Demand Instances are used instead of Spot Instances. This does not meet the "minimizes cost" requirement as effectively.
- [ ] Option D is wrong because: Similar to option C, using On-Demand Instances in an EKS managed node group is more expensive than using Spot Instances. For workloads that are disruption-tolerant, Spot Instances provide a much better cost profile.

</details>

<details>
  <summary>Question 129</summary>

A company is running a multi-tier web application on premises. The web application is containerized and runs on a number of Linux hosts connected to a PostgreSQL database that contains user records. The operational overhead of maintaining the infrastructure and capacity planning is limiting the company's growth. A solutions architect must improve the application's infrastructure.

Which combination of actions should the solutions architect take to accomplish this? (Choose two.)

- [ ] A. Migrate the PostgreSQL database to Amazon Aurora.
- [ ] B. Migrate the web application to be hosted on Amazon EC2 instances.
- [ ] C. Set up an Amazon CloudFront distribution for the web application content.
- [ ] D. Set up Amazon ElastiCache between the web application and the PostgreSQL database.
- [ ] E. Migrate the web application to be hosted on AWS Fargate with Amazon Elastic Container Service (Amazon ECS).

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Migrate the PostgreSQL database to Amazon Aurora.
- [ ] E. Migrate the web application to be hosted on AWS Fargate with Amazon Elastic Container Service (Amazon ECS).

Why these are the correct answers:

This question focuses on reducing operational overhead and improving scalability for an existing containerized application and its database.

A. Migrate the PostgreSQL database to Amazon Aurora.
- [ ] Reduced Database Operational Overhead: Amazon Aurora is a fully managed relational database service compatible with MySQL and PostgreSQL.
- [ ] Migrating the on-premises PostgreSQL database to Amazon Aurora (PostgreSQL-compatible edition) significantly reduces the operational burden associated with database administration tasks like patching, backups, scaling, and ensuring high availability, as AWS manages these aspects.
- [ ] Improved Scalability and Availability: Aurora is designed for high performance, scalability, and availability, which helps in supporting the company's growth.

E. Migrate the web application to be hosted on AWS Fargate with Amazon Elastic Container Service (Amazon ECS).
- [ ] Reduced Web Tier Operational Overhead: The web application is already containerized. AWS Fargate is a serverless compute engine for containers that works with Amazon ECS (a container orchestration service).
- [ ] By migrating the containerized application to ECS using the Fargate launch type, the company eliminates the need to provision, manage, patch, or scale the underlying EC2 instances (the current Linux hosts). This directly addresses the problem of "operational overhead of maintaining the infrastructure and capacity planning" for the application tier.
- [ ] Scalability: ECS with Fargate allows the application to scale seamlessly based on demand without manual intervention for the underlying compute resources.
Combining these two solutions modernizes both the database and application tiers to managed/serverless AWS services, thereby reducing operational overhead and improving scalability.

<hr> Why are the other answers wrong? <hr>

- [ ] Option B is wrong because: Migrating the web application to self-managed Amazon EC2 instances, even if using Auto Scaling, still requires the company to manage the operating systems, patching, and some aspects of capacity planning for those instances. This does not minimize operational overhead as effectively as using a serverless container platform like AWS Fargate (Option E).
- [ ] Option C is wrong because: Amazon CloudFront is a content delivery network (CDN) that can improve website performance and reduce load by caching content. While beneficial, it doesn't directly address the core issue of operational overhead in maintaining the on-premises application servers or the database infrastructure. It's a complementary optimization, not a foundational solution for reducing the stated overhead.
- [ ] Option D is wrong because: Amazon ElastiCache is an in-memory caching service that can reduce latency and database load by caching frequently accessed data. While this can improve performance, it does not reduce the operational overhead of managing the PostgreSQL database itself or the application hosting infrastructure. Migrating to managed services like Aurora and Fargate addresses the operational overhead more directly.


</details>

<details>
  <summary>Question 130</summary>

An application runs on Amazon EC2 instances across multiple Availability Zones. The instances run in an Amazon EC2 Auto Scaling group behind an Application Load Balancer. The application performs best when the CPU utilization of the EC2 instances is at or near 40%.

What should a solutions architect do to maintain the desired performance across all instances in the group?

- [ ] A. Use a simple scaling policy to dynamically scale the Auto Scaling group.
- [ ] B. Use a target tracking policy to dynamically scale the Auto Scaling group.
- [ ] C. Use an AWS Lambda function to update the desired Auto Scaling group capacity.
- [ ] D. Use scheduled scaling actions to scale up and scale down the Auto Scaling group.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Use a target tracking policy to dynamically scale the Auto Scaling group.

Why this is the correct answer:

- [ ] Target Tracking Scaling Policy: This is an Amazon EC2 Auto Scaling feature that allows you to define a target value for a specific CloudWatch metric (such as average CPU utilization for the Auto Scaling group). The Auto Scaling group then automatically adjusts the number of instances (scales out or scales in) as needed to keep the specified metric at, or close to, the target value you've set.
- [ ] Maintaining Specific CPU Utilization: The requirement is to keep the CPU utilization of the EC2 instances "at or near 40%." A target tracking scaling policy with the AverageCPUUtilization metric set to a target of 40 is precisely designed for this scenario. It provides a dynamic and responsive way to maintain the desired performance level.
- [ ] Simplicity and Efficiency: Target tracking policies are generally simpler to configure and manage than simple or step scaling policies because you only need to define the target metric and value. AWS Auto Scaling handles the underlying calculations for how many instances to add or remove.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: Simple scaling policies adjust the current capacity based on a single scaling adjustment after a CloudWatch alarm threshold is breached. They require a cooldown period after each scaling activity before they can respond to further alarms. This can lead to slower or less precise adjustments compared to target tracking, which continuously works to keep the metric near the target.
- [ ] Option C is wrong because: While it's possible to use an AWS Lambda function to monitor CloudWatch metrics and programmatically adjust the Auto Scaling group's desired capacity, this is a custom solution that involves writing, deploying, and maintaining code. This introduces more operational overhead compared to using the built-in target tracking scaling policies provided by Auto Scaling.
- [ ] Option D is wrong because: Scheduled scaling actions are used to scale capacity based on predictable, time-based patterns (e.g., increasing capacity during business hours and decreasing it at night). The scenario does not indicate a predictable load schedule but rather a desired CPU utilization level that needs to be maintained dynamically regardless of the time. Target tracking scaling is more appropriate for responding to actual load and maintaining a performance metric.

</details>

# AWS-SAA-PRACTICE-EXAM Questions 131-140

<details>
  <summary>Question 131</summary>

- [ ] A.  Turn  
A company is developing a file-sharing application that will use an Amazon S3 bucket for storage. The company wants to serve all the files through an Amazon CloudFront distribution. The company does not want the files to be accessible through direct navigation to the S3 URL.

What should a solutions architect do to meet these requirements?

- [ ] A. Write individual policies for each S3 bucket to grant read permission for only CloudFront access.
- [ ] B. Create an IAM user. Grant the user read permission to objects in the S3 bucket. Assign the user to CloudFront.
- [ ] C. Write an S3 bucket policy that assigns the CloudFront distribution ID as the Principal and assigns the target S3 bucket as the Amazon Resource Name (ARN).
- [ ] D. Create an origin access identity (OAI). Assign the OAI to the CloudFront distribution. Configure the S3 bucket permissions so that only the OAI has read permission.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Create an origin access identity (OAI). Assign the OAI to the CloudFront distribution. Configure the S3 bucket permissions so that only the OAI has read permission.

Why this is the correct answer:

This solution uses the standard and recommended method for restricting S3 bucket access to only a CloudFront distribution:

- [ ] Origin Access Identity (OAI): An OAI is a special CloudFront user identity that you create and associate with your CloudFront distribution. This OAI acts as a trusted principal that CloudFront uses when accessing your S3 bucket.
- [ ] Restricting S3 Bucket Permissions to OAI: You then modify the S3 bucket policy (or ACLs, though bucket policies are generally preferred for more granular control) to grant read permissions (e.g., s3:GetObject) only to this specific OAI. All other public access or access by other IAM principals to the S3 bucket is denied or not granted.
- [ ] Result: When users request files through the CloudFront distribution, CloudFront uses its associated OAI to fetch the files from the S3 bucket. Since the bucket policy only allows the OAI to read the objects, direct S3 URLs will not work for users, thus meeting the requirement that files are not accessible through direct S3 navigation.
- [ ] Note: AWS has also introduced Origin Access Control (OAC) as an enhanced alternative to OAI, offering better security and features, but OAI is the classic method and still valid, especially in the context of these exam options.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: While bucket policies are used, this option is too vague. "Grant read permission for only CloudFront access" needs a specific mechanism. Simply stating this doesn't explain how CloudFront's access is identified and restricted. Option D provides that specific mechanism (OAI).
- [ ] Option B is wrong because: You do not assign an IAM user to CloudFront for origin access. CloudFront does not use IAM user credentials in this manner to access S3 buckets. The OAI (or OAC) mechanism is used for this purpose.
- [ ] Option C is wrong because: You cannot directly assign a CloudFront distribution ID as a Principal in an S3 bucket policy in the way described. The Principal in the bucket policy, when using OAI, refers to the canonical user ID of the OAI. If using OAC, the principal is cloudfront.amazonaws.com with a condition based on the distribution.

</details>  

<details>
  <summary>Question 132</summary>

A company's website provides users with downloadable historical performance reports. The website needs a solution that will scale to meet the company's website demands globally. The solution should be cost-effective, limit the provisioning of infrastructure resources, and provide the fastest possible response time.

Which combination should a solutions architect recommend to meet these requirements?

- [ ] A. Amazon CloudFront and Amazon S3
- [ ] B. AWS Lambda and Amazon DynamoDB
- [ ] C. Application Load Balancer with Amazon EC2 Auto Scaling
- [ ] D. Amazon Route 53 with internal Application Load Balancers

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Amazon CloudFront and Amazon S3

Why this is the correct answer:

This solution leverages AWS services designed for scalable, cost-effective, and low-latency global content delivery:

- [ ] Amazon S3 for Storing Reports: Amazon S3 (Simple Storage Service) is an ideal place to store the "downloadable historical performance reports." S3 offers high durability, scalability, and is very cost-effective for storing static files.
- [ ] Amazon CloudFront for Global Delivery and Performance: Amazon CloudFront is a global content delivery network (CDN). By configuring CloudFront with the S3 bucket as its origin, the performance reports will be cached at edge locations geographically closer to users around the world. When a user requests a report, it is served from the nearest edge location, which significantly reduces latency and provides the "fastest possible response time."
- [ ] Scalability: Both S3 and CloudFront are highly scalable services that can automatically handle large and fluctuating demand globally.
- [ ] Cost-Effectiveness and Limited Infrastructure Provisioning: This is a serverless approach for content storage and delivery. There are no servers to manage, which "limits the provisioning of infrastructure resources" and reduces operational overhead. This combination is generally very "cost-effective" for distributing static content globally.

<hr> Why are the other answers wrong? <hr>

- [ ] Option B is wrong because: AWS Lambda and Amazon DynamoDB are powerful serverless components for building application backends (e.g., processing requests, storing structured data). However, for serving static downloadable files like performance reports globally with the fastest response time, S3 and CloudFront are more direct and optimized. Lambda and DynamoDB might be used to generate the reports or manage metadata about them, but not for the primary delivery of the files themselves.
- [ ] Option C is wrong because: Using an Application Load Balancer with Amazon EC2 Auto Scaling is a solution for hosting dynamic web applications that require compute instances. For serving downloadable static reports, this approach involves managing EC2 instances, which increases operational overhead and cost compared to a serverless S3/CloudFront solution. It does not align well with "limit the provisioning of infrastructure resources."
- [ ] Option D is wrong because: Amazon Route 53 is a DNS service used for routing users to endpoints. Internal Application Load Balancers are designed for distributing traffic to applications within your VPC, not for serving content globally to external users with the fastest possible response time. CloudFront is the key AWS service for global, low-latency content delivery to end-users.

</details>

<details>
  <summary>Question 133</summary>

A company runs an Oracle database on premises. As part of the company's migration to AWS, the company wants to upgrade the database to the most recent available version. The company also wants to set up disaster recovery (DR) for the database. The company needs to minimize the operational overhead for normal operations and DR setup. The company also needs to maintain access to the database's underlying operating system.

Which solution will meet these requirements?

- [ ] A. Migrate the Oracle database to an Amazon EC2 instance. Set up database replication to a different AWS Region.
- [ ] B. Migrate the Oracle database to Amazon RDS for Oracle. Activate Cross-Region automated backups to replicate the snapshots to another AWS Region.
- [ ] C. Migrate the Oracle database to Amazon RDS Custom for Oracle. Create a read replica for the database in another AWS Region.
- [ ] D. Migrate the Oracle database to Amazon RDS for Oracle. Create a standby database in another Availability Zone.

</details>

<details>
  <summary>Answer</summary>

- [ ] C. Migrate the Oracle database to Amazon RDS Custom for Oracle. Create a read replica for the database in another AWS Region.

Why this is the correct answer:

This question has several key requirements: upgrade flexibility, disaster recovery (DR), minimized operational overhead, and access to the underlying operating system.

- [ ] Maintain Access to Underlying OS (Amazon RDS Custom for Oracle): A critical requirement is to "maintain access to the database's underlying operating system." Standard Amazon RDS is fully managed and does not provide OS access. Amazon RDS Custom for Oracle, however, gives you administrative access to the underlying EC2 instance and the database environment. This allows for customizations, specific configurations, and the ability to apply patches or upgrades that might not yet be supported by standard RDS.
- [ ] Upgrade to Most Recent Version: With OS and database environment access provided by RDS Custom, the company has more control and flexibility to "upgrade the database to the most recent available version," even if that version isn't immediately available or fully supported in standard RDS.
- [ ] Disaster Recovery (Cross-Region Read Replica): For Amazon RDS Custom for Oracle, you can set up a cross-region read replica. In a DR scenario, this read replica in a different AWS Region can be promoted to become a standalone, writable instance. This provides a DR solution.
- [ ] Minimize Operational Overhead (Relative to EC2): While RDS Custom requires more customer management responsibility than standard RDS (due to OS access), it still automates some database administration tasks (like provisioning and some backup aspects) compared to running an Oracle database entirely on self-managed EC2 instances. For setting up DR via a managed read replica, it is also less overhead than manually configuring and managing replication for Oracle on EC2.
- [ ] This option best balances the need for OS access with managed features for DR and somewhat reduced operational overhead compared to a full EC2 deployment.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: Migrating to an Oracle database on a self-managed Amazon EC2 instance provides full OS access. However, setting up and managing database replication to a different AWS Region for DR would involve significant operational overhead for configuration, monitoring, patching the OS and database, and managing the replication process itself. This contradicts the requirement to "minimize the operational overhead for... DR setup."
- [ ] Option B is wrong because: Standard Amazon RDS for Oracle is a fully managed service and does not provide access to the underlying operating system, which is a key requirement. While Cross-Region automated backups are a valid DR mechanism for standard RDS, the lack of OS access makes this option unsuitable.
- [ ] Option D is wrong because:
Standard Amazon RDS for Oracle does not provide OS access.
Creating a standby database in another Availability Zone (a Multi-AZ deployment) is for high availability within a single region, protecting against an AZ failure. It is not a cross-region disaster recovery solution.

</details>

<details>
  <summary>Question 134</summary>

- [ ] A.  Turn  
A company wants to move its application to a serverless solution. The serverless solution needs to analyze existing and new data by using SQL. The company stores the data in an Amazon S3 bucket. The data requires encryption and must be replicated to a different AWS Region.

Which solution will meet these requirements with the LEAST operational overhead?

- [ ] A. Create a new S3 bucket. Load the data into the new S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with AWS KMS multi-Region keys (SSE-KMS). Use Amazon Athena to query the data.
- [ ] B. Create a new S3 bucket. Load the data into the new S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with AWS KMS multi-Region keys (SSE-KMS). Use Amazon RDS to query the data.
- [ ] C. Load the data into the existing S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use Amazon Athena to query the data.
- [ ] D. Load the data into the existing S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use Amazon RDS to query the data.

</details>

<details>
  <summary>Answer</summary>

- [ ] C. Load the data into the existing S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use Amazon Athena to query the data.

Why this is the correct answer:

This solution focuses on serverless components and minimizing operational overhead.

- [ ] Using Existing S3 Bucket: Loading data into an existing S3 bucket is straightforward and avoids the setup of a new one if not strictly necessary.
S3 Cross-Region Replication (CRR): CRR automatically and asynchronously copies objects across buckets in different AWS Regions. This meets the requirement that "data must be replicated to a different AWS Region."
- [ ] Server-Side Encryption with S3-Managed Keys (SSE-S3): SSE-S3 provides encryption at rest where Amazon S3 manages both the encryption process and the encryption keys. This is the simplest way to achieve encryption in S3 with the absolute "LEAST operational overhead" because there are no keys for the customer to manage. When using CRR with SSE-S3 encrypted objects, S3 handles the re-encryption in the destination region transparently.
- [ ] Amazon Athena for SQL Analysis: Amazon Athena is a serverless, interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. This perfectly fits the requirement to "analyze existing and new data by using SQL" in a serverless manner with minimal setup or management.   
- [ ] Overall Least Operational Overhead: This combination leverages fully managed and serverless AWS services for storage, replication, encryption, and querying, leading to the lowest possible operational burden.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: While using server-side encryption with AWS KMS multi-Region keys (SSE-KMS) provides more customer control over the encryption keys and allows the same key to be used across regions, it generally involves slightly more operational overhead than SSE-S3. With SSE-KMS, you need to create and manage the KMS key, its policies, and potentially its rotation (though automatic rotation is available for KMS-managed CMKs). SSE-S3 is simpler from a key management perspective if the only requirement is that data "requires encryption."
- [ ] Option B is wrong because:
Similar to option A, SSE-KMS has slightly more operational overhead for key management than SSE-S3.
More significantly, using Amazon RDS (a relational database service) to query data stored directly in an Amazon S3 bucket is not its primary design. While some RDS engines might offer ways to load data from S3 or link to it (like Aurora querying S3), Amazon Athena is the purpose-built, serverless SQL query engine for data in S3. Using RDS would introduce the operational overhead of managing a database instance.
- [ ] Option D is wrong because: Similar to option B, using Amazon RDS to query data directly from S3 is not the most operationally efficient or standard serverless approach. Athena is the correct tool for serverless SQL queries on S3.

</details>

<details>
  <summary>Question 135</summary>

A company runs workloads on AWS. The company needs to connect to a service from an external provider. The service is hosted in the provider's VPC. According to the company's security team, the connectivity must be private and must be restricted to the target service. The connection must be initiated only from the company's VPC.

Which solution will meet these requirements?

- [ ] A. Create a VPC peering connection between the company's VPC and the provider's VPC. Update the route table to connect to the target service.
- [ ] B. Ask the provider to create a virtual private gateway in its VPC. Use AWS PrivateLink to connect to the target service.
- [ ] C. Create a NAT gateway in a public subnet of the company's VPC. Update the route table to connect to the target service.
- [ ] D. Ask the provider to create a VPC endpoint for the target service. Use AWS PrivateLink to connect to the target service.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Ask the provider to create a VPC endpoint for the target service. Use AWS PrivateLink to connect to the target service.

Why this is the correct answer:

- [ ] (Note: The PDF phrasing "provider to create a VPC endpoint" is slightly imprecise. The provider creates an endpoint service, and the company creates an interface VPC endpoint to connect to that service.)
- [ ] AWS PrivateLink for Private and Secure Connectivity: AWS PrivateLink is designed to provide secure, private connectivity between your VPCs, AWS services, and your on-premises applications, without exposing your traffic to the public internet. This directly addresses the "connectivity must be private" requirement.
- [ ] Provider Creates Endpoint Service: The external provider would host their service behind a Network Load Balancer (NLB) and then create a VPC endpoint service configuration for it. This makes their service available for private connections via PrivateLink.
- [ ] Company Creates Interface Endpoint: The company (consumer) then creates an interface VPC endpoint in their own VPC. This endpoint gets a private IP address from the company's subnet and acts as an entry point to the provider's service.
- [ ] Restricted to Target Service: Connectivity via PrivateLink is specific to the service endpoint defined by the provider. This ensures that access is "restricted to the target service," not the provider's entire VPC.
- [ ] Connection Initiated from Company's VPC: The connection is initiated from the company's VPC through the interface endpoint to the provider's service. Traffic flows unidirectionally in terms of initiation, from consumer to provider service.
- [ ] No Internet Exposure: Traffic stays within the AWS private network.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: VPC peering establishes a network connection between two VPCs, enabling them to communicate as if they are on the same private network. However, VPC peering connects the VPCs at a network level, potentially exposing more than just the target service unless very strict security group and network ACL rules are applied. It also doesn't work if the VPCs have overlapping CIDR blocks. PrivateLink offers more granular, service-specific private connectivity.
- [ ] Option B is wrong because: A virtual private gateway is used on the VPC side to establish a VPN connection or an AWS Direct Connect connection, typically to an on-premises network. It's not the component a service provider creates in their VPC for consumers to connect via PrivateLink. The provider would create an endpoint service.
- [ ] Option C is wrong because: A NAT gateway is used to enable instances in a private subnet to initiate outbound connections to the internet. This solution would route traffic over the public internet to access the provider's service (if it had a public endpoint), which violates the "connectivity must be private" requirement.

</details>

<details>
  <summary>Question 136</summary>

A company is migrating its on-premises PostgreSQL database to Amazon Aurora PostgreSQL. The on-premises database must remain online and accessible during the migration. The Aurora database must remain synchronized with the on-premises database.

Which combination of actions must a solutions architect take to meet these requirements? (Choose two.)

- [ ] A. Create an ongoing replication task.
- [ ] B. Create a database backup of the on-premises database.
- [ ] C. Create an AWS Database Migration Service (AWS DMS) replication server.
- [ ] D. Convert the database schema by using the AWS Schema Conversion Tool (AWS SCT).
- [ ] E. Create an Amazon EventBridge (Amazon CloudWatch Events) rule to monitor the database synchronization.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Create an ongoing replication task.
- [ ] C. Create an AWS Database Migration Service (AWS DMS) replication server.

Why these are the correct answers:

This scenario describes a database migration with minimal downtime where the source remains online and the target needs to be kept in sync. AWS Database Migration Service (DMS) is designed for this.

C. Create an AWS Database Migration Service (AWS DMS) replication server.
- [ ] DMS Replication Instance: AWS DMS uses a replication instance (often referred to conceptually as a replication server) to perform the actual data migration between the source and target databases.
- [ ] This replication instance is an EC2 instance managed by DMS that runs the replication tasks. Creating and configuring this replication instance is a fundamental step in using DMS.

A. Create an ongoing replication task.
- [ ] Full Load and Change Data Capture (CDC): To migrate the database while it remains online and then keep it synchronized, AWS DMS performs an initial full load of the existing data and then captures and applies ongoing changes from the source to the target.
- [ ] This ongoing synchronization is configured as part of a DMS task, often referred to as Change Data Capture (CDC) or ongoing replication.
- [ ] This ensures the "Aurora database must remain synchronized with the on-premises database."
- [ ] These two steps are essential components of using AWS DMS for a migration that involves ongoing synchronization.

<hr> Why are the other answers wrong? <hr>

- [ ] Option B is wrong because: While creating an initial database backup might be part of some migration strategies (and DMS can perform a full load which is like restoring from a consistent point), this option alone does not address the requirement for the Aurora database to "remain synchronized" with the on-premises database while the on-premises database is still online and accepting changes. Ongoing replication is needed for that.
- [ ] Option D is wrong because: The AWS Schema Conversion Tool (AWS SCT) is primarily used for heterogeneous database migrations, where the source and target database engines are different (e.g., Oracle to PostgreSQL). In this case, the migration is from on-premises PostgreSQL to Amazon Aurora PostgreSQL, which is a homogeneous migration (PostgreSQL to PostgreSQL-compatible). While SCT can be used for schema analysis or minor adjustments even in homogeneous migrations, it's not a mandatory step for basic schema compatibility, and it doesn't handle the data synchronization. The core of keeping data in sync is DMS ongoing replication.
- [ ] Option E is wrong because: While monitoring database synchronization is important (and DMS provides its own monitoring capabilities), creating an Amazon EventBridge rule to monitor synchronization is a monitoring activity, not an action that performs the migration or ensures synchronization itself. The primary actions involve setting up the DMS components to perform the replication.

</details>

<details>
  <summary>Question 137</summary>

A company uses AWS Organizations to create dedicated AWS accounts for each business unit to manage each business unit's account independently upon request. The root email recipient missed a notification that was sent to the root user email address of one account. The company wants to ensure that all future notifications are not missed. Future notifications must be limited to account administrators.

Which solution will meet these requirements?

- [ ] A. Configure the company's email server to forward notification email messages that are sent to the AWS account root user email address to all users in the organization.
- [ ] B. Configure all AWS account root user email addresses as distribution lists that go to a few administrators who can respond to alerts. Configure AWS account alternate contacts in the AWS Organizations console or programmatically.
- [ ] C. Configure all AWS account root user email messages to be sent to one administrator who is responsible for monitoring alerts and forwarding those alerts to the appropriate groups.
- [ ] D. Configure all existing AWS accounts and all newly created accounts to use the same root user email address. Configure AWS account alternate contacts in the AWS Organizations console or programmatically.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Configure all AWS account root user email addresses as distribution lists that go to a few administrators who can respond to alerts. Configure AWS account alternate contacts in the AWS Organizations console or programmatically.

Why this is the correct answer:

This solution addresses the requirements of ensuring notifications are not missed and are directed to the appropriate personnel:

- [ ] Root User Email as a Distribution List: The email address associated with the AWS account root user receives critical notifications from AWS regarding billing, security, and operations. Configuring this root email address as a distribution list (e.g., aws-account-XYZ-admins@company.com) that forwards emails to "a few administrators who can respond to alerts" ensures that multiple responsible individuals receive these important communications.
- [ ] This redundancy reduces the likelihood of a notification being missed if one person is unavailable. It also limits the notifications to the intended "account administrators."
- [ ] AWS Account Alternate Contacts: AWS allows you to configure alternate contacts for billing, operations, and security for each AWS account. These contacts will also receive relevant notifications from AWS pertaining to their specific area.
- [ ] Configuring appropriate administrators as alternate contacts provides an additional, targeted channel for important alerts and information to reach the right people. This can be managed centrally through the AWS Organizations console or programmatically for accounts within an Organization.
- [ ] This combination provides multiple layers of notification to the correct group of administrators, enhancing reliability.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: Forwarding all root user email notifications to "all users in the organization" is overly broad and would likely lead to alert fatigue, with many users receiving irrelevant emails. This violates the requirement that "Future notifications must be limited to account administrators."
- [ ] Option C is wrong because: Sending all root user email messages to only "one administrator" reintroduces a single point of failure. If that single administrator is on leave, overlooks an email, or their email account has issues, critical notifications could still be missed. This does not adequately solve the problem of ensuring notifications are not missed.
- [ ] Option D is wrong because: Using the same root user email address for all existing and newly created AWS accounts is a security anti-pattern and generally not recommended. Each AWS account should have a unique root user email address for better security, isolation, and administrative clarity. While configuring alternate contacts is a good practice, the premise of using a shared root email is flawed.

</details>

<details>
  <summary>Question 138</summary>

A company runs its ecommerce application on AWS. Every new order is published as a message in a RabbitMQ queue that runs on an Amazon EC2 instance in a single Availability Zone. These messages are processed by a different application that runs on a separate EC2 instance. This application stores the details in a PostgreSQL database on another EC2 instance. All the EC2 instances are in the same Availability Zone. The company needs to redesign its architecture to provide the highest availability with the least operational overhead.   

What should a solutions architect do to meet these requirements?

- [ ] A. Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ. Create a Multi-AZ Auto Scaling group for EC2 instances that host the application. Create another Multi-AZ Auto Scaling group for EC2 instances that host the PostgreSQL database.
- [ ] B. Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ. Create a Multi-AZ Auto Scaling group for EC2 instances that host the application. Migrate the database to run on a Multi-AZ deployment of Amazon RDS for PostgreSQL.
- [ ] C. Create a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue. Create another Multi-AZ Auto Scaling group for EC2 instances that host the application. Migrate the database to run on a Multi-AZ deployment of Amazon RDS for PostgreSQL.
- [ ] D. Create a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue. Create another Multi-AZ Auto Scaling group for EC2 instances that host the application. Create a third Multi-AZ Auto Scaling group for EC2 instances that host the PostgreSQL database

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ. Create a Multi-AZ Auto Scaling group for EC2 instances that host the application. Migrate the database to run on a Multi-AZ deployment of Amazon RDS for PostgreSQL.

Why this is the correct answer:

This solution focuses on using managed services to achieve high availability and reduce operational overhead for all tiers of the application:

- [ ] Amazon MQ for RabbitMQ: The existing RabbitMQ on a single EC2 instance is a single point of failure. Migrating this to Amazon MQ, which supports RabbitMQ as an engine, allows for a managed message broker service. Configuring Amazon MQ with active/standby brokers deployed across multiple Availability Zones provides high availability for the message queuing tier with automatic failover. This significantly reduces the operational overhead of managing RabbitMQ.
- [ ] Multi-AZ Auto Scaling Group for Application EC2 Instances: For the message processing application running on EC2, using an Auto Scaling group that launches instances across multiple Availability Zones ensures high availability and scalability for this tier. If an instance or AZ fails, the Auto Scaling group can maintain the desired capacity.
- [ ] Amazon RDS for PostgreSQL with Multi-AZ: Migrating the self-managed PostgreSQL database on EC2 to Amazon RDS for PostgreSQL with a Multi-AZ deployment provides a highly available, durable, and managed database solution. RDS Multi-AZ automatically replicates data synchronously to a standby instance in a different AZ and handles failover automatically, minimizing downtime and operational burden.
- [ ] This combination addresses all components (queue, application, database) with solutions that enhance availability while using managed services to ensure the "least operational overhead."

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: While it correctly addresses the queue with Amazon MQ and the application with a Multi-AZ Auto Scaling group, it suggests using "another Multi-AZ Auto Scaling group for EC2 instances that host the PostgreSQL database." This means still self-managing the PostgreSQL database on EC2 instances, which incurs higher operational overhead for tasks like patching, backups, scaling, and ensuring high availability compared to using a managed service like Amazon RDS for PostgreSQL with Multi-AZ (as in option B).
- [ ] Option C is wrong because: Creating a "Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue" means the company would continue to self-manage RabbitMQ, albeit with some improved availability through the Auto Scaling group. Amazon MQ (as suggested in options A and B) is a fully managed message broker service that would provide higher availability for RabbitMQ with less operational overhead.
- [ ] Option D is wrong because: This option proposes self-managing both RabbitMQ and PostgreSQL on EC2 instances, even if within Multi-AZ Auto Scaling groups. This approach involves the highest operational overhead for managing all three tiers (queue, application, database), including OS patching, software updates, backup management, and high availability configuration for each. It does not leverage managed AWS services for the queue and database effectively to reduce operational overhead.

</details>

<details>
  <summary>Question 139</summary>

A reporting team receives files each day in an Amazon S3 bucket. The reporting team manually reviews and copies the files from this initial S3 bucket to an analysis S3 bucket each day at the same time to use with Amazon QuickSight. Additional teams are starting to send more files in larger sizes to the initial S3 bucket. The reporting team wants to move the files automatically to the analysis S3 bucket as the files enter the initial S3 bucket. The reporting team also wants to use AWS Lambda functions to run pattern-matching code on the copied data. In addition, the reporting team wants to send the data files to a pipeline in Amazon SageMaker Pipelines.

What should a solutions architect do to meet these requirements with the LEAST operational overhead?

- [ ] A. Create a Lambda function to copy the files to the analysis S3 bucket. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type.
- [ ] B. Create a Lambda function to copy the files to the analysis S3 bucket. Configure the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines as targets for the rule.
- [ ] C. Configure S3 replication between the S3 buckets. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type.
- [ ] D. Configure S3 replication between the S3 buckets. Configure the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines as targets for the rule.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Configure S3 replication between the S3 buckets. Configure the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines as targets for the rule.

Why this is the correct answer:

This solution uses managed services to automate the workflow with minimal operational overhead:

- [ ] Configure S3 Replication: To automatically move/copy files from the initial S3 bucket to the analysis S3 bucket "as the files enter the initial S3 bucket," Amazon S3 Replication (Same-Region Replication or Cross-Region Replication) is the most appropriate and operationally efficient method. It's a managed feature that automatically replicates new objects.
- [ ] Event Notification on Analysis Bucket to EventBridge: Once files are replicated to the analysis S3 bucket, an S3 event notification can be configured for s3:ObjectCreated:* events on this analysis bucket. Sending these events to Amazon EventBridge provides a flexible and scalable way to route these events to multiple downstream services.
- [ ] EventBridge Rule and Multiple Targets: An EventBridge rule can be set up to listen for these S3 object creation events from the analysis bucket. This rule can then invoke multiple targets:
- [ ] An AWS Lambda function to "run pattern-matching code on the copied data."
- [ ] Amazon SageMaker Pipelines (as EventBridge can trigger SageMaker Pipelines). This allows for parallel processing or fanning out the event to different services as required.
- [ ] Least Operational Overhead: This architecture relies heavily on managed services (S3 Replication, S3 Events, EventBridge, Lambda, SageMaker Pipelines integration), minimizing the need for custom infrastructure or complex orchestration logic.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: Using an AWS Lambda function to copy files between S3 buckets (instead of S3 Replication) adds custom code to develop and maintain, which increases operational overhead compared to the managed S3 Replication feature. While S3 event notifications can directly target Lambda, EventBridge (as in option D) offers more flexibility for routing to multiple distinct targets like Lambda and SageMaker Pipelines from a single event.
- [ ] Option B is wrong because: Similar to option A, it uses a Lambda function for the file copy, which is less operationally efficient than S3 Replication. The rest of the event handling via EventBridge is good, but the initial copy mechanism is suboptimal.
- [ ] Option C is wrong because: While S3 Replication is correctly used for copying files, and S3 event notifications can trigger Lambda, directly configuring multiple, distinct downstream services like Lambda and SageMaker Pipelines as direct destinations of a single S3 event notification might be less flexible or manageable than using Amazon EventBridge as an intermediary. EventBridge provides a centralized event bus with more powerful routing and filtering capabilities for multiple targets.

</details>

<details>
  <summary>Question 140</summary>

A solutions architect needs to help a company optimize the cost of running an application on AWS. The application will use Amazon EC2 instances, AWS Fargate, and AWS Lambda for compute within the architecture. The EC2 instances will run the data ingestion layer of the application. EC2 usage will be sporadic and unpredictable. Workloads that run on EC2 instances can be interrupted at any time. The application front end will run on Fargate, and Lambda will serve the API layer. The front-end utilization and API layer utilization will be predictable over the course of the next year.

Which combination of purchasing options will provide the MOST cost-effective solution for hosting this application? (Choose two.)

- [ ] A. Use Spot Instances for the data ingestion layer
- [ ] B. Use On-Demand Instances for the data ingestion layer
- [ ] C. Purchase a 1-year Compute Savings Plan for the front end and API layer.
- [ ] D. Purchase 1-year All Upfront Reserved instances for the data ingestion layer.
- [ ] E. Purchase a 1-year EC2 Instance Savings Plan for the front end and API layer.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Use Spot Instances for the data ingestion layer
- [ ] C. Purchase a 1-year Compute Savings Plan for the front end and API layer.

Why these are the correct answers:

This question requires optimizing costs across different types of compute workloads with varying characteristics.

A. Use Spot Instances for the data ingestion layer
- [ ] EC2 Workload Characteristics: The data ingestion layer running on EC2 has "sporadic and unpredictable" usage, and significantly, "Workloads that run on EC2 instances can be interrupted at any time."
- [ ] Cost-Effectiveness of Spot Instances: Amazon EC2 Spot Instances allow you to take advantage of spare AWS compute capacity at discounts of up to 90% compared to On-Demand prices.
- [ ] They are ideal for fault-tolerant, flexible, and interruptible workloads. Given these characteristics of the data ingestion layer, Spot Instances provide the MOST cost-effective option for this component.

C. Purchase a 1-year Compute Savings Plan for the front end and API layer.
- [ ] Fargate and Lambda Workload Characteristics: The application front end (AWS Fargate) and API layer (AWS Lambda) have "predictable utilization over the course of the next year."
- [ ] Compute Savings Plans Flexibility: Compute Savings Plans offer significant discounts (comparable to Reserved Instances) in exchange for a commitment to a consistent amount of compute usage (measured in $/hour) over a 1-year or 3-year term.
- [ ] A key benefit is their flexibility: Compute Savings Plans automatically apply to EC2 instance usage across different instance families, sizes, Availability Zones, Regions, and operating systems, and they also apply to AWS Fargate and AWS Lambda usage.
- [ ] Cost-Effectiveness for Predictable Fargate/Lambda: For predictable Fargate and Lambda usage, a Compute Savings Plan will provide substantial cost savings compared to paying On-Demand rates (for Fargate) or standard per-request/duration charges (for Lambda).

<hr> Why are the other answers wrong? <hr>

- [ ] Option B is wrong because: Using On-Demand Instances for the data ingestion layer, which is sporadic, unpredictable, and interruptible, is not the most cost-effective choice. Spot Instances (Option A) offer much greater potential for savings for such workloads.
- [ ] Option D is wrong because: Purchasing 1-year All Upfront Reserved Instances for the data ingestion layer (EC2) is not suitable because its usage is "sporadic and unpredictable." Reserved Instances provide the best value when utilized consistently throughout their term. For interruptible and sporadic workloads, Spot Instances are more appropriate.
- [ ] Option E is wrong because: An "EC2 Instance Savings Plan" provides discounts primarily for Amazon EC2 instance usage within a specific instance family and AWS Region. It does not apply to AWS Fargate or AWS Lambda usage. Since the front end runs on Fargate and the API on Lambda, an EC2 Instance Savings Plan would not cover these components. A "Compute Savings Plan" (Option C) is needed for broader coverage including Fargate and Lambda.

</details>

# AWS-SAA-PRACTICE-EXAM Questions 141-150

<details>
  <summary>Question 141</summary>

A company runs a web-based portal that provides users with global breaking news, local alerts, and weather updates. The portal delivers each user a personalized view by using mixture of static and dynamic content. Content is served over HTTPS through an API server running on an Amazon EC2 instance behind an Application Load Balancer (ALB). The company wants the portal to provide this content to its users across the world as quickly as possible.

How should a solutions architect design the application to ensure the LEAST amount of latency for all users?

- [ ] A. Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve all static and dynamic content by specifying the ALB as an origin.
- [ ] B. Deploy the application stack in two AWS Regions. Use an Amazon Route 53 latency routing policy to serve all content from the ALB in the closest Region.
- [ ] C. Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve the static content. Serve the dynamic content directly from the ALB.
- [ ] D. Deploy the application stack in two AWS Regions. Use an Amazon Route 53 geolocation routing policy to serve all content from the ALB in the closest Region.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve all static and dynamic content by specifying the ALB as an origin.

Why this is the correct answer:

- [ ] Amazon CloudFront for Global Low Latency: Amazon CloudFront is a global content delivery network (CDN) service. It significantly reduces latency by caching static content at edge locations geographically closer to users worldwide. For dynamic content, CloudFront forwards requests to the origin (in this case, the ALB) over optimized network paths and can maintain persistent connections, which also helps reduce latency for dynamic requests.
- [ ] Serving Both Static and Dynamic Content via CloudFront: CloudFront can be configured to serve both static assets (which it can cache extensively) and dynamic content (by forwarding requests to the ALB origin). Using CloudFront for all content types ensures that users benefit from its global network and performance optimizations for their entire experience.
- [ ] Single AWS Region for Origin Simplifies Management: Deploying the application stack (EC2 instances and ALB) in a single AWS Region simplifies infrastructure management and data consistency compared to a multi-Region active-active setup. CloudFront's global presence provides the low-latency access for users worldwide without requiring a multi-Region application backend for this primary goal.
- [ ] This approach leverages CloudFront's strength in global content delivery to minimize latency for all users, even with a single-region origin for the application itself.

<hr> Why are the other answers wrong? <hr>

- [ ] Option B is wrong because: While deploying in two AWS Regions with Route 53 latency routing can reduce latency by directing users to the nearest region, it adds significant complexity and cost in terms of managing a multi-Region application stack and ensuring data synchronization. CloudFront (as in Option A) typically provides a more extensive network of edge locations (often closer to users than full AWS Regions) and can achieve comparable or better latency reduction for web content with less backend complexity.
- [ ] Option C is wrong because: Only using CloudFront for static content and serving dynamic content directly from the ALB means that users accessing dynamic content would not benefit from CloudFront's global network optimizations (like optimized routing to the origin and connection management at the edge) for those requests. This would result in higher latency for dynamic content for users far from the origin region compared to routing dynamic content through CloudFront as well.
- [ ] Option D is wrong because: Similar to option B, this involves the complexity of a multi-Region application deployment. Amazon Route 53 geolocation routing directs users based on their geographic location, which is useful for serving region-specific content or meeting data sovereignty requirements. Latency-based routing is generally more appropriate for minimizing latency. However, CloudFront (Option A) is still a more effective solution for overall web content latency reduction.

</details>

<details>
  <summary>Question 142</summary>

A gaming company is designing a highly available architecture. The application runs on a modified Linux kernel and supports only UDP-based traffic. The company needs the front-end tier to provide the best possible user experience. That tier must have low latency, route traffic to the nearest edge location, and provide static IP addresses for entry into the application endpoints.

What should a solutions architect do to meet these requirements?

- [ ] A. Configure Amazon Route 53 to forward requests to an Application Load Balancer. Use AWS Lambda for the application in AWS Application Auto Scaling.
- [ ] B. Configure Amazon CloudFront to forward requests to a Network Load Balancer. Use AWS Lambda for the application in an AWS Application Auto Scaling group.
- [ ] C. Configure AWS Global Accelerator to forward requests to a Network Load Balancer. Use Amazon EC2 instances for the application in an EC2 Auto Scaling group.
- [ ] D. Configure Amazon API Gateway to forward requests to an Application Load Balancer. Use Amazon EC2 instances for the application in an EC2 Auto Scaling group.

</details>

<details>
  <summary>Answer</summary>

- [ ] C. Configure AWS Global Accelerator to forward requests to a Network Load Balancer. Use Amazon EC2 instances for the application in an EC2 Auto Scaling group.

Why this is the correct answer:

This solution addresses all the specific requirements for a UDP-based, low-latency, globally accessible gaming application:

- [ ] AWS Global Accelerator for UDP, Low Latency, Static IPs, and Edge Routing:
- [ ] UDP-based Traffic: Global Accelerator supports both TCP and UDP traffic, which is essential for the gaming application.
- [ ] Low Latency & Nearest Edge Location: It directs user traffic over the AWS global network to the optimal application endpoint (in this case, a Network Load Balancer), leveraging AWS edge locations to reduce latency and improve performance for global users.
- [ ] Static IP Addresses: Global Accelerator provides a set of static anycast IP addresses that serve as a fixed entry point for your application, simplifying client configurations and DNS.
- [ ] Network Load Balancer (NLB) for UDP Load Balancing: A Network Load Balancer operates at Layer 4 (transport layer) and is designed to handle high-throughput, low-latency TCP and UDP traffic. It's the appropriate load balancer type for UDP-based gaming applications.
- [ ] Amazon EC2 Instances in an Auto Scaling Group: Since the application runs on a "modified Linux kernel," it's best suited for Amazon EC2 instances. Using an EC2 Auto Scaling group ensures that the application tier is scalable to handle varying loads and highly available by maintaining a fleet of healthy instances across Availability Zones.
- [ ] This combination meets all specified requirements: UDP support, low latency, nearest edge routing, static IP entry points, and a scalable/highly available application tier.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because:
Application Load Balancer (ALB): ALBs are designed for Layer 7 (HTTP/HTTPS) traffic and do not support UDP-based traffic.
AWS Lambda for Modified Linux Kernel: An application running on a "modified Linux kernel" typically implies a traditional server environment hosted on EC2, not a serverless AWS Lambda function.
- [ ] Option B is wrong because:
Amazon CloudFront for UDP: CloudFront is a content delivery network (CDN) primarily for caching and delivering HTTP/HTTPS web content. It does not natively support or optimize general UDP traffic for gaming applications in the way Global Accelerator does.
AWS Lambda for Modified Linux Kernel: Same issue as in option A.
- [ ] Option D is wrong because:
Amazon API Gateway for UDP: API Gateway is used for creating and managing RESTful and WebSocket APIs (HTTP/HTTPS based). It is not designed to handle raw UDP traffic for a gaming application.
Application Load Balancer (ALB): Same issue as in option A; ALBs do not support UDP.

</details>

<details>
  <summary>Question 143</summary>

A company wants to migrate its existing on-premises monolithic application to AWS. The company wants to keep as much of the front-end code and the backend code as possible. However, the company wants to break the application into smaller applications. A different team will manage each application. The company needs a highly scalable solution that minimizes operational overhead.

Which solution will meet these requirements?

- [ ] A. Host the application on AWS Lambda. Integrate the application with Amazon API Gateway.
- [ ] B. Host the application with AWS Amplify. Connect the application to an Amazon API Gateway API that is integrated with AWS Lambda.
- [ ] C. Host the application on Amazon EC2 instances. Set up an Application Load Balancer with EC2 instances in an Auto Scaling group as targets.
- [ ] D. Host the application on Amazon Elastic Container Service (Amazon ECS). Set up an Application Load Balancer with Amazon ECS as the target.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Host the application on Amazon Elastic Container Service (Amazon ECS). Set up an Application Load Balancer with Amazon ECS as the target.

Why this is the correct answer:

This question describes a common scenario of migrating a monolith and refactoring it into microservices while minimizing code changes and operational overhead.

- [ ] Containerization for Code Reusability and Microservices: The company wants to "keep as much of the front-end code and the backend code as possible" while breaking the application into "smaller applications." Containerizing these smaller application components (microservices) using Docker is an effective way to package existing code and its dependencies.
- [ ] Amazon ECS for Container Orchestration: Amazon Elastic Container Service (ECS) is a fully managed container orchestration service that simplifies the deployment, management, and scaling of containerized applications. It is well-suited for running microservices.   
- [ ] Scalability: ECS can automatically scale the number of container tasks up or down based on demand, providing a "highly scalable solution."
- [ ] Minimized Operational Overhead (especially with Fargate): ECS can be used with AWS Fargate as the compute engine. Fargate is serverless, meaning AWS manages the underlying EC2 instances, so the company doesn't have to provision, patch, or scale servers. This directly addresses the need to "minimize operational overhead." Even if using the EC2 launch type with ECS, ECS itself manages the orchestration, reducing some overhead compared to manually managing containers on EC2.
- [ ] Application Load Balancer (ALB) for Traffic Distribution: An ALB can distribute incoming traffic across the various containerized microservices managed by ECS, providing a single entry point and load balancing.
- [ ] Team Management: Different teams can manage their respective microservices (as distinct ECS services) independently.
- [ ] This approach allows the company to leverage its existing code by containerizing it, adopt a microservices architecture, and benefit from the scalability and reduced operational overhead of a managed container platform.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: Migrating an existing monolithic application (even when broken into smaller pieces) to a fully serverless architecture with AWS Lambda and Amazon API Gateway often requires significant code refactoring to fit Lambda's event-driven, stateless execution model and its specific runtime requirements and limits. This might conflict with the goal to "keep as much of the front-end code and the backend code as possible" without substantial changes.
- [ ] Option B is wrong because: AWS Amplify is a set of tools and services for building full-stack web and mobile applications, often with a serverless backend (like API Gateway and Lambda). While it can be used for new applications or specific front-end deployments, migrating a complex, existing monolithic backend (even when decomposed) primarily using Amplify might involve more refactoring than simply containerizing the components and running them on ECS. The core requirement is to improve the infrastructure for an existing, decomposed application.
- [ ] Option C is wrong because: Hosting the application directly on Amazon EC2 instances, even with an ALB and Auto Scaling, generally involves more operational overhead than using a container orchestration service like ECS, especially with the Fargate launch type. With EC2, the company is responsible for managing the operating systems, patching, security configurations of the instances, and the container runtime if not using an orchestrator. This does not minimize operational overhead as effectively as ECS/Fargate.

</details>

<details>
  <summary>Question 144</summary>

A company recently started using Amazon Aurora as the data store for its global ecommerce application. When large reports are run, developers report that the ecommerce application is performing poorly. After reviewing metrics in Amazon CloudWatch, a solutions architect finds that the ReadIOPS and CPUUtilization metrics are spiking when monthly reports run.

What is the MOST cost-effective solution?

- [ ] A. Migrate the monthly reporting to Amazon Redshift.
- [ ] B. Migrate the monthly reporting to an Aurora Replica.
- [ ] C. Migrate the Aurora database to a larger instance class.
- [ ] D. Increase the Provisioned IOPS on the Aurora instance.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Migrate the monthly reporting to an Aurora Replica.

Why this is the correct answer:

- [ ] Problem Identification: The issue is that "large reports" are causing spikes in ReadIOPS and CPUUtilization on the primary Aurora database, which degrades the performance of the main ecommerce application. This indicates that the read-heavy reporting workload is consuming resources needed by the transactional application.
- [ ] Aurora Replicas for Read Scaling: Amazon Aurora allows you to create up to 15 Aurora Replicas (read replicas) within an Aurora DB cluster. These replicas share the same underlying storage as the primary instance but are dedicated to serving read traffic. By directing the "monthly reporting" queries to an Aurora Replica, the read load is effectively offloaded from the primary (writer) instance.
- [ ] Improved Application Performance: This separation ensures that the resource-intensive reporting queries do not impact the performance of the primary instance, which can then dedicate its resources to the ecommerce application's transactional workload.
- [ ] Cost-Effective: Creating an Aurora Replica is generally more cost-effective than significantly scaling up the primary instance (Option C) or setting up and maintaining a separate data warehousing solution like Amazon Redshift (Option A) solely for these monthly reports, especially if the reports can run against a slightly delayed copy of the production data (Aurora Replicas have low replica lag). Aurora Replicas utilize the shared storage, so you primarily pay for the compute capacity of the replica instance.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: Migrating monthly reporting to Amazon Redshift, a data warehousing service, involves setting up ETL (Extract, Transform, Load) processes to move data from Aurora to Redshift. This adds complexity, potential data latency for reports, and the cost of running a Redshift cluster. While Redshift is powerful for analytics, it might be overkill and less cost-effective than an Aurora Replica if the reporting needs can be met by querying a replica of the operational database.
- [ ] Option C is wrong because: Migrating the entire Aurora database to a larger instance class would increase the cost of the primary database instance 24/7, even when the resource-intensive reports are not running. This is not the "MOST cost-effective solution" for a problem caused by periodic read-heavy workloads. Offloading the specific workload to a read replica is a more targeted and economical approach.
- [ ] Option D is wrong because: The problem states that both ReadIOPS and CPUUtilization are spiking. While increasing Provisioned IOPS (if the Aurora cluster is configured with Provisioned IOPS storage, though Aurora standard storage automatically scales IOPS with storage size) might address the I/O bottleneck, it would not resolve the high CPU utilization caused by the reporting queries if they are also CPU-intensive. An Aurora Replica provides separate compute resources, addressing both potential bottlenecks for the reporting load.

</details>

<details>
  <summary>Question 145</summary>

A company hosts a website analytics application on a single Amazon EC2 On-Demand Instance. The analytics software is written in PHP and uses a MySQL database. The analytics software, the web server that provides PHP, and the database server are all hosted on the EC2 instance. The application is showing signs of performance degradation during busy times and is presenting 5xx errors. The company needs to make the application scale seamlessly.

Which solution will meet these requirements MOST cost-effectively?

- [ ] A. Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use an Application Load Balancer to distribute the load to each EC2 instance.
- [ ] B. Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use Amazon Route 53 weighted routing to distribute the load across the two EC2 instances.
- [ ] C. Migrate the database to an Amazon Aurora MySQL DB instance. Create an AWS Lambda function to stop the EC2 instance and change the instance type. Create an Amazon CloudWatch alarm to invoke the Lambda function when CPU utilization surpasses 75%.
- [ ] D. Migrate the database to an Amazon Aurora MySQL DB instance. Create an AMI of the web application. Apply the AMI to a launch template. Create an Auto Scaling group with the launch template Configure the launch template to use a Spot Fleet. Attach an Application Load Balancer to the Auto Scaling group.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Migrate the database to an Amazon Aurora MySQL DB instance. Create an AMI of the web application. Apply the AMI to a launch template. Create an Auto Scaling group with the launch template Configure the launch template to use a Spot Fleet. Attach an Application Load Balancer to the Auto Scaling group.

Why this is the correct answer:

This solution addresses the performance degradation and scaling issues by decoupling the database and web tiers and implementing scalable, cost-effective AWS services:

- [ ] Migrate Database to Amazon Aurora MySQL: Moving the MySQL database from the EC2 instance to Amazon Aurora MySQL (a managed, MySQL-compatible database service) improves database performance, scalability, availability, and reduces operational overhead. Aurora is designed for high performance and can scale more easily than a self-managed database on EC2.
- [ ] Decouple Web Tier and Use Auto Scaling:
- [ ] Creating an Amazon Machine Image (AMI) of the web application (PHP software and web server) allows for consistent deployment of web server instances.
- [ ] Using this AMI with a launch template and an EC2 Auto Scaling group enables the web tier to "scale seamlessly." The Auto Scaling group can automatically launch or terminate EC2 instances based on demand (e.g., CPU utilization, network traffic), ensuring the application can handle busy times.
- [ ] Application Load Balancer (ALB): An ALB is essential for distributing incoming user traffic across the multiple EC2 instances in the Auto Scaling group. It also performs health checks on the instances, routing traffic only to healthy ones, which improves availability.
- [ ] Spot Fleet for Cost-Effectiveness: Configuring the launch template (used by the Auto Scaling group) to use a Spot Fleet (or directly configuring the Auto Scaling group to use Spot Instances) can significantly reduce EC2 compute costs. Since web tiers are often stateless, they can be good candidates for Spot Instances, which offer large discounts for interruptible capacity. This addresses the "MOST cost-effectively" requirement.
- [ ] This comprehensive approach makes both the database and application tiers scalable, highly available, and optimizes for cost.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: While migrating the database to Amazon RDS for MySQL and using an ALB are good steps, only launching a "second EC2 On-Demand Instance" does not provide seamless or automatic scaling. It's a fixed capacity increase and doesn't adapt to varying loads as effectively as an Auto Scaling group. It's less scalable and potentially less cost-effective than option D.
- [ ] Option B is wrong because: Using Amazon Route 53 weighted routing for load balancing across EC2 instances is generally less effective than using an Application Load Balancer. ALBs provide more sophisticated load balancing features, including application-level health checks, session stickiness, and better integration with Auto Scaling groups. Route 53 DNS-based load balancing has limitations like DNS caching and slower reaction to instance failures.
- [ ] Option C is wrong because: This solution proposes vertical scaling (changing the instance type) of a single EC2 instance using a Lambda function when CPU utilization is high. This approach has several drawbacks:
It causes downtime when the instance is stopped and restarted with a new type.
It's a reactive approach and doesn't provide seamless scaling for concurrent users. A single, larger instance can still become a bottleneck.
Horizontal scaling (adding more instances, as in option D) is generally preferred for web application scalability.

</details>

<details>
  <summary>Question 146</summary>

A company runs a stateless web application in production on a group of Amazon EC2 On-Demand Instances behind an Application Load Balancer. The application experiences heavy usage during an 8-hour period each business day. Application usage is moderate and steady overnight. Application usage is low during weekends.

The company wants to minimize its EC2 costs without affecting the availability of the application.

Which solution will meet these requirements?

- [ ] A. Use Spot Instances for the entire workload.
- [ ] B. Use Reserved Instances for the baseline level of usage. Use Spot instances for any additional capacity that the application needs.
- [ ] C. Use On-Demand Instances for the baseline level of usage. Use Spot Instances for any additional capacity that the application needs.
- [ ] D. Use Dedicated Instances for the baseline level of usage. Use On-Demand Instances for any additional capacity that the application needs.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Use Reserved Instances for the baseline level of usage. Use Spot instances for any additional capacity that the application needs.

Why this is the correct answer:

This solution uses a blended approach of EC2 purchasing options to optimize costs based on the described usage patterns:

- [ ] Reserved Instances (RIs) for Baseline Usage: The application has "moderate and steady overnight" usage and also "low during weekends." This constitutes a predictable baseline level of compute capacity that is always needed. Purchasing Reserved Instances for this baseline capacity provides a significant discount (up to 72%) compared to On-Demand pricing, in exchange for a 1-year or 3-year commitment. This is the most cost-effective way to cover the continuous, predictable part of the workload.
- [ ] Spot Instances for Peak/Additional Capacity: For the "heavy usage during an 8-hour period each business day" and any other dynamic scaling needs beyond the baseline, Spot Instances can be used. Spot Instances offer the largest discounts (up to 90% off On-Demand) by allowing you to use spare EC2 capacity. Since the web application is "stateless," it is generally well-suited to handle potential interruptions that can occur with Spot Instances (as Auto Scaling can launch replacements). This makes Spot Instances highly cost-effective for handling variable and peak loads.
- [ ] This combination ensures that the always-on baseline is covered at a discounted rate with RIs, while the fluctuating peak demand is handled very cheaply with Spot Instances, thus minimizing overall EC2 costs without impacting availability (assuming the application and Auto Scaling are configured to handle Spot Instance lifecycle).

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: While Spot Instances are very cost-effective, relying on them for the entire workload, including the baseline capacity, might not be ideal for ensuring consistent availability for a production web application if there are periods of high Spot prices or low Spot availability leading to interruptions. A more common strategy is to cover the baseline with a more stable option like RIs or Savings Plans.
- [ ] Option C is wrong because: Using On-Demand Instances for the baseline level of usage is more expensive than using Reserved Instances. If there's a predictable, continuous baseline, RIs (or Savings Plans) will offer significant cost savings over On-Demand for that portion of the workload.
- [ ] Option D is wrong because: Dedicated Instances run on hardware that is dedicated to a single customer's account. They are typically used for specific compliance, licensing, or security requirements that necessitate physical server isolation. Dedicated Instances are more expensive than standard On-Demand or Reserved Instances and are not primarily a cost-minimization strategy for a general web application unless such strict isolation is a requirement (which is not stated in the question). Using On-Demand for additional capacity is also less cost-effective than Spot Instances for this stateless application.

</details>

<details>
  <summary>Question 147</summary>

A company needs to retain application log files for a critical application for 10 years. The application team regularly accesses logs from the past month for troubleshooting, but logs older than 1 month are rarely accessed. The application generates more than 10 TB of logs per month.

Which storage option meets these requirements MOST cost-effectively?

- [ ] A. Store the logs in Amazon S3. Use AWS Backup to move logs more than 1 month old to S3 Glacier Deep Archive.
- [ ] B. Store the logs in Amazon S3. Use S3 Lifecycle policies to move logs more than 1 month old to S3 Glacier Deep Archive.
- [ ] C. Store the logs in Amazon CloudWatch Logs. Use AWS Backup to move logs more than 1 month old to S3 Glacier Deep Archive.
- [ ] D. Store the logs in Amazon CloudWatch Logs. Use Amazon S3 Lifecycle policies to move logs more than 1 month old to S3 Glacier Deep Archive.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Store the logs in Amazon S3. Use S3 Lifecycle policies to move logs more than 1 month old to S3 Glacier Deep Archive.

Why this is the correct answer:

This solution provides a balance of accessibility for recent logs and extreme cost-effectiveness for long-term archival:

- [ ] Amazon S3 for Initial Log Storage: Storing the large volume of application logs (10 TB/month) in Amazon S3 (initially, likely in S3 Standard or S3 Standard-Infrequent Access if access patterns within the first month allow) provides durable, scalable, and cost-effective storage. Logs from the "past month" that are regularly accessed for troubleshooting can be readily retrieved from these S3 tiers.
- [ ] S3 Lifecycle Policies for Automated Tiering: Amazon S3 Lifecycle policies allow you to define rules to automatically transition objects to different storage classes based on their age. This is ideal for managing data with changing access patterns over time.
- [ ] Transition to S3 Glacier Deep Archive: For logs "older than 1 month" that are "rarely accessed" and need to be retained for 10 years, transitioning them to Amazon S3 Glacier Deep Archive is the MOST cost-effective solution. S3 Glacier Deep Archive offers the lowest storage cost in AWS for long-term data archiving. An S3 Lifecycle policy can be configured to automatically move these logs after 30 days (1 month).
- [ ] Cost-Effectiveness: This approach ensures that recent, frequently accessed logs are available with good performance, while older, rarely accessed logs are moved to the cheapest possible storage tier for long-term retention, significantly reducing overall storage costs over the 10-year period.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: While AWS Backup can be used to back up data in S3 and manage its lifecycle, including moving it to colder tiers, S3 Lifecycle policies are the native, more direct, and generally more operationally efficient way to manage object transitions between storage classes within S3 itself, including to S3 Glacier Deep Archive. Using AWS Backup for this specific intra-S3 tiering task adds an unnecessary management layer.
- [ ] Option C is wrong because: Storing 10 TB of logs per month directly in Amazon CloudWatch Logs for long periods can become significantly more expensive than storing them in Amazon S3. CloudWatch Logs is optimized for real-time monitoring, analysis, and shorter-term retention or streaming to other services. For very long-term, high-volume storage, S3 is more cost-effective. While AWS Backup could then archive from CloudWatch Logs (or S3 if logs are first exported), starting with S3 (Option B) is better for initial cost.
- [ ] Option D is wrong because: Similar to option C, storing high volumes of logs in CloudWatch Logs for the initial month before any archival considerations is less cost-effective than using S3 from the start for bulk storage. S3 Lifecycle policies apply to objects within S3; while you can export CloudWatch Logs to S3 and then apply lifecycle policies, Option B is more direct by using S3 as the primary storage from which lifecycle policies operate.

</details>

<details>
  <summary>Question 148</summary>

A company has a data ingestion workflow that includes the following components:

An Amazon Simple Notification Service (Amazon SNS) topic that receives notifications about new data deliveries
An AWS Lambda function that processes and stores the data
The ingestion workflow occasionally fails because of network connectivity issues. When failure occurs, the corresponding data is not ingested unless the company manually reruns the job.

What should a solutions architect do to ensure that all notifications are eventually processed?

- [ ] A. Configure the Lambda function for deployment across multiple Availability Zones.
- [ ] B. Modify the Lambda function's configuration to increase the CPU and memory allocations for the function.
- [ ] C. Configure the SNS topic's retry strategy to increase both the number of retries and the wait time between retries.
- [ ] D. Configure an Amazon Simple Queue Service (Amazon SQS) queue as the on-failure destination. Modify the Lambda function to process messages in the queue.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Configure an Amazon Simple Queue Service (Amazon SQS) queue as the on-failure destination. Modify the Lambda function to process messages in the queue.

Why this is the correct answer:

This solution addresses the problem of lost messages due to Lambda processing failures by introducing a dead-letter queue (DLQ) mechanism.

- [ ] Problem: Lost Messages on Failure: Currently, if the Lambda function fails to process a notification from SNS (due to "network connectivity issues"), and if SNS's built-in retries to the Lambda are also exhausted or unsuccessful, the notification (and the data processing it triggers) is lost.
- [ ] SQS as an On-Failure Destination (Dead-Letter Queue): Amazon SNS allows you to configure a dead-letter queue (DLQ), which is an Amazon SQS queue. If SNS cannot successfully deliver a message to a subscribed endpoint (like the Lambda function) after exhausting its retry attempts, the message is sent to this SQS DLQ instead of being discarded. This ensures that failed notifications are captured and not lost.
- [ ] Processing from the DLQ: Once messages are in the SQS DLQ, they can be processed. The phrase "Modify the Lambda function to process messages in the queue" implies that there will be a mechanism to handle these captured messages. This could involve:
A separate Lambda function that polls the DLQ to analyze the failures and attempt reprocessing.
- [ ] An operational process where administrators are alerted to messages in the DLQ and can manually trigger reprocessing or investigate the root cause of the failures. This ensures that "all notifications are eventually processed" by providing a durable holding place for failed attempts.

<hr> Why are the other answers wrong? <hr>

- [ ] Option A is wrong because: AWS Lambda functions are inherently deployed and run in a highly available environment that spans multiple Availability Zones within a region by default. Users do not manually configure Lambda deployment across AZs. This does not address the issue of what happens when an individual invocation fails due to transient issues like network connectivity after all retries.
- [ ] Option B is wrong because: Increasing the CPU and memory for the Lambda function might help if failures were due to resource exhaustion within the Lambda function itself. However, the problem states that failures are "because of network connectivity issues." While more resources might allow the function to complete faster if it does connect, it doesn't solve the problem of a persistent network issue during the invocation or guarantee that the message won't be lost if all invocation attempts fail.
- [ ] Option C is wrong because: Amazon SNS has built-in retry policies for delivering messages to subscribers like Lambda functions. While you can configure these retry policies (number of retries, backoff strategy), if all configured retries ultimately fail due to a persistent (even if temporary) issue, the message can still be discarded by SNS if a DLQ is not configured. A DLQ (as in option D) provides a safety net for messages that exhaust these retries.

</details>

<details>
  <summary>Question 149</summary>

A company has a service that produces event data. The company wants to use AWS to process the event data as it is received. The data is written in a specific order that must be maintained throughout processing. The company wants to implement a solution that minimizes operational overhead.

How should a solutions architect accomplish this?

- [ ] A. Create an Amazon Simple Queue Service (Amazon SQS) FIFO queue to hold messages. Set up an AWS Lambda function to process messages from the queue.
- [ ] B. Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing payloads to process. Configure an AWS Lambda function as a subscriber.
- [ ] C. Create an Amazon Simple Queue Service (Amazon SQS) standard queue to hold messages. Set up an AWS Lambda function to process messages from the queue independently.
- [ ] D. Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing payloads to process. Configure an Amazon Simple Queue Service (Amazon SQS) queue as a subscriber.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Create an Amazon Simple Queue Service (Amazon SQS) FIFO queue to hold messages. Set up an AWS Lambda function to process messages from the queue.

Why this is the correct answer:

- [ ] Maintaining Order with SQS FIFO Queues: The critical requirement is that "The data is written in a specific order that must be maintained throughout processing." Amazon SQS FIFO (First-In, First-Out) queues are specifically designed for applications where the order of operations and events is critical. They preserve the exact order in which messages are sent and received.
- [ ] Processing with AWS Lambda: An AWS Lambda function can be configured as a consumer for the SQS FIFO queue. Lambda will process the messages from the queue in the order they are received (within a message group, if message group IDs are used). This is a serverless approach, which helps to "minimize operational overhead."
- [ ] Decoupling and Reliability: SQS queues decouple the event-producing service from the Lambda processing function, providing a durable buffer for the event data. This increases the reliability of the system.
This solution directly addresses the need for ordered processing with minimal operational burden using managed AWS services.

<hr> Why are the other answers wrong? <hr>

- [ ] Option B is wrong because: Amazon SNS standard topics do not guarantee the order in which messages are delivered to subscribers (like an AWS Lambda function). While SNS is good for fanning out messages, it's not suitable when strict end-to-end order preservation is required for processing. (SNS FIFO topics do exist but are not specified here, and even then, the SQS FIFO queue in option A is a more direct fit for an ordered processing pipeline).
- [ ] Option C is wrong because: Amazon SQS standard queues provide "best-effort" ordering. This means that while messages are generally delivered in the order they are sent, there is no guarantee, and messages might be delivered out of order, especially at scale. This violates the requirement that "order that must be maintained throughout processing."
- [ ] Option D is wrong because: This option describes using SNS to deliver to an SQS queue. If the SNS topic is a standard topic, it will not guarantee ordered delivery to the SQS queue. If the SQS queue is a standard queue, it also won't preserve order. While an SNS FIFO topic delivering to an SQS FIFO queue could work, option A is a more direct and simpler solution focusing on a single SQS FIFO queue for ordered message processing by Lambda.

</details>

<details>
  <summary>Question 150</summary>

A company is migrating an application from on-premises servers to Amazon EC2 instances. As part of the migration design requirements, a solutions architect must implement infrastructure metric alarms. The company does not need to take action if CPU utilization increases to more than 50% for a short burst of time. However, if the CPU utilization increases to more than 50% and read IOPS on the disk are high at the same time, the company needs to act as soon as possible. The solutions architect also must reduce false alarms.

What should the solutions architect do to meet these requirements?

- [ ] A. Create Amazon CloudWatch composite alarms where possible.
- [ ] B. Create Amazon CloudWatch dashboards to visualize the metrics and react to issues quickly.
- [ ] C. Create Amazon CloudWatch Synthetics canaries to monitor the application and raise an alarm.
- [ ] D. Create single Amazon CloudWatch metric alarms with multiple metric thresholds where possible.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Create Amazon CloudWatch composite alarms where possible.
      
Why this is the correct answer:

- [ ] Amazon CloudWatch Composite Alarms: The core requirement is to trigger an action only when multiple conditions are met simultaneously (CPU utilization > 50% AND high read IOPS). Amazon CloudWatch composite alarms are designed for exactly this scenario. A composite alarm allows you to create an alarm based on the state of other CloudWatch alarms (which would be individual metric alarms for CPU utilization and read IOPS). The composite alarm will only go into an ALARM state when all the underlying rule conditions are met.
- [ ] Reduces False Alarms: By requiring both conditions to be true, composite alarms help "reduce false alarms." An alarm won't trigger if only CPU utilization briefly spikes without high IOPS, or vice-versa, aligning with the company's requirement not to act on short bursts of a single metric.
- [ ] Timely Action: When the combined critical condition (high CPU and high IOPS) is met, the composite alarm will trigger, enabling the company to "act as soon as possible."

<hr> Why are the other answers wrong? <hr>

- [ ] Option B is wrong because: Amazon CloudWatch dashboards are excellent for visualizing metrics and monitoring trends. However, dashboards themselves do not create or trigger alarms based on specific conditions. They are for observation and manual analysis, not for automated alerting based on combined metric states.
- [ ] Option C is wrong because: Amazon CloudWatch Synthetics canaries are used to create configurable scripts that monitor your application endpoints and APIs, simulating user traffic (outside-in monitoring). They are useful for checking availability, latency, and user experience aspects. They are not primarily designed for monitoring internal infrastructure metrics like EC2 CPU utilization or disk IOPS and creating alarms based on combinations of these specific instance-level metrics.
- [ ] Option D is wrong because: Standard Amazon CloudWatch metric alarms are based on a single metric crossing a single threshold for a specified number of evaluation periods. While you can create multiple alarms for different thresholds of the same metric, a "single Amazon CloudWatch metric alarm with multiple metric thresholds" that combines different metrics (like CPU and IOPS) to trigger a single alarm state is not a standard feature. Composite alarms (Option A) are the AWS service feature designed to achieve this "AND" logic across different alarms.

</details>











