<details>
  <summary>Question 501</summary>

A company wants to ingest customer payment data into the company's data lake in Amazon S3.
The company receives payment data every minute on average.
The company wants to analyze the payment data in real time.
Then the company wants to ingest the data into the data lake.
Which solution will meet these requirements with the MOST operational efficiency?

-   [ ] A. Use Amazon Kinesis Data Streams to ingest data.
    Use AWS Lambda to analyze the data in real time.
-   [ ] B. Use AWS Glue to ingest data.
    Use Amazon Kinesis Data Analytics to analyze the data in real time.
-   [ ] C. Use Amazon Kinesis Data Firehose to ingest data.
    Use Amazon Kinesis Data Analytics to analyze the data in real time.
-   [ ] D. Use Amazon API Gateway to ingest data.
    Use AWS Lambda to analyze the data in real time.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Use Amazon Kinesis Data Firehose to ingest data.
    Use Amazon Kinesis Data Analytics to analyze the data in real time.

Why these are the correct answers:

C. Use Amazon Kinesis Data Firehose to ingest data.
Use Amazon Kinesis Data Analytics to analyze the data in real time.

-   [ ] Amazon Kinesis Data Firehose is designed for near real-time ingestion of streaming data into destinations like Amazon S3.
-   [ ] Amazon Kinesis Data Analytics can analyze streaming data in real time.

Why are the other answers wrong?

-   [ ] A. Kinesis Data Streams requires more management for scaling and data delivery compared to Firehose.
    Lambda is not designed for real-time analytics on streaming data.
-   [ ] B. AWS Glue is an ETL service, not optimized for real-time data ingestion and analysis.
-   [ ] D. API Gateway is for API management, not for high-volume streaming data ingestion.
    Lambda is not suited for real-time analytics on streams.

Therefore, Option C is the most operationally efficient solution.

</details>
<details>
  <summary>Question 502</summary>

A company runs a website that uses a content management system (CMS) on Amazon EC2.
The CMS runs on a single EC2 instance and uses an Amazon Aurora MySQL Multi-AZ DB instance for the data tier.
Website images are stored on an Amazon Elastic Block Store (Amazon EBS) volume that is mounted inside the EC2 instance.
Which combination of actions should a solutions architect take to improve the performance and resilience of the website? (Choose two.)

-   [ ] A. Move the website images into an Amazon S3 bucket that is mounted on every EC2 instance
-   [ ] B. Share the website images by using an NFS share from the primary EC2 instance.
    Mount this share on the other EC2 instances.
-   [ ] C. Move the website images onto an Amazon Elastic File System (Amazon EFS) file system that is mounted on every EC2 instance.
-   [ ] D. Create an Amazon Machine Image (AMI) from the existing EC2 instance.
    Use the AMI to provision new instances behind an Application Load Balancer as part of an Auto Scaling group.
    Configure an accelerator in AWS Global Accelerator for the website
-   [ ] E. Create an Amazon Machine Image (AMI) from the existing EC2 instance.
    Use the AMI to provision new instances behind an Application Load Balancer as part of an Auto Scaling group.
    Configure an Amazon CloudFront distribution for the website.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Move the website images onto an Amazon Elastic File System (Amazon EFS) file system that is mounted on every EC2 instance.
-   [ ] E. Create an Amazon Machine Image (AMI) from the existing EC2 instance.
    Use the AMI to provision new instances behind an Application Load Balancer as part of an Auto Scaling group.
    Configure an Amazon CloudFront distribution for the website.

Why these are the correct answers:

C. Move the website images onto an Amazon Elastic File System (Amazon EFS) file system that is mounted on every EC2 instance.

-   [ ] EFS provides shared file storage that can be accessed by multiple EC2 instances, improving scalability and availability.

E. Create an Amazon Machine Image (AMI) from the existing EC2 instance.
Use the AMI to provision new instances behind an Application Load Balancer as part of an Auto Scaling group.
Configure an Amazon CloudFront distribution for the website.

-   [ ] Auto Scaling groups and Application Load Balancers distribute traffic and improve availability.
-   [ ] CloudFront caches static content, improving performance.

Why are the other answers wrong?

-   [ ] A. Mounting an S3 bucket on EC2 instances is not efficient for serving frequently accessed files.
-   [ ] B. NFS shares from a single EC2 instance introduce a single point of failure.
-   [ ] D. Global Accelerator is for improving global application performance, not for scaling within a Region.

Therefore, Options C and E provide the best solution for performance and resilience.

</details>
<details>
  <summary>Question 503</summary>

A company runs an infrastructure monitoring service.
The company is building a new feature that will enable the service to monitor data in customer AWS accounts.
The new feature will call AWS APIs in customer accounts to describe Amazon EC2 instances and read Amazon CloudWatch metrics.
What should the company do to obtain access to customer accounts in the MOST secure way?

-   [ ] A. Ensure that the customers create an IAM role in their account with read-only EC2 and CloudWatch permissions and a trust policy to the company's account.
-   [ ] B. Create a serverless API that implements a token vending machine to provide temporary AWS credentials for a role with read-only EC2 and CloudWatch permissions.
-   [ ] C. Ensure that the customers create an IAM user in their account with read-only EC2 and CloudWatch permissions.
    Encrypt and store customer access and secret keys in a secrets management system.
-   [ ] D. Ensure that the customers create an Amazon Cognito user in their account to use an IAM role with read-only EC2 and CloudWatch permissions.
    Encrypt and store the Amazon Cognito user and password in a secrets management system.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Ensure that the customers create an IAM role in their account with read-only EC2 and CloudWatch permissions and a trust policy to the company's account.

Why these are the correct answers:

A. Ensure that the customers create an IAM role in their account with read-only EC2 and CloudWatch permissions and a trust policy to the company's account.

-   [ ] IAM roles allow for secure delegation of permissions without sharing long-term credentials.
-   [ ] The trust policy grants the company's account permission to assume the role.

Why are the other answers wrong?

-   [ ] B. A token vending machine adds complexity and is not necessary for this use case.
-   [ ] C. Sharing IAM user credentials (access keys) is insecure.
-   [ ] D. Amazon Cognito is for user authentication, not for granting AWS service permissions.

Therefore, Option A is the most secure solution.

</details>
<details>
  <summary>Question 504</summary>

A company needs to connect several VPCs in the us-east-1 Region that span hundreds of AWS accounts.
The company's networking team has its own AWS account to manage the cloud network.
What is the MOST operationally efficient solution to connect the VPCs?

-   [ ] A. Set up VPC peering connections between each VPC.
    Update each associated subnet's route table
-   [ ] B. Configure a NAT gateway and an internet gateway in each VPC to connect each VPC through the internet
-   [ ] C. Create an AWS Transit Gateway in the networking team's AWS account.
    Configure static routes from each VPC.
-   [ ] D. Deploy VPN gateways in each VPC.
    Create a transit VPC in the networking team's AWS account to connect to each VPC.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Create an AWS Transit Gateway in the networking team's AWS account.
    Configure static routes from each VPC.

Why these are the correct answers:

C. Create an AWS Transit Gateway in the networking team's AWS account.
Configure static routes from each VPC.

-   [ ] AWS Transit Gateway simplifies the management of connections between multiple VPCs.
-   [ ] It reduces the complexity of managing numerous peering connections.

Why are the other answers wrong?

-   [ ] A. VPC peering is complex to manage with hundreds of VPCs.
-   [ ] B. Connecting VPCs through the internet is inefficient and insecure.
-   [ ] D. VPN gateways and transit VPCs add complexity and overhead.

Therefore, Option C is the most operationally efficient solution.

</details>
<details>
  <summary>Question 505</summary>

A company has Amazon EC2 instances that run nightly batch jobs to process data.
The EC2 instances run in an Auto Scaling group that uses On-Demand billing.
If a job fails on one instance, another instance will reprocess the job.
The batch jobs run between 12:00 AM and 06:00 AM local time every day.
Which solution will provide EC2 instances to meet these requirements MOST cost-effectively?

-   [ ] A. Purchase a 1-year Savings Plan for Amazon EC2 that covers the instance family of the Auto Scaling group that the batch job uses.
-   [ ] B. Purchase a 1-year Reserved Instance for the specific instance type and operating system of the instances in the Auto Scaling group that the batch job uses.
-   [ ] C. Create a new launch template for the Auto Scaling group.
    Set the instances to Spot Instances.
    Set a policy to scale out based on CPU usage.
-   [ ] D. Create a new launch template for the Auto Scaling group.
    Increase the instance size.
    Set a policy to scale out based on CPU usage.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Create a new launch template for the Auto Scaling group.
    Set the instances to Spot Instances.
    Set a policy to scale out based on CPU usage.

Why these are the correct answers:

C. Create a new launch template for the Auto Scaling group.
Set the instances to Spot Instances.
Set a policy to scale out based on CPU usage.

-   [ ] Spot Instances are cost-effective for fault-tolerant batch processing.
-   [ ] Auto Scaling can replace any interrupted Spot Instances.
-   [ ] Scaling based on CPU usage optimizes resource utilization.

Why are the other answers wrong?

-   [ ] A and B. Savings Plans and Reserved Instances are cost-effective for continuous usage, not for sporadic nightly jobs.
-   [ ] D. Increasing instance size is less cost-effective than using Spot Instances.

Therefore, Option C is the most cost-effective solution.

</details>
<details>
  <summary>Question 506</summary>

A social media company is building a feature for its website.
The feature will give users the ability to upload photos.
The company expects significant increases in demand during large events and must ensure that the website can handle the upload traffic from users.
Which solution meets these requirements with the MOST scalability?

-   [ ] A. Upload files from the user's browser to the application servers.
    Transfer the files to an Amazon S3 bucket.
-   [ ] B. Provision an AWS Storage Gateway file gateway.
    Upload files directly from the user's browser to the file gateway.
-   [ ] C. Generate Amazon S3 presigned URLs in the application.
    Upload files directly from the user's browser into an S3 bucket.
-   [ ] D. Provision an Amazon Elastic File System (Amazon EFS) file system.
    Upload files directly from the user's browser to the file system.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Generate Amazon S3 presigned URLs in the application.
    Upload files directly from the user's browser into an S3 bucket.

Why these are the correct answers:

C. Generate Amazon S3 presigned URLs in the application.
Upload files directly from the user's browser into an S3 bucket.

-   [ ] S3 presigned URLs allow users to upload files directly to S3, offloading the traffic from the application servers.
-   [ ] S3 is highly scalable and can handle large volumes of uploads.

Why are the other answers wrong?

-   [ ] A. Uploading files to application servers and then transferring them to S3 increases the load on the servers.
-   [ ] B. Storage Gateway is for hybrid cloud storage, not for handling large volumes of direct user uploads.
-   [ ] D. EFS is a file system and is not designed for handling direct uploads from numerous users.

Therefore, Option C is the most scalable solution.

</details>
<details>
  <summary>Question 507</summary>

A company has a web application for travel ticketing.
The application is based on a database that runs in a single data center in North America.
The company wants to expand the application to serve a global user base.
The company needs to deploy the application to multiple AWS Regions.
Average latency must be less than 1 second on updates to the reservation database.
The company wants to have separate deployments of its web platform across multiple Regions.
However, the company must maintain a single primary reservation database that is globally consistent.
Which solution should a solutions architect recommend to meet these requirements?

-   [ ] A. Convert the application to use Amazon DynamoDB.
    Use a global table for the center reservation table.
    Use the correct Regional endpoint in each Regional deployment.
-   [ ] B. Migrate the database to an Amazon Aurora MySQL database.
    Deploy Aurora Read Replicas in each Region.
    Use the correct Regional endpoint in each Regional deployment for access to the database.
-   [ ] C. Migrate the database to an Amazon RDS for MySQL database.
    Deploy MySQL read replicas in each Region.
    Use the correct Regional endpoint in each Regional deployment for access to the database.
-   [ ] D. Migrate the application to an Amazon Aurora Serverless database.
    Deploy instances of the database to each Region.
    Use the correct Regional endpoint in each Regional deployment to access the database.
    Use AWS Lambda functions to process event streams in each Region to synchronize the databases.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Migrate the database to an Amazon Aurora MySQL database.
    Deploy Aurora Read Replicas in each Region.
    Use the correct Regional endpoint in each Regional deployment for access to the database.

Why these are the correct answers:

B. Migrate the database to an Amazon Aurora MySQL database.
Deploy Aurora Read Replicas in each Region.
Use the correct Regional endpoint in each Regional deployment for access to the database.

-   [ ] Aurora provides high performance and scalability.
-   [ ] Read Replicas in each Region can serve local read requests, reducing latency.
-   [ ] A single primary Aurora database ensures global consistency.

Why are the other answers wrong?

-   [ ] A. DynamoDB is a NoSQL database and may require significant application changes.
-   [ ] C. RDS MySQL read replicas do not provide the same level of performance and global consistency as Aurora.
-   [ ] D. Aurora Serverless is not ideal for high-performance, low-latency global applications.
    Lambda and event streams add complexity.

Therefore, Option B is the most suitable solution.

</details>
<details>
  <summary>Question 508</summary>

A company has migrated multiple Microsoft Windows Server workloads to Amazon EC2 instances that run in the us-west-1 Region.
The company manually backs up the workloads to create an image as needed.
In the event of a natural disaster in the us-west-1 Region, the company wants to recover workloads quickly in the us-west-2 Region.
The company wants no more than 24 hours of data loss on the EC2 instances.
The company also wants to automate any backups of the EC2 instances.
Which solutions will meet these requirements with the LEAST administrative effort? (Choose two.)

-   [ ] A. Create an Amazon EC2-backed Amazon Machine Image (AMI) lifecycle policy to create a backup based on tags.
    Schedule the backup to run twice daily.
    Copy the image on demand.
-   [ ] B. Create an Amazon EC2-backed Amazon Machine Image (AMI) lifecycle policy to create a backup based on tags.
    Schedule the backup to run twice daily.
    Configure the copy to the us-west-2 Region.
-   [ ] C. Create backup vaults in us-west-1 and in us-west-2 by using AWS Backup.
    Create a backup plan for the EC2 instances based on tag values.
    Create an AWS Lambda function to run as a scheduled job to copy the backup data to us-west-2.
-   [ ] D. Create a backup vault by using AWS Backup.
    Use AWS Backup to create a backup plan for the EC2 instances based on tag values.
    Define the destination for the copy as us-west-2.
    Specify the backup schedule to run twice daily.
-   [ ] E. Create a backup vault by using AWS Backup.
    Use AWS Backup to create a backup plan for the EC2 instances based on tag values.
    Specify the backup schedule to run twice daily.
    Copy on demand to us-west-2.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Create an Amazon EC2-backed Amazon Machine Image (AMI) lifecycle policy to create a backup based on tags.
    Schedule the backup to run twice daily.
    Configure the copy to the us-west-2 Region.
-   [ ] D. Create a backup vault by using AWS Backup.
    Use AWS Backup to create a backup plan for the EC2 instances based on tag values.
    Define the destination for the copy as us-west-2.
    Specify the backup schedule to run twice daily.

Why these are the correct answers:

B. Create an Amazon EC2-backed Amazon Machine Image (AMI) lifecycle policy to create a backup based on tags.
Schedule the backup to run twice daily.
Configure the copy to the us-west-2 Region.

-   [ ] AMI lifecycle policies automate AMI creation.
-   [ ] Scheduling backups twice daily meets the RPO of 24 hours.
-   [ ] Automating the copy to us-west-2 simplifies the DR process.

D. Create a backup vault by using AWS Backup.
Use AWS Backup to create a backup plan for the EC2 instances based on tag values.
Define the destination for the copy as us-west-2.
Specify the backup schedule to run twice daily.

-   [ ] AWS Backup centralizes backup management.
-   [ ] It automates backups and cross-Region copies.
-   [ ] Tag-based backups simplify management.

Why are the other answers wrong?

-   [ ] A. Copying images on demand adds manual steps.
-   [ ] C. Using Lambda functions adds complexity.
-   [ ] E. Copying on demand adds manual steps.

Therefore, Options B and D provide the most automated and efficient solutions.

</details>
<details>
  <summary>Question 509</summary>

A company operates a two-tier application for image processing.
The application uses two Availability Zones, each with one public subnet and one private subnet.
An Application Load Balancer (ALB) for the web tier uses the public subnets.
Amazon EC2 instances for the application tier use the private subnets.
Users report that the application is running more slowly than expected.
A security audit of the web server log files shows that the application is receiving millions of illegitimate requests from a small number of IP addresses.
A solutions architect needs to resolve the immediate performance problem while the company investigates a more permanent solution.
What should the solutions architect recommend to meet this requirement?

-   [ ] A. Modify the inbound security group for the web tier.
    Add a deny rule for the IP addresses that are consuming resources.
-   [ ] B. Modify the network ACL for the web tier subnets.
    Add an inbound deny rule for the IP addresses that are consuming resources.
-   [ ] C. Modify the inbound security group for the application tier.
    Add a deny rule for the IP addresses that are consuming resources.
-   [ ] D. Modify the network ACL for the application tier subnets.
    Add an inbound deny rule for the IP addresses that are consuming resources.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Modify the network ACL for the web tier subnets.
    Add an inbound deny rule for the IP addresses that are consuming resources.

Why these are the correct answers:

B. Modify the network ACL for the web tier subnets.
Add an inbound deny rule for the IP addresses that are consuming resources.

-   [ ] Network ACLs operate at the subnet level and can quickly block traffic, reducing the load on the web tier.
-   [ ] This is an immediate solution to mitigate the performance issue.

Why are the other answers wrong?

-   [ ] A and C. Security groups operate at the instance level and are less efficient for blocking traffic at the subnet level.
-   [ ] D. Modifying the application tier's network ACLs does not address the immediate problem of illegitimate requests hitting the web tier.

Therefore, Option B is the most appropriate solution for immediate mitigation.

</details>
<details>
  <summary>Question 510</summary>

A global marketing company has applications that run in the ap-southeast-2 Region and the eu-west-1 Region.
Applications that run in a VPC in eu-west-1 need to communicate securely with databases that run in a VPC in ap-southeast-2.
Which network design will meet these requirements?

-   [ ] A. Create a VPC peering connection between the eu-west-1 VPC and the ap-southeast-2 VPC.
    Create an inbound rule in the eu-west-1 application security group that allows traffic from the database server IP addresses in the ap-southeast-2 security group.
-   [ ] B. Configure a VPC peering connection between the ap-southeast-2 VPC and the eu-west-1 VPC.
    Update the subnet route tables.
    Create an inbound rule in the ap-southeast-2 database security group that references the security group ID of the application servers in eu-west-1.
-   [ ] C. Configure a VPC peering connection between the ap-southeast-2 VPC and the eu-west-1 VPUpdate the subnet route tables.
    Create an inbound rule in the ap-southeast-2 database security group that allows traffic from the eu-west-1 application server IP addresses.
-   [ ] D. Create a transit gateway with a peering attachment between the eu-west-1 VPC and the ap- southeast-2 VPC.
    After the transit gateways are properly peered and routing is configured, create an inbound rule in the database security group that references the security group ID of the application servers in eu-west-1.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Configure a VPC peering connection between the ap-southeast-2 VPC and the eu-west-1 VPUpdate the subnet route tables.
    Create an inbound rule in the ap-southeast-2 database security group that allows traffic from the eu-west-1 application server IP addresses.

Why these are the correct answers:

C. Configure a VPC peering connection between the ap-southeast-2 VPC and the eu-west-1 VPUpdate the subnet route tables.
Create an inbound rule in the ap-southeast-2 database security group that allows traffic from the eu-west-1 application server IP addresses.

-   [ ] VPC peering connects two VPCs, enabling communication.
-   [ ] Updating route tables directs traffic between the VPCs.
-   [ ] Security groups control traffic at the instance level.

Why are the other answers wrong?

-   [ ] A. Security groups cannot reference IP addresses in another security group across VPC peering connections.
-   [ ] B. While security groups can reference other security groups, the direction of the peering connection setup is important and the description is incorrect.
-   [ ] D. Transit Gateway is for connecting many VPCs, not just two, and adds complexity.

Therefore, Option C is the correct solution.

</details>

<details>
  <summary>Question 501</summary>

A company wants to ingest customer payment data into the company's data lake in Amazon S3.
The company receives payment data every minute on average.
The company wants to analyze the payment data in real time.
Then the company wants to ingest the data into the data lake.
Which solution will meet these requirements with the MOST operational efficiency?

-   [ ] A.
    Use Amazon Kinesis Data Streams to ingest data.
    Use AWS Lambda to analyze the data in real time.
-   [ ] B.
    Use AWS Glue to ingest data.
    Use Amazon Kinesis Data Analytics to analyze the data in real time.
-   [ ] C.
    Use Amazon Kinesis Data Firehose to ingest data.
    Use Amazon Kinesis Data Analytics to analyze the data in real time.
-   [ ] D.
    Use Amazon API Gateway to ingest data.
    Use AWS Lambda to analyze the data in real time.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C.
    Use Amazon Kinesis Data Firehose to ingest data.
    Use Amazon Kinesis Data Analytics to analyze the data in real time.

Why these are the correct answers:

C.
Use Amazon Kinesis Data Firehose to ingest data.
Use Amazon Kinesis Data Analytics to analyze the data in real time.

-   [ ] Amazon Kinesis Data Firehose is designed for real-time streaming data ingestion into destinations like Amazon S3.
-   [ ] Amazon Kinesis Data Analytics can analyze streaming data in real time, providing insights as the data arrives.

Why are the other answers wrong?

-   [ ] A.
    Kinesis Data Streams requires more management for scaling and data delivery than Firehose.
    Lambda is not designed for real-time analytics on streaming data.
-   [ ] B.
    AWS Glue is an ETL service, not optimized for real-time data ingestion and analysis.
-   [ ] D.
    API Gateway is for API management, not for high-volume streaming data ingestion.
    Lambda is not suited for real-time analytics on streams.

Therefore, Option C is the most operationally efficient solution.

</details>
<details>
  <summary>Question 502</summary>

A company runs a website that uses a content management system (CMS) on Amazon EC2.
The CMS runs on a single EC2 instance and uses an Amazon Aurora MySQL Multi-AZ DB instance for the data tier.
Website images are stored on an Amazon Elastic Block Store (Amazon EBS) volume that is mounted inside the EC2 instance.
Which combination of actions should a solutions architect take to improve the performance and resilience of the website?
(Choose two.)

-   [ ] A.
    Move the website images into an Amazon S3 bucket that is mounted on every EC2 instance
-   [ ] B.
    Share the website images by using an NFS share from the primary EC2 instance.
    Mount this share on the other EC2 instances.
-   [ ] C.
    Move the website images onto an Amazon Elastic File System (Amazon EFS) file system that is mounted on every EC2 instance.
-   [ ] D.
    Create an Amazon Machine Image (AMI) from the existing EC2 instance.
    Use the AMI to provision new instances behind an Application Load Balancer as part of an Auto Scaling group.
    Configure an accelerator in AWS Global Accelerator for the website
-   [ ] E.
    Create an Amazon Machine Image (AMI) from the existing EC2 instance.
    Use the AMI to provision new instances behind an Application Load Balancer as part of an Auto Scaling group.
    Configure an Amazon CloudFront distribution for the website.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C.
    Move the website images onto an Amazon Elastic File System (Amazon EFS) file system that is mounted on every EC2 instance.
-   [ ] E.
    Create an Amazon Machine Image (AMI) from the existing EC2 instance.
    Use the AMI to provision new instances behind an Application Load Balancer as part of an Auto Scaling group.
    Configure an Amazon CloudFront distribution for the website.

Why these are the correct answers:

C.
Move the website images onto an Amazon Elastic File System (Amazon EFS) file system that is mounted on every EC2 instance.

-   [ ] Amazon EFS provides shared file storage that can be accessed by multiple EC2 instances, improving scalability and availability.

E.
Create an Amazon Machine Image (AMI) from the existing EC2 instance.
Use the AMI to provision new instances behind an Application Load Balancer as part of an Auto Scaling group.
Configure an Amazon CloudFront distribution for the website.

-   [ ] Auto Scaling groups and Application Load Balancers distribute traffic and improve availability.
-   [ ] CloudFront caches static content, improving performance.

Why are the other answers wrong?

-   [ ] A.
    Mounting an S3 bucket on EC2 instances is not efficient for serving frequently accessed files.
-   [ ] B.
    NFS shares from a single EC2 instance introduce a single point of failure.
-   [ ] D.
    Global Accelerator is for improving global application performance, not for scaling within a Region.

Therefore, Options C and E provide the best solution for performance and resilience.

</details>
<details>
  <summary>Question 503</summary>

A company runs an infrastructure monitoring service.
The company is building a new feature that will enable the service to monitor data in customer AWS accounts.
The new feature will call AWS APIs in customer accounts to describe Amazon EC2 instances and read Amazon CloudWatch metrics.
What should the company do to obtain access to customer accounts in the MOST secure way?

-   [ ] A.
    Ensure that the customers create an IAM role in their account with read-only EC2 and CloudWatch permissions and a trust policy to the company's account.
-   [ ] B.
    Create a serverless API that implements a token vending machine to provide temporary AWS credentials for a role with read-only EC2 and CloudWatch permissions.
-   [ ] C.
    Ensure that the customers create an IAM user in their account with read-only EC2 and CloudWatch permissions.
    Encrypt and store customer access and secret keys in a secrets management system.
-   [ ] D.
    Ensure that the customers create an Amazon Cognito user in their account to use an IAM role with read-only EC2 and CloudWatch permissions.
    Encrypt and store the Amazon Cognito user and password in a secrets management system.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A.
    Ensure that the customers create an IAM role in their account with read-only EC2 and CloudWatch permissions and a trust policy to the company's account.

Why these are the correct answers:

A.
Ensure that the customers create an IAM role in their account with read-only EC2 and CloudWatch permissions and a trust policy to the company's account.

-   [ ] IAM roles allow for secure delegation of permissions without sharing long-term credentials.
-   [ ] The trust policy grants the company's account permission to assume the role.

Why are the other answers wrong?

-   [ ] B.
    A token vending machine adds complexity and is not necessary for this use case.
-   [ ] C.
    Sharing IAM user credentials (access keys) is insecure.
-   [ ] D.
    Amazon Cognito is for user authentication, not for granting AWS service permissions.

Therefore, Option A is the most secure solution.

</details>
<details>
  <summary>Question 504</summary>

A company needs to connect several VPCs in the us-east-1 Region that span hundreds of AWS accounts.
The company's networking team has its own AWS account to manage the cloud network.
What is the MOST operationally efficient solution to connect the VPCs?

-   [ ] A.
    Set up VPC peering connections between each VPC.
    Update each associated subnet's route table
-   [ ] B.
    Configure a NAT gateway and an internet gateway in each VPC to connect each VPC through the internet
-   [ ] C.
    Create an AWS Transit Gateway in the networking team's AWS account.
    Configure static routes from each VPC.
-   [ ] D.
    Deploy VPN gateways in each VPC.
    Create a transit VPC in the networking team's AWS account to connect to each VPC.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C.
    Create an AWS Transit Gateway in the networking team's AWS account.
    Configure static routes from each VPC.

Why these are the correct answers:

C.
Create an AWS Transit Gateway in the networking team's AWS account.
Configure static routes from each VPC.

-   [ ] AWS Transit Gateway simplifies the management of connections between multiple VPCs.
-   [ ] It reduces the complexity of managing numerous peering connections.

Why are the other answers wrong?

-   [ ] A.
    VPC peering is complex to manage with hundreds of VPCs.
-   [ ] B.
    Connecting VPCs through the internet is inefficient and insecure.
-   [ ] D.
    VPN gateways and transit VPCs add complexity and overhead.

Therefore, Option C is the most operationally efficient solution.

</details>
<details>
  <summary>Question 505</summary>

A company has Amazon EC2 instances that run nightly batch jobs to process data.
The EC2 instances run in an Auto Scaling group that uses On-Demand billing.
If a job fails on one instance, another instance will reprocess the job.
The batch jobs run between 12:00 AM and 06:00 AM local time every day.
Which solution will provide EC2 instances to meet these requirements MOST cost-effectively?

-   [ ] A.
    Purchase a 1-year Savings Plan for Amazon EC2 that covers the instance family of the Auto Scaling group that the batch job uses.
-   [ ] B.
    Purchase a 1-year Reserved Instance for the specific instance type and operating system of the instances in the Auto Scaling group that the batch job uses.
-   [ ] C.
    Create a new launch template for the Auto Scaling group.
    Set the instances to Spot Instances.
    Set a policy to scale out based on CPU usage.
-   [ ] D.
    Create a new launch template for the Auto Scaling group.
    Increase the instance size.
    Set a policy to scale out based on CPU usage.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C.
    Create a new launch template for the Auto Scaling group.
    Set the instances to Spot Instances.
    Set a policy to scale out based on CPU usage.

Why these are the correct answers:

C.
Create a new launch template for the Auto Scaling group.
Set the instances to Spot Instances.
Set a policy to scale out based on CPU usage.

-   [ ] Spot Instances are cost-effective for fault-tolerant batch processing.
-   [ ] Auto Scaling can replace any interrupted Spot Instances.
-   [ ] Scaling based on CPU usage optimizes resource utilization.

Why are the other answers wrong?

-   [ ] A and B.
    Savings Plans and Reserved Instances are cost-effective for continuous usage, not for sporadic nightly jobs.
-   [ ] D.
    Increasing instance size is less cost-effective than using Spot Instances.

Therefore, Option C is the most cost-effective solution.

</details>
<details>
  <summary>Question 506</summary>

A social media company is building a feature for its website.
The feature will give users the ability to upload photos.
The company expects significant increases in demand during large events and must ensure that the website can handle the upload traffic from users.
Which solution meets these requirements with the MOST scalability?

-   [ ] A.
    Upload files from the user's browser to the application servers.
    Transfer the files to an Amazon S3 bucket.
-   [ ] B.
    Provision an AWS Storage Gateway file gateway.
    Upload files directly from the user's browser to the file gateway.
-   [ ] C.
    Generate Amazon S3 presigned URLs in the application.
    Upload files directly from the user's browser into an S3 bucket.
-   [ ] D.
    Provision an Amazon Elastic File System (Amazon EFS) file system.
    Upload files directly from the user's browser to the file system.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C.
    Generate Amazon S3 presigned URLs in the application.
    Upload files directly from the user's browser into an S3 bucket.

Why these are the correct answers:

C.
Generate Amazon S3 presigned URLs in the application.
Upload files directly from the user's browser into an S3 bucket.

-   [ ] S3 presigned URLs allow users to upload files directly to S3, offloading the traffic from the application servers.
-   [ ] S3 is highly scalable and can handle large volumes of uploads.

Why are the other answers wrong?

-   [ ] A.
    Uploading files to application servers and then transferring them to S3 increases the load on the servers.
-   [ ] B.
    Storage Gateway is for hybrid cloud storage, not for handling large volumes of direct user uploads.
-   [ ] D.
    EFS is a file system and is not designed for handling direct uploads from numerous users.

Therefore, Option C is the most scalable solution.

</details>
<details>
  <summary>Question 507</summary>

A company has a web application for travel ticketing.
The application is based on a database that runs in a single data center in North America.
The company wants to expand the application to serve a global user base.
The company needs to deploy the application to multiple AWS Regions.
Average latency must be less than 1 second on updates to the reservation database.
The company wants to have separate deployments of its web platform across multiple Regions.
However, the company must maintain a single primary reservation database that is globally consistent.
Which solution should a solutions architect recommend to meet these requirements?

-   [ ] A.
    Convert the application to use Amazon DynamoDB.
    Use a global table for the center reservation table.
    Use the correct Regional endpoint in each Regional deployment.
-   [ ] B.
    Migrate the database to an Amazon Aurora MySQL database.
    Deploy Aurora Read Replicas in each Region.
    Use the correct Regional endpoint in each Regional deployment for access to the database.
-   [ ] C.
    Migrate the database to an Amazon RDS for MySQL database.
    Deploy MySQL read replicas in each Region.
    Use the correct Regional endpoint in each Regional deployment for access to the database.
-   [ ] D.
    Migrate the application to an Amazon Aurora Serverless database.
    Deploy instances of the database to each Region.
    Use the correct Regional endpoint in each Regional deployment to access the database.
    Use AWS Lambda functions to process event streams in each Region to synchronize the databases.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B.
    Migrate the database to an Amazon Aurora MySQL database.
    Deploy Aurora Read Replicas in each Region.
    Use the correct Regional endpoint in each Regional deployment for access to the database.

Why these are the correct answers:

B.
Migrate the database to an Amazon Aurora MySQL database.
Deploy Aurora Read Replicas in each Region.
Use the correct Regional endpoint in each Regional deployment for access to the database.

-   [ ] Aurora provides high performance and scalability.
-   [ ] Read Replicas in each Region can serve local read requests, reducing latency.
-   [ ] A single primary Aurora database ensures global consistency.

Why are the other answers wrong?

-   [ ] A.
    DynamoDB is a NoSQL database and may require significant application changes.
-   [ ] C.
    RDS MySQL read replicas do not provide the same level of performance and global consistency as Aurora.
-   [ ] D.
    Aurora Serverless is not ideal for high-performance, low-latency global applications.
    Lambda and event streams add complexity.

Therefore, Option B is the most suitable solution.

</details>
<details>
  <summary>Question 508</summary>

A company has migrated multiple Microsoft Windows Server workloads to Amazon EC2 instances that run in the us-west-1 Region.
The company manually backs up the workloads to create an image as needed.
In the event of a natural disaster in the us-west-1 Region, the company wants to recover workloads quickly in the us-west-2 Region.
The company wants no more than 24 hours of data loss on the EC2 instances.
The company also wants to automate any backups of the EC2 instances.
Which solutions will meet these requirements with the LEAST administrative effort?
(Choose two.)

-   [ ] A.
    Create an Amazon EC2-backed Amazon Machine Image (AMI) lifecycle policy to create a backup based on tags.
    Schedule the backup to run twice daily.
    Copy the image on demand.
-   [ ] B.
    Create an Amazon EC2-backed Amazon Machine Image (AMI) lifecycle policy to create a backup based on tags.
    Schedule the backup to run twice daily.
    Configure the copy to the us-west-2 Region.
-   [ ] C.
    Create backup vaults in us-west-1 and in us-west-2 by using AWS Backup.
    Create a backup plan for the EC2 instances based on tag values.
    Create an AWS Lambda function to run as a scheduled job to copy the backup data to us-west-2.
-   [ ] D.
    Create a backup vault by using AWS Backup.
    Use AWS Backup to create a backup plan for the EC2 instances based on tag values.
    Define the destination for the copy as us-west-2.
    Specify the backup schedule to run twice daily.
-   [ ] E.
    Create a backup vault by using AWS Backup.
    Use AWS Backup to create a backup plan for the EC2 instances based on tag values.
    Specify the backup schedule to run twice daily.
    Copy on demand to us-west-2.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B.
    Create an Amazon EC2-backed Amazon Machine Image (AMI) lifecycle policy to create a backup based on tags.
    Schedule the backup to run twice daily.
    Configure the copy to the us-west-2 Region.
-   [ ] D.
    Create a backup vault by using AWS Backup.
    Use AWS Backup to create a backup plan for the EC2 instances based on tag values.
    Define the destination for the copy as us-west-2.
    Specify the backup schedule to run twice daily.

Why these are the correct answers:

B.
Create an Amazon EC2-backed Amazon Machine Image (AMI) lifecycle policy to create a backup based on tags.
Schedule the backup to run twice daily.
Configure the copy to the us-west-2 Region.

-   [ ] AMI lifecycle policies automate AMI creation.
-   [ ] Scheduling backups twice daily meets the RPO of 24 hours.
-   [ ] Automating the copy to us-west-2 simplifies the DR process.

D.
Create a backup vault by using AWS Backup.
Use AWS Backup to create a backup plan for the EC2 instances based on tag values.
Define the destination for the copy as us-west-2.
Specify the backup schedule to run twice daily.

-   [ ] AWS Backup centralizes backup management.
-   [ ] It automates backups and cross-Region copies.
-   [ ] Tag-based backups simplify management.

Why are the other answers wrong?

-   [ ] A.
    Copying images on demand adds manual steps.
-   [ ] C.
    Using Lambda functions adds complexity.
-   [ ] E.
    Copying on demand adds manual steps.

Therefore, Options B and D provide the most automated and efficient solutions.

</details>
<details>
  <summary>Question 509</summary>

A company operates a two-tier application for image processing.
The application uses two Availability Zones, each with one public subnet and one private subnet.
An Application Load Balancer (ALB) for the web tier uses the public subnets.
Amazon EC2 instances for the application tier use the private subnets.
Users report that the application is running more slowly than expected.
A security audit of the web server log files shows that the application is receiving millions of illegitimate requests from a small number of IP addresses.
A solutions architect needs to resolve the immediate performance problem while the company investigates a more permanent solution.
What should the solutions architect recommend to meet this requirement?

-   [ ] A.
    Modify the inbound security group for the web tier.
    Add a deny rule for the IP addresses that are consuming resources.
-   [ ] B.
    Modify the network ACL for the web tier subnets.
    Add an inbound deny rule for the IP addresses that are consuming resources.
-   [ ] C.
    Modify the inbound security group for the application tier.
    Add a deny rule for the IP addresses that are consuming resources.
-   [ ] D.
    Modify the network ACL for the application tier subnets.
    Add an inbound deny rule for the IP addresses that are consuming resources.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B.
    Modify the network ACL for the web tier subnets.
    Add an inbound deny rule for the IP addresses that are consuming resources.

Why these are the correct answers:

B.
Modify the network ACL for the web tier subnets.
Add an inbound deny rule for the IP addresses that are consuming resources.

-   [ ] Network ACLs operate at the subnet level and can quickly block traffic, reducing the load on the web tier.
-   [ ] This is an immediate solution to mitigate the performance issue.

Why are the other answers wrong?

-   [ ] A and C.
    Security groups operate at the instance level and are less efficient for blocking traffic at the subnet level.
-   [ ] D.
    Modifying the application tier's network ACLs does not address the immediate problem of illegitimate requests hitting the web tier.

Therefore, Option B is the most appropriate solution for immediate mitigation.

</details>
<details>
  <summary>Question 510</summary>

A global marketing company has applications that run in the ap-southeast-2 Region and the eu-west-1 Region.
Applications that run in a VPC in eu-west-1 need to communicate securely with databases that run in a VPC in ap-southeast-2.
Which network design will meet these requirements?

-   [ ] A.
    Create a VPC peering connection between the eu-west-1 VPC and the ap-southeast-2 VPC.
    Create an inbound rule in the eu-west-1 application security group that allows traffic from the database server IP addresses in the ap-southeast-2 security group.
-   [ ] B.
    Configure a VPC peering connection between the ap-southeast-2 VPC and the eu-west-1 VPC.
    Update the subnet route tables.
    Create an inbound rule in the ap-southeast-2 database security group that references the security group ID of the application servers in eu-west-1.
-   [ ] C.
    Configure a VPC peering connection between the ap-southeast-2 VPC and the eu-west-1 VPUpdate the subnet route tables.
    Create an inbound rule in the ap-southeast-2 database security group that allows traffic from the eu-west-1 application server IP addresses.
-   [ ] D.
    Create a transit gateway with a peering attachment between the eu-west-1 VPC and the ap- southeast-2 VPC.
    After the transit gateways are properly peered and routing is configured, create an inbound rule in the database security group that references the security group ID of the application servers in eu-west-1.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C.
    Configure a VPC peering connection between the ap-southeast-2 VPC and the eu-west-1 VPUpdate the subnet route tables.
    Create an inbound rule in the ap-southeast-2 database security group that allows traffic from the eu-west-1 application server IP addresses.

Why these are the correct answers:

C.
Configure a VPC peering connection between the ap-southeast-2 VPC and the eu-west-1 VPUpdate the subnet route tables.
Create an inbound rule in the ap-southeast-2 database security group that allows traffic from the eu-west-1 application server IP addresses.

-   [ ] VPC peering connects two VPCs, enabling communication.
-   [ ] Updating route tables directs traffic between the VPCs.
-   [ ] Security groups control traffic at the instance level.

Why are the other answers wrong?

-   [ ] A.
    Security groups cannot reference IP addresses in another security group across VPC peering connections.
-   [ ] B.
    While security groups can reference other security groups, the direction of the peering connection setup is important and the description is incorrect.
-   [ ] D.
    Transit Gateway is for connecting many VPCs, not just two, and adds complexity.

Therefore, Option C is the correct solution.

</details>

































