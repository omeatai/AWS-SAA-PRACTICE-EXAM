<details>
  <summary>Question 501</summary>

A company wants to ingest customer payment data into the company's data lake in Amazon S3.
The company receives payment data every minute on average.
The company wants to analyze the payment data in real time.
Then the company wants to ingest the data into the data lake.
Which solution will meet these requirements with the MOST operational efficiency?

-   [ ] A. Use Amazon Kinesis Data Streams to ingest data.
    Use AWS Lambda to analyze the data in real time.
-   [ ] B. Use AWS Glue to ingest data.
    Use Amazon Kinesis Data Analytics to analyze the data in real time.
-   [ ] C. Use Amazon Kinesis Data Firehose to ingest data.
    Use Amazon Kinesis Data Analytics to analyze the data in real time.
-   [ ] D. Use Amazon API Gateway to ingest data.
    Use AWS Lambda to analyze the data in real time.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Use Amazon Kinesis Data Firehose to ingest data.
    Use Amazon Kinesis Data Analytics to analyze the data in real time.

Why these are the correct answers:

C. Use Amazon Kinesis Data Firehose to ingest data.
Use Amazon Kinesis Data Analytics to analyze the data in real time.

-   [ ] Amazon Kinesis Data Firehose is designed for near real-time ingestion of streaming data into destinations like Amazon S3.
-   [ ] Amazon Kinesis Data Analytics can analyze streaming data in real time.

Why are the other answers wrong?

-   [ ] A. Kinesis Data Streams requires more management for scaling and data delivery compared to Firehose.
    Lambda is not designed for real-time analytics on streaming data.
-   [ ] B. AWS Glue is an ETL service, not optimized for real-time data ingestion and analysis.
-   [ ] D. API Gateway is for API management, not for high-volume streaming data ingestion.
    Lambda is not suited for real-time analytics on streams.

Therefore, Option C is the most operationally efficient solution.

</details>
<details>
  <summary>Question 502</summary>

A company runs a website that uses a content management system (CMS) on Amazon EC2.
The CMS runs on a single EC2 instance and uses an Amazon Aurora MySQL Multi-AZ DB instance for the data tier.
Website images are stored on an Amazon Elastic Block Store (Amazon EBS) volume that is mounted inside the EC2 instance.
Which combination of actions should a solutions architect take to improve the performance and resilience of the website? (Choose two.)

-   [ ] A. Move the website images into an Amazon S3 bucket that is mounted on every EC2 instance
-   [ ] B. Share the website images by using an NFS share from the primary EC2 instance.
    Mount this share on the other EC2 instances.
-   [ ] C. Move the website images onto an Amazon Elastic File System (Amazon EFS) file system that is mounted on every EC2 instance.
-   [ ] D. Create an Amazon Machine Image (AMI) from the existing EC2 instance.
    Use the AMI to provision new instances behind an Application Load Balancer as part of an Auto Scaling group.
    Configure an accelerator in AWS Global Accelerator for the website
-   [ ] E. Create an Amazon Machine Image (AMI) from the existing EC2 instance.
    Use the AMI to provision new instances behind an Application Load Balancer as part of an Auto Scaling group.
    Configure an Amazon CloudFront distribution for the website.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Move the website images onto an Amazon Elastic File System (Amazon EFS) file system that is mounted on every EC2 instance.
-   [ ] E. Create an Amazon Machine Image (AMI) from the existing EC2 instance.
    Use the AMI to provision new instances behind an Application Load Balancer as part of an Auto Scaling group.
    Configure an Amazon CloudFront distribution for the website.

Why these are the correct answers:

C. Move the website images onto an Amazon Elastic File System (Amazon EFS) file system that is mounted on every EC2 instance.

-   [ ] EFS provides shared file storage that can be accessed by multiple EC2 instances, improving scalability and availability.

E. Create an Amazon Machine Image (AMI) from the existing EC2 instance.
Use the AMI to provision new instances behind an Application Load Balancer as part of an Auto Scaling group.
Configure an Amazon CloudFront distribution for the website.

-   [ ] Auto Scaling groups and Application Load Balancers distribute traffic and improve availability.
-   [ ] CloudFront caches static content, improving performance.

Why are the other answers wrong?

-   [ ] A. Mounting an S3 bucket on EC2 instances is not efficient for serving frequently accessed files.
-   [ ] B. NFS shares from a single EC2 instance introduce a single point of failure.
-   [ ] D. Global Accelerator is for improving global application performance, not for scaling within a Region.

Therefore, Options C and E provide the best solution for performance and resilience.

</details>
<details>
  <summary>Question 503</summary>

A company runs an infrastructure monitoring service.
The company is building a new feature that will enable the service to monitor data in customer AWS accounts.
The new feature will call AWS APIs in customer accounts to describe Amazon EC2 instances and read Amazon CloudWatch metrics.
What should the company do to obtain access to customer accounts in the MOST secure way?

-   [ ] A. Ensure that the customers create an IAM role in their account with read-only EC2 and CloudWatch permissions and a trust policy to the company's account.
-   [ ] B. Create a serverless API that implements a token vending machine to provide temporary AWS credentials for a role with read-only EC2 and CloudWatch permissions.
-   [ ] C. Ensure that the customers create an IAM user in their account with read-only EC2 and CloudWatch permissions.
    Encrypt and store customer access and secret keys in a secrets management system.
-   [ ] D. Ensure that the customers create an Amazon Cognito user in their account to use an IAM role with read-only EC2 and CloudWatch permissions.
    Encrypt and store the Amazon Cognito user and password in a secrets management system.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Ensure that the customers create an IAM role in their account with read-only EC2 and CloudWatch permissions and a trust policy to the company's account.

Why these are the correct answers:

A. Ensure that the customers create an IAM role in their account with read-only EC2 and CloudWatch permissions and a trust policy to the company's account.

-   [ ] IAM roles allow for secure delegation of permissions without sharing long-term credentials.
-   [ ] The trust policy grants the company's account permission to assume the role.

Why are the other answers wrong?

-   [ ] B. A token vending machine adds complexity and is not necessary for this use case.
-   [ ] C. Sharing IAM user credentials (access keys) is insecure.
-   [ ] D. Amazon Cognito is for user authentication, not for granting AWS service permissions.

Therefore, Option A is the most secure solution.

</details>
<details>
  <summary>Question 504</summary>

A company needs to connect several VPCs in the us-east-1 Region that span hundreds of AWS accounts.
The company's networking team has its own AWS account to manage the cloud network.
What is the MOST operationally efficient solution to connect the VPCs?

-   [ ] A. Set up VPC peering connections between each VPC.
    Update each associated subnet's route table
-   [ ] B. Configure a NAT gateway and an internet gateway in each VPC to connect each VPC through the internet
-   [ ] C. Create an AWS Transit Gateway in the networking team's AWS account.
    Configure static routes from each VPC.
-   [ ] D. Deploy VPN gateways in each VPC.
    Create a transit VPC in the networking team's AWS account to connect to each VPC.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Create an AWS Transit Gateway in the networking team's AWS account.
    Configure static routes from each VPC.

Why these are the correct answers:

C. Create an AWS Transit Gateway in the networking team's AWS account.
Configure static routes from each VPC.

-   [ ] AWS Transit Gateway simplifies the management of connections between multiple VPCs.
-   [ ] It reduces the complexity of managing numerous peering connections.

Why are the other answers wrong?

-   [ ] A. VPC peering is complex to manage with hundreds of VPCs.
-   [ ] B. Connecting VPCs through the internet is inefficient and insecure.
-   [ ] D. VPN gateways and transit VPCs add complexity and overhead.

Therefore, Option C is the most operationally efficient solution.

</details>
<details>
  <summary>Question 505</summary>

A company has Amazon EC2 instances that run nightly batch jobs to process data.
The EC2 instances run in an Auto Scaling group that uses On-Demand billing.
If a job fails on one instance, another instance will reprocess the job.
The batch jobs run between 12:00 AM and 06:00 AM local time every day.
Which solution will provide EC2 instances to meet these requirements MOST cost-effectively?

-   [ ] A. Purchase a 1-year Savings Plan for Amazon EC2 that covers the instance family of the Auto Scaling group that the batch job uses.
-   [ ] B. Purchase a 1-year Reserved Instance for the specific instance type and operating system of the instances in the Auto Scaling group that the batch job uses.
-   [ ] C. Create a new launch template for the Auto Scaling group.
    Set the instances to Spot Instances.
    Set a policy to scale out based on CPU usage.
-   [ ] D. Create a new launch template for the Auto Scaling group.
    Increase the instance size.
    Set a policy to scale out based on CPU usage.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Create a new launch template for the Auto Scaling group.
    Set the instances to Spot Instances.
    Set a policy to scale out based on CPU usage.

Why these are the correct answers:

C. Create a new launch template for the Auto Scaling group.
Set the instances to Spot Instances.
Set a policy to scale out based on CPU usage.

-   [ ] Spot Instances are cost-effective for fault-tolerant batch processing.
-   [ ] Auto Scaling can replace any interrupted Spot Instances.
-   [ ] Scaling based on CPU usage optimizes resource utilization.

Why are the other answers wrong?

-   [ ] A and B. Savings Plans and Reserved Instances are cost-effective for continuous usage, not for sporadic nightly jobs.
-   [ ] D. Increasing instance size is less cost-effective than using Spot Instances.

Therefore, Option C is the most cost-effective solution.

</details>
<details>
  <summary>Question 506</summary>

A social media company is building a feature for its website.
The feature will give users the ability to upload photos.
The company expects significant increases in demand during large events and must ensure that the website can handle the upload traffic from users.
Which solution meets these requirements with the MOST scalability?

-   [ ] A. Upload files from the user's browser to the application servers.
    Transfer the files to an Amazon S3 bucket.
-   [ ] B. Provision an AWS Storage Gateway file gateway.
    Upload files directly from the user's browser to the file gateway.
-   [ ] C. Generate Amazon S3 presigned URLs in the application.
    Upload files directly from the user's browser into an S3 bucket.
-   [ ] D. Provision an Amazon Elastic File System (Amazon EFS) file system.
    Upload files directly from the user's browser to the file system.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Generate Amazon S3 presigned URLs in the application.
    Upload files directly from the user's browser into an S3 bucket.

Why these are the correct answers:

C. Generate Amazon S3 presigned URLs in the application.
Upload files directly from the user's browser into an S3 bucket.

-   [ ] S3 presigned URLs allow users to upload files directly to S3, offloading the traffic from the application servers.
-   [ ] S3 is highly scalable and can handle large volumes of uploads.

Why are the other answers wrong?

-   [ ] A. Uploading files to application servers and then transferring them to S3 increases the load on the servers.
-   [ ] B. Storage Gateway is for hybrid cloud storage, not for handling large volumes of direct user uploads.
-   [ ] D. EFS is a file system and is not designed for handling direct uploads from numerous users.

Therefore, Option C is the most scalable solution.

</details>
<details>
  <summary>Question 507</summary>

A company has a web application for travel ticketing.
The application is based on a database that runs in a single data center in North America.
The company wants to expand the application to serve a global user base.
The company needs to deploy the application to multiple AWS Regions.
Average latency must be less than 1 second on updates to the reservation database.
The company wants to have separate deployments of its web platform across multiple Regions.
However, the company must maintain a single primary reservation database that is globally consistent.
Which solution should a solutions architect recommend to meet these requirements?

-   [ ] A. Convert the application to use Amazon DynamoDB.
    Use a global table for the center reservation table.
    Use the correct Regional endpoint in each Regional deployment.
-   [ ] B. Migrate the database to an Amazon Aurora MySQL database.
    Deploy Aurora Read Replicas in each Region.
    Use the correct Regional endpoint in each Regional deployment for access to the database.
-   [ ] C. Migrate the database to an Amazon RDS for MySQL database.
    Deploy MySQL read replicas in each Region.
    Use the correct Regional endpoint in each Regional deployment for access to the database.
-   [ ] D. Migrate the application to an Amazon Aurora Serverless database.
    Deploy instances of the database to each Region.
    Use the correct Regional endpoint in each Regional deployment to access the database.
    Use AWS Lambda functions to process event streams in each Region to synchronize the databases.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Migrate the database to an Amazon Aurora MySQL database.
    Deploy Aurora Read Replicas in each Region.
    Use the correct Regional endpoint in each Regional deployment for access to the database.

Why these are the correct answers:

B. Migrate the database to an Amazon Aurora MySQL database.
Deploy Aurora Read Replicas in each Region.
Use the correct Regional endpoint in each Regional deployment for access to the database.

-   [ ] Aurora provides high performance and scalability.
-   [ ] Read Replicas in each Region can serve local read requests, reducing latency.
-   [ ] A single primary Aurora database ensures global consistency.

Why are the other answers wrong?

-   [ ] A. DynamoDB is a NoSQL database and may require significant application changes.
-   [ ] C. RDS MySQL read replicas do not provide the same level of performance and global consistency as Aurora.
-   [ ] D. Aurora Serverless is not ideal for high-performance, low-latency global applications.
    Lambda and event streams add complexity.

Therefore, Option B is the most suitable solution.

</details>
<details>
  <summary>Question 508</summary>

A company has migrated multiple Microsoft Windows Server workloads to Amazon EC2 instances that run in the us-west-1 Region.
The company manually backs up the workloads to create an image as needed.
In the event of a natural disaster in the us-west-1 Region, the company wants to recover workloads quickly in the us-west-2 Region.
The company wants no more than 24 hours of data loss on the EC2 instances.
The company also wants to automate any backups of the EC2 instances.
Which solutions will meet these requirements with the LEAST administrative effort? (Choose two.)

-   [ ] A. Create an Amazon EC2-backed Amazon Machine Image (AMI) lifecycle policy to create a backup based on tags.
    Schedule the backup to run twice daily.
    Copy the image on demand.
-   [ ] B. Create an Amazon EC2-backed Amazon Machine Image (AMI) lifecycle policy to create a backup based on tags.
    Schedule the backup to run twice daily.
    Configure the copy to the us-west-2 Region.
-   [ ] C. Create backup vaults in us-west-1 and in us-west-2 by using AWS Backup.
    Create a backup plan for the EC2 instances based on tag values.
    Create an AWS Lambda function to run as a scheduled job to copy the backup data to us-west-2.
-   [ ] D. Create a backup vault by using AWS Backup.
    Use AWS Backup to create a backup plan for the EC2 instances based on tag values.
    Define the destination for the copy as us-west-2.
    Specify the backup schedule to run twice daily.
-   [ ] E. Create a backup vault by using AWS Backup.
    Use AWS Backup to create a backup plan for the EC2 instances based on tag values.
    Specify the backup schedule to run twice daily.
    Copy on demand to us-west-2.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Create an Amazon EC2-backed Amazon Machine Image (AMI) lifecycle policy to create a backup based on tags.
    Schedule the backup to run twice daily.
    Configure the copy to the us-west-2 Region.
-   [ ] D. Create a backup vault by using AWS Backup.
    Use AWS Backup to create a backup plan for the EC2 instances based on tag values.
    Define the destination for the copy as us-west-2.
    Specify the backup schedule to run twice daily.

Why these are the correct answers:

B. Create an Amazon EC2-backed Amazon Machine Image (AMI) lifecycle policy to create a backup based on tags.
Schedule the backup to run twice daily.
Configure the copy to the us-west-2 Region.

-   [ ] AMI lifecycle policies automate AMI creation.
-   [ ] Scheduling backups twice daily meets the RPO of 24 hours.
-   [ ] Automating the copy to us-west-2 simplifies the DR process.

D. Create a backup vault by using AWS Backup.
Use AWS Backup to create a backup plan for the EC2 instances based on tag values.
Define the destination for the copy as us-west-2.
Specify the backup schedule to run twice daily.

-   [ ] AWS Backup centralizes backup management.
-   [ ] It automates backups and cross-Region copies.
-   [ ] Tag-based backups simplify management.

Why are the other answers wrong?

-   [ ] A. Copying images on demand adds manual steps.
-   [ ] C. Using Lambda functions adds complexity.
-   [ ] E. Copying on demand adds manual steps.

Therefore, Options B and D provide the most automated and efficient solutions.

</details>
<details>
  <summary>Question 509</summary>

A company operates a two-tier application for image processing.
The application uses two Availability Zones, each with one public subnet and one private subnet.
An Application Load Balancer (ALB) for the web tier uses the public subnets.
Amazon EC2 instances for the application tier use the private subnets.
Users report that the application is running more slowly than expected.
A security audit of the web server log files shows that the application is receiving millions of illegitimate requests from a small number of IP addresses.
A solutions architect needs to resolve the immediate performance problem while the company investigates a more permanent solution.
What should the solutions architect recommend to meet this requirement?

-   [ ] A. Modify the inbound security group for the web tier.
    Add a deny rule for the IP addresses that are consuming resources.
-   [ ] B. Modify the network ACL for the web tier subnets.
    Add an inbound deny rule for the IP addresses that are consuming resources.
-   [ ] C. Modify the inbound security group for the application tier.
    Add a deny rule for the IP addresses that are consuming resources.
-   [ ] D. Modify the network ACL for the application tier subnets.
    Add an inbound deny rule for the IP addresses that are consuming resources.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Modify the network ACL for the web tier subnets.
    Add an inbound deny rule for the IP addresses that are consuming resources.

Why these are the correct answers:

B. Modify the network ACL for the web tier subnets.
Add an inbound deny rule for the IP addresses that are consuming resources.

-   [ ] Network ACLs operate at the subnet level and can quickly block traffic, reducing the load on the web tier.
-   [ ] This is an immediate solution to mitigate the performance issue.

Why are the other answers wrong?

-   [ ] A and C. Security groups operate at the instance level and are less efficient for blocking traffic at the subnet level.
-   [ ] D. Modifying the application tier's network ACLs does not address the immediate problem of illegitimate requests hitting the web tier.

Therefore, Option B is the most appropriate solution for immediate mitigation.

</details>
<details>
  <summary>Question 510</summary>

A global marketing company has applications that run in the ap-southeast-2 Region and the eu-west-1 Region.
Applications that run in a VPC in eu-west-1 need to communicate securely with databases that run in a VPC in ap-southeast-2.
Which network design will meet these requirements?

-   [ ] A. Create a VPC peering connection between the eu-west-1 VPC and the ap-southeast-2 VPC.
    Create an inbound rule in the eu-west-1 application security group that allows traffic from the database server IP addresses in the ap-southeast-2 security group.
-   [ ] B. Configure a VPC peering connection between the ap-southeast-2 VPC and the eu-west-1 VPC.
    Update the subnet route tables.
    Create an inbound rule in the ap-southeast-2 database security group that references the security group ID of the application servers in eu-west-1.
-   [ ] C. Configure a VPC peering connection between the ap-southeast-2 VPC and the eu-west-1 VPUpdate the subnet route tables.
    Create an inbound rule in the ap-southeast-2 database security group that allows traffic from the eu-west-1 application server IP addresses.
-   [ ] D. Create a transit gateway with a peering attachment between the eu-west-1 VPC and the ap- southeast-2 VPC.
    After the transit gateways are properly peered and routing is configured, create an inbound rule in the database security group that references the security group ID of the application servers in eu-west-1.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Configure a VPC peering connection between the ap-southeast-2 VPC and the eu-west-1 VPUpdate the subnet route tables.
    Create an inbound rule in the ap-southeast-2 database security group that allows traffic from the eu-west-1 application server IP addresses.

Why these are the correct answers:

C. Configure a VPC peering connection between the ap-southeast-2 VPC and the eu-west-1 VPUpdate the subnet route tables.
Create an inbound rule in the ap-southeast-2 database security group that allows traffic from the eu-west-1 application server IP addresses.

-   [ ] VPC peering connects two VPCs, enabling communication.
-   [ ] Updating route tables directs traffic between the VPCs.
-   [ ] Security groups control traffic at the instance level.

Why are the other answers wrong?

-   [ ] A. Security groups cannot reference IP addresses in another security group across VPC peering connections.
-   [ ] B. While security groups can reference other security groups, the direction of the peering connection setup is important and the description is incorrect.
-   [ ] D. Transit Gateway is for connecting many VPCs, not just two, and adds complexity.

Therefore, Option C is the correct solution.

</details>

<details>
  <summary>Question 511</summary>

A company is developing software that uses a PostgreSQL database schema.
The company needs to configure multiple development environments and databases for the company's developers.
On average, each development environment is used for half of the 8-hour workday.
Which solution will meet these requirements MOST cost-effectively?

-   [ ] A.
    Configure each development environment with its own Amazon Aurora PostgreSQL database
-   [ ] B.
    Configure each development environment with its own Amazon RDS for PostgreSQL Single-AZ DB instances
-   [ ] C.
    Configure each development environment with its own Amazon Aurora On-Demand PostgreSQL-Compatible database
-   [ ] D.
    Configure each development environment with its own Amazon S3 bucket by using Amazon S3 Object Select

</details>

<details>
  <summary>Answer</summary>

-   [ ] C.
    Configure each development environment with its own Amazon Aurora On-Demand PostgreSQL-Compatible database

Why these are the correct answers:

C.
Configure each development environment with its own Amazon Aurora On-Demand PostgreSQL-Compatible database

-   [ ] Aurora Serverless v1 (which is what "On-Demand" refers to in the document) automatically starts up, shuts down, and scales capacity based on application needs, making it cost-effective for intermittent use.
-   [ ] It provides PostgreSQL compatibility, so developers can use their existing schema.

Why are the other answers wrong?

-   [ ] A and B.
    Provisioning a full Aurora PostgreSQL database or RDS for PostgreSQL instance for each developer is more expensive, even if not fully utilized.
-   [ ] D.
    Amazon S3 and S3 Object Select are for object storage and retrieval, not for running a PostgreSQL database.

Therefore, Option C is the most cost-effective solution for development environments with intermittent use.

</details>
<details>
  <summary>Question 512</summary>

A company uses AWS Organizations with resources tagged by account.
The company also uses AWS Backup to back up its AWS infrastructure resources.
The company needs to back up all AWS resources.
Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A.
    Use AWS Config to identify all untagged resources.
    Tag the identified resources programmatically.
    Use tags in the backup plan.
-   [ ] B.
    Use AWS Config to identify all resources that are not running.
    Add those resources to the backup vault.
-   [ ] C.
    Require all AWS account owners to review their resources to identify the resources that need to be backed up.
-   [ ] D.
    Use Amazon Inspector to identify all noncompliant resources.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A.
    Use AWS Config to identify all untagged resources.
    Tag the identified resources programmatically.
    Use tags in the backup plan.

Why these are the correct answers:

A.
Use AWS Config to identify all untagged resources.
Tag the identified resources programmatically.
Use tags in the backup plan.

-   [ ] AWS Config can identify resources that are not tagged, allowing for automated tagging.
-   [ ] AWS Backup can use tags to select resources for backup, simplifying backup management.

Why are the other answers wrong?

-   [ ] B.
    Backing up only non-running resources does not meet the requirement to back up all resources.
-   [ ] C.
    Manual review is time-consuming and error-prone.
-   [ ] D.
    AWS Inspector is for security vulnerability assessments, not for backup management.

Therefore, Option A is the most efficient and automated solution.

</details>
<details>
  <summary>Question 513</summary>

A social media company wants to allow its users to upload images in an application that is hosted in the AWS Cloud.
The company needs a solution that automatically resizes the images so that the images can be displayed on multiple device types.
The application experiences unpredictable traffic patterns throughout the day.
The company is seeking a highly available solution that maximizes scalability.
What should a solutions architect do to meet these requirements?

-   [ ] A.
    Create a static website hosted in Amazon S3 that invokes AWS Lambda functions to resize the images and store the images in an Amazon S3 bucket.
-   [ ] B.
    Create a static website hosted in Amazon CloudFront that invokes AWS Step Functions to resize the images and store the images in an Amazon RDS database.
-   [ ] C.
    Create a dynamic website hosted on a web server that runs on an Amazon EC2 instance.
    Configure a process that runs on the EC2 instance to resize the images and store the images in an Amazon S3 bucket.
-   [ ] D.
    Create a dynamic website hosted on an automatically scaling Amazon Elastic Container Service (Amazon ECS) cluster that creates a resize job in Amazon Simple Queue Service (Amazon SQS).
    Set up an image-resizing program that runs on an Amazon EC2 instance to process the resize jobs.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A.
    Create a static website hosted in Amazon S3 that invokes AWS Lambda functions to resize the images and store the images in an Amazon S3 bucket.

Why these are the correct answers:

A.
Create a static website hosted in Amazon S3 that invokes AWS Lambda functions to resize the images and store the images in an Amazon S3 bucket.

-   [ ] S3 is highly scalable for storing images.
-   [ ] Lambda functions can automatically resize images and scale based on traffic, providing a serverless and cost-effective solution.

Why are the other answers wrong?

-   [ ] B.
    CloudFront is a CDN and not for hosting a static website.
    Step Functions are for orchestrating workflows, not for image resizing.
    RDS is not suitable for storing images.
-   [ ] C.
    EC2 instances require more management and do not scale as efficiently as Lambda.
-   [ ] D.
    ECS and SQS add complexity and are not necessary for simple image resizing.

Therefore, Option A is the most scalable and cost-effective solution.

</details>
<details>
  <summary>Question 514</summary>

A company is running a microservices application on Amazon EC2 instances.
The company wants to migrate the application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster for scalability.
The company must configure the Amazon EKS control plane with endpoint private access set to true and endpoint public access set to false to maintain security compliance.
The company must also put the data plane in private subnets.
However, the company has received error notifications because the node cannot join the cluster.
Which solution will allow the node to join the cluster?

-   [ ] A.
    Grant the required permission in AWS Identity and Access Management (IAM) to the AmazonEKSNodeRole IAM role.
-   [ ] B.
    Create interface VPC endpoints to allow nodes to access the control plane.
-   [ ] C.
    Recreate nodes in the public subnet.
    Restrict security groups for EC2 nodes.
-   [ ] D.
    Allow outbound traffic in the security group of the nodes.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B.
    Create interface VPC endpoints to allow nodes to access the control plane.

Why these are the correct answers:

B.
Create interface VPC endpoints to allow nodes to access the control plane.

-   [ ] When the EKS control plane has private access enabled, nodes in private subnets need a way to communicate with it.
-   [ ] Interface VPC endpoints provide this private connectivity.

Why are the other answers wrong?

-   [ ] A.
    IAM permissions are necessary but not sufficient for network connectivity.
-   [ ] C.
    Recreating nodes in public subnets violates the security compliance requirement.
-   [ ] D.
    Outbound traffic permissions are necessary but do not address the need for a private connection to the control plane.

Therefore, Option B is the correct solution.

</details>
<details>
  <summary>Question 515</summary>

A company is migrating an on-premises application to AWS.
The company wants to use Amazon Redshift as a solution.
Which use cases are suitable for Amazon Redshift in this scenario?
(Choose three.)

-   [ ] A.
    Supporting data APIs to access data with traditional, containerized, and event-driven applications
-   [ ] B.
    Supporting client-side and server-side encryption
-   [ ] C.
    Building analytics workloads during specified hours and when the application is not active
-   [ ] D.
    Caching data to reduce the pressure on the backend database
-   [ ] E.
    Scaling globally to support petabytes of data and tens of millions of requests per minute
-   [ ] F.
    Creating a secondary replica of the cluster by using the AWS Management Console

</details>

<details>
  <summary>Answer</summary>

-   [ ] B.
    Supporting client-side and server-side encryption
-   [ ] C.
    Building analytics workloads during specified hours and when the application is not active
-   [ ] E.
    Scaling globally to support petabytes of data and tens of millions of requests per minute

Why these are the correct answers:

B.
Supporting client-side and server-side encryption

-   [ ] Redshift supports encryption, which is important for data security.

C.
Building analytics workloads during specified hours and when the application is not active

-   [ ] Redshift is optimized for analytical workloads.

E.
Scaling globally to support petabytes of data and tens of millions of requests per minute

-   [ ] Redshift is designed for large-scale data warehousing.

Why are the other answers wrong?

-   [ ] A.
    Redshift is not designed for serving data APIs for applications.
-   [ ] D.
    Redshift is a data warehouse, not a caching solution.
-   [ ] F.
    Creating a secondary replica is not a primary use case; Redshift focuses on analytics.

Therefore, Options B, C, and E are the correct use cases.

</details>
<details>
  <summary>Question 516</summary>

A company provides an API interface to customers so the customers can retrieve their financial information.
Ehe company expects a larger number of requests during peak usage times of the year.
The company requires the API to respond consistently with low latency to ensure customer satisfaction.
The company needs to provide a compute host for the API.
Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A.
    Use an Application Load Balancer and Amazon Elastic Container Service (Amazon ECS).
-   [ ] B.
    Use Amazon API Gateway and AWS Lambda functions with provisioned concurrency.
-   [ ] C.
    Use an Application Load Balancer and an Amazon Elastic Kubernetes Service (Amazon EKS) cluster.
-   [ ] D.
    Use Amazon API Gateway and AWS Lambda functions with reserved concurrency.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B.
    Use Amazon API Gateway and AWS Lambda functions with provisioned concurrency.

Why these are the correct answers:

B.
Use Amazon API Gateway and AWS Lambda functions with provisioned concurrency.

-   [ ] API Gateway and Lambda are serverless, reducing operational overhead.
-   [ ] Provisioned concurrency ensures low latency by keeping Lambda functions initialized.

Why are the other answers wrong?

-   [ ] A and C.
    ECS and EKS require more operational overhead for managing infrastructure.
-   [ ] D.
    Reserved concurrency is deprecated; provisioned concurrency is the current best practice.

Therefore, Option B is the most efficient solution.

</details>
<details>
  <summary>Question 517</summary>

A company wants to send all AWS Systems Manager Session Manager logs to an Amazon S3 bucket for archival purposes.
Which solution will meet this requirement with the MOST operational efficiency?

-   [ ] A.
    Enable S3 logging in the Systems Manager console.
    Choose an S3 bucket to send the session data to.
-   [ ] B.
    Install the Amazon CloudWatch agent.
    Push all logs to a CloudWatch log group.
    Export the logs to an S3 bucket from the group for archival purposes.
-   [ ] C.
    Create a Systems Manager document to upload all server logs to a central S3 bucket.
    Use Amazon EventBridge to run the Systems Manager document against all servers that are in the account daily.
-   [ ] D.
    Install an Amazon CloudWatch agent.
    Push all logs to a CloudWatch log group.
    Create a CloudWatch logs subscription that pushes any incoming log events to an Amazon Kinesis Data Firehose delivery stream.
    Set Amazon S3 as the destination.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A.
    Enable S3 logging in the Systems Manager console.
    Choose an S3 bucket to send the session data to.

Why these are the correct answers:

A.
Enable S3 logging in the Systems Manager console.
Choose an S3 bucket to send the session data to.

-   [ ] Systems Manager directly integrates with S3 for logging, providing the simplest and most efficient way to archive logs.

Why are the other answers wrong?

-   [ ] B, C, and D.
    These options involve additional services and configurations, increasing operational overhead.

Therefore, Option A is the most efficient solution.

</details>
<details>
  <summary>Question 518</summary>

An application uses an Amazon RDS MySQL DB instance.
The RDS database is becoming low on disk space.
A solutions architect wants to increase the disk space without downtime.
Which solution meets these requirements with the LEAST amount of effort?

-   [ ] A.
    Enable storage autoscaling in RDS
-   [ ] B.
    Increase the RDS database instance size
-   [ ] C.
    Change the RDS database instance storage type to Provisioned IOPS
-   [ ] D.
    Back up the RDS database, increase the storage capacity, restore the database, and stop the previous instance

</details>

<details>
  <summary>Answer</summary>

-   [ ] A.
    Enable storage autoscaling in RDS

Why these are the correct answers:

A.
Enable storage autoscaling in RDS

-   [ ] RDS storage autoscaling automatically increases storage capacity when needed without downtime.

Why are the other answers wrong?

-   [ ] B.
    Increasing instance size scales compute, not storage.
-   [ ] C.
    Changing storage type does not directly address storage capacity.
-   [ ] D.
    Backing up and restoring involves downtime.

Therefore, Option A is the simplest and most efficient solution.

</details>
<details>
  <summary>Question 519</summary>

A consulting company provides professional services to customers worldwide.
The company provides solutions and tools for customers to expedite gathering and analyzing data on AWS.
The company needs to centrally manage and deploy a common set of solutions and tools for customers to use for self-service purposes.
Which solution will meet these requirements?

-   [ ] A.
    Create AWS CloudFormation templates for the customers.
-   [ ] B.
    Create AWS Service Catalog products for the customers.
-   [ ] C.
    Create AWS Systems Manager templates for the customers.
-   [ ] D.
    Create AWS Config items for the customers.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B.
    Create AWS Service Catalog products for the customers.

Why these are the correct answers:

B.
Create AWS Service Catalog products for the customers.

-   [ ] AWS Service Catalog allows organizations to create and manage catalogs of IT services that are approved for use on AWS.
-   [ ] It enables centralized management and self-service access for customers.

Why are the other answers wrong?

-   [ ] A.
    CloudFormation templates define infrastructure but do not provide self-service access management.
-   [ ] C.
    Systems Manager templates automate tasks on EC2 instances, not for deploying solutions.
-   [ ] D.
    AWS Config items track resource configuration, not for deploying solutions.

Therefore, Option B is the most suitable solution.

</details>
<details>
  <summary>Question 520</summary>

A company is designing a new web application that will run on Amazon EC2 Instances.
The application will use Amazon DynamoDB for backend data storage.
The application traffic will be unpredictable.
The company expects that the application read and write throughput to the database will be moderate to high.
The company needs to scale in response to application traffic.
Which DynamoDB table configuration will meet these requirements MOST cost-effectively?

-   [ ] A.
    Configure DynamoDB with provisioned read and write by using the DynamoDB Standard table class.
    Set DynamoDB auto scaling to a maximum defined capacity.
-   [ ] B.
    Configure DynamoDB in on-demand mode by using the DynamoDB Standard table class.
-   [ ] C.
    Configure DynamoDB with provisioned read and write by using the DynamoDB Standard Infrequent Access (DynamoDB Standard-IA) table class.
    Set DynamoDB auto scaling to a maximum defined capacity.
-   [ ] D.
    Configure DynamoDB in on-demand mode by using the DynamoDB Standard Infrequent Access (DynamoDB Standard-IA) table class.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B.
    Configure DynamoDB in on-demand mode by using the DynamoDB Standard table class.

Why these are the correct answers:

B.
Configure DynamoDB in on-demand mode by using the DynamoDB Standard table class.

-   [ ] On-demand mode automatically scales read and write capacity in response to application traffic, making it suitable for unpredictable workloads.
-   [ ] The Standard table class is appropriate for moderate to high throughput.

Why are the other answers wrong?

-   [ ] A and C.
    Provisioned capacity requires specifying read and write capacity units, which is less cost-effective for unpredictable traffic.
    Auto Scaling adds complexity.
-   [ ] D.
    Standard-IA is for infrequently accessed data, which does not align with moderate to high throughput.

Therefore, Option B is the most cost-effective solution.

</details>

































