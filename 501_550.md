# AWS-SAA-PRACTICE-EXAM Questions 501-510

<details>
  <summary>Question 501</summary>

A company wants to ingest customer payment data into the company's data lake in Amazon S3.
The company receives payment data every minute on average.
The company wants to analyze the payment data in real time.
Then the company wants to ingest the data into the data lake.
Which solution will meet these requirements with the MOST operational efficiency?

-   [ ] A. Use Amazon Kinesis Data Streams to ingest data.
    Use AWS Lambda to analyze the data in real time.
-   [ ] B. Use AWS Glue to ingest data.
    Use Amazon Kinesis Data Analytics to analyze the data in real time.
-   [ ] C. Use Amazon Kinesis Data Firehose to ingest data.
    Use Amazon Kinesis Data Analytics to analyze the data in real time.
-   [ ] D. Use Amazon API Gateway to ingest data.
    Use AWS Lambda to analyze the data in real time.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Use Amazon Kinesis Data Firehose to ingest data.
    Use Amazon Kinesis Data Analytics to analyze the data in real time.

Why these are the correct answers:

C. Use Amazon Kinesis Data Firehose to ingest data.
Use Amazon Kinesis Data Analytics to analyze the data in real time.

-   [ ] Amazon Kinesis Data Firehose is designed for near real-time ingestion of streaming data into destinations like Amazon S3.
-   [ ] Amazon Kinesis Data Analytics can analyze streaming data in real time.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Kinesis Data Streams requires more management for scaling and data delivery compared to Firehose.
    Lambda is not designed for real-time analytics on streaming data.
-   [ ] B. AWS Glue is an ETL service, not optimized for real-time data ingestion and analysis.
-   [ ] D. API Gateway is for API management, not for high-volume streaming data ingestion.
    Lambda is not suited for real-time analytics on streams.

Therefore, Option C is the most operationally efficient solution.

</details>
<details>
  <summary>Question 502</summary>

A company runs a website that uses a content management system (CMS) on Amazon EC2.
The CMS runs on a single EC2 instance and uses an Amazon Aurora MySQL Multi-AZ DB instance for the data tier.
Website images are stored on an Amazon Elastic Block Store (Amazon EBS) volume that is mounted inside the EC2 instance.
Which combination of actions should a solutions architect take to improve the performance and resilience of the website? (Choose two.)

-   [ ] A. Move the website images into an Amazon S3 bucket that is mounted on every EC2 instance
-   [ ] B. Share the website images by using an NFS share from the primary EC2 instance.
    Mount this share on the other EC2 instances.
-   [ ] C. Move the website images onto an Amazon Elastic File System (Amazon EFS) file system that is mounted on every EC2 instance.
-   [ ] D. Create an Amazon Machine Image (AMI) from the existing EC2 instance.
    Use the AMI to provision new instances behind an Application Load Balancer as part of an Auto Scaling group.
    Configure an accelerator in AWS Global Accelerator for the website
-   [ ] E. Create an Amazon Machine Image (AMI) from the existing EC2 instance.
    Use the AMI to provision new instances behind an Application Load Balancer as part of an Auto Scaling group.
    Configure an Amazon CloudFront distribution for the website.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Move the website images onto an Amazon Elastic File System (Amazon EFS) file system that is mounted on every EC2 instance.
-   [ ] E. Create an Amazon Machine Image (AMI) from the existing EC2 instance.
    Use the AMI to provision new instances behind an Application Load Balancer as part of an Auto Scaling group.
    Configure an Amazon CloudFront distribution for the website.

Why these are the correct answers:

C. Move the website images onto an Amazon Elastic File System (Amazon EFS) file system that is mounted on every EC2 instance.

-   [ ] EFS provides shared file storage that can be accessed by multiple EC2 instances, improving scalability and availability.

E. Create an Amazon Machine Image (AMI) from the existing EC2 instance.
Use the AMI to provision new instances behind an Application Load Balancer as part of an Auto Scaling group.
Configure an Amazon CloudFront distribution for the website.

-   [ ] Auto Scaling groups and Application Load Balancers distribute traffic and improve availability.
-   [ ] CloudFront caches static content, improving performance.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Mounting an S3 bucket on EC2 instances is not efficient for serving frequently accessed files.
-   [ ] B. NFS shares from a single EC2 instance introduce a single point of failure.
-   [ ] D. Global Accelerator is for improving global application performance, not for scaling within a Region.

Therefore, Options C and E provide the best solution for performance and resilience.

</details>
<details>
  <summary>Question 503</summary>

A company runs an infrastructure monitoring service.
The company is building a new feature that will enable the service to monitor data in customer AWS accounts.
The new feature will call AWS APIs in customer accounts to describe Amazon EC2 instances and read Amazon CloudWatch metrics.
What should the company do to obtain access to customer accounts in the MOST secure way?

-   [ ] A. Ensure that the customers create an IAM role in their account with read-only EC2 and CloudWatch permissions and a trust policy to the company's account.
-   [ ] B. Create a serverless API that implements a token vending machine to provide temporary AWS credentials for a role with read-only EC2 and CloudWatch permissions.
-   [ ] C. Ensure that the customers create an IAM user in their account with read-only EC2 and CloudWatch permissions.
    Encrypt and store customer access and secret keys in a secrets management system.
-   [ ] D. Ensure that the customers create an Amazon Cognito user in their account to use an IAM role with read-only EC2 and CloudWatch permissions.
    Encrypt and store the Amazon Cognito user and password in a secrets management system.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Ensure that the customers create an IAM role in their account with read-only EC2 and CloudWatch permissions and a trust policy to the company's account.

Why these are the correct answers:

A. Ensure that the customers create an IAM role in their account with read-only EC2 and CloudWatch permissions and a trust policy to the company's account.

-   [ ] IAM roles allow for secure delegation of permissions without sharing long-term credentials.
-   [ ] The trust policy grants the company's account permission to assume the role.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. A token vending machine adds complexity and is not necessary for this use case.
-   [ ] C. Sharing IAM user credentials (access keys) is insecure.
-   [ ] D. Amazon Cognito is for user authentication, not for granting AWS service permissions.

Therefore, Option A is the most secure solution.

</details>
<details>
  <summary>Question 504</summary>

A company needs to connect several VPCs in the us-east-1 Region that span hundreds of AWS accounts.
The company's networking team has its own AWS account to manage the cloud network.
What is the MOST operationally efficient solution to connect the VPCs?

-   [ ] A. Set up VPC peering connections between each VPC.
    Update each associated subnet's route table
-   [ ] B. Configure a NAT gateway and an internet gateway in each VPC to connect each VPC through the internet
-   [ ] C. Create an AWS Transit Gateway in the networking team's AWS account.
    Configure static routes from each VPC.
-   [ ] D. Deploy VPN gateways in each VPC.
    Create a transit VPC in the networking team's AWS account to connect to each VPC.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Create an AWS Transit Gateway in the networking team's AWS account.
    Configure static routes from each VPC.

Why these are the correct answers:

C. Create an AWS Transit Gateway in the networking team's AWS account.
Configure static routes from each VPC.

-   [ ] AWS Transit Gateway simplifies the management of connections between multiple VPCs.
-   [ ] It reduces the complexity of managing numerous peering connections.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. VPC peering is complex to manage with hundreds of VPCs.
-   [ ] B. Connecting VPCs through the internet is inefficient and insecure.
-   [ ] D. VPN gateways and transit VPCs add complexity and overhead.

Therefore, Option C is the most operationally efficient solution.

</details>
<details>
  <summary>Question 505</summary>

A company has Amazon EC2 instances that run nightly batch jobs to process data.
The EC2 instances run in an Auto Scaling group that uses On-Demand billing.
If a job fails on one instance, another instance will reprocess the job.
The batch jobs run between 12:00 AM and 06:00 AM local time every day.
Which solution will provide EC2 instances to meet these requirements MOST cost-effectively?

-   [ ] A. Purchase a 1-year Savings Plan for Amazon EC2 that covers the instance family of the Auto Scaling group that the batch job uses.
-   [ ] B. Purchase a 1-year Reserved Instance for the specific instance type and operating system of the instances in the Auto Scaling group that the batch job uses.
-   [ ] C. Create a new launch template for the Auto Scaling group.
    Set the instances to Spot Instances.
    Set a policy to scale out based on CPU usage.
-   [ ] D. Create a new launch template for the Auto Scaling group.
    Increase the instance size.
    Set a policy to scale out based on CPU usage.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Create a new launch template for the Auto Scaling group.
    Set the instances to Spot Instances.
    Set a policy to scale out based on CPU usage.

Why these are the correct answers:

C. Create a new launch template for the Auto Scaling group.
Set the instances to Spot Instances.
Set a policy to scale out based on CPU usage.

-   [ ] Spot Instances are cost-effective for fault-tolerant batch processing.
-   [ ] Auto Scaling can replace any interrupted Spot Instances.
-   [ ] Scaling based on CPU usage optimizes resource utilization.

<hr> Why are the other answers wrong? <hr>

-   [ ] A and B. Savings Plans and Reserved Instances are cost-effective for continuous usage, not for sporadic nightly jobs.
-   [ ] D. Increasing instance size is less cost-effective than using Spot Instances.

Therefore, Option C is the most cost-effective solution.

</details>
<details>
  <summary>Question 506</summary>

A social media company is building a feature for its website.
The feature will give users the ability to upload photos.
The company expects significant increases in demand during large events and must ensure that the website can handle the upload traffic from users.
Which solution meets these requirements with the MOST scalability?

-   [ ] A. Upload files from the user's browser to the application servers.
    Transfer the files to an Amazon S3 bucket.
-   [ ] B. Provision an AWS Storage Gateway file gateway.
    Upload files directly from the user's browser to the file gateway.
-   [ ] C. Generate Amazon S3 presigned URLs in the application.
    Upload files directly from the user's browser into an S3 bucket.
-   [ ] D. Provision an Amazon Elastic File System (Amazon EFS) file system.
    Upload files directly from the user's browser to the file system.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Generate Amazon S3 presigned URLs in the application.
    Upload files directly from the user's browser into an S3 bucket.

Why these are the correct answers:

C. Generate Amazon S3 presigned URLs in the application.
Upload files directly from the user's browser into an S3 bucket.

-   [ ] S3 presigned URLs allow users to upload files directly to S3, offloading the traffic from the application servers.
-   [ ] S3 is highly scalable and can handle large volumes of uploads.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Uploading files to application servers and then transferring them to S3 increases the load on the servers.
-   [ ] B. Storage Gateway is for hybrid cloud storage, not for handling large volumes of direct user uploads.
-   [ ] D. EFS is a file system and is not designed for handling direct uploads from numerous users.

Therefore, Option C is the most scalable solution.

</details>
<details>
  <summary>Question 507</summary>

A company has a web application for travel ticketing.
The application is based on a database that runs in a single data center in North America.
The company wants to expand the application to serve a global user base.
The company needs to deploy the application to multiple AWS Regions.
Average latency must be less than 1 second on updates to the reservation database.
The company wants to have separate deployments of its web platform across multiple Regions.
However, the company must maintain a single primary reservation database that is globally consistent.
Which solution should a solutions architect recommend to meet these requirements?

-   [ ] A. Convert the application to use Amazon DynamoDB.
    Use a global table for the center reservation table.
    Use the correct Regional endpoint in each Regional deployment.
-   [ ] B. Migrate the database to an Amazon Aurora MySQL database.
    Deploy Aurora Read Replicas in each Region.
    Use the correct Regional endpoint in each Regional deployment for access to the database.
-   [ ] C. Migrate the database to an Amazon RDS for MySQL database.
    Deploy MySQL read replicas in each Region.
    Use the correct Regional endpoint in each Regional deployment for access to the database.
-   [ ] D. Migrate the application to an Amazon Aurora Serverless database.
    Deploy instances of the database to each Region.
    Use the correct Regional endpoint in each Regional deployment to access the database.
    Use AWS Lambda functions to process event streams in each Region to synchronize the databases.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Migrate the database to an Amazon Aurora MySQL database.
    Deploy Aurora Read Replicas in each Region.
    Use the correct Regional endpoint in each Regional deployment for access to the database.

Why these are the correct answers:

B. Migrate the database to an Amazon Aurora MySQL database.
Deploy Aurora Read Replicas in each Region.
Use the correct Regional endpoint in each Regional deployment for access to the database.

-   [ ] Aurora provides high performance and scalability.
-   [ ] Read Replicas in each Region can serve local read requests, reducing latency.
-   [ ] A single primary Aurora database ensures global consistency.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. DynamoDB is a NoSQL database and may require significant application changes.
-   [ ] C. RDS MySQL read replicas do not provide the same level of performance and global consistency as Aurora.
-   [ ] D. Aurora Serverless is not ideal for high-performance, low-latency global applications.
    Lambda and event streams add complexity.

Therefore, Option B is the most suitable solution.

</details>
<details>
  <summary>Question 508</summary>

A company has migrated multiple Microsoft Windows Server workloads to Amazon EC2 instances that run in the us-west-1 Region.
The company manually backs up the workloads to create an image as needed.
In the event of a natural disaster in the us-west-1 Region, the company wants to recover workloads quickly in the us-west-2 Region.
The company wants no more than 24 hours of data loss on the EC2 instances.
The company also wants to automate any backups of the EC2 instances.
Which solutions will meet these requirements with the LEAST administrative effort? (Choose two.)

-   [ ] A. Create an Amazon EC2-backed Amazon Machine Image (AMI) lifecycle policy to create a backup based on tags.
    Schedule the backup to run twice daily.
    Copy the image on demand.
-   [ ] B. Create an Amazon EC2-backed Amazon Machine Image (AMI) lifecycle policy to create a backup based on tags.
    Schedule the backup to run twice daily.
    Configure the copy to the us-west-2 Region.
-   [ ] C. Create backup vaults in us-west-1 and in us-west-2 by using AWS Backup.
    Create a backup plan for the EC2 instances based on tag values.
    Create an AWS Lambda function to run as a scheduled job to copy the backup data to us-west-2.
-   [ ] D. Create a backup vault by using AWS Backup.
    Use AWS Backup to create a backup plan for the EC2 instances based on tag values.
    Define the destination for the copy as us-west-2.
    Specify the backup schedule to run twice daily.
-   [ ] E. Create a backup vault by using AWS Backup.
    Use AWS Backup to create a backup plan for the EC2 instances based on tag values.
    Specify the backup schedule to run twice daily.
    Copy on demand to us-west-2.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Create an Amazon EC2-backed Amazon Machine Image (AMI) lifecycle policy to create a backup based on tags.
    Schedule the backup to run twice daily.
    Configure the copy to the us-west-2 Region.
-   [ ] D. Create a backup vault by using AWS Backup.
    Use AWS Backup to create a backup plan for the EC2 instances based on tag values.
    Define the destination for the copy as us-west-2.
    Specify the backup schedule to run twice daily.

Why these are the correct answers:

B. Create an Amazon EC2-backed Amazon Machine Image (AMI) lifecycle policy to create a backup based on tags.
Schedule the backup to run twice daily.
Configure the copy to the us-west-2 Region.

-   [ ] AMI lifecycle policies automate AMI creation.
-   [ ] Scheduling backups twice daily meets the RPO of 24 hours.
-   [ ] Automating the copy to us-west-2 simplifies the DR process.

D. Create a backup vault by using AWS Backup.
Use AWS Backup to create a backup plan for the EC2 instances based on tag values.
Define the destination for the copy as us-west-2.
Specify the backup schedule to run twice daily.

-   [ ] AWS Backup centralizes backup management.
-   [ ] It automates backups and cross-Region copies.
-   [ ] Tag-based backups simplify management.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Copying images on demand adds manual steps.
-   [ ] C. Using Lambda functions adds complexity.
-   [ ] E. Copying on demand adds manual steps.

Therefore, Options B and D provide the most automated and efficient solutions.

</details>
<details>
  <summary>Question 509</summary>

A company operates a two-tier application for image processing.
The application uses two Availability Zones, each with one public subnet and one private subnet.
An Application Load Balancer (ALB) for the web tier uses the public subnets.
Amazon EC2 instances for the application tier use the private subnets.
Users report that the application is running more slowly than expected.
A security audit of the web server log files shows that the application is receiving millions of illegitimate requests from a small number of IP addresses.
A solutions architect needs to resolve the immediate performance problem while the company investigates a more permanent solution.
What should the solutions architect recommend to meet this requirement?

-   [ ] A. Modify the inbound security group for the web tier.
    Add a deny rule for the IP addresses that are consuming resources.
-   [ ] B. Modify the network ACL for the web tier subnets.
    Add an inbound deny rule for the IP addresses that are consuming resources.
-   [ ] C. Modify the inbound security group for the application tier.
    Add a deny rule for the IP addresses that are consuming resources.
-   [ ] D. Modify the network ACL for the application tier subnets.
    Add an inbound deny rule for the IP addresses that are consuming resources.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Modify the network ACL for the web tier subnets.
    Add an inbound deny rule for the IP addresses that are consuming resources.

Why these are the correct answers:

B. Modify the network ACL for the web tier subnets.
Add an inbound deny rule for the IP addresses that are consuming resources.

-   [ ] Network ACLs operate at the subnet level and can quickly block traffic, reducing the load on the web tier.
-   [ ] This is an immediate solution to mitigate the performance issue.

<hr> Why are the other answers wrong? <hr>

-   [ ] A and C. Security groups operate at the instance level and are less efficient for blocking traffic at the subnet level.
-   [ ] D. Modifying the application tier's network ACLs does not address the immediate problem of illegitimate requests hitting the web tier.

Therefore, Option B is the most appropriate solution for immediate mitigation.

</details>
<details>
  <summary>Question 510</summary>

A global marketing company has applications that run in the ap-southeast-2 Region and the eu-west-1 Region.
Applications that run in a VPC in eu-west-1 need to communicate securely with databases that run in a VPC in ap-southeast-2.
Which network design will meet these requirements?

-   [ ] A. Create a VPC peering connection between the eu-west-1 VPC and the ap-southeast-2 VPC.
    Create an inbound rule in the eu-west-1 application security group that allows traffic from the database server IP addresses in the ap-southeast-2 security group.
-   [ ] B. Configure a VPC peering connection between the ap-southeast-2 VPC and the eu-west-1 VPC.
    Update the subnet route tables.
    Create an inbound rule in the ap-southeast-2 database security group that references the security group ID of the application servers in eu-west-1.
-   [ ] C. Configure a VPC peering connection between the ap-southeast-2 VPC and the eu-west-1 VPUpdate the subnet route tables.
    Create an inbound rule in the ap-southeast-2 database security group that allows traffic from the eu-west-1 application server IP addresses.
-   [ ] D. Create a transit gateway with a peering attachment between the eu-west-1 VPC and the ap- southeast-2 VPC.
    After the transit gateways are properly peered and routing is configured, create an inbound rule in the database security group that references the security group ID of the application servers in eu-west-1.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Configure a VPC peering connection between the ap-southeast-2 VPC and the eu-west-1 VPUpdate the subnet route tables.
    Create an inbound rule in the ap-southeast-2 database security group that allows traffic from the eu-west-1 application server IP addresses.

Why these are the correct answers:

C. Configure a VPC peering connection between the ap-southeast-2 VPC and the eu-west-1 VPUpdate the subnet route tables.
Create an inbound rule in the ap-southeast-2 database security group that allows traffic from the eu-west-1 application server IP addresses.

-   [ ] VPC peering connects two VPCs, enabling communication.
-   [ ] Updating route tables directs traffic between the VPCs.
-   [ ] Security groups control traffic at the instance level.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Security groups cannot reference IP addresses in another security group across VPC peering connections.
-   [ ] B. While security groups can reference other security groups, the direction of the peering connection setup is important and the description is incorrect.
-   [ ] D. Transit Gateway is for connecting many VPCs, not just two, and adds complexity.

Therefore, Option C is the correct solution.

</details>

# AWS-SAA-PRACTICE-EXAM Questions 511-520

<details>
  <summary>Question 511</summary>

A company is developing software that uses a PostgreSQL database schema.
The company needs to configure multiple development environments and databases for the company's developers.
On average, each development environment is used for half of the 8-hour workday.
Which solution will meet these requirements MOST cost-effectively?

-   [ ] A.
    Configure each development environment with its own Amazon Aurora PostgreSQL database
-   [ ] B.
    Configure each development environment with its own Amazon RDS for PostgreSQL Single-AZ DB instances
-   [ ] C.
    Configure each development environment with its own Amazon Aurora On-Demand PostgreSQL-Compatible database
-   [ ] D.
    Configure each development environment with its own Amazon S3 bucket by using Amazon S3 Object Select

</details>

<details>
  <summary>Answer</summary>

-   [ ] C.
    Configure each development environment with its own Amazon Aurora On-Demand PostgreSQL-Compatible database

Why these are the correct answers:

C.
Configure each development environment with its own Amazon Aurora On-Demand PostgreSQL-Compatible database

-   [ ] Aurora Serverless v1 (which is what "On-Demand" refers to in the document) automatically starts up, shuts down, and scales capacity based on application needs, making it cost-effective for intermittent use.
-   [ ] It provides PostgreSQL compatibility, so developers can use their existing schema.

<hr> Why are the other answers wrong? <hr>

-   [ ] A and B.
    Provisioning a full Aurora PostgreSQL database or RDS for PostgreSQL instance for each developer is more expensive, even if not fully utilized.
-   [ ] D.
    Amazon S3 and S3 Object Select are for object storage and retrieval, not for running a PostgreSQL database.

Therefore, Option C is the most cost-effective solution for development environments with intermittent use.

</details>
<details>
  <summary>Question 512</summary>

A company uses AWS Organizations with resources tagged by account.
The company also uses AWS Backup to back up its AWS infrastructure resources.
The company needs to back up all AWS resources.
Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A.
    Use AWS Config to identify all untagged resources.
    Tag the identified resources programmatically.
    Use tags in the backup plan.
-   [ ] B.
    Use AWS Config to identify all resources that are not running.
    Add those resources to the backup vault.
-   [ ] C.
    Require all AWS account owners to review their resources to identify the resources that need to be backed up.
-   [ ] D.
    Use Amazon Inspector to identify all noncompliant resources.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A.
    Use AWS Config to identify all untagged resources.
    Tag the identified resources programmatically.
    Use tags in the backup plan.

Why these are the correct answers:

A.
Use AWS Config to identify all untagged resources.
Tag the identified resources programmatically.
Use tags in the backup plan.

-   [ ] AWS Config can identify resources that are not tagged, allowing for automated tagging.
-   [ ] AWS Backup can use tags to select resources for backup, simplifying backup management.

<hr> Why are the other answers wrong? <hr>

-   [ ] B.
    Backing up only non-running resources does not meet the requirement to back up all resources.
-   [ ] C.
    Manual review is time-consuming and error-prone.
-   [ ] D.
    AWS Inspector is for security vulnerability assessments, not for backup management.

Therefore, Option A is the most efficient and automated solution.

</details>
<details>
  <summary>Question 513</summary>

A social media company wants to allow its users to upload images in an application that is hosted in the AWS Cloud.
The company needs a solution that automatically resizes the images so that the images can be displayed on multiple device types.
The application experiences unpredictable traffic patterns throughout the day.
The company is seeking a highly available solution that maximizes scalability.
What should a solutions architect do to meet these requirements?

-   [ ] A.
    Create a static website hosted in Amazon S3 that invokes AWS Lambda functions to resize the images and store the images in an Amazon S3 bucket.
-   [ ] B.
    Create a static website hosted in Amazon CloudFront that invokes AWS Step Functions to resize the images and store the images in an Amazon RDS database.
-   [ ] C.
    Create a dynamic website hosted on a web server that runs on an Amazon EC2 instance.
    Configure a process that runs on the EC2 instance to resize the images and store the images in an Amazon S3 bucket.
-   [ ] D.
    Create a dynamic website hosted on an automatically scaling Amazon Elastic Container Service (Amazon ECS) cluster that creates a resize job in Amazon Simple Queue Service (Amazon SQS).
    Set up an image-resizing program that runs on an Amazon EC2 instance to process the resize jobs.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A.
    Create a static website hosted in Amazon S3 that invokes AWS Lambda functions to resize the images and store the images in an Amazon S3 bucket.

Why these are the correct answers:

A.
Create a static website hosted in Amazon S3 that invokes AWS Lambda functions to resize the images and store the images in an Amazon S3 bucket.

-   [ ] S3 is highly scalable for storing images.
-   [ ] Lambda functions can automatically resize images and scale based on traffic, providing a serverless and cost-effective solution.

<hr> Why are the other answers wrong? <hr>

-   [ ] B.
    CloudFront is a CDN and not for hosting a static website.
    Step Functions are for orchestrating workflows, not for image resizing.
    RDS is not suitable for storing images.
-   [ ] C.
    EC2 instances require more management and do not scale as efficiently as Lambda.
-   [ ] D.
    ECS and SQS add complexity and are not necessary for simple image resizing.

Therefore, Option A is the most scalable and cost-effective solution.

</details>
<details>
  <summary>Question 514</summary>

A company is running a microservices application on Amazon EC2 instances.
The company wants to migrate the application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster for scalability.
The company must configure the Amazon EKS control plane with endpoint private access set to true and endpoint public access set to false to maintain security compliance.
The company must also put the data plane in private subnets.
However, the company has received error notifications because the node cannot join the cluster.
Which solution will allow the node to join the cluster?

-   [ ] A.
    Grant the required permission in AWS Identity and Access Management (IAM) to the AmazonEKSNodeRole IAM role.
-   [ ] B.
    Create interface VPC endpoints to allow nodes to access the control plane.
-   [ ] C.
    Recreate nodes in the public subnet.
    Restrict security groups for EC2 nodes.
-   [ ] D.
    Allow outbound traffic in the security group of the nodes.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B.
    Create interface VPC endpoints to allow nodes to access the control plane.

Why these are the correct answers:

B.
Create interface VPC endpoints to allow nodes to access the control plane.

-   [ ] When the EKS control plane has private access enabled, nodes in private subnets need a way to communicate with it.
-   [ ] Interface VPC endpoints provide this private connectivity.

<hr> Why are the other answers wrong? <hr>

-   [ ] A.
    IAM permissions are necessary but not sufficient for network connectivity.
-   [ ] C.
    Recreating nodes in public subnets violates the security compliance requirement.
-   [ ] D.
    Outbound traffic permissions are necessary but do not address the need for a private connection to the control plane.

Therefore, Option B is the correct solution.

</details>
<details>
  <summary>Question 515</summary>

A company is migrating an on-premises application to AWS.
The company wants to use Amazon Redshift as a solution.
Which use cases are suitable for Amazon Redshift in this scenario?
(Choose three.)

-   [ ] A.
    Supporting data APIs to access data with traditional, containerized, and event-driven applications
-   [ ] B.
    Supporting client-side and server-side encryption
-   [ ] C.
    Building analytics workloads during specified hours and when the application is not active
-   [ ] D.
    Caching data to reduce the pressure on the backend database
-   [ ] E.
    Scaling globally to support petabytes of data and tens of millions of requests per minute
-   [ ] F.
    Creating a secondary replica of the cluster by using the AWS Management Console

</details>

<details>
  <summary>Answer</summary>

-   [ ] B.
    Supporting client-side and server-side encryption
-   [ ] C.
    Building analytics workloads during specified hours and when the application is not active
-   [ ] E.
    Scaling globally to support petabytes of data and tens of millions of requests per minute

Why these are the correct answers:

B.
Supporting client-side and server-side encryption

-   [ ] Redshift supports encryption, which is important for data security.

C.
Building analytics workloads during specified hours and when the application is not active

-   [ ] Redshift is optimized for analytical workloads.

E.
Scaling globally to support petabytes of data and tens of millions of requests per minute

-   [ ] Redshift is designed for large-scale data warehousing.

<hr> Why are the other answers wrong? <hr>

-   [ ] A.
    Redshift is not designed for serving data APIs for applications.
-   [ ] D.
    Redshift is a data warehouse, not a caching solution.
-   [ ] F.
    Creating a secondary replica is not a primary use case; Redshift focuses on analytics.

Therefore, Options B, C, and E are the correct use cases.

</details>
<details>
  <summary>Question 516</summary>

A company provides an API interface to customers so the customers can retrieve their financial information.
Ehe company expects a larger number of requests during peak usage times of the year.
The company requires the API to respond consistently with low latency to ensure customer satisfaction.
The company needs to provide a compute host for the API.
Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A.
    Use an Application Load Balancer and Amazon Elastic Container Service (Amazon ECS).
-   [ ] B.
    Use Amazon API Gateway and AWS Lambda functions with provisioned concurrency.
-   [ ] C.
    Use an Application Load Balancer and an Amazon Elastic Kubernetes Service (Amazon EKS) cluster.
-   [ ] D.
    Use Amazon API Gateway and AWS Lambda functions with reserved concurrency.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B.
    Use Amazon API Gateway and AWS Lambda functions with provisioned concurrency.

Why these are the correct answers:

B.
Use Amazon API Gateway and AWS Lambda functions with provisioned concurrency.

-   [ ] API Gateway and Lambda are serverless, reducing operational overhead.
-   [ ] Provisioned concurrency ensures low latency by keeping Lambda functions initialized.

<hr> Why are the other answers wrong? <hr>

-   [ ] A and C.
    ECS and EKS require more operational overhead for managing infrastructure.
-   [ ] D.
    Reserved concurrency is deprecated; provisioned concurrency is the current best practice.

Therefore, Option B is the most efficient solution.

</details>
<details>
  <summary>Question 517</summary>

A company wants to send all AWS Systems Manager Session Manager logs to an Amazon S3 bucket for archival purposes.
Which solution will meet this requirement with the MOST operational efficiency?

-   [ ] A.
    Enable S3 logging in the Systems Manager console.
    Choose an S3 bucket to send the session data to.
-   [ ] B.
    Install the Amazon CloudWatch agent.
    Push all logs to a CloudWatch log group.
    Export the logs to an S3 bucket from the group for archival purposes.
-   [ ] C.
    Create a Systems Manager document to upload all server logs to a central S3 bucket.
    Use Amazon EventBridge to run the Systems Manager document against all servers that are in the account daily.
-   [ ] D.
    Install an Amazon CloudWatch agent.
    Push all logs to a CloudWatch log group.
    Create a CloudWatch logs subscription that pushes any incoming log events to an Amazon Kinesis Data Firehose delivery stream.
    Set Amazon S3 as the destination.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A.
    Enable S3 logging in the Systems Manager console.
    Choose an S3 bucket to send the session data to.

Why these are the correct answers:

A.
Enable S3 logging in the Systems Manager console.
Choose an S3 bucket to send the session data to.

-   [ ] Systems Manager directly integrates with S3 for logging, providing the simplest and most efficient way to archive logs.

<hr> Why are the other answers wrong? <hr>

-   [ ] B, C, and D.
    These options involve additional services and configurations, increasing operational overhead.

Therefore, Option A is the most efficient solution.

</details>
<details>
  <summary>Question 518</summary>

An application uses an Amazon RDS MySQL DB instance.
The RDS database is becoming low on disk space.
A solutions architect wants to increase the disk space without downtime.
Which solution meets these requirements with the LEAST amount of effort?

-   [ ] A.
    Enable storage autoscaling in RDS
-   [ ] B.
    Increase the RDS database instance size
-   [ ] C.
    Change the RDS database instance storage type to Provisioned IOPS
-   [ ] D.
    Back up the RDS database, increase the storage capacity, restore the database, and stop the previous instance

</details>

<details>
  <summary>Answer</summary>

-   [ ] A.
    Enable storage autoscaling in RDS

Why these are the correct answers:

A.
Enable storage autoscaling in RDS

-   [ ] RDS storage autoscaling automatically increases storage capacity when needed without downtime.

<hr> Why are the other answers wrong? <hr>

-   [ ] B.
    Increasing instance size scales compute, not storage.
-   [ ] C.
    Changing storage type does not directly address storage capacity.
-   [ ] D.
    Backing up and restoring involves downtime.

Therefore, Option A is the simplest and most efficient solution.

</details>
<details>
  <summary>Question 519</summary>

A consulting company provides professional services to customers worldwide.
The company provides solutions and tools for customers to expedite gathering and analyzing data on AWS.
The company needs to centrally manage and deploy a common set of solutions and tools for customers to use for self-service purposes.
Which solution will meet these requirements?

-   [ ] A.
    Create AWS CloudFormation templates for the customers.
-   [ ] B.
    Create AWS Service Catalog products for the customers.
-   [ ] C.
    Create AWS Systems Manager templates for the customers.
-   [ ] D.
    Create AWS Config items for the customers.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B.
    Create AWS Service Catalog products for the customers.

Why these are the correct answers:

B.
Create AWS Service Catalog products for the customers.

-   [ ] AWS Service Catalog allows organizations to create and manage catalogs of IT services that are approved for use on AWS.
-   [ ] It enables centralized management and self-service access for customers.

<hr> Why are the other answers wrong? <hr>

-   [ ] A.
    CloudFormation templates define infrastructure but do not provide self-service access management.
-   [ ] C.
    Systems Manager templates automate tasks on EC2 instances, not for deploying solutions.
-   [ ] D.
    AWS Config items track resource configuration, not for deploying solutions.

Therefore, Option B is the most suitable solution.

</details>
<details>
  <summary>Question 520</summary>

A company is designing a new web application that will run on Amazon EC2 Instances.
The application will use Amazon DynamoDB for backend data storage.
The application traffic will be unpredictable.
The company expects that the application read and write throughput to the database will be moderate to high.
The company needs to scale in response to application traffic.
Which DynamoDB table configuration will meet these requirements MOST cost-effectively?

-   [ ] A.
    Configure DynamoDB with provisioned read and write by using the DynamoDB Standard table class.
    Set DynamoDB auto scaling to a maximum defined capacity.
-   [ ] B.
    Configure DynamoDB in on-demand mode by using the DynamoDB Standard table class.
-   [ ] C.
    Configure DynamoDB with provisioned read and write by using the DynamoDB Standard Infrequent Access (DynamoDB Standard-IA) table class.
    Set DynamoDB auto scaling to a maximum defined capacity.
-   [ ] D.
    Configure DynamoDB in on-demand mode by using the DynamoDB Standard Infrequent Access (DynamoDB Standard-IA) table class.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B.
    Configure DynamoDB in on-demand mode by using the DynamoDB Standard table class.

Why these are the correct answers:

B.
Configure DynamoDB in on-demand mode by using the DynamoDB Standard table class.

-   [ ] On-demand mode automatically scales read and write capacity in response to application traffic, making it suitable for unpredictable workloads.
-   [ ] The Standard table class is appropriate for moderate to high throughput.

<hr> Why are the other answers wrong? <hr>

-   [ ] A and C.
    Provisioned capacity requires specifying read and write capacity units, which is less cost-effective for unpredictable traffic.
    Auto Scaling adds complexity.
-   [ ] D.
    Standard-IA is for infrequently accessed data, which does not align with moderate to high throughput.

Therefore, Option B is the most cost-effective solution.

</details>

# AWS-SAA-PRACTICE-EXAM Questions 521-530

<details>
  <summary>Question 521</summary>

A retail company has several businesses.
The IT team for each business manages its own AWS account.
Each team account is part of an organization in AWS Organizations.
Each team monitors its product inventory levels in an Amazon DynamoDB table in the team's own AWS account.
The company is deploying a central inventory reporting application into a shared AWS account.
The application must be able to read items from all the teams' DynamoDB tables.
Which authentication option will meet these requirements MOST securely?

-   [ ] A.
    Integrate DynamoDB with AWS Secrets Manager in the inventory application account.
    Configure the application to use the correct secret from Secrets Manager to authenticate and read the DynamoDB table.
    Schedule secret rotation for every 30 days.
-   [ ] B.
    In every business account, create an IAM user that has programmatic access.
    Configure the application to use the correct IAM user access key ID and secret access key to authenticate and read the DynamoDB table.
    Manually rotate IAM access keys every 30 days.
-   [ ] C.
    In every business account, create an IAM role named BU_ROLE with a policy that gives the role access to the DynamoDB table and a trust policy to trust a specific role in the inventory application account.
    In the inventory account, create a role named APP_ROLE that allows access to the STS AssumeRole API operation.
    Configure the application to use APP_ROLE and assume the crossaccount role BU_ROLE to read the DynamoDB table.
-   [ ] D.
    Integrate DynamoDB with AWS Certificate Manager (ACM).
    Generate identity certificates to authenticate DynamoDB.
    Configure the application to use the correct certificate to authenticate and read the DynamoDB table.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C.
    In every business account, create an IAM role named BU_ROLE with a policy that gives the role access to the DynamoDB table and a trust policy to trust a specific role in the inventory application account.
    In the inventory account, create a role named APP_ROLE that allows access to the STS AssumeRole API operation.
    Configure the application to use APP_ROLE and assume the crossaccount role BU_ROLE to read the DynamoDB table.

Why these are the correct answers:

C.
In every business account, create an IAM role named BU_ROLE with a policy that gives the role access to the DynamoDB table and a trust policy to trust a specific role in the inventory application account.
In the inventory account, create a role named APP_ROLE that allows access to the STS AssumeRole API operation.
Configure the application to use APP_ROLE and assume the crossaccount role BU_ROLE to read the DynamoDB table.

-   [ ] IAM roles provide secure cross-account access by allowing the application to assume a role in each business account.
-   [ ] This method avoids sharing or embedding long-term credentials.

<hr> Why are the other answers wrong? <hr>

-   [ ] A.
    Secrets Manager is for storing secrets, not for cross-account authentication for DynamoDB access.
-   [ ] B.
    Sharing IAM user credentials (access keys) is insecure and requires manual rotation.
-   [ ] D.
    ACM is for managing SSL/TLS certificates, not for DynamoDB authentication.

Therefore, Option C is the most secure solution.

</details>
<details>
  <summary>Question 522</summary>

A company runs container applications by using Amazon Elastic Kubernetes Service (Amazon EKS).
The company's workload is not consistent throughout the day.
The company wants Amazon EKS to scale in and out according to the workload.
Which combination of steps will meet these requirements with the LEAST operational overhead?
(Choose two.)

-   [ ] A.
    Use an AWS Lambda function to resize the EKS cluster.
-   [ ] B.
    Use the Kubernetes Metrics Server to activate horizontal pod autoscaling.
-   [ ] C.
    Use the Kubernetes Cluster Autoscaler to manage the number of nodes in the cluster.
-   [ ] D.
    Use Amazon API Gateway and connect it to Amazon EKS.
-   [ ] E.
    Use AWS App Mesh to observe network activity.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B.
    Use the Kubernetes Metrics Server to activate horizontal pod autoscaling.
-   [ ] C.
    Use the Kubernetes Cluster Autoscaler to manage the number of nodes in the cluster.

Why these are the correct answers:

B.
Use the Kubernetes Metrics Server to activate horizontal pod autoscaling.

-   [ ] Kubernetes Metrics Server collects resource usage data, enabling horizontal pod autoscaling to scale pods within the cluster.

C.
Use the Kubernetes Cluster Autoscaler to manage the number of nodes in the cluster.

-   [ ] Kubernetes Cluster Autoscaler automatically adjusts the number of nodes in the EKS cluster based on the needs of the pods.

<hr> Why are the other answers wrong? <hr>

-   [ ] A.
    Using Lambda to resize the EKS cluster adds complexity and requires custom code.
-   [ ] D.
    API Gateway is for managing APIs, not for scaling EKS clusters.
-   [ ] E.
    App Mesh is for managing microservices communication, not for cluster scaling.

Therefore, Options B and C provide the most efficient and automated scaling solution.

</details>
<details>
  <summary>Question 523</summary>

A company runs a microservice-based serverless web application.
The application must be able to retrieve data from multiple Amazon DynamoDB tables A solutions architect needs to give the application the ability to retrieve the data with no impact on the baseline performance of the application.
Which solution will meet these requirements in the MOST operationally efficient way?

-   [ ] A.
    AWS AppSync pipeline resolvers
-   [ ] B.
    Amazon CloudFront with Lambda@Edge functions
-   [ ] C.
    Edge-optimized Amazon API Gateway with AWS Lambda functions
-   [ ] D.
    Amazon Athena Federated Query with a DynamoDB connector

</details>

<details>
  <summary>Answer</summary>

-   [ ] D.
    Amazon Athena Federated Query with a DynamoDB connector

Why these are the correct answers:

D.
Amazon Athena Federated Query with a DynamoDB connector

-   [ ] Athena Federated Query allows you to query data across multiple data sources, including DynamoDB, with standard SQL.
-   [ ] It is serverless and does not impact the performance of the application.

<hr> Why are the other answers wrong? <hr>

-   [ ] A.
    AppSync is a GraphQL service, not designed for simple data retrieval from multiple DynamoDB tables in a RESTful manner.
-   [ ] B.
    CloudFront and Lambda@Edge are for content delivery and edge computing, not for querying databases.
-   [ ] C.
    API Gateway and Lambda can retrieve data, but it requires more code and management than Athena Federated Query.

Therefore, Option D is the most operationally efficient solution.

</details>
<details>
  <summary>Question 524</summary>

A company wants to analyze and troubleshoot Access Denied errors and Unauthorized errors that are related to IAM permissions.
The company has AWS CloudTrail turned on.
Which solution will meet these requirements with the LEAST effort?

-   [ ] A.
    Use AWS Glue and write custom scripts to query CloudTrail logs for the errors.
-   [ ] B.
    Use AWS Batch and write custom scripts to query CloudTrail logs for the errors.
-   [ ] C.
    Search CloudTrail logs with Amazon Athena queries to identify the errors.
-   [ ] D.
    Search CloudTrail logs with Amazon QuickSight.
    Create a dashboard to identify the errors.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C.
    Search CloudTrail logs with Amazon Athena queries to identify the errors.

Why these are the correct answers:

C.
Search CloudTrail logs with Amazon Athena queries to identify the errors.

-   [ ] Amazon Athena allows you to query CloudTrail logs using SQL, making it efficient for analyzing specific error types.

<hr> Why are the other answers wrong? <hr>

-   [ ] A and B.
    AWS Glue and AWS Batch require writing and managing custom scripts, which is more complex.
-   [ ] D.
    QuickSight is for visualizing data, not for directly querying logs.

Therefore, Option C is the most efficient solution.

</details>
<details>
  <summary>Question 525</summary>

A company wants to add its existing AWS usage cost to its operation cost dashboard.
A solutions architect needs to recommend a solution that will give the company access to its usage cost programmatically.
The company must be able to access cost data for the current year and forecast costs for the next 12 months.
Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A.
    Access usage cost-related data by using the AWS Cost Explorer API with pagination.
-   [ ] B.
    Access usage cost-related data by using downloadable AWS Cost Explorer report.csv files.
-   [ ] C.
    Configure AWS Budgets actions to send usage cost data to the company through FTP.
-   [ ] D.
    Create AWS Budgets reports for usage cost data.
    Send the data to the company through SMTP.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A.
    Access usage cost-related data by using the AWS Cost Explorer API with pagination.

Why these are the correct answers:

A.
Access usage cost-related data by using the AWS Cost Explorer API with pagination.

-   [ ] The Cost Explorer API provides programmatic access to cost and usage data.
-   [ ] Pagination allows for efficient retrieval of large datasets.

<hr> Why are the other answers wrong? <hr>

-   [ ] B.
    Downloading CSV files is manual and not programmatic.
-   [ ] C and D.
    Budgets and their actions are for monitoring and alerting on costs, not for providing detailed cost data programmatically.
    FTP and SMTP add complexity.

Therefore, Option A is the most efficient solution.

</details>
<details>
  <summary>Question 526</summary>

A solutions architect is reviewing the resilience of an application.
The solutions architect notices that a database administrator recently failed over the application's Amazon Aurora PostgreSQL database writer instance as part of a scaling exercise.
The failover resulted in 3 minutes of downtime for the application.
Which solution will reduce the downtime for scaling exercises with the LEAST operational overhead?

-   [ ] A.
    Create more Aurora PostgreSQL read replicas in the cluster to handle the load during failover.
-   [ ] B.
    Set up a secondary Aurora PostgreSQL cluster in the same AWS Region.
    During failover, update the application to use the secondary cluster's writer endpoint.
-   [ ] C.
    Create an Amazon ElastiCache for Memcached cluster to handle the load during failover.
-   [ ] D.
    Set up an Amazon RDS proxy for the database.
    Update the application to use the proxy endpoint.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D.
    Set up an Amazon RDS proxy for the database.
    Update the application to use the proxy endpoint.

Why these are the correct answers:

D.
Set up an Amazon RDS proxy for the database.
Update the application to use the proxy endpoint.

-   [ ] Amazon RDS Proxy minimizes downtime during failovers by maintaining database connections.
-   [ ] It is transparent to the application, reducing operational overhead.

<hr> Why are the other answers wrong? <hr>

-   [ ] A.
    Read replicas do not reduce downtime during writer instance failovers.
-   [ ] B.
    Setting up a secondary cluster and updating the application endpoint adds complexity.
-   [ ] C.
    ElastiCache is for caching, not for database failovers.

Therefore, Option D is the most efficient solution.

</details>
<details>
  <summary>Question 527</summary>

A company has a regional subscription-based streaming service that runs in a single AWS Region.
The architecture consists of web servers and application servers on Amazon EC2 instances.
The EC2 instances are in Auto Scaling groups behind Elastic Load Balancers.
The architecture includes an Amazon Aurora global database cluster that extends across multiple Availability Zones.
The company wants to expand globally and to ensure that its application has minimal downtime.
Which solution will provide the MOST fault tolerance?

-   [ ] A.
    Extend the Auto Scaling groups for the web tier and the application tier to deploy instances in Availability Zones in a second Region.
    Use an Aurora global database to deploy the database in the primary Region and the second Region.
    Use Amazon Route 53 health checks with a failover routing policy to the second Region.
-   [ ] B.
    Deploy the web tier and the application tier to a second Region.
    Add an Aurora PostgreSQL cross-Region Aurora Replica in the second Region.
    Use Amazon Route 53 health checks with a failover routing policy to the second Region.
    Promote the secondary to primary as needed.
-   [ ] C.
    Deploy the web tier and the application tier to a second Region.
    Create an Aurora PostgreSQL database in the second Region.
    Use AWS Database Migration Service (AWS DMS) to replicate the primary database to the second Region.
    Use Amazon Route 53 health checks with a failover routing policy to the second Region.
-   [ ] D.
    Deploy the web tier and the application tier to a second Region.
    Use an Amazon Aurora global database to deploy the database in the primary Region and the second Region.
    Use Amazon Route 53 health checks with a failover routing policy to the second Region.
    Promote the secondary to primary as needed.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D.
    Deploy the web tier and the application tier to a second Region.
    Use an Amazon Aurora global database to deploy the database in the primary Region and the second Region.
    Use Amazon Route 53 health checks with a failover routing policy to the second Region.
    Promote the secondary to primary as needed.

Why these are the correct answers:

D.
Deploy the web tier and the application tier to a second Region.
Use an Amazon Aurora global database to deploy the database in the primary Region and the second Region.
Use Amazon Route 53 health checks with a failover routing policy to the second Region.
Promote the secondary to primary as needed.

-   [ ] Deploying the application tiers in a second Region provides redundancy.
-   [ ] Aurora global database ensures database replication across Regions.
-   [ ] Route 53 health checks and failover routing enable automatic failover.
-   [ ] Promoting the secondary to primary allows for controlled failover.

<hr> Why are the other answers wrong? <hr>

-   [ ] A.
    Extending Auto Scaling groups across Regions is not the most efficient way to achieve regional redundancy.
-   [ ] B.
    Aurora Replicas are for read scaling, not for failover with minimal downtime.
    Manual promotion adds complexity.
-   [ ] C.
    DMS is for database migration, not for continuous replication for failover.

Therefore, Option D provides the most fault-tolerant solution.

</details>
<details>
  <summary>Question 528</summary>

A data analytics company wants to migrate its batch processing system to AWS.
The company receives thousands of small data files periodically during the day through FTP.
An on-premises batch job processes the data files overnight.
However, the batch job takes hours to finish running.
The company wants the AWS solution to process incoming data files as soon as possible with minimal changes to the FTP clients that send the files.
The solution must delete the incoming data files after the files have been processed successfully.
Processing for each file needs to take 3-8 minutes.
Which solution will meet these requirements in the MOST operationally efficient way?

-   [ ] A.
    Use an Amazon EC2 instance that runs an FTP server to store incoming files as objects in Amazon S3 Glacier Flexible Retrieval.
    Configure a job queue in AWS Batch.
    Use Amazon EventBridge rules to invoke the job to process the objects nightly from S3 Glacier Flexible Retrieval.
    Delete the objects after the job has processed the objects.
-   [ ] B.
    Use an Amazon EC2 instance that runs an FTP server to store incoming files on an Amazon Elastic Block Store (Amazon EBS) volume.
    Configure a job queue in AWS Batch.
    Use Amazon EventBridge rules to invoke the job to process the files nightly from the EBS volume.
    Delete the files after the job has processed the files.
-   [ ] C.
    Use AWS Transfer Family to create an FTP server to store incoming files on an Amazon Elastic Block Store (Amazon EBS) volume.
    Configure a job queue in AWS Batch.
    Use an Amazon S3 event notification when each file arrives to invoke the job in AWS Batch.
    Delete the files after the job has processed the files.
-   [ ] D.
    Use AWS Transfer Family to create an FTP server to store incoming files in Amazon S3 Standard.
    Create an AWS Lambda function to process the files and to delete the files after they are processed.
    Use an S3 event notification to invoke the Lambda function when the files arrive.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D.
    Use AWS Transfer Family to create an FTP server to store incoming files in Amazon S3 Standard.
    Create an AWS Lambda function to process the files and to delete the files after they are processed.
    Use an S3 event notification to invoke the Lambda function when the files arrive.

Why these are the correct answers:

D.
Use AWS Transfer Family to create an FTP server to store incoming files in Amazon S3 Standard.
Create an AWS Lambda function to process the files and to delete the files after they are processed.
Use an S3 event notification to invoke the Lambda function when the files arrive.

-   [ ] AWS Transfer Family provides a managed FTP server.
-   [ ] Lambda functions can process files immediately upon arrival in S3.
-   [ ] S3 event notifications trigger Lambda, enabling near real-time processing.
-   [ ] Lambda can delete files after processing.

<hr> Why are the other answers wrong? <hr>

-   [ ] A and B.
    Using EC2 for FTP servers requires more management.
    AWS Batch is for batch processing, not for immediate file processing.
    S3 Glacier is for archival, not for immediate processing.
-   [ ] C.
    Using EBS for FTP storage adds complexity.
    Batch is not suitable for immediate processing.

Therefore, Option D is the most efficient solution.

</details>
<details>
  <summary>Question 529</summary>

A company is migrating its workloads to AWS. The company has transactional and sensitive data in its databases.
The company wants to use AWS Cloud solutions to increase security and reduce operational overhead for the databases.
Which solution will meet these requirements?

-   [ ] A.
    Migrate the databases to Amazon EC2.
    Use an AWS Key Management Service (AWS KMS) AWS managed key for encryption.
-   [ ] B.
    Migrate the databases to Amazon RDS Configure encryption at rest.
-   [ ] C.
    Migrate the data to Amazon S3 Use Amazon Macie for data security and protection
-   [ ] D.
    Migrate the database to Amazon RDS.
    Use Amazon CloudWatch Logs for data security and protection.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B.
    Migrate the databases to Amazon RDS Configure encryption at rest.

Why these are the correct answers:

B.
Migrate the databases to Amazon RDS Configure encryption at rest.

-   [ ] Amazon RDS is a managed database service that reduces operational overhead.
-   [ ] RDS supports encryption at rest, providing data security.

<hr> Why are the other answers wrong? <hr>

-   [ ] A.
    EC2 requires more management than RDS.
-   [ ] C.
    S3 is object storage, not suitable for transactional data.
    Macie is for data discovery, not database security.
-   [ ] D.
    CloudWatch Logs is for logging, not for database security.

Therefore, Option B is the most suitable solution.

</details>
<details>
  <summary>Question 530</summary>

A company has an online gaming application that has TCP and UDP multiplayer gaming capabilities.
The company uses Amazon Route 53 to point the application traffic to multiple Network Load Balancers (NLBs) in different AWS Regions.
The company needs to improve application performance and decrease latency for the online game in preparation for user growth.
Which solution will meet these requirements?

-   [ ] A.
    Add an Amazon CloudFront distribution in front of the NLBs.
    Increase the Cache-Control max- age parameter.
-   [ ] B.
    Replace the NLBs with Application Load Balancers (ALBs).
    Configure Route 53 to use latency- based routing.
-   [ ] C.
    Add AWS Global Accelerator in front of the NLBs.
    Configure a Global Accelerator endpoint to use the correct listener ports.
-   [ ] D.
    Add an Amazon API Gateway endpoint behind the NLBs.
    Enable API caching.
    Override method caching for the different stages.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C.
    Add AWS Global Accelerator in front of the NLBs.
    Configure a Global Accelerator endpoint to use the correct listener ports.

Why these are the correct answers:

C.
Add AWS Global Accelerator in front of the NLBs.
Configure a Global Accelerator endpoint to use the correct listener ports.

-   [ ] AWS Global Accelerator improves performance for global applications by routing traffic through AWS's global network.
-   [ ] It supports both TCP and UDP, which is essential for gaming applications.

<hr> Why are the other answers wrong? <hr>

-   [ ] A.
    CloudFront is a CDN and is not optimized for real-time, low-latency traffic like gaming.
-   [ ] B.
    ALBs are for HTTP/HTTPS traffic, not TCP/UDP.
    Route 53 latency-based routing does not provide the same performance improvements as Global Accelerator.
-   [ ] D.
    API Gateway is for API management, not for gaming traffic.

Therefore, Option C is the most suitable solution.

</details>

# AWS-SAA-PRACTICE-EXAM Questions 531-540

<details>
  <summary>Question 531</summary>

A company needs to integrate with a third-party data feed.
The data feed sends a webhook to notify an external service when new data is ready for consumption.
A developer wrote an AWS Lambda function to retrieve data when the company receives a webhook callback.
The developer must make the Lambda function available for the third party to call.
Which solution will meet these requirements with the MOST operational efficiency?

-   [ ] A.
    Create a function URL for the Lambda function.
    Provide the Lambda function URL to the third party for the webhook.
-   [ ] B.
    Deploy an Application Load Balancer (ALB) in front of the Lambda function.
    Provide the ALB URL to the third party for the webhook.
-   [ ] C.
    Create an Amazon Simple Notification Service (Amazon SNS) topic.
    Attach the topic to the Lambda function.
    Provide the public hostname of the SNS topic to the third party for the webhook.
-   [ ] D.
    Create an Amazon Simple Queue Service (Amazon SQS) queue.
    Attach the queue to the Lambda function.
    Provide the public hostname of the SQS queue to the third party for the webhook.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A.
    Create a function URL for the Lambda function.
    Provide the Lambda function URL to the third party for the webhook.

Why these are the correct answers:

A.
Create a function URL for the Lambda function.
Provide the Lambda function URL to the third party for the webhook.

-   [ ] Lambda function URLs provide a dedicated HTTP(S) endpoint for your Lambda function.
-   [ ] This is the simplest and most efficient way to allow a third party to trigger a Lambda function via a webhook.

<hr> Why are the other answers wrong? <hr>

-   [ ] B.
    An Application Load Balancer is designed for load balancing HTTP/HTTPS traffic to application servers, not for triggering Lambda functions directly from webhooks.
-   [ ] C.
    SNS is a publish/subscribe messaging service, not designed for direct webhook calls.
-   [ ] D.
    SQS is a message queuing service, not for direct webhook calls.

Therefore, Option A is the most operationally efficient solution.

</details>
<details>
  <summary>Question 532</summary>

A company has a workload in an AWS Region.
Customers connect to and access the workload by using an Amazon API Gateway REST API.
The company uses Amazon Route 53 as its DNS provider.
The company wants to provide individual and secure URLs for all customers.
Which combination of steps will meet these requirements with the MOST operational efficiency?
(Choose three.)

-   [ ] A.
    Register the required domain in a registrar.
    Create a wildcard custom domain name in a Route 53 hosted zone and record in the zone that points to the API Gateway endpoint.
-   [ ] B.
    Request a wildcard certificate that matches the domains in AWS Certificate Manager (ACM) in a different Region.
-   [ ] C.
    Create hosted zones for each customer as required in Route 53.
    Create zone records that point to the API Gateway endpoint.
-   [ ] D.
    Request a wildcard certificate that matches the custom domain name in AWS Certificate Manager (ACM) in the same Region.
-   [ ] E.
    Create multiple API endpoints for each customer in API Gateway.
-   [ ] F.
    Create a custom domain name in API Gateway for the REST API.
    Import the certificate from AWS Certificate Manager (ACM).

</details>

<details>
  <summary>Answer</summary>

-   [ ] A.
    Register the required domain in a registrar.
    Create a wildcard custom domain name in a Route 53 hosted zone and record in the zone that points to the API Gateway endpoint.
-   [ ] D.
    Request a wildcard certificate that matches the custom domain name in AWS Certificate Manager (ACM) in the same Region.
-   [ ] F.
    Create a custom domain name in API Gateway for the REST API.
    Import the certificate from AWS Certificate Manager (ACM).

Why these are the correct answers:

A.
Register the required domain in a registrar.
Create a wildcard custom domain name in a Route 53 hosted zone and record in the zone that points to the API Gateway endpoint.

-   [ ] Registering a domain is necessary for creating custom URLs.
-   [ ] A wildcard custom domain and Route 53 record simplify the management of multiple subdomains.

D.
Request a wildcard certificate that matches the custom domain name in AWS Certificate Manager (ACM) in the same Region.

-   [ ] ACM certificates are required for secure (HTTPS) custom domains.
-   [ ] Wildcard certificates cover multiple subdomains.
-   [ ] ACM certificates must be in the same Region as the API Gateway.

F.
Create a custom domain name in API Gateway for the REST API.
Import the certificate from AWS Certificate Manager (ACM).

-   [ ] API Gateway custom domain names allow for user-friendly URLs.
-   [ ] ACM certificates are used to secure these custom domains.

<hr> Why are the other answers wrong? <hr>

-   [ ] B.
    ACM certificates must be in the same Region as the API Gateway.
-   [ ] C.
    Creating separate hosted zones for each customer adds significant operational overhead.
-   [ ] E.
    Creating multiple API endpoints is not necessary for providing individual URLs.

Therefore, Options A, D, and F are the correct steps.

</details>
<details>
  <summary>Question 533</summary>

A company stores data in Amazon S3.
According to regulations, the data must not contain personally identifiable information (PII).
The company recently discovered that S3 buckets have some objects that contain PII.
The company needs to automatically detect PII in S3 buckets and to notify the company's security team.
Which solution will meet these requirements?

-   [ ] A.
    Use Amazon Macie.
    Create an Amazon EventBridge rule to filter the SensitiveData event type from Macie findings and to send an Amazon Simple Notification Service (Amazon SNS) notification to the security team.
-   [ ] B.
    Use Amazon GuardDuty.
    Create an Amazon EventBridge rule to filter the CRITICAL event type from GuardDuty findings and to send an Amazon Simple Notification Service (Amazon SNS) notification to the security team.
-   [ ] C.
    Use Amazon Macie.
    Create an Amazon EventBridge rule to filter the SensitiveData:S3Object/Personal event type from Macie findings and to send an Amazon Simple Queue Service (Amazon SQS) notification to the security team.
-   [ ] D.
    Use Amazon GuardDuty.
    Create an Amazon EventBridge rule to filter the CRITICAL event type from GuardDuty findings and to send an Amazon Simple Queue Service (Amazon SQS) notification to the security team.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A.
    Use Amazon Macie.
    Create an Amazon EventBridge rule to filter the SensitiveData event type from Macie findings and to send an Amazon Simple Notification Service (Amazon SNS) notification to the security team.

Why these are the correct answers:

A.
Use Amazon Macie.
Create an Amazon EventBridge rule to filter the SensitiveData event type from Macie findings and to send an Amazon Simple Notification Service (Amazon SNS) notification to the security team.

-   [ ] Amazon Macie is designed to discover and protect sensitive data in S3.
-   [ ] EventBridge allows for automated event handling and notifications.
-   [ ] SNS provides a simple way to notify the security team.

<hr> Why are the other answers wrong? <hr>

-   [ ] B and D.
    Amazon GuardDuty is a threat detection service, not for PII detection.
    CRITICAL event types are not related to PII detection.
-   [ ] C.
    While Macie is correct, SQS is a queuing service and not typically used for immediate notifications to a security team.
    SNS is more suitable for this purpose.
    The specific event type filter might not be universally applicable.

Therefore, Option A is the most appropriate solution.

</details>
<details>
  <summary>Question 534</summary>

A company wants to build a logging solution for its multiple AWS accounts.
The company currently stores the logs from all accounts in a centralized account.
The company has created an Amazon S3 bucket in the centralized account to store the VPC flow logs and AWS CloudTrail logs.
All logs must be highly available for 30 days for frequent analysis, retained for an additional 60 days for backup purposes, and deleted 90 days after creation.
Which solution will meet these requirements MOST cost-effectively?

-   [ ] A.
    Transition objects to the S3 Standard storage class 30 days after creation.
    Write an expiration action that directs Amazon S3 to delete objects after 90 days.
-   [ ] B.
    Transition objects to the S3 Standard-Infrequent Access (S3 Standard-IA) storage class 30 days after creation.
    Move all objects to the S3 Glacier Flexible Retrieval storage class after 90 days.
    Write an expiration action that directs Amazon S3 to delete objects after 90 days.
-   [ ] C.
    Transition objects to the S3 Glacier Flexible Retrieval storage class 30 days after creation.
    Write an expiration action that directs Amazon S3 to delete objects after 90 days.
-   [ ] D.
    Transition objects to the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class 30 days after creation.
    Move all objects to the S3 Glacier Flexible Retrieval storage class after 90 days.
    Write an expiration action that directs Amazon S3 to delete objects after 90 days.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C.
    Transition objects to the S3 Glacier Flexible Retrieval storage class 30 days after creation.
    Write an expiration action that directs Amazon S3 to delete objects after 90 days.

Why these are the correct answers:

C.
Transition objects to the S3 Glacier Flexible Retrieval storage class 30 days after creation.
Write an expiration action that directs Amazon S3 to delete objects after 90 days.

-   [ ] S3 Glacier Flexible Retrieval is cost-effective for data that needs to be retained for backup purposes but is not frequently accessed.
-   [ ] S3 Lifecycle policies automate the transition and deletion of objects.

<hr> Why are the other answers wrong? <hr>

-   [ ] A.
    S3 Standard is more expensive than Glacier for long-term backup.
-   [ ] B and D.
    S3 Standard-IA and S3 One Zone-IA are more expensive than Glacier for long-term backup.
    Moving data to Glacier after 90 days is unnecessary since the requirement is to delete after 90 days.

Therefore, Option C is the most cost-effective solution.

</details>
<details>
  <summary>Question 535</summary>

A company is building an Amazon Elastic Kubernetes Service (Amazon EKS) cluster for its workloads.
All secrets that are stored in Amazon EKS must be encrypted in the Kubernetes etcd key-value store.
Which solution will meet these requirements?

-   [ ] A.
    Create a new AWS Key Management Service (AWS KMS) key.
    Use AWS Secrets Manager to manage, rotate, and store all secrets in Amazon EKS.
-   [ ] B.
    Create a new AWS Key Management Service (AWS KMS) key.
    Enable Amazon EKS KMS secrets encryption on the Amazon EKS cluster.
-   [ ] C.
    Create the Amazon EKS cluster with default options.
    Use the Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) driver as an add-on.
-   [ ] D.
    Create a new AWS Key Management Service (AWS KMS) key with the alias/aws/ebs alias.
    Enable default Amazon Elastic Block Store (Amazon EBS) volume encryption for the account.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B.
    Create a new AWS Key Management Service (AWS KMS) key.
    Enable Amazon EKS KMS secrets encryption on the Amazon EKS cluster.

Why these are the correct answers:

B.
Create a new AWS Key Management Service (AWS KMS) key.
Enable Amazon EKS KMS secrets encryption on the Amazon EKS cluster.

-   [ ] Amazon EKS supports encrypting Kubernetes secrets at rest in the etcd key-value store using KMS keys.
-   [ ] This is the most direct and secure way to meet the requirement.

<hr> Why are the other answers wrong? <hr>

-   [ ] A.
    Secrets Manager is for storing and managing secrets, not for encrypting secrets within the EKS cluster's etcd store.
-   [ ] C.
    The EBS CSI driver is for managing EBS volumes, not for encrypting etcd secrets.
-   [ ] D.
    EBS volume encryption is for encrypting volumes, not etcd secrets.
    The `alias/aws/ebs` alias is for EBS encryption.

Therefore, Option B is the correct solution.

</details>
<details>
  <summary>Question 536</summary>

A company wants to provide data scientists with near real-time read-only access to the company's production Amazon RDS for PostgreSQL database.
The database is currently configured as a Single-AZ database.
The data scientists use complex queries that will not affect the production database.
The company needs a solution that is highly available.
Which solution will meet these requirements MOST cost-effectively?

-   [ ] A.
    Scale the existing production database in a maintenance window to provide enough power for the data scientists.
-   [ ] B.
    Change the setup from a Single-AZ to a Multi-AZ instance deployment with a larger secondary standby instance.
    Provide the data scientists access to the secondary instance.
-   [ ] C.
    Change the setup from a Single-AZ to a Multi-AZ instance deployment.
    Provide two additional read replicas for the data scientists.
-   [ ] D.
    Change the setup from a Single-AZ to a Multi-AZ cluster deployment with two readable standby instances.
    Provide read endpoints to the data scientists.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D.
    Change the setup from a Single-AZ to a Multi-AZ cluster deployment with two readable standby instances.
    Provide read endpoints to the data scientists.

Why these are the correct answers:

D.
Change the setup from a Single-AZ to a Multi-AZ cluster deployment with two readable standby instances.
Provide read endpoints to the data scientists.

-   [ ] Aurora PostgreSQL Multi-AZ cluster provides high availability.
-   [ ] Readable standby instances allow for read-only access without impacting the primary instance.
-   [ ] This is more cost-effective than provisioning larger instances or additional clusters.

<hr> Why are the other answers wrong? <hr>

-   [ ] A.
    Scaling the existing database can impact performance and availability.
-   [ ] B and C.
    Multi-AZ instance deployments are for high availability, not for scaling reads.

Therefore, Option D is the most cost-effective and suitable solution.

</details>
<details>
  <summary>Question 537</summary>

A company runs a three-tier web application in the AWS Cloud that operates across three Availability Zones.
The application architecture has an Application Load Balancer, an Amazon EC2 web server that hosts user session states, and a MySQL database that runs on an EC2 instance.
The company expects sudden increases in application traffic.
The company wants to be able to scale to meet future application capacity demands and to ensure high availability across all three Availability Zones.
Which solution will meet these requirements?

-   [ ] A.
    Migrate the MySQL database to Amazon RDS for MySQL with a Multi-AZ DB cluster deployment.
    Use Amazon ElastiCache for Redis with high availability to store session data and to cache reads.
    Migrate the web server to an Auto Scaling group that is in three Availability Zones.
-   [ ] B.
    Migrate the MySQL database to Amazon RDS for MySQL with a Multi-AZ DB cluster deployment.
    Use Amazon ElastiCache for Memcached with high availability to store session data and to cache reads.
    Migrate the web server to an Auto Scaling group that is in three Availability Zones.
-   [ ] C.
    Migrate the MySQL database to Amazon DynamoDB Use DynamoDB Accelerator (DAX) to cache reads.
    Store the session data in DynamoDB.
    Migrate the web server to an Auto Scaling group that is in three Availability Zones.
-   [ ] D.
    Migrate the MySQL database to Amazon RDS for MySQL in a single Availability Zone.
    Use Amazon ElastiCache for Redis with high availability to store session data and to cache reads.
    Migrate the web server to an Auto Scaling group that is in three Availability Zones.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A.
    Migrate the MySQL database to Amazon RDS for MySQL with a Multi-AZ DB cluster deployment.
    Use Amazon ElastiCache for Redis with high availability to store session data and to cache reads.
    Migrate the web server to an Auto Scaling group that is in three Availability Zones.

Why these are the correct answers:

A.
Migrate the MySQL database to Amazon RDS for MySQL with a Multi-AZ DB cluster deployment.
Use Amazon ElastiCache for Redis with high availability to store session data and to cache reads.
Migrate the web server to an Auto Scaling group that is in three Availability Zones.

-   [ ] RDS Multi-AZ DB cluster provides high availability for the database.
-   [ ] ElastiCache for Redis is suitable for session data storage and caching.
-   [ ] Auto Scaling groups in multiple AZs ensure scalability and availability for the web tier.

<hr> Why are the other answers wrong? <hr>

-   [ ] B.
    Memcached does not support the same level of data persistence and features as Redis.
-   [ ] C.
    DynamoDB is a NoSQL database and may require significant application changes.
-   [ ] D.
    A single-AZ RDS deployment does not provide high availability.

Therefore, Option A is the correct solution.

</details>
<details>
  <summary>Question 538</summary>

A global video streaming company uses Amazon CloudFront as a content distribution network (CDN).
The company wants to roll out content in a phased manner across multiple countries.
The company needs to ensure that viewers who are outside the countries to which the company rolls out content are not able to view the content.
Which solution will meet these requirements?

-   [ ] A.
    Add geographic restrictions to the content in CloudFront by using an allow list.
    Set up a custom error message.
-   [ ] B.
    Set up a new URL tor restricted content.
    Authorize access by using a signed URL and cookies.
    Set up a custom error message.
-   [ ] C.
    Encrypt the data for the content that the company distributes.
    Set up a custom error message.
-   [ ] D.
    Create a new URL for restricted content.
    Set up a time-restricted access policy for signed URLs.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A.
    Add geographic restrictions to the content in CloudFront by using an allow list.
    Set up a custom error message.

Why these are the correct answers:

A.
Add geographic restrictions to the content in CloudFront by using an allow list.
Set up a custom error message.

-   [ ] CloudFront geographic restrictions (geo-restriction) allow you to control the countries in which your content is distributed.
-   [ ] An allow list specifies the countries where the content is allowed.
-   [ ] A custom error message provides a user-friendly response to blocked viewers.

<hr> Why are the other answers wrong? <hr>

-   [ ] B.
    Signed URLs and cookies are for controlling access to individual files, not for country-based restrictions.
-   [ ] C.
    Encryption does not prevent access based on location.
-   [ ] D.
    Time-restricted access policies are for controlling access duration, not location.

Therefore, Option A is the correct solution.

</details>
<details>
  <summary>Question 539</summary>

A company wants to use the AWS Cloud to improve its on-premises disaster recovery (DR) configuration.
The company's core production business application uses Microsoft SQL Server Standard, which runs on a virtual machine (VM).
The application has a recovery point objective (RPO) of 30 seconds or fewer and a recovery time objective (RTO) of 60 minutes.
The DR solution needs to minimize costs wherever possible.
Which solution will meet these requirements?

-   [ ] A.
    Configure a multi-site active/active setup between the on-premises server and AWS by using Microsoft SQL Server Enterprise with Always On availability groups.
-   [ ] B.
    Configure a warm standby Amazon RDS for SQL Server database on AWS.
    Configure AWS Database Migration Service (AWS DMS) to use change data capture (CDC).
-   [ ] C.
    Use AWS Elastic Disaster Recovery configured to replicate disk changes to AWS as a pilot light.
-   [ ] D.
    Use third-party backup software to capture backups every night.
    Store a secondary set of backups in Amazon S3.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B.
    Configure a warm standby Amazon RDS for SQL Server database on AWS.
    Configure AWS Database Migration Service (AWS DMS) to use change data capture (CDC).

Why these are the correct answers:

B.
Configure a warm standby Amazon RDS for SQL Server database on AWS.
Configure AWS Database Migration Service (AWS DMS) to use change data capture (CDC).

-   [ ] Amazon RDS provides a managed database service, reducing operational overhead.
-   [ ] DMS with CDC replicates changes in near real-time, meeting the RPO requirement.
-   [ ] A warm standby setup allows for quicker recovery compared to a cold standby.

<hr> Why are the other answers wrong? <hr>

-   [ ] A.
    SQL Server Enterprise and Always On availability groups are more expensive than RDS for SQL Server Standard.
-   [ ] C.
    Elastic Disaster Recovery is designed for broader disaster recovery of EC2 instances, not optimized for database replication.
-   [ ] D.
    Third-party backups and S3 storage do not meet the RPO and RTO requirements.

Therefore, Option B is the most cost-effective solution that meets the requirements.

</details>
<details>
  <summary>Question 540</summary>

A company has an on-premises server that uses an Oracle database to process and store customer information.
The company wants to use an AWS database service to achieve higher availability and to improve application performance.
The company also wants to offload reporting from its primary database system.
Which solution will meet these requirements in the MOST operationally efficient way?

-   [ ] A.
    Use AWS Database Migration Service (AWS DMS) to create an Amazon RDS DB instance in multiple AWS Regions.
    Point the reporting functions toward a separate DB instance from the primary DB instance.
-   [ ] B.
    Use Amazon RDS in a Single-AZ deployment to create an Oracle database.
    Create a read replica in the same zone as the primary DB instance.
    Direct the reporting functions to the read replica.
-   [ ] C.
    Use Amazon RDS deployed in a Multi-AZ cluster deployment to create an Oracle database.
    Direct the reporting functions to use the reader instance in the cluster deployment.
-   [ ] D.
    Use Amazon RDS deployed in a Multi-AZ instance deployment to create an Amazon Aurora database.
    Direct the reporting functions to the reader instances.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D.
    Use Amazon RDS deployed in a Multi-AZ instance deployment to create an Amazon Aurora database.
    Direct the reporting functions to the reader instances.

Why these are the correct answers:

D.
Use Amazon RDS deployed in a Multi-AZ instance deployment to create an Amazon Aurora database.
Direct the reporting functions to the reader instances.

-   [ ] Amazon Aurora is a managed database service that provides high availability and performance.
-   [ ] Multi-AZ deployments enhance availability.
-   [ ] Reader instances allow for offloading reporting workload.

<hr> Why are the other answers wrong? <hr>

-   [ ] A.
    DMS is for database migration, not for ongoing database operations.
-   [ ] B.
    Single-AZ deployments do not provide high availability.
-   [ ] C.
    Multi-AZ cluster deployments are not a standard term for Aurora.

Therefore, Option D is the most operationally efficient solution.

</details>

# AWS-SAA-PRACTICE-EXAM Questions 541-550

<details>
  <summary>Question 541</summary>

A company wants to build a web application on AWS. Client access requests to the website are not predictable and can be idle for a long time. Only customers who have paid a subscription fee can have the ability to sign in and use the web application. Which combination of steps will meet these requirements MOST cost-effectively? (Choose three.)

- [ ] A. Create an AWS Lambda function to retrieve user information from Amazon DynamoDB. Create an Amazon API Gateway endpoint to accept RESTful APIs. Send the API calls to the Lambda function.
- [ ] B. Create an Amazon Elastic Container Service (Amazon ECS) service behind an Application Load Balancer to retrieve user information from Amazon RDS. Create an Amazon API Gateway endpoint to accept RESTful APIs. Send the API calls to the Lambda function.
- [ ] C. Create an Amazon Cognito user pool to authenticate users.
- [ ] D. Create an Amazon Cognito identity pool to authenticate users.
- [ ] E. Use AWS Amplify to serve the frontend web content with HTML, CSS, and JS. Use an integrated Amazon CloudFront configuration.
- [ ] F. Use Amazon S3 static web hosting with PHP, CSS, and JS. Use Amazon CloudFront to serve the frontend web content.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Create an AWS Lambda function to retrieve user information from Amazon DynamoDB. Create an Amazon API Gateway endpoint to accept RESTful APIs. Send the API calls to the Lambda function.
- [ ] C. Create an Amazon Cognito user pool to authenticate users.
- [ ] E. Use AWS Amplify to serve the frontend web content with HTML, CSS, and JS. Use an integrated Amazon CloudFront configuration.

Why these are the correct answers:

A. Create an AWS Lambda function to retrieve user information from Amazon DynamoDB. Create an Amazon API Gateway endpoint to accept RESTful APIs. Send the API calls to the Lambda function.

-   [ ] AWS Lambda is a cost-effective compute service that scales automatically and charges only for compute time consumed. This aligns well with unpredictable access patterns and idle times.
-   [ ] Amazon API Gateway allows you to create RESTful APIs that can trigger Lambda functions, providing a scalable and managed way to access backend logic.
-   [ ] Using Lambda and API Gateway together is suitable for building web applications with dynamic backend processing.

C. Create an Amazon Cognito user pool to authenticate users.

-   [ ] Amazon Cognito user pools provide user sign-up, sign-in, and access control. They handle authentication, which is essential for ensuring only paying customers can access the application.
-   [ ] Cognito integrates well with API Gateway and other AWS services, simplifying the implementation of user authentication and authorization.

E. Use AWS Amplify to serve the frontend web content with HTML, CSS, and JS. Use an integrated Amazon CloudFront configuration.

-   [ ] AWS Amplify simplifies the development and deployment of frontend web applications. It allows you to host static content (HTML, CSS, JS) and integrate with backend services.
-   [ ] Amazon CloudFront is a content delivery network (CDN) that improves performance by caching content and reducing latency. It also offers scalability and security benefits.
-   [ ] Using Amplify with CloudFront is a cost-effective way to host and deliver static web content.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. While ECS and Application Load Balancer can handle web traffic, they are generally more suitable for applications with consistent traffic. Lambda is more cost-effective for unpredictable and idle traffic.
-   [ ] D. Amazon Cognito identity pools provide temporary AWS credentials for users to access AWS services directly. They are typically used for authorizing access to AWS resources, not for user authentication in web applications. User pools (Option C) are the correct choice for authentication.
-   [ ] F. While S3 can host static web content, it doesn't natively support PHP. AWS Amplify is a more suitable choice for hosting modern web applications with static content and simplified deployment.

Therefore, Options A, C, and E provide the most cost-effective solution for a web application with unpredictable traffic and subscription-based access.

</details>

<details>
  <summary>Question 542</summary>

A media company uses an Amazon CloudFront distribution to deliver content over the internet. The company wants only premium customers to have access to the media streams and file content. The company stores all content in an Amazon S3 bucket. The company also delivers content on demand to customers for a specific purpose, such as movie rentals or music downloads. Which solution will meet these requirements?

-   [ ] A. Generate and provide S3 signed cookies to premium customers.
-   [ ] B. Generate and provide CloudFront signed URLs to premium customers.
-   [ ] C. Use origin access control (OAC) to limit the access of non-premium customers.
-   [ ] D. Generate and activate field-level encryption to block non-premium customers.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Generate and provide CloudFront signed URLs to premium customers.

Why this is the correct answer:

B. Generate and provide CloudFront signed URLs to premium customers.

-   [ ] CloudFront signed URLs allow you to control access to your content. You can generate URLs that are valid for a limited time, or for specific users, giving you fine-grained control over who can access your media streams and files.
-   [ ] This method is ideal for on-demand content delivery, as it allows you to grant access to individual customers for specific content and durations, such as for movie rentals or music downloads.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. S3 signed cookies are used to control access to S3 buckets directly, not CloudFront distributions. While you can use S3 signed URLs, CloudFront signed URLs offer better integration with CloudFront's caching and distribution capabilities.
-   [ ] C. Origin Access Control (OAC) is used to secure the communication between CloudFront and the origin (in this case, S3). It doesn't provide a mechanism to grant or restrict access to individual users or premium customers.
-   [ ] D. Field-level encryption is used to encrypt specific data fields within a request, not to control access to content based on user privileges. It's designed to protect sensitive data during transmission and storage.

Therefore, Option B is the most suitable solution for controlling access to media streams and files for premium customers using CloudFront.

</details>
<details>
  <summary>Question 543</summary>

A company runs Amazon EC2 instances in multiple AWS accounts that are individually billed. The company recently purchased a Savings Plan. Because of changes in the company's business requirements, the company has decommissioned a large number of EC2 instances. The company wants to use its Savings Plan discounts on its other AWS accounts. Which combination of steps will meet these requirements? (Choose two.)

-   [ ] A. From the AWS Account Management Console of the management account, turn on discount sharing from the billing preferences section.
-   [ ] B. From the AWS Account Management Console of the account that purchased the existing Savings Plan, turn on discount sharing from the billing preferences section. Include all accounts.
-   [ ] C. From the AWS Organizations management account, use AWS Resource Access Manager (AWS RAM) to share the Savings Plan with other accounts.
-   [ ] D. Create an organization in AWS Organizations in a new payer account. Invite the other AWS accounts to join the organization from the management account.
-   [ ] E. Create an organization in AWS Organizations in the existing AWS account with the existing EC2 instances and Savings Plan. Invite the other AWS accounts to join the organization from the management account.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. From the AWS Account Management Console of the management account, turn on discount sharing from the billing preferences section.
-   [ ] E. Create an organization in AWS Organizations in the existing AWS account with the existing EC2 instances and Savings Plan. Invite the other AWS accounts to join the organization from the management account.

Why these are the correct answers:

A. From the AWS Account Management Console of the management account, turn on discount sharing from the billing preferences section.

-   [ ] To share Savings Plans across multiple accounts, you need to enable Savings Plan sharing. This is done within the AWS Billing and Cost Management console.
-   [ ] The management account (or payer account) is where you configure the settings to allow other accounts to benefit from the Savings Plan discounts.

E. Create an organization in AWS Organizations in the existing AWS account with the existing EC2 instances and Savings Plan. Invite the other AWS accounts to join the organization from the management account.

-   [ ] AWS Organizations allows you to centrally manage billing, control access, and share resources across multiple AWS accounts.
-   [ ] To effectively share Savings Plans, you need to have your accounts organized within AWS Organizations. The existing account with the Savings Plan can become the management account for the organization.
-   [ ] By inviting other accounts to join the organization, you enable the Savings Plan discounts to be applied to their EC2 usage.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. Discount sharing is not enabled from the AWS Account Management Console of the account that purchased the Savings Plan. It's managed from the management account in AWS Billing and Cost Management.
-   [ ] C. AWS Resource Access Manager (AWS RAM) is used for sharing specific AWS resources, such as subnets, with other AWS accounts. It is not used for sharing Savings Plans.
-   [ ] D. Creating a new payer account and migrating accounts is an unnecessary step. It's simpler to use the existing account with the Savings Plan as the management account for AWS Organizations.

Therefore, Options A and E are the correct steps to share Savings Plan discounts across multiple AWS accounts.

</details>
<details>
  <summary>Question 544</summary>

A retail company uses a regional Amazon API Gateway API for its public REST APIs. The API Gateway endpoint is a custom domain name that points to an Amazon Route 53 alias record. A solutions architect needs to create a solution that has minimal effects on customers and minimal data loss to release the new version of APIs. Which solution will meet these requirements?

-   [ ] A. Create a canary release deployment stage for API Gateway. Deploy the latest API version. Point an appropriate percentage of traffic to the canary stage. After API verification, promote the canary stage to the production stage.
-   [ ] B. Create a new API Gateway endpoint with a new version of the API in OpenAPI YAML file format. Use the import-to-update operation in merge mode into the API in API Gateway. Deploy the new version of the API to the production stage.
-   [ ] C. Create a new API Gateway endpoint with a new version of the API in OpenAPI JSON file format. Use the import-to-update operation in overwrite mode into the API in API Gateway. Deploy the new version of the API to the production stage.
-   [ ] D. Create a new API Gateway endpoint with new versions of the API definitions. Create a custom domain name for the new API Gateway API. Point the Route 53 alias record to the new API Gateway API custom domain name.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Create a canary release deployment stage for API Gateway. Deploy the latest API version. Point an appropriate percentage of traffic to the canary stage. After API verification, promote the canary stage to the production stage.

Why this is the correct answer:

A. Create a canary release deployment stage for API Gateway. Deploy the latest API version. Point an appropriate percentage of traffic to the canary stage. After API verification, promote the canary stage to the production stage.

-   [ ] A canary release is a deployment strategy that releases a new version of software to a small subset of users before rolling it out to the entire user base.
-   [ ] In API Gateway, a canary deployment stage allows you to deploy the new API version to a small percentage of traffic, while the majority of traffic continues to use the existing version.
-   [ ] This minimizes the impact on customers and allows you to test the new API version in a production environment.
-   [ ] After verifying the new API version, you can promote the canary stage to the production stage, making it available to all users.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. While importing a new API version in merge mode can update the API, it doesn't provide a controlled rollout like a canary release. Deploying directly to the production stage can lead to issues if the new version has errors.
-   [ ] C. Overwriting the existing API version with the new version carries a higher risk of disrupting service for all users. If there are any issues with the new version, all users will be affected.
-   [ ] D. Creating a new API Gateway endpoint and updating the Route 53 record is a more disruptive approach. It requires DNS changes, which can take time to propagate, and it involves switching all traffic at once, increasing the risk of impacting all users.

Therefore, Option A is the most suitable solution for releasing a new API version with minimal impact on customers and minimal data loss.

</details>
<details>
  <summary>Question 545</summary>

A company wants to direct its users to a backup static error page if the company's primary website is unavailable. The primary website's DNS records are hosted in Amazon Route 53. The domain is pointing to an Application Load Balancer (ALB). The company needs a solution that minimizes changes and infrastructure overhead. Which solution will meet these requirements?

-   [ ] A. Update the Route 53 records to use a latency routing policy. Add a static error page that is hosted in an Amazon S3 bucket to the records so that the traffic is sent to the most responsive endpoints.
-   [ ] B. Set up a Route 53 active-passive failover configuration. Direct traffic to a static error page that is hosted in an Amazon S3 bucket when Route 53 health checks determine that the ALB endpoint is unhealthy.
-   [ ] C. Set up a Route 53 active-active configuration with the ALB and an Amazon EC2 instance that hosts a static error page as endpoints. Configure Route 53 to send requests to the instance only if the health checks fail for the ALB.
-   [ ] D. Update the Route 53 records to use a multivalue answer routing policy. Create a health check. Direct traffic to the website if the health check passes. Direct traffic to a static error page that is hosted in Amazon S3 if the health check does not pass.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Set up a Route 53 active-passive failover configuration. Direct traffic to a static error page that is hosted in an Amazon S3 bucket when Route 53 health checks determine that the ALB endpoint is unhealthy.

Why this is the correct answer:

B. Set up a Route 53 active-passive failover configuration. Direct traffic to a static error page that is hosted in an Amazon S3 bucket when Route 53 health checks determine that the ALB endpoint is unhealthy.

-   [ ] Route 53 active-passive failover configuration allows you to have a primary endpoint (the ALB) and a secondary endpoint (the S3 static error page).
-   [ ] Route 53 health checks monitor the health of the primary endpoint. If the health check fails (indicating unavailability), Route 53 automatically routes traffic to the secondary endpoint.
-   [ ] This solution minimizes changes, as it primarily involves configuring Route 53 and setting up a simple S3 static website for the error page. It also has minimal infrastructure overhead, as S3 is used for hosting the static page.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Latency routing policy routes traffic to the endpoint with the lowest latency. It doesn't provide a failover mechanism to redirect traffic to an error page when the primary website is unavailable.
-   [ ] C. Active-active configuration distributes traffic across all healthy endpoints. It doesn't provide a way to redirect all traffic to an error page only when the primary website is unavailable. Also, using an EC2 instance for a static error page adds unnecessary infrastructure overhead.
-   [ ] D. Multivalue answer routing policy returns multiple IP addresses for each query and doesn't guarantee failover. While health checks can be used, this policy is designed for load balancing, not for directing all traffic to a backup error page during an outage.

Therefore, Option B is the most suitable solution for directing users to a backup static error page with minimal changes and infrastructure overhead.

</details>

<details>
  <summary>Question 546</summary>

A recent analysis of a company's IT expenses highlights the need to reduce backup costs. The company's chief information officer wants to simplify the on-premises backup infrastructure and reduce costs by eliminating the use of physical backup tapes. The company must preserve the existing investment in the on-premises backup applications and workflows.

What should a solutions architect recommend?

-   [ ] A. Set up AWS Storage Gateway to connect with the backup applications using the NFS interface.
-   [ ] B. Set up an Amazon EFS file system that connects with the backup applications using the NFS interface.
-   [ ] C. Set up an Amazon EFS file system that connects with the backup applications using the iSCSI interface.
-   [ ] D. Set up AWS Storage Gateway to connect with the backup applications using the iSCSI-virtual tape library (VTL) interface.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Set up AWS Storage Gateway to connect with the backup applications using the iSCSI-virtual tape library (VTL) interface.

Why this is the correct answer:

D. Set up AWS Storage Gateway to connect with the backup applications using the iSCSI-virtual tape library (VTL) interface.

-   [ ] AWS Storage Gateway's Virtual Tape Library (VTL) interface allows you to present cloud-based storage to on-premises backup applications as virtual tape libraries.
-   [ ] This enables you to replace physical tape infrastructure with a more cost-effective and scalable cloud-based solution while preserving your existing investment in backup applications and workflows.
-   [ ] The VTL interface integrates seamlessly with existing backup software, minimizing the need for changes to backup processes.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Using the NFS interface with AWS Storage Gateway is suitable for file shares, not for tape backups. It would require significant changes to the backup applications.
-   [ ] B. Amazon EFS is a file storage service for use with EC2 instances. It is not designed to replace tape backups or integrate with existing backup applications.
-   [ ] C. Similar to Option B, Amazon EFS with the iSCSI interface is not designed for tape backup replacement and would require significant changes to existing backup workflows.

Therefore, Option D is the most appropriate solution for reducing backup costs and simplifying infrastructure while preserving existing backup applications.
</details>
<details>
  <summary>Question 547</summary>

A company has data collection sensors at different locations. The data collection sensors stream a high volume of data to the company. The company wants to design a platform on AWS to ingest and process high-volume streaming data. The solution must be scalable and support data collection in near real time. The company must store the data in Amazon S3 for future reporting. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Use Amazon Kinesis Data Firehose to deliver streaming data to Amazon S3.
-   [ ] B. Use AWS Glue to deliver streaming data to Amazon S3.
-   [ ] C. Use AWS Lambda to deliver streaming data and store the data to Amazon S3.
-   [ ] D. Use AWS Database Migration Service (AWS DMS) to deliver streaming data to Amazon S3.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Use Amazon Kinesis Data Firehose to deliver streaming data to Amazon S3.

Why this is the correct answer:

A. Use Amazon Kinesis Data Firehose to deliver streaming data to Amazon S3.

-   [ ] Amazon Kinesis Data Firehose is designed to easily and reliably stream data to data lakes, data stores, and analytics services. It can capture, transform, and load streaming data into Amazon S3.
-   [ ] Kinesis Data Firehose is fully managed, which means it automatically scales to handle high volumes of data and requires minimal operational overhead.
-   [ ] It provides near real-time data ingestion and delivery, making it suitable for the company's requirements.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. AWS Glue is a fully managed extract, transform, and load (ETL) service. While it can move data, it is primarily designed for batch processing and ETL jobs, not for real-time streaming ingestion.
-   [ ] C. AWS Lambda can process streaming data, but using it to directly handle high-volume streams and deliver them to S3 would require significant custom code for scaling, buffering, and error handling, increasing operational overhead.
-   [ ] D. AWS Database Migration Service (AWS DMS) is designed for migrating databases to AWS. It is not suitable for ingesting and processing high-volume streaming data from sensors.

Therefore, Option A is the most operationally efficient solution for ingesting and processing high-volume streaming data from sensors.
</details>
<details>
  <summary>Question 548</summary>

A company has separate AWS accounts for its finance, data analytics, and development departments. Because of costs and security concerns, the company wants to control which services each AWS account can use. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Use AWS Systems Manager templates to control which AWS services each department can use.
-   [ ] B. Create organization units (OUs) for each department in AWS Organizations. Attach service control policies (SCPs) to the OUs.
-   [ ] C. Use AWS CloudFormation to automatically provision only the AWS services that each department can use.
-   [ ] D. Set up a list of products in AWS Service Catalog in the AWS accounts to manage and control the usage of specific AWS services.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Create organization units (OUs) for each department in AWS Organizations. Attach service control policies (SCPs) to the OUs.

Why this is the correct answer:

B. Create organization units (OUs) for each department in AWS Organizations. Attach service control policies (SCPs) to the OUs.

-   [ ] AWS Organizations allows you to centrally manage multiple AWS accounts.
-   [ ] Organization units (OUs) enable you to group accounts hierarchically.
-   [ ] Service control policies (SCPs) allow you to define AWS service permissions at the OU or organization level, providing centralized control over which services each account can use.
-   [ ] This approach minimizes operational overhead by enforcing controls at a higher level, rather than managing them individually in each account.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. AWS Systems Manager is designed for managing EC2 instances and other AWS resources, not for controlling which AWS services accounts can use.
-   [ ] C. AWS CloudFormation is an infrastructure-as-code service for provisioning AWS resources. While it can control what is provisioned, it does not provide ongoing control over which services an account can use.
-   [ ] D. AWS Service Catalog allows you to create and manage catalogs of approved AWS services. It helps in standardizing resource usage but does not provide the same level of preventive control as SCPs.

Therefore, Option B is the most operationally efficient solution for controlling which AWS services each department can use.

</details>

<details>
  <summary>Question 549</summary>

A company has created a multi-tier application for its ecommerce website. The website uses an Application Load Balancer that resides in the public subnets, a web tier in the public subnets, and a MySQL cluster hosted on Amazon EC2 instances in the private subnets. The MySQL database needs to retrieve product catalog and pricing information that is hosted on the internet by a third-party provider. A solutions architect must devise a strategy that maximizes security without increasing operational overhead.

What should the solutions architect do to meet these requirements?

-   [ ] A. Deploy a NAT instance in the VPC. Route all the internet-based traffic through the NAT instance.
-   [ ] B. Deploy a NAT gateway in the public subnets. Modify the private subnet route table to direct all internet-bound traffic to the NAT gateway.
-   [ ] C. Configure an internet gateway and attach it to the VPModify the private subnet route table to direct internet-bound traffic to the internet gateway.
-   [ ] D. Configure a virtual private gateway and attach it to the VPC. Modify the private subnet route table to direct internet-bound traffic to the virtual private gateway.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Deploy a NAT gateway in the public subnets. Modify the private subnet route table to direct all internet-bound traffic to the NAT gateway.

Why this is the correct answer:

B. Deploy a NAT gateway in the public subnets. Modify the private subnet route table to direct all internet-bound traffic to the NAT gateway.

-   [ ] NAT gateways allow instances in private subnets to connect to the internet while preventing the internet from initiating connections with those instances. This enhances security.
-   [ ] By placing the NAT gateway in the public subnet and routing traffic from the private subnets through it, the MySQL instances can retrieve information from the third-party provider without being directly exposed to the internet.
-   [ ] NAT gateways are managed by AWS, reducing operational overhead compared to managing NAT instances.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. NAT instances perform a similar function to NAT gateways, but they require more management, such as scaling and patching, which increases operational overhead.
-   [ ] C. An internet gateway allows direct communication between the VPC and the internet. This would expose the MySQL instances in the private subnet directly to the internet, which is a security risk.
-   [ ] D. A virtual private gateway is used to create a VPN connection between your VPC and an on-premises network. It is not used for enabling internet access for instances in private subnets.

Therefore, Option B provides the most secure and operationally efficient solution for allowing the MySQL instances to access the third-party provider.

</details>

<details>
  <summary>Question 550</summary>

A company is using AWS Key Management Service (AWS KMS) keys to encrypt AWS Lambda environment variables. A solutions architect needs to ensure that the required permissions are in place to decrypt and use the environment variables. Which steps must the solutions architect take to implement the correct permissions? (Choose two.)

-   [ ] A. Add AWS KMS permissions in the Lambda resource policy.
-   [ ] B. Add AWS KMS permissions in the Lambda execution role.
-   [ ] C. Add AWS KMS permissions in the Lambda function policy.
-   [ ] D. Allow the Lambda execution role in the AWS KMS key policy.
-   [ ] E. Allow the Lambda resource policy in the AWS KMS key policy.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Add AWS KMS permissions in the Lambda execution role.
-   [ ] D. Allow the Lambda execution role in the AWS KMS key policy.

Why these are the correct answers:

B. Add AWS KMS permissions in the Lambda execution role.

-   [ ] The Lambda execution role is an IAM role that grants the Lambda function permissions to access AWS services.
-   [ ] To allow the Lambda function to decrypt environment variables encrypted with KMS, you must add KMS permissions to this role.
-   [ ] This typically involves granting the `kms:Decrypt` permission.

D. Allow the Lambda execution role in the AWS KMS key policy.

-   [ ] The KMS key policy controls who can use the KMS key.
-   [ ] To allow the Lambda function to use the KMS key for decryption, you must allow the Lambda execution role in the KMS key policy.
-   [ ] This is done by adding a statement to the key policy that grants the execution role the necessary permissions.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Lambda resource policies are used to grant other AWS accounts or services permission to invoke the Lambda function. They are not used to grant the Lambda function permissions to access other AWS services.
-   [ ] C. Lambda function policies are the same as Lambda resource policies. They do not grant the Lambda function permissions to access other AWS services.
-   [ ] E. The Lambda resource policy does not need to be allowed in the KMS key policy. The execution role is what needs permission to use the KMS key.

Therefore, Options B and D are the correct steps to ensure the Lambda function has the necessary permissions to decrypt environment variables encrypted with KMS.

</details>





















