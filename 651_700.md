# AWS-SAA-PRACTICE-EXAM Questions 651-660

<details>
    <summary>Question 651</summary>

A company stores a large volume of image files in an Amazon S3 bucket. The images need to be readily available for the first 180 days. The images are infrequently accessed for the next 180 days. After 360 days, the images need to be archived but must be available instantly upon request. After 5 years, only auditors can access the images. The auditors must be able to retrieve the images within 12 hours. The images cannot be lost during this process. A developer will use S3 Standard storage for the first 180 days. The developer needs to configure an S3 Lifecycle rule. Which solution will meet these requirements MOST cost-effectively?

-   [ ] A. Transition the objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 180 days.
    S3 Glacier Instant Retrieval after 360 days, and S3 Glacier Deep Archive after 5 years.
-   [ ] B. Transition the objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 180 days.
    S3 Glacier Flexible Retrieval after 360 days, and S3 Glacier Deep Archive after 5 years.
-   [ ] C. Transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 180 days, S3 Glacier Instant Retrieval after 360 days, and S3 Glacier Deep Archive after 5 years.
-   [ ] D. Transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 180 days, S3 Glacier Flexible Retrieval after 360 days, and S3 Glacier Deep Archive after 5 years.

</details>

<details>
    <summary>Answer</summary>

-   [ ] C. Transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 180 days, S3 Glacier Instant Retrieval after 360 days, and S3 Glacier Deep Archive after 5 years.

Why these are the correct answers:

C. Transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 180 days, S3 Glacier Instant Retrieval after 360 days, and S3 Glacier Deep Archive after 5 years.

-   [ ] S3 Standard-IA is cost-effective for infrequently accessed data.
-   [ ] S3 Glacier Instant Retrieval provides instant access for archived data.
-   [ ] S3 Glacier Deep Archive is suitable for long-term archival with retrieval times within 12 hours.
-   [ ] This combination meets all the requirements in a cost-effective manner.

<hr> Why are the other answers wrong? <hr>

-   [ ] A and B. S3 One Zone-IA is cheaper but less resilient than S3 Standard-IA.
-   [ ] D. S3 Glacier Flexible Retrieval has retrieval times of several hours, not instant.

Therefore, Option C is the most cost-effective solution that meets the requirements.
</details>
<details>
    <summary>Question 652</summary>

A company has a large data workload that runs for 6 hours each day. The company cannot lose any data while the process is running. A solutions architect is designing an Amazon EMR cluster configuration to support this critical data workload. Which solution will meet these requirements MOST cost-effectively?

-   [ ] A. Configure a long-running cluster that runs the primary node and core nodes on On-Demand Instances and the task nodes on Spot Instances.
-   [ ] B. Configure a transient cluster that runs the primary node and core nodes on On-Demand Instances and the task nodes on Spot Instances.
-   [ ] C. Configure a transient cluster that runs the primary node on an On-Demand Instance and the core nodes and task nodes on Spot Instances.
-   [ ] D. Configure a long-running cluster that runs the primary node on an On-Demand Instance, the core nodes on Spot Instances, and the task nodes on Spot Instances.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Configure a transient cluster that runs the primary node and core nodes on On-Demand Instances and the task nodes on Spot Instances.

Why these are the correct answers:

B. Configure a transient cluster that runs the primary node and core nodes on On-Demand Instances and the task nodes on Spot Instances.

-   [ ] Transient clusters are cost-effective for short-lived workloads.
-   [ ] On-Demand Instances for primary and core nodes ensure data is not lost.
-   [ ] Spot Instances for task nodes reduce costs for compute capacity.

<hr> Why are the other answers wrong? <hr>

-   [ ] A and D. Long-running clusters are more expensive for daily 6-hour workloads.
-   [ ] C. Using Spot Instances for core nodes risks data loss.

Therefore, Option B is the most cost-effective solution for this scenario.
</details>
<details>
    <summary>Question 653</summary>

A company maintains an Amazon RDS database that maps users to cost centers. The company has accounts in an organization in AWS Organizations. The company needs a solution that will tag all resources that are created in a specific AWS account in the organization. The solution must tag each resource with the cost center ID of the user who created the resource. Which solution will meet these requirements?

-   [ ] A. Move the specific AWS account to a new organizational unit (OU) in Organizations from the management account.
    Create a service control policy (SCP) that requires all existing resources to have the correct cost center tag before the resources are created.
    Apply the SCP to the new OU.
-   [ ] B. Create an AWS Lambda function to tag the resources after the Lambda function looks up the appropriate cost center from the RDS database.
    Configure an Amazon EventBridge rule that reacts to AWS CloudTrail events to invoke the Lambda function.
-   [ ] C. Create an AWS CloudFormation stack to deploy an AWS Lambda function.
    Configure the Lambda function to look up the appropriate cost center from the RDS database and to tag resources.
    Create an Amazon EventBridge scheduled rule to invoke the CloudFormation stack.
-   [ ] D. Create an AWS Lambda function to tag the resources with a default value.
    Configure an Amazon EventBridge rule that reacts to AWS CloudTrail events to invoke the Lambda function when a resource is missing the cost center tag.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Create an AWS Lambda function to tag the resources after the Lambda function looks up the appropriate cost center from the RDS database.
    Configure an Amazon EventBridge rule that reacts to AWS CloudTrail events to invoke the Lambda function.

Why these are the correct answers:

B. Create an AWS Lambda function to tag the resources after the Lambda function looks up the appropriate cost center from the RDS database. Configure an Amazon EventBridge rule that reacts to AWS CloudTrail events to invoke the Lambda function.

-   [ ] Lambda can look up the cost center ID from the RDS database.
-   [ ] EventBridge can trigger the Lambda function when resources are created.
-   [ ] This ensures all new resources are tagged correctly.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. SCPs do not tag resources; they control permissions.
-   [ ] C. CloudFormation is for provisioning resources, not for continuous tagging.
-   [ ] D. Tagging with a default value does not meet the requirement to use the user's cost center ID.

Therefore, Option B is the correct solution for tagging resources with user-specific cost center IDs.
</details>
<details>
    <summary>Question 654</summary>

A company recently migrated its web application to the AWS Cloud. The company uses an Amazon EC2 instance to run multiple processes to host the application. The processes include an Apache web server that serves static content. The Apache web server makes requests to a PHP application that uses a local Redis server for user sessions. The company wants to redesign the architecture to be highly available and to use AWS managed solutions. Which solution will meet these requirements?

-   [ ] A. Use AWS Elastic Beanstalk to host the static content and the PHP application.
    Configure Elastic Beanstalk to deploy its EC2 instance into a public subnet.
    Assign a public IP address.
-   [ ] B. Use AWS Lambda to host the static content and the PHP application.
    Use an Amazon API Gateway REST API to proxy requests to the Lambda function.
    Set the API Gateway CORS configuration to respond to the domain name.
    Configure Amazon ElastiCache for Redis to handle session information.
-   [ ] C. Keep the backend code on the EC2 instance.
    Create an Amazon ElastiCache for Redis cluster that has Multi-AZ enabled.
    Configure the ElastiCache for Redis cluster in cluster mode.
    Copy the frontend resources to Amazon S3.
    Configure the backend code to reference the EC2 instance.
-   [ ] D. Configure an Amazon CloudFront distribution with an Amazon S3 endpoint to an S3 bucket that is configured to host the static content.
    Configure an Application Load Balancer that targets an Amazon Elastic Container Service (Amazon ECS) service that runs AWS Fargate tasks for the PHP application.
    Configure the PHP application to use an Amazon ElastiCache for Redis cluster that runs in multiple Availability Zones.

</details>

<details>
    <summary>Answer</summary>

-   [ ] D. Configure an Amazon CloudFront distribution with an Amazon S3 endpoint to an S3 bucket that is configured to host the static content.
    Configure an Application Load Balancer that targets an Amazon Elastic Container Service (Amazon ECS) service that runs AWS Fargate tasks for the PHP application.
    Configure the PHP application to use an Amazon ElastiCache for Redis cluster that runs in multiple Availability Zones.

Why these are the correct answers:

D. Configure an Amazon CloudFront distribution with an Amazon S3 endpoint to an S3 bucket that is configured to host the static content. Configure an Application Load Balancer that targets an Amazon Elastic Container Service (Amazon ECS) service that runs AWS Fargate tasks for the PHP application. Configure the PHP application to use an Amazon ElastiCache for Redis cluster that runs in multiple Availability Zones.

-   [ ] CloudFront and S3 provide highly available hosting for static content.
-   [ ] ECS Fargate with an ALB provides scalable and managed compute for the PHP application.
-   [ ] ElastiCache for Redis in Multi-AZ ensures highly available session management.
-   [ ] This architecture uses managed services for high availability.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Elastic Beanstalk simplifies deployment but does not provide the same level of scalability and management as other services.
-   [ ] B. Lambda is not suitable for hosting long-running applications like a web server.
-   [ ] C. Keeping the backend on EC2 does not fully leverage managed services for scalability and availability.

Therefore, Option D is the best solution for a highly available and managed architecture.
</details>
<details>
    <summary>Question 655</summary>

A company runs a web application on Amazon EC2 instances in an Auto Scaling group that has a target group. The company designed the application to work with session affinity (sticky sessions) for a better user experience. The application must be available publicly over the internet as an endpoint. A WAF must be applied to the endpoint for additional security. Session affinity (sticky sessions) must be configured on the endpoint. Which combination of steps will meet these requirements? (Choose two.)

-   [ ] A. Create a public Network Load Balancer.
    Specify the application target group.
-   [ ] B. Create a Gateway Load Balancer.
    Specify the application target group.
-   [ ] C. Create a public Application Load Balancer.
    Specify the application target group.
-   [ ] D. Create a second target group.
    Add Elastic IP addresses to the EC2 instances.
-   [ ] E. Create a web ACL in AWS WAF.
    Associate the web ACL with the endpoint

</details>

<details>
    <summary>Answer</summary>

-   [ ] C. Create a public Application Load Balancer.
    Specify the application target group.
-   [ ] E. Create a web ACL in AWS WAF.
    Associate the web ACL with the endpoint

Why these are the correct answers:

C. Create a public Application Load Balancer. Specify the application target group.

-   [ ] Application Load Balancers (ALBs) support session affinity (sticky sessions) for web applications.
-   [ ] ALBs can be public, allowing access over the internet.

E. Create a web ACL in AWS WAF. Associate the web ACL with the endpoint

-   [ ] AWS WAF provides web application firewall capabilities.
-   [ ] It can be associated with an ALB for security.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Network Load Balancers (NLBs) do not support session affinity.
-   [ ] B. Gateway Load Balancers are for third-party virtual appliances.
-   [ ] D. Creating a second target group with Elastic IPs is not necessary for this setup.

Therefore, Options C and E are the correct steps to meet the requirements.
</details>
<details>
    <summary>Question 656</summary>

A company runs a website that stores images of historical events. Website users need the ability to search and view images based on the year that the event in the image occurred. On average, users request each image only once or twice a year. The company wants a highly available solution to store and deliver the images to users. Which solution will meet these requirements MOST cost-effectively?

-   [ ] A. Store images in Amazon Elastic Block Store (Amazon EBS).
    Use a web server that runs on Amazon EC2.
-   [ ] B. Store images in Amazon Elastic File System (Amazon EFS).
    Use a web server that runs on Amazon EC2.
-   [ ] C. Store images in Amazon S3 Standard.
    Use S3 Standard to directly deliver images by using a static website.
-   [ ] D. Store images in Amazon S3 Standard-Infrequent Access (S3 Standard-IA).
    Use S3 Standard-IA to directly deliver images by using a static website.

</details>

<details>
    <summary>Answer</summary>

-   [ ] D. Store images in Amazon S3 Standard-Infrequent Access (S3 Standard-IA).
    Use S3 Standard-IA to directly deliver images by using a static website.

Why these are the correct answers:

D. Store images in Amazon S3 Standard-Infrequent Access (S3 Standard-IA). Use S3 Standard-IA to directly deliver images by using a static website.

-   [ ] S3 Standard-IA is designed for data that is infrequently accessed, which reduces storage costs.
-   [ ] S3 can directly deliver images, simplifying the architecture.
-   [ ] This solution is highly available and cost-effective.

<hr> Why are the other answers wrong? <hr>

-   [ ] A and B. EBS and EFS are more expensive for storing large amounts of infrequently accessed data.
-   [ ] C. S3 Standard is more expensive than S3 Standard-IA for infrequently accessed data.

Therefore, Option D is the most cost-effective solution for storing and delivering images.
</details>
<details>
    <summary>Question 657</summary>

A company has multiple AWS accounts in an organization in AWS Organizations that different business units use. The company has multiple offices around the world. The company needs to update security group rules to allow new office CIDR ranges or to remove old CIDR ranges across the organization. The company wants to centralize the management of security group rules to minimize the administrative overhead that updating CIDR ranges requires. Which solution will meet these requirements MOST cost-effectively?

-   [ ] A. Create VPC security groups in the organization's management account.
    Update the security groups when a CIDR range update is necessary.
-   [ ] B. Create a VPC customer managed prefix list that contains the list of CIDRs.
    Use AWS Resource Access Manager (AWS RAM) to share the prefix list across the organization.
    Use the prefix list in the security groups across the organization.
-   [ ] C. Create an AWS managed prefix list.
    Use an AWS Security Hub policy to enforce the security group update across the organization.
    Use an AWS Lambda function to update the prefix list automatically when the CIDR ranges change.
-   [ ] D. Create security groups in a central administrative AWS account.
    Create an AWS Firewall Manager common security group policy for the whole organization.
    Select the previously created security groups as primary groups in the policy.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Create a VPC customer managed prefix list that contains the list of CIDRs.
    Use AWS Resource Access Manager (AWS RAM) to share the prefix list across the organization.
    Use the prefix list in the security groups across the organization.

Why these are the correct answers:

B. Create a VPC customer managed prefix list that contains the list of CIDRs. Use AWS Resource Access Manager (AWS RAM) to share the prefix list across the organization. Use the prefix list in the security groups across the organization.

-   [ ] Prefix lists allow you to group multiple CIDR blocks and reference them in security groups.
-   [ ] AWS RAM allows sharing prefix lists across AWS accounts.
-   [ ] This centralizes CIDR management and reduces overhead.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Managing security groups in the management account does not scale well.
-   [ ] C. AWS Security Hub does not enforce security group rules.
-   [ ] D. AWS Firewall Manager is more complex and expensive for this simple use case.

Therefore, Option B is the most cost-effective solution for centralized CIDR management.
</details>
<details>
    <summary>Question 658</summary>

A company uses an on-premises network-attached storage (NAS) system to provide file shares to its high performance computing (HPC) workloads. The company wants to migrate its latency-sensitive HPC workloads and its storage to the AWS Cloud. The company must be able to provide NFS and SMB multi-protocol access from the file system. Which solution will meet these requirements with the LEAST latency? (Choose two.)

-   [ ] A. Deploy compute optimized EC2 instances into a cluster placement group.
-   [ ] B. Deploy compute optimized EC2 instances into a partition placement group.
-   [ ] C. Attach the EC2 instances to an Amazon FSx for Lustre file system.
-   [ ] D. Attach the EC2 instances to an Amazon FSx for OpenZFS file system.
-   [ ] E. Attach the EC2 instances to an Amazon FSx for NetApp ONTAP file system.

</details>

<details>
    <summary>Answer</summary>

-   [ ] A. Deploy compute optimized EC2 instances into a cluster placement group.
-   [ ] E. Attach the EC2 instances to an Amazon FSx for NetApp ONTAP file system.

Why these are the correct answers:

A. Deploy compute optimized EC2 instances into a cluster placement group.

-   [ ] Cluster placement groups reduce latency by placing instances close together.
-   [ ] Compute-optimized instances provide high performance for HPC.

E. Attach the EC2 instances to an Amazon FSx for NetApp ONTAP file system.

-   [ ] FSx for NetApp ONTAP provides multi-protocol access (NFS and SMB) and high performance.
-   [ ] It is suitable for latency-sensitive workloads.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. Partition placement groups are for fault tolerance, not minimizing latency.
-   [ ] C and D. FSx for Lustre and FSx for OpenZFS are not the best options for multi-protocol access compared to FSx for NetApp ONTAP.

Therefore, Options A and E are the correct choices for minimizing latency and providing multi-protocol access.
</details>
<details>
    <summary>Question 659</summary>

A company is relocating its data center and wants to securely transfer 50 TB of data to AWS within 2 weeks. The existing data center has a Site-to-Site VPN connection to AWS that is 90% utilized. Which AWS service should a solutions architect use to meet these requirements?

-   [ ] A. AWS DataSync with a VPC endpoint
-   [ ] B. AWS Direct Connect
-   [ ] C. AWS Snowball Edge Storage Optimized
-   [ ] D. AWS Storage Gateway

</details>

<details>
    <summary>Answer</summary>

-   [ ] C. AWS Snowball Edge Storage Optimized

Why these are the correct answers:

C. AWS Snowball Edge Storage Optimized

-   [ ] Snowball Edge is designed for large-scale data transfers.
-   [ ] It bypasses the VPN, which is already heavily utilized.
-   [ ] It can transfer 50 TB within 2 weeks.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. DataSync is for online data transfers, which would be slow over a congested VPN.
-   [ ] B. Direct Connect takes time to establish and is not suitable for a quick, one-time migration.
-   [ ] D. Storage Gateway is for hybrid storage, not for large-scale data migration.

Therefore, Option C is the most suitable service for this scenario.
</details>
<details>
    <summary>Question 660</summary>

A company hosts an application on Amazon EC2 On-Demand Instances in an Auto Scaling group. Application peak hours occur at the same time each day. Application users report slow application performance at the start of peak hours. The application performs normally 2-3 hours after peak hours begin. The company wants to ensure that the application works properly at the start of peak hours. Which solution will meet these requirements?

-   [ ] A. Configure an Application Load Balancer to distribute traffic properly to the instances.
-   [ ] B. Configure a dynamic scaling policy for the Auto Scaling group to launch new instances based on memory utilization.
-   [ ] C. Configure a dynamic scaling policy for the Auto Scaling group to launch new instances based on CPU utilization.
-   [ ] D. Configure a scheduled scaling policy for the Auto Scaling group to launch new instances before peak hours.

</details>

<details>
    <summary>Answer</summary>

-   [ ] D. Configure a scheduled scaling policy for the Auto Scaling group to launch new instances before peak hours.

Why these are the correct answers:

D. Configure a scheduled scaling policy for the Auto Scaling group to launch new instances before peak hours.

-   [ ] Scheduled scaling allows you to add instances in advance of known peak times.
-   [ ] This ensures capacity is available when needed, preventing performance issues.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. An ALB distributes traffic but does not address scaling.
-   [ ] B and C. Dynamic scaling reacts to increased load, but in this case, the load is predictable, so scheduled scaling is more efficient.

Therefore, Option D is the best solution for predictable peak hours.
</details>

# AWS-SAA-PRACTICE-EXAM Questions 661-670

<details>
    <summary>Question 661</summary>

A company runs applications on AWS that connect to the company's Amazon RDS database. The applications scale on weekends and at peak times of the year. The company wants to scale the database more effectively for its applications that connect to the database. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Use Amazon DynamoDB with connection pooling with a target group configuration for the database.
    Change the applications to use the DynamoDB endpoint.
-   [ ] B. Use Amazon RDS Proxy with a target group for the database.
    Change the applications to use the RDS Proxy endpoint.
-   [ ] C. Use a custom proxy that runs on Amazon EC2 as an intermediary to the database.
    Change the applications to use the custom proxy endpoint.
-   [ ] D. Use an AWS Lambda function to provide connection pooling with a target group configuration for the database.
    Change the applications to use the Lambda function.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Use Amazon RDS Proxy with a target group for the database.
    Change the applications to use the RDS Proxy endpoint.

Why these are the correct answers:

B. Use Amazon RDS Proxy with a target group for the database. Change the applications to use the RDS Proxy endpoint.

-   [ ] Amazon RDS Proxy manages database connections, improving scalability and efficiency.
-   [ ] It reduces the overhead of establishing new connections.
-   [ ] This is a managed service, minimizing operational overhead.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. DynamoDB is a NoSQL database and requires significant application changes.
-   [ ] C. A custom proxy on EC2 requires managing EC2 instances.
-   [ ] D. Lambda is not suitable for managing database connections.

Therefore, Option B is the most suitable solution for efficient database scaling with minimal overhead.
</details>
<details>
    <summary>Question 662</summary>

A company uses AWS Cost Explorer to monitor its AWS costs. The company notices that Amazon Elastic Block Store (Amazon EBS) storage and snapshot costs increase every month. However, the company does not purchase additional EBS storage every month. The company wants to optimize monthly costs for its current storage usage. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Use logs in Amazon CloudWatch Logs to monitor the storage utilization of Amazon EBS.
    Use Amazon EBS Elastic Volumes to reduce the size of the EBS volumes.
-   [ ] B. Use a custom script to monitor space usage.
    Use Amazon EBS Elastic Volumes to reduce the size of the EBS volumes.
-   [ ] C. Delete all expired and unused snapshots to reduce snapshot costs.
-   [ ] D. Delete all nonessential snapshots.
    Use Amazon Data Lifecycle Manager to create and manage the snapshots according to the company's snapshot policy requirements.

</details>

<details>
    <summary>Answer</summary>

-   [ ] D. Delete all nonessential snapshots.
    Use Amazon Data Lifecycle Manager to create and manage the snapshots according to the company's snapshot policy requirements.

Why these are the correct answers:

D. Delete all nonessential snapshots. Use Amazon Data Lifecycle Manager to create and manage the snapshots according to the company's snapshot policy requirements.

-   [ ] Snapshots consume storage and increase costs.
-   [ ] Deleting nonessential snapshots reduces costs.
-   [ ] Data Lifecycle Manager automates snapshot management.

<hr> Why are the other answers wrong? <hr>

-   [ ] A and B. Monitoring storage utilization does not directly reduce snapshot costs.
-   [ ] C. Manually deleting snapshots is operationally intensive.

Therefore, Option D is the most efficient solution for optimizing EBS costs.
</details>
<details>
    <summary>Question 663</summary>

A company is developing a new application on AWS. The application consists of an Amazon Elastic Container Service (Amazon ECS) cluster, an Amazon S3 bucket that contains assets for the application, and an Amazon RDS for MySQL database that contains the dataset for the application. The dataset contains sensitive information. The company wants to ensure that only the ECS cluster can access the data in the RDS for MySQL database and the data in the S3 bucket. Which solution will meet these requirements?

-   [ ] A. Create a new AWS Key Management Service (AWS KMS) customer managed key to encrypt both the S3 bucket and the RDS for MySQL database.
    Ensure that the KMS key policy includes encrypt and decrypt permissions for the ECS task execution role.
-   [ ] B. Create an AWS Key Management Service (AWS KMS) AWS managed key to encrypt both the S3 bucket and the RDS for MySQL database.
    Ensure that the S3 bucket policy specifies the ECS task execution role as a user.
-   [ ] C. Create an S3 bucket policy that restricts bucket access to the ECS task execution role.
    Create a VPC endpoint for Amazon RDS for MySQL.
    Update the RDS for MySQL security group to allow access from only the subnets that the ECS cluster will generate tasks in.
-   [ ] D. Create a VPC endpoint for Amazon RDS for MySQL.
    Update the RDS for MySQL security group to allow access from only the subnets that the ECS cluster will generate tasks in.
    Create a VPC endpoint for Amazon S3.
    Update the S3 bucket policy to allow access from only the S3 VPC endpoint.

</details>

<details>
    <summary>Answer</summary>

-   [ ] A. Create a new AWS Key Management Service (AWS KMS) customer managed key to encrypt both the S3 bucket and the RDS for MySQL database.
    Ensure that the KMS key policy includes encrypt and decrypt permissions for the ECS task execution role.

Why these are the correct answers:

A. Create a new AWS Key Management Service (AWS KMS) customer managed key to encrypt both the S3 bucket and the RDS for MySQL database. Ensure that the KMS key policy includes encrypt and decrypt permissions for the ECS task execution role.

-   [ ] KMS encrypts data, controlling access through key policies.
-   [ ] A customer-managed key provides control over encryption.
-   [ ] Granting ECS task execution role permissions in the KMS key policy allows the ECS cluster to access the data.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. AWS managed keys do not provide fine-grained access control.
-   [ ] C. S3 bucket policies and VPC endpoints do not directly control access to encrypted data.
-   [ ] D. VPC endpoints control network access but do not encrypt data.

Therefore, Option A is the most secure solution for controlling data access.
</details>
<details>
    <summary>Question 664</summary>

A company has a web application that runs on premises. The application experiences latency issues during peak hours. The latency issues occur twice each month. At the start of a latency issue, the application's CPU utilization immediately increases to 10 times its normal amount. The company wants to migrate the application to AWS to improve latency. The company also wants to scale the application automatically when application demand increases. The company will use AWS Elastic Beanstalk for application deployment. Which solution will meet these requirements?

-   [ ] A. Configure an Elastic Beanstalk environment to use burstable performance instances in unlimited mode.
    Configure the environment to scale based on requests.
-   [ ] B. Configure an Elastic Beanstalk environment to use compute optimized instances.
    Configure the environment to scale based on requests.
-   [ ] C. Configure an Elastic Beanstalk environment to use compute optimized instances.
    Configure the environment to scale on a schedule.
-   [ ] D. Configure an Elastic Beanstalk environment to use burstable performance instances in unlimited mode.
    Configure the environment to scale on predictive metrics.

</details>

<details>
    <summary>Answer</summary>

-   [ ] A. Configure an Elastic Beanstalk environment to use burstable performance instances in unlimited mode.
    Configure the environment to scale based on requests.

Why these are the correct answers:

A. Configure an Elastic Beanstalk environment to use burstable performance instances in unlimited mode. Configure the environment to scale based on requests.

-   [ ] Burstable performance instances in unlimited mode can handle CPU spikes.
-   [ ] Scaling based on requests allows for automatic scaling.
-   [ ] This combination addresses both latency and scaling needs.

<hr> Why are the other answers wrong? <hr>

-   [ ] B and C. Compute-optimized instances are more expensive and may not be necessary for occasional spikes.
-   [ ] D. Predictive metrics are less effective for sudden, unpredictable spikes.

Therefore, Option A is the most cost-effective solution for handling latency and scaling.
</details>
<details>
    <summary>Question 665</summary>

A company has customers located across the world. The company wants to use automation to secure its systems and network infrastructure. The company's security team must be able to track and audit all incremental changes to the infrastructure. Which solution will meet these requirements?

-   [ ] A. Use AWS Organizations to set up the infrastructure.
    Use AWS Config to track changes.
-   [ ] B. Use AWS CloudFormation to set up the infrastructure.
    Use AWS Config to track changes.
-   [ ] C. Use AWS Organizations to set up the infrastructure.
    Use AWS Service Catalog to track changes.
-   [ ] D. Use AWS CloudFormation to set up the infrastructure.
    Use AWS Service Catalog to track changes.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Use AWS CloudFormation to set up the infrastructure.
    Use AWS Config to track changes.

Why these are the correct answers:

B. Use AWS CloudFormation to set up the infrastructure. Use AWS Config to track changes.

-   [ ] CloudFormation automates infrastructure deployment.
-   [ ] AWS Config tracks configuration changes for auditing.
-   [ ] This combination provides automation and security tracking.

<hr> Why are the other answers wrong? <hr>

-   [ ] A and C. AWS Organizations is for managing AWS accounts, not infrastructure.
-   [ ] D. AWS Service Catalog is for creating and managing catalogs of IT services.

Therefore, Option B is the correct solution for automating infrastructure setup and tracking changes.
</details>
<details>
    <summary>Question 666</summary>

A startup company is hosting a website for its customers on an Amazon EC2 instance. The website consists of a stateless Python application and a MySQL database. The website serves only a small amount of traffic. The company is concerned about the reliability of the instance and needs to migrate to a highly available architecture. The company cannot modify the application code. Which combination of actions should a solutions architect take to achieve high availability for the website? (Choose two.)

-   [ ] A. Provision an internet gateway in each Availability Zone in use.
-   [ ] B. Migrate the database to an Amazon RDS for MySQL Multi-AZ DB instance.
-   [ ] C. Migrate the database to Amazon DynamoDB, and enable DynamoDB auto scaling.
-   [ ] D. Use AWS DataSync to synchronize the database data across multiple EC2 instances.
-   [ ] E. Create an Application Load Balancer to distribute traffic to an Auto Scaling group of EC2 instances that are distributed across two Availability Zones.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Migrate the database to an Amazon RDS for MySQL Multi-AZ DB instance.
-   [ ] E. Create an Application Load Balancer to distribute traffic to an Auto Scaling group of EC2 instances that are distributed across two Availability Zones.

Why these are the correct answers:

B. Migrate the database to an Amazon RDS for MySQL Multi-AZ DB instance.

-   [ ] RDS Multi-AZ provides high availability for databases.
-   [ ] It automatically handles failover in case of an outage.

E. Create an Application Load Balancer to distribute traffic to an Auto Scaling group of EC2 instances that are distributed across two Availability Zones.

-   [ ] An ALB and Auto Scaling group provide high availability for the application.
-   [ ] They distribute traffic and scale instances across AZs.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Internet Gateways are for VPC connectivity, not high availability.
-   [ ] C. DynamoDB requires application code changes.
-   [ ] D. DataSync is for data transfer, not high availability.

Therefore, Options B and E are the correct actions for achieving high availability.
</details>
<details>
    <summary>Question 667</summary>

A company is moving its data and applications to AWS during a multiyear migration project. The company wants to securely access data on Amazon S3 from the company's AWS Region and from the company's on-premises location. The data must not traverse the internet. The company has established an AWS Direct Connect connection between its Region and its on-premises location. Which solution will meet these requirements?

-   [ ] A. Create gateway endpoints for Amazon S3.
    Use the gateway endpoints to securely access the data from the Region and the on-premises location.
-   [ ] B. Create a gateway in AWS Transit Gateway to access Amazon S3 securely from the Region and the on-premises location.
-   [ ] C. Create interface endpoints for Amazon S3.
    Use the interface endpoints to securely access the data from the Region and the on-premises location.
-   [ ] D. Use an AWS Key Management Service (AWS KMS) key to access the data securely from the Region and the on-premises location.

</details>

<details>
    <summary>Answer</summary>

-   [ ] C. Create interface endpoints for Amazon S3.
    Use the interface endpoints to securely access the data from the Region and the on-premises location.

Why these are the correct answers:

C. Create interface endpoints for Amazon S3. Use the interface endpoints to securely access the data from the Region and the on-premises location.

-   [ ] Interface endpoints use private IP addresses within your VPC.
-   [ ] They keep traffic within the AWS network and the Direct Connect connection.
-   [ ] This ensures data does not traverse the internet.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Gateway endpoints use gateway resources, not private IPs.
-   [ ] B. Transit Gateway does not directly provide S3 access.
-   [ ] D. KMS is for encryption, not network access.

Therefore, Option C is the correct solution for secure S3 access.
</details>
<details>
    <summary>Question 668</summary>

A company created a new organization in AWS Organizations. The organization has multiple accounts for the company's development teams. The development team members use AWS IAM Identity Center (AWS Single Sign-On) to access the accounts. For each of the company's applications, the development teams must use a predefined application name to tag resources that are created. A solutions architect needs to design a solution that gives the development team the ability to create resources only if the application name tag has an approved value. Which solution will meet these requirements?

-   [ ] A. Create an IAM group that has a conditional Allow policy that requires the application name tag to be specified for resources to be created.
-   [ ] B. Create a cross-account role that has a Deny policy for any resource that has the application name tag.
-   [ ] C. Create a resource group in AWS Resource Groups to validate that the tags are applied to all resources in all accounts.
-   [ ] D. Create a tag policy in Organizations that has a list of allowed application names.

</details>

<details>
    <summary>Answer</summary>

-   [ ] D. Create a tag policy in Organizations that has a list of allowed application names.

Why these are the correct answers:

D. Create a tag policy in Organizations that has a list of allowed application names.

-   [ ] Tag policies enforce tagging consistency across AWS Organizations.
-   [ ] They allow you to specify allowed values for tags.
-   [ ] This ensures resources are created with approved tags.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. IAM policies can require tags but cannot enforce allowed values.
-   [ ] B. Deny policies are not the best approach for enforcing allowed values.
-   [ ] C. Resource Groups organize resources but do not enforce tagging.

Therefore, Option D is the correct solution for enforcing approved tag values.
</details>
<details>
    <summary>Question 669</summary>

A company runs its databases on Amazon RDS for PostgreSQL. The company wants a secure solution to manage the master user password by rotating the password every 30 days. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Use Amazon EventBridge to schedule a custom AWS Lambda function to rotate the password every 30 days.
-   [ ] B. Use the modify-db-instance command in the AWS CLI to change the password.
-   [ ] C. Integrate AWS Secrets Manager with Amazon RDS for PostgreSQL to automate password rotation.
-   [ ] D. Integrate AWS Systems Manager Parameter Store with Amazon RDS for PostgreSQL to automate password rotation.

</details>

<details>
    <summary>Answer</summary>

-   [ ] C. Integrate AWS Secrets Manager with Amazon RDS for PostgreSQL to automate password rotation.

Why these are the correct answers:

C. Integrate AWS Secrets Manager with Amazon RDS for PostgreSQL to automate password rotation.

-   [ ] AWS Secrets Manager can automatically rotate RDS passwords.
-   [ ] It is a managed service, minimizing operational overhead.
-   [ ] This provides a secure and automated solution.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Lambda and EventBridge require custom code and management.
-   [ ] B. AWS CLI requires manual intervention.
-   [ ] D. Parameter Store is for storing parameters, not automating password rotation.

Therefore, Option C is the most efficient solution for automated password rotation.
</details>
<details>
    <summary>Question 670</summary>

A company performs tests on an application that uses an Amazon DynamoDB table. The tests run for 4 hours once a week. The company knows how many read and write operations the application performs to the table each second during the tests. The company does not currently use DynamoDB for any other use case. A solutions architect needs to optimize the costs for the table. Which solution will meet these requirements?

-   [ ] A. Choose on-demand mode.
    Update the read and write capacity units appropriately.
-   [ ] B. Choose provisioned mode.
    Update the read and write capacity units appropriately.
-   [ ] C. Purchase DynamoDB reserved capacity for a 1-year term.
-   [ ] D. Purchase DynamoDB reserved capacity for a 3-year term.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Choose provisioned mode.
    Update the read and write capacity units appropriately.

Why these are the correct answers:

B. Choose provisioned mode. Update the read and write capacity units appropriately.

-   [ ] Provisioned mode is cost-effective when you know the workload capacity.
-   [ ] Since the company knows the read/write operations, they can provision accordingly.
-   [ ] This avoids the costs of on-demand mode for predictable workloads.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. On-demand mode is for unpredictable workloads, which is not the case here.
-   [ ] C and D. Reserved capacity is for sustained usage, not for 4-hour weekly tests.

Therefore, Option B is the best solution for cost optimization with known capacity needs.
</details>

# AWS-SAA-PRACTICE-EXAM Questions 671-680

<details>
    <summary>Question 671</summary>

A company runs its applications on Amazon EC2 instances. The company performs periodic financial assessments of its AWS costs. The company recently identified unusual spending. The company needs a solution to prevent unusual spending. The solution must monitor costs and notify responsible stakeholders in the event of unusual spending. Which solution will meet these requirements?

-   [ ] A. Use an AWS Budgets template to create a zero spend budget.
-   [ ] B. Create an AWS Cost Anomaly Detection monitor in the AWS Billing and Cost Management console.
-   [ ] C. Create AWS Pricing Calculator estimates for the current running workload pricing details.
-   [ ] D. Use Amazon CloudWatch to monitor costs and to identify unusual spending.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Create an AWS Cost Anomaly Detection monitor in the AWS Billing and Cost Management console.

Why these are the correct answers:

B. Create an AWS Cost Anomaly Detection monitor in the AWS Billing and Cost Management console.

-   [ ] AWS Cost Anomaly Detection monitors costs and identifies unusual spending patterns.
-   [ ] It notifies stakeholders about anomalies.
-   [ ] This solution is designed to prevent unexpected costs.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. A zero-spend budget does not monitor or notify about unusual spending.
-   [ ] C. AWS Pricing Calculator estimates costs but does not monitor spending.
-   [ ] D. CloudWatch monitors metrics, not cost anomalies.

Therefore, Option B is the most suitable solution for preventing unusual spending.
</details>
<details>
    <summary>Question 672</summary>

A marketing company receives a large amount of new clickstream data in Amazon S3 from a marketing campaign. The company needs to analyze the clickstream data in Amazon S3 quickly. Then the company needs to determine whether to process the data further in the data pipeline. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Create external tables in a Spark catalog.
    Configure jobs in AWS Glue to query the data.
-   [ ] B. Configure an AWS Glue crawler to crawl the data.
    Configure Amazon Athena to query the data.
-   [ ] C. Create external tables in a Hive metastore.
    Configure Spark jobs in Amazon EMR to query the data.
-   [ ] D. Configure an AWS Glue crawler to crawl the data.
    Configure Amazon Kinesis Data Analytics to use SQL to query the data.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Configure an AWS Glue crawler to crawl the data.
    Configure Amazon Athena to query the data.

Why these are the correct answers:

B. Configure an AWS Glue crawler to crawl the data. Configure Amazon Athena to query the data.

-   [ ] AWS Glue crawler automatically infers the schema of data in S3.
-   [ ] Amazon Athena allows querying data in S3 using SQL.
-   [ ] This combination is efficient and has minimal overhead.

<hr> Why are the other answers wrong? <hr>

-   [ ] A and C. Spark and EMR involve more setup and management overhead.
-   [ ] D. Kinesis Data Analytics is for streaming data, not batch analysis of S3 data.

Therefore, Option B is the most efficient solution for quick analysis of clickstream data.
</details>
<details>
    <summary>Question 673</summary>

A company runs an SMB file server in its data center. The file server stores large files that the company frequently accesses for up to 7 days after the file creation date. After 7 days, the company needs to be able to access the files with a maximum retrieval time of 24 hours. Which solution will meet these requirements?

-   [ ] A. Use AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS.
-   [ ] B. Create an Amazon S3 File Gateway to increase the company's storage space.
    Create an S3 Lifecycle policy to transition the data to S3 Glacier Deep Archive after 7 days.
-   [ ] C. Create an Amazon FSx File Gateway to increase the company's storage space.
    Create an Amazon S3 Lifecycle policy to transition the data after 7 days.
-   [ ] D. Configure access to Amazon S3 for each user.
    Create an S3 Lifecycle policy to transition the data to S3 Glacier Flexible Retrieval after 7 days.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Create an Amazon S3 File Gateway to increase the company's storage space.
    Create an S3 Lifecycle policy to transition the data to S3 Glacier Deep Archive after 7 days.

Why these are the correct answers:

B. Create an Amazon S3 File Gateway to increase the company's storage space. Create an S3 Lifecycle policy to transition the data to S3 Glacier Deep Archive after 7 days.

-   [ ] S3 File Gateway provides on-premises access to S3.
-   [ ] S3 Lifecycle policy can move data to Glacier Deep Archive.
-   [ ] Glacier Deep Archive meets the 24-hour retrieval requirement.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. DataSync is for migration, not ongoing storage.
-   [ ] C. FSx File Gateway is not a valid service.
-   [ ] D. Configuring S3 access for each user is complex, and Glacier Flexible Retrieval does not guarantee 24-hour retrieval.

Therefore, Option B is the correct solution for storing and retrieving files.
</details>
<details>
    <summary>Question 674</summary>

A company runs a web application on Amazon EC2 instances in an Auto Scaling group. The application uses a database that runs on an Amazon RDS for PostgreSQL DB instance. The application performs slowly when traffic increases. The database experiences a heavy read load during periods of high traffic. Which actions should a solutions architect take to resolve these performance issues? (Choose two.)

-   [ ] A. Turn on auto scaling for the DB instance.
-   [ ] B. Create a read replica for the DB instance.
    Configure the application to send read traffic to the read replica.
-   [ ] C. Convert the DB instance to a Multi-AZ DB instance deployment.
    Configure the application to send read traffic to the standby DB instance.
-   [ ] D. Create an Amazon ElastiCache cluster.
    Configure the application to cache query results in the ElastiCache cluster.
-   [ ] E. Configure the Auto Scaling group subnets to ensure that the EC2 instances are provisioned in the same Availability Zone as the DB instance.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Create a read replica for the DB instance.
    Configure the application to send read traffic to the read replica.
-   [ ] D. Create an Amazon ElastiCache cluster.
    Configure the application to cache query results in the ElastiCache cluster.

Why these are the correct answers:

B. Create a read replica for the DB instance. Configure the application to send read traffic to the read replica.

-   [ ] Read replicas offload read traffic from the primary database.
-   [ ] This improves database performance.

D. Create an Amazon ElastiCache cluster. Configure the application to cache query results in the ElastiCache cluster.

-   [ ] ElastiCache reduces database load by caching frequently accessed data.
-   [ ] This improves application performance.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Auto Scaling for RDS is not a direct solution for read-heavy workloads.
-   [ ] C. Multi-AZ is for high availability, not read scaling.
-   [ ] E. AZ placement does not directly address database read load.

Therefore, Options B and D are the correct actions to resolve performance issues.
</details>
<details>
    <summary>Question 675</summary>

A company uses Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS) volumes to run an application. The company creates one snapshot of each EBS volume every day to meet compliance requirements. The company wants to implement an architecture that prevents the accidental deletion of EBS volume snapshots. The solution must not change the administrative rights of the storage administrator user. Which solution will meet these requirements with the LEAST administrative effort?

-   [ ] A. Create an IAM role that has permission to delete snapshots.
    Attach the role to a new EC2 instance.
    Use the AWS CLI from the new EC2 instance to delete snapshots.
-   [ ] B. Create an IAM policy that denies snapshot deletion.
    Attach the policy to the storage administrator user.
-   [ ] C. Add tags to the snapshots.
    Create retention rules in Recycle Bin for EBS snapshots that have the tags.
-   [ ] D. Lock the EBS snapshots to prevent deletion.

</details>

<details>
    <summary>Answer</summary>

-   [ ] D. Lock the EBS snapshots to prevent deletion.

Why these are the correct answers:

D. Lock the EBS snapshots to prevent deletion.

-   [ ] Snapshot locking prevents deletion.
-   [ ] It does not require changing administrator rights.
-   [ ] This is the simplest way to prevent accidental deletion.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Using a separate EC2 instance and IAM role adds complexity.
-   [ ] B. Denying snapshot deletion changes administrator rights.
-   [ ] C. Recycle Bin is for recovery, not prevention.

Therefore, Option D is the most straightforward solution.
</details>
<details>
    <summary>Question 676</summary>

A company's application uses Network Load Balancers, Auto Scaling groups, Amazon EC2 instances, and databases that are deployed in an Amazon VPC. The company wants to capture information about traffic to and from the network interfaces in near real time in its Amazon VPC. The company wants to send the information to Amazon OpenSearch Service for analysis. Which solution will meet these requirements?

-   [ ] A. Create a log group in Amazon CloudWatch Logs.
    Configure VPC Flow Logs to send the log data to the log group.
    Use Amazon Kinesis Data Streams to stream the logs from the log group to OpenSearch Service.
-   [ ] B. Create a log group in Amazon CloudWatch Logs.
    Configure VPC Flow Logs to send the log data to the log group.
    Use Amazon Kinesis Data Firehose to stream the logs from the log group to OpenSearch Service.
-   [ ] C. Create a trail in AWS CloudTrail.
    Configure VPC Flow Logs to send the log data to the trail.
    Use Amazon Kinesis Data Streams to stream the logs from the trail to OpenSearch Service.
-   [ ] D. Create a trail in AWS CloudTrail.
    Configure VPC Flow Logs to send the log data to the trail.
    Use Amazon Kinesis Data Firehose to stream the logs from the trail to OpenSearch Service.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Create a log group in Amazon CloudWatch Logs.
    Configure VPC Flow Logs to send the log data to the log group.
    Use Amazon Kinesis Data Firehose to stream the logs from the log group to OpenSearch Service.

Why these are the correct answers:

B. Create a log group in Amazon CloudWatch Logs. Configure VPC Flow Logs to send the log data to the log group. Use Amazon Kinesis Data Firehose to stream the logs from the log group to OpenSearch Service.

-   [ ] VPC Flow Logs capture information about IP traffic.
-   [ ] CloudWatch Logs stores the log data.
-   [ ] Kinesis Data Firehose streams logs to OpenSearch Service.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Kinesis Data Streams is for real-time data streams, not log delivery.
-   [ ] C and D. CloudTrail records API calls, not network traffic.

Therefore, Option B is the correct solution for capturing and analyzing network traffic.
</details>
<details>
    <summary>Question 677</summary>

A company is developing an application that will run on a production Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS cluster has managed node groups that are provisioned with On-Demand Instances. The company needs a dedicated EKS cluster for development work. The company will use the development cluster infrequently to test the resiliency of the application. The EKS cluster must manage all the nodes. Which solution will meet these requirements MOST cost-effectively?

-   [ ] A. Create a managed node group that contains only Spot Instances.
-   [ ] B. Create two managed node groups.
    Provision one node group with On-Demand Instances.
    Provision the second node group with Spot Instances.
-   [ ] C. Create an Auto Scaling group that has a launch configuration that uses Spot Instances.
    Configure the user data to add the nodes to the EKS cluster.
-   [ ] D. Create a managed node group that contains only On-Demand Instances.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Create two managed node groups.
    Provision one node group with On-Demand Instances.
    Provision the second node group with Spot Instances.

Why these are the correct answers:

B. Create two managed node groups. Provision one node group with On-Demand Instances. Provision the second node group with Spot Instances.

-   [ ] Managed node groups simplify node management.
-   [ ] Spot Instances are cost-effective for infrequent testing.
-   [ ] Having one On-Demand group provides a fallback.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Only Spot Instances may lead to interruptions.
-   [ ] C. Auto Scaling groups require more management than managed node groups.
-   [ ] D. On-Demand Instances are more expensive for infrequent use.

Therefore, Option B is the most cost-effective solution for a development EKS cluster.
</details>
<details>
    <summary>Question 678</summary>

A company stores sensitive data in Amazon S3. A solutions architect needs to create an encryption solution. The company needs to fully control the ability of users to create, rotate, and disable encryption keys with minimal effort for any data that must be encrypted. Which solution will meet these requirements?

-   [ ] A. Use default server-side encryption with Amazon S3 managed encryption keys (SSE-S3) to store the sensitive data.
-   [ ] B. Create a customer managed key by using AWS Key Management Service (AWS KMS).
    Use the new key to encrypt the S3 objects by using server-side encryption with AWS KMS keys (SSE-KMS).
-   [ ] C. Create an AWS managed key by using AWS Key Management Service (AWS KMS).
    Use the new key to encrypt the S3 objects by using server-side encryption with AWS KMS keys (SSE-KMS).
-   [ ] D. Download S3 objects to an Amazon EC2 instance.
    Encrypt the objects by using customer managed keys.
    Upload the encrypted objects back into Amazon S3.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Create a customer managed key by using AWS Key Management Service (AWS KMS).
    Use the new key to encrypt the S3 objects by using server-side encryption with AWS KMS keys (SSE-KMS).

Why these are the correct answers:

B. Create a customer managed key by using AWS Key Management Service (AWS KMS). Use the new key to encrypt the S3 objects by using server-side encryption with AWS KMS keys (SSE-KMS).

-   [ ] Customer managed keys give you full control over the key lifecycle.
-   [ ] SSE-KMS simplifies encryption and decryption.
-   [ ] This solution meets the control and management requirements.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. SSE-S3 keys are managed by AWS, not the customer.
-   [ ] C. AWS managed keys do not provide full control.
-   [ ] D. Encrypting on EC2 adds complexity and overhead.

Therefore, Option B is the correct solution for controlling encryption keys.
</details>
<details>
    <summary>Question 679</summary>

A company wants to back up its on-premises virtual machines (VMs) to AWS. The company's backup solution exports on-premises backups to an Amazon S3 bucket as objects. The S3 backups must be retained for 30 days and must be automatically deleted after 30 days. Which combination of steps will meet these requirements? (Choose three.)

-   [ ] A. Create an S3 bucket that has S3 Object Lock enabled.
-   [ ] B. Create an S3 bucket that has object versioning enabled.
-   [ ] C. Configure a default retention period of 30 days for the objects.
-   [ ] D. Configure an S3 Lifecycle policy to protect the objects for 30 days.
-   [ ] E. Configure an S3 Lifecycle policy to expire the objects after 30 days.
-   [ ] F. Configure the backup solution to tag the objects with a 30-day retention period

</details>

<details>
    <summary>Answer</summary>

-   [ ] A. Create an S3 bucket that has S3 Object Lock enabled.
-   [ ] C. Configure a default retention period of 30 days for the objects.
-   [ ] E. Configure an S3 Lifecycle policy to expire the objects after 30 days.

Why these are the correct answers:

A. Create an S3 bucket that has S3 Object Lock enabled.

-   [ ] Object Lock prevents objects from being deleted or overwritten.
-   [ ] This ensures backups are retained.

C. Configure a default retention period of 30 days for the objects.

-   [ ] This enforces the 30-day retention requirement.

E. Configure an S3 Lifecycle policy to expire the objects after 30 days.

-   [ ] Lifecycle policies automate object deletion.
-   [ ] This ensures automatic deletion after 30 days.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. Versioning keeps multiple versions, not retention.
-   [ ] D. Lifecycle policies do not protect objects for 30 days; they manage transitions or expiration.
-   [ ] F. Backup solution tagging does not enforce retention in S3.

Therefore, Options A, C, and E are the correct steps for backup retention and deletion.
</details>
<details>
    <summary>Question 680</summary>

A solutions architect needs to copy files from an Amazon S3 bucket to an Amazon Elastic File System (Amazon EFS) file system and another S3 bucket. The files must be copied continuously. New files are added to the original S3 bucket consistently. The copied files should be overwritten only if the source file changes. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Create an AWS DataSync location for both the destination S3 bucket and the EFS file system.
    Create a task for the destination S3 bucket and the EFS file system.
    Set the transfer mode to transfer only data that has changed.
-   [ ] B. Create an AWS Lambda function.
    Mount the file system to the function.
    Set up an S3 event notification to invoke the function when files are created and changed in Amazon S3.
    Configure the function to copy files to the file system and the destination S3 bucket.
-   [ ] C. Create an AWS DataSync location for both the destination S3 bucket and the EFS file system.
    Create a task for the destination S3 bucket and the EFS file system.
    Set the transfer mode to transfer all data.
-   [ ] D. Launch an Amazon EC2 instance in the same VPC as the file system.
    Mount the file system.
    Create a script to routinely synchronize all objects that changed in the origin S3 bucket to the destination S3 bucket and the mounted file system.

</details>

<details>
    <summary>Answer</summary>

-   [ ] A. Create an AWS DataSync location for both the destination S3 bucket and the EFS file system.
    Create a task for the destination S3 bucket and the EFS file system.
    Set the transfer mode to transfer only data that has changed.

Why these are the correct answers:

A. Create an AWS DataSync location for both the destination S3 bucket and the EFS file system. Create a task for the destination S3 bucket and the EFS file system. Set the transfer mode to transfer only data that has changed.

-   [ ] AWS DataSync automates data transfer between AWS storage services.
-   [ ] It can transfer only changed data, optimizing efficiency.
-   [ ] This solution minimizes operational overhead.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. Lambda requires more setup and management.
-   [ ] C. Transferring all data is inefficient.
-   [ ] D. EC2 and a custom script involve more manual work.

Therefore, Option A is the most efficient solution for continuous file copying.
</details>

# AWS-SAA-PRACTICE-EXAM Questions 681-690

<details>
    <summary>Question 681</summary>

A company uses Amazon EC2 instances and stores data on Amazon Elastic Block Store (Amazon EBS) volumes. The company must ensure that all data is encrypted at rest by using AWS Key Management Service (AWS KMS). The company must be able to control rotation of the encryption keys. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Create a customer managed key.
    Use the key to encrypt the EBS volumes.
-   [ ] B. Use an AWS managed key to encrypt the EBS volumes.
    Use the key to configure automatic key rotation.
-   [ ] C. Create an external KMS key with imported key material.
    Use the key to encrypt the EBS volumes.
-   [ ] D. Use an AWS owned key to encrypt the EBS volumes.

</details>

<details>
    <summary>Answer</summary>

-   [ ] A. Create a customer managed key.
    Use the key to encrypt the EBS volumes.

Why these are the correct answers:

A. Create a customer managed key. Use the key to encrypt the EBS volumes.

-   [ ] Customer managed keys provide control over key rotation.
-   [ ] KMS simplifies encryption and decryption of EBS volumes.
-   [ ] This solution meets the requirements with minimal overhead.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. AWS managed keys do not allow customer control over key rotation.
-   [ ] C. External KMS keys add complexity with key management.
-   [ ] D. AWS owned keys do not provide any control over key management.

Therefore, Option A is the most suitable solution for EBS encryption.
</details>
<details>
    <summary>Question 682</summary>

A company needs a solution to enforce data encryption at rest on Amazon EC2 instances. The solution must automatically identify noncompliant resources and enforce compliance policies on findings. Which solution will meet these requirements with the LEAST administrative overhead?

-   [ ] A. Use an IAM policy that allows users to create only encrypted Amazon Elastic Block Store (Amazon EBS) volumes.
    Use AWS Config and AWS Systems Manager to automate the detection and remediation of unencrypted EBS volumes.
-   [ ] B. Use AWS Key Management Service (AWS KMS) to manage access to encrypted Amazon Elastic Block Store (Amazon EBS) volumes.
    Use AWS Lambda and Amazon EventBridge to automate the detection and remediation of unencrypted EBS volumes.
-   [ ] C. Use Amazon Macie to detect unencrypted Amazon Elastic Block Store (Amazon EBS) volumes.
    Use AWS Systems Manager Automation rules to automatically encrypt existing and new EBS volumes.
-   [ ] D. Use Amazon inspector to detect unencrypted Amazon Elastic Block Store (Amazon EBS) volumes.
    Use AWS Systems Manager Automation rules to automatically encrypt existing and new EBS volumes.

</details>

<details>
    <summary>Answer</summary>

-   [ ] A. Use an IAM policy that allows users to create only encrypted Amazon Elastic Block Store (Amazon EBS) volumes.
    Use AWS Config and AWS Systems Manager to automate the detection and remediation of unencrypted EBS volumes.

Why these are the correct answers:

A. Use an IAM policy that allows users to create only encrypted Amazon Elastic Block Store (Amazon EBS) volumes. Use AWS Config and AWS Systems Manager to automate the detection and remediation of unencrypted EBS volumes.

-   [ ] IAM policies enforce encryption during volume creation.
-   [ ] AWS Config detects non-compliant volumes.
-   [ ] AWS Systems Manager automates remediation.
-   [ ] This combination provides comprehensive enforcement.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. Lambda and EventBridge require more custom setup.
-   [ ] C. Amazon Macie is for sensitive data discovery, not EBS encryption enforcement.
-   [ ] D. Amazon Inspector is for security vulnerabilities, not encryption enforcement.

Therefore, Option A is the most efficient solution for enforcing encryption.
</details>
<details>
    <summary>Question 683</summary>

A company is migrating its multi-tier on-premises application to AWS. The application consists of a single-node MySQL database and a multi-node web tier. The company must minimize changes to the application during the migration. The company wants to improve application resiliency after the migration. Which combination of steps will meet these requirements? (Choose two.)

-   [ ] A. Migrate the web tier to Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer.
-   [ ] B. Migrate the database to Amazon EC2 instances in an Auto Scaling group behind a Network Load Balancer.
-   [ ] C. Migrate the database to an Amazon RDS Multi-AZ deployment.
-   [ ] D. Migrate the web tier to an AWS Lambda function.
-   [ ] E. Migrate the database to an Amazon DynamoDB table.

</details>

<details>
    <summary>Answer</summary>

-   [ ] A. Migrate the web tier to Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer.
-   [ ] C. Migrate the database to an Amazon RDS Multi-AZ deployment.

Why these are the correct answers:

A. Migrate the web tier to Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer.

-   [ ] Auto Scaling groups provide resiliency for the web tier.
-   [ ] Application Load Balancers distribute traffic.
-   [ ] EC2 instances minimize application changes.

C. Migrate the database to an Amazon RDS Multi-AZ deployment.

-   [ ] RDS Multi-AZ provides high availability for the database.
-   [ ] It minimizes changes compared to other database options.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. EC2 Auto Scaling groups are not ideal for databases.
-   [ ] D and E. Lambda and DynamoDB require significant application changes.

Therefore, Options A and C are the correct steps for migration.
</details>
<details>
    <summary>Question 684</summary>

A company wants to migrate its web applications from on premises to AWS. The company is located close to the eu-central-1 Region. Because of regulations, the company cannot launch some of its applications in eu-central-1. The company wants to achieve single-digit millisecond latency. Which solution will meet these requirements?

-   [ ] A. Deploy the applications in eu-central-1.
    Extend the company's VPC from eu-central-1 to an edge location in Amazon CloudFront.
-   [ ] B. Deploy the applications in AWS Local Zones by extending the company's VPC from eu-central-1 to the chosen Local Zone.
-   [ ] C. Deploy the applications in eu-central-1.
    Extend the company's VPC from eu-central-1 to the regional edge caches in Amazon CloudFront.
-   [ ] D. Deploy the applications in AWS Wavelength Zones by extending the company's VPC from eu-central-1 to the chosen Wavelength Zone.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Deploy the applications in AWS Local Zones by extending the company's VPC from eu-central-1 to the chosen Local Zone.

Why these are the correct answers:

B. Deploy the applications in AWS Local Zones by extending the company's VPC from eu-central-1 to the chosen Local Zone.

-   [ ] Local Zones place compute and storage closer to the company.
-   [ ] This reduces latency while complying with regulations.
-   [ ] Extending the VPC provides seamless connectivity.

<hr> Why are the other answers wrong? <hr>

-   [ ] A and C. CloudFront edge locations and regional edge caches are for caching, not for running applications with low latency.
-   [ ] D. Wavelength Zones are for 5G applications, not general web applications.

Therefore, Option B is the correct solution for low-latency access.
</details>
<details>
    <summary>Question 685</summary>

A company's ecommerce website has unpredictable traffic and uses AWS Lambda functions to directly access a private Amazon RDS for PostgreSQL DB instance. The company wants to maintain predictable database performance and ensure that the Lambda invocations do not overload the database with too many connections. What should a solutions architect do to meet these requirements?

-   [ ] A. Point the client driver at an RDS custom endpoint.
    Deploy the Lambda functions inside a VPC.
-   [ ] B. Point the client driver at an RDS proxy endpoint.
    Deploy the Lambda functions inside a VPC.
-   [ ] C. Point the client driver at an RDS custom endpoint.
    Deploy the Lambda functions outside a VPC.
-   [ ] D. Point the client driver at an RDS proxy endpoint.
    Deploy the Lambda functions outside a VPC.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Point the client driver at an RDS proxy endpoint.
    Deploy the Lambda functions inside a VPC.

Why these are the correct answers:

B. Point the client driver at an RDS proxy endpoint. Deploy the Lambda functions inside a VPC.

-   [ ] RDS Proxy manages database connections, preventing overload.
-   [ ] Lambda functions in a VPC can access private RDS instances.
-   [ ] This combination ensures predictable performance.

<hr> Why are the other answers wrong? <hr>

-   [ ] A and C. RDS custom endpoints do not manage connections.
-   [ ] C and D. Lambda functions outside a VPC cannot directly access private RDS instances.

Therefore, Option B is the correct solution for database performance.
</details>
<details>
    <summary>Question 686</summary>

A company is creating an application. The company stores data from tests of the application in multiple on-premises locations. The company needs to connect the on-premises locations to VPCs in an AWS Region in the AWS Cloud. The number of accounts and VPCs will increase during the next year. The network architecture must simplify the administration of new connections and must provide the ability to scale. Which solution will meet these requirements with the LEAST administrative overhead?

-   [ ] A. Create a peering connection between the VPCs.
    Create a VPN connection between the VPCs and the on-premises locations.
-   [ ] B. Launch an Amazon EC2 instance.
    On the instance, include VPN software that uses a VPN connection to connect all VPCs and on-premises locations.
-   [ ] C. Create a transit gateway.
    Create VPC attachments for the VPC connections.
    Create VPN attachments for the on-premises connections.
-   [ ] D. Create an AWS Direct Connect connection between the on-premises locations and a central VPC.
    Connect the central VPC to other VPCs by using peering connections.

</details>

<details>
    <summary>Answer</summary>

-   [ ] C. Create a transit gateway.
    Create VPC attachments for the VPC connections.
    Create VPN attachments for the on-premises connections.

Why these are the correct answers:

C. Create a transit gateway. Create VPC attachments for the VPC connections. Create VPN attachments for the on-premises connections.

-   [ ] Transit Gateway simplifies network connectivity.
-   [ ] It scales easily and reduces administrative overhead.
-   [ ] This solution is ideal for multiple VPCs and on-premises connections.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Peering connections are complex to manage at scale.
-   [ ] B. An EC2 instance with VPN software is a single point of failure.
-   [ ] D. Direct Connect with VPC peering is complex and less scalable.

Therefore, Option C is the most efficient solution for network connectivity.
</details>
<details>
    <summary>Question 687</summary>

A company that uses AWS needs a solution to predict the resources needed for manufacturing processes each month. The solution must use historical values that are currently stored in an Amazon S3 bucket. The company has no machine learning (ML) experience and wants to use a managed service for the training and predictions. Which combination of steps will meet these requirements? (Choose two.)

-   [ ] A. Deploy an Amazon SageMaker model.
    Create a SageMaker endpoint for inference.
-   [ ] B. Use Amazon SageMaker to train a model by using the historical data in the S3 bucket.
-   [ ] C. Configure an AWS Lambda function with a function URL that uses Amazon SageMaker endpoints to create predictions based on the inputs.
-   [ ] D. Configure an AWS Lambda function with a function URL that uses an Amazon Forecast predictor to create a prediction based on the inputs.
-   [ ] E. Train an Amazon Forsecast predictor by using the historical data in the S3 bucket.

</details>

<details>
    <summary>Answer</summary>

-   [ ] A. Deploy an Amazon SageMaker model.
    Create a SageMaker endpoint for inference.
-   [ ] B. Use Amazon SageMaker to train a model by using the historical data in the S3 bucket.

Why these are the correct answers:

A. Deploy an Amazon SageMaker model. Create a SageMaker endpoint for inference.

-   [ ] SageMaker endpoints provide managed inference.

B. Use Amazon SageMaker to train a model by using the historical data in the S3 bucket.

-   [ ] SageMaker simplifies model training.
-   [ ] This combination provides a managed ML solution.

<hr> Why are the other answers wrong? <hr>

-   [ ] C and D. Lambda is not needed for managed predictions.
-   [ ] E. Forecast is for time-series data, which may not be suitable for all manufacturing predictions.

Therefore, Options A and B are the correct steps for managed ML.
</details>
<details>
    <summary>Question 688</summary>

A company manages AWS accounts in AWS Organizations. AWS IAM Identity Center (AWS Single Sign-On) and AWS Control Tower are configured for the accounts. The company wants to manage multiple user permissions across all the accounts. The permissions will be used by multiple IAM users and must be split between the developer and administrator teams. Each team requires different permissions. The company wants a solution that includes new users that are hired on both teams. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Create individual users in IAM Identity Center for each account.
    Create separate developer and administrator groups in IAM Identity Center.
    Assign the users to the appropriate groups.
    Create a custom IAM policy for each group to set fine-grained permissions.
-   [ ] B. Create individual users in IAM Identity Center for each account.
    Create separate developer and administrator groups in IAM Identity Center.
    Assign the users to the appropriate groups.
    Attach AWS managed IAM policies to each user as needed for fine-grained permissions.
-   [ ] C. Create individual users in IAM Identity Center.
    Create new developer and administrator groups in IAM Identity Center.
    Create new permission sets that include the appropriate IAM policies for each group.
    Assign the new groups to the appropriate accounts.
    Assign the new permission sets to the new groups.
    When new users are hired, add them to the appropriate group.
-   [ ] D. Create individual users in IAM Identity Center.
    Create new permission sets that include the appropriate IAM policies for each user.
    Assign the users to the appropriate accounts.
    Grant additional IAM permissions to the users from within specific accounts.
    When new users are hired, add them to IAM Identity Center and assign them to the accounts.

</details>

<details>
    <summary>Answer</summary>

-   [ ] C. Create individual users in IAM Identity Center.
    Create new developer and administrator groups in IAM Identity Center.
    Create new permission sets that include the appropriate IAM policies for each group.
    Assign the new groups to the appropriate accounts.
    Assign the new permission sets to the new groups.
    When new users are hired, add them to the appropriate group.

Why these are the correct answers:

C. Create individual users in IAM Identity Center. Create new developer and administrator groups in IAM Identity Center. Create new permission sets that include the appropriate IAM policies for each group. Assign the new groups to the appropriate accounts. Assign the new permission sets to the new groups. When new users are hired, add them to the appropriate group.

-   [ ] IAM Identity Center centralizes user management.
-   [ ] Permission sets define permissions for groups.
-   [ ] This simplifies management of user permissions across accounts.

<hr> Why are the other answers wrong? <hr>

-   [ ] A and B. Creating users in each account is complex.
-   [ ] D. Granting IAM permissions in specific accounts adds overhead.

Therefore, Option C is the most efficient solution for managing permissions.
</details>
<details>
    <summary>Question 689</summary>

A company wants to standardize its Amazon Elastic Block Store (Amazon EBS) volume encryption strategy. The company also wants to minimize the cost and configuration effort required to operate the volume encryption check. Which solution will meet these requirements?

-   [ ] A. Write API calls to describe the EBS volumes and to confirm the EBS volumes are encrypted.
    Use Amazon EventBridge to schedule an AWS Lambda function to run the API calls.
-   [ ] B. Write API calls to describe the EBS volumes and to confirm the EBS volumes are encrypted.
    Run the API calls on an AWS Fargate task.
-   [ ] C. Create an AWS Identity and Access Management (IAM) policy that requires the use of tags on EBS volumes.
    Use AWS Cost Explorer to display resources that are not properly tagged.
    Encrypt the untagged resources manually.
-   [ ] D. Create an AWS Config rule for Amazon EBS to evaluate if a volume is encrypted and to flag the volume if it is not encrypted.

</details>

<details>
    <summary>Answer</summary>

-   [ ] D. Create an AWS Config rule for Amazon EBS to evaluate if a volume is encrypted and to flag the volume if it is not encrypted.

Why these are the correct answers:

D. Create an AWS Config rule for Amazon EBS to evaluate if a volume is encrypted and to flag the volume if it is not encrypted.

-   [ ] AWS Config rules automate compliance checks.
-   [ ] They flag non-compliant EBS volumes.
-   [ ] This solution is efficient and has minimal overhead.

<hr> Why are the other answers wrong? <hr>

-   [ ] A and B. Lambda and Fargate require more setup and management.
-   [ ] C. IAM policies and tags do not enforce encryption.

Therefore, Option D is the most efficient solution for EBS encryption compliance.
</details>
<details>
    <summary>Question 690</summary>

A company regularly uploads GB-sized files to Amazon S3. After the company uploads the files, the company uses a fleet of Amazon EC2 Spot Instances to transcode the file format. The company needs to scale throughput when the company uploads data from the on-premises data center to Amazon S3 and when the company downloads data from Amazon S3 to the EC2 instances. Which solutions will meet these requirements? (Choose two.)

-   [ ] A. Use the S3 bucket access point instead of accessing the S3 bucket directly.
-   [ ] B. Upload the files into multiple S3 buckets.
-   [ ] C. Use S3 multipart uploads.
-   [ ] D. Fetch multiple byte-ranges of an object in parallel.
-   [ ] E. Add a random prefix to each object when uploading the files.

</details>

<details>
    <summary>Answer</summary>

-   [ ] C. Use S3 multipart uploads.
-   [ ] D. Fetch multiple byte-ranges of an object in parallel.

Why these are the correct answers:

C. Use S3 multipart uploads.

-   [ ] Multipart uploads improve throughput for large files.
-   [ ] They allow for parallel uploads and retries.

D. Fetch multiple byte-ranges of an object in parallel.

-   [ ] Parallel fetching increases download throughput.
-   [ ] It allows EC2 instances to download parts of a file simultaneously.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. S3 access points manage access, not throughput.
-   [ ] B. Uploading to multiple buckets does not inherently increase throughput.
-   [ ] E. Random prefixes are for distribution, not throughput.

Therefore, Options C and D are the correct solutions for scaling throughput.
</details>

# AWS-SAA-PRACTICE-EXAM Questions 691-700

<details>
    <summary>Question 691</summary>

A solutions architect is designing a shared storage solution for a web application that is deployed across multiple Availability Zones. The web application runs on Amazon EC2 instances that are in an Auto Scaling group. The company plans to make frequent changes to the content. The solution must have strong consistency in returning the new content as soon as the changes occur. Which solutions meet these requirements? (Choose two.)

-   [ ] A. Use AWS Storage Gateway Volume Gateway Internet Small Computer Systems Interface (iSCSI) block storage that is mounted to the individual EC2 instances.
-   [ ] B. Create an Amazon Elastic File System (Amazon EFS) file system.
    Mount the EFS file system on the individual EC2 instances.
-   [ ] C. Create a shared Amazon Elastic Block Store (Amazon EBS) volume.
    Mount the EBS volume on the individual EC2 instances.
-   [ ] D. Use AWS DataSync to perform continuous synchronization of data between EC2 hosts in the Auto Scaling group.
-   [ ] E. Create an Amazon S3 bucket to store the web content.
    Set the metadata for the Cache-Control header to no-cache.
    Use Amazon CloudFront to deliver the content.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Create an Amazon Elastic File System (Amazon EFS) file system.
    Mount the EFS file system on the individual EC2 instances.
-   [ ] E. Create an Amazon S3 bucket to store the web content.
    Set the metadata for the Cache-Control header to no-cache.
    Use Amazon CloudFront to deliver the content.

Why these are the correct answers:

B. Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on the individual EC2 instances.

-   [ ] Amazon EFS provides shared file storage that can be accessed by multiple EC2 instances across Availability Zones.
-   [ ] It offers strong consistency.

E. Create an Amazon S3 bucket to store the web content. Set the metadata for the Cache-Control header to no-cache. Use Amazon CloudFront to deliver the content.

-   [ ] S3 stores the content, and CloudFront delivers it globally.
-   [ ] Cache-Control: no-cache ensures strong consistency by bypassing caching.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Storage Gateway is for on-premises integration, not shared storage in AWS.
-   [ ] C. EBS volumes cannot be mounted to multiple EC2 instances across Availability Zones.
-   [ ] D. DataSync is for data transfer, not shared storage.

Therefore, Options B and E are the correct solutions for shared storage with strong consistency.
</details>
<details>
    <summary>Question 692</summary>

A company is deploying an application in three AWS Regions using an Application Load Balancer. Amazon Route 53 will be used to distribute traffic between these Regions. Which Route 53 configuration should a solutions architect use to provide the MOST high-performing experience?

-   [ ] A. Create an A record with a latency policy.
-   [ ] B. Create an A record with a geolocation policy.
-   [ ] C. Create a CNAME record with a failover policy.
-   [ ] D. Create a CNAME record with a geoproximity policy.

</details>

<details>
    <summary>Answer</summary>

-   [ ] A. Create an A record with a latency policy.

Why these are the correct answers:

A. Create an A record with a latency policy.

-   [ ] Latency-based routing directs traffic to the region with the lowest latency.
-   [ ] This provides the best performance for users.
-   [ ] A records are used to map domain names to IP addresses.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. Geolocation routing routes traffic based on user location, not performance.
-   [ ] C. Failover policies are for high availability, not performance. CNAME records are used to alias one name to another.
-   [ ] D. Geoproximity routing routes traffic based on resource location and can consider bias, but it's more complex than latency-based routing for performance. CNAME records are used to alias one name to another.

Therefore, Option A is the most suitable solution for high-performing global applications.
</details>
<details>
    <summary>Question 693</summary>

A company has a web application that includes an embedded NoSQL database. The application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances run in an Amazon EC2 Auto Scaling group in a single Availability Zone. A recent increase in traffic requires the application to be highly available and for the database to be eventually consistent. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Replace the ALB with a Network Load Balancer.
    Maintain the embedded NoSQL database with its replication service on the EC2 instances.
-   [ ] B. Replace the ALB with a Network Load Balancer.
    Migrate the embedded NoSQL database to Amazon DynamoDB by using AWS Database Migration Service (AWS DMS).
-   [ ] C. Modify the Auto Scaling group to use EC2 instances across three Availability Zones.
    Maintain the embedded NoSQL database with its replication service on the EC2 instances.
-   [ ] D. Modify the Auto Scaling group to use EC2 instances across three Availability Zones.
    Migrate the embedded NoSQL database to Amazon DynamoDB by using AWS Database Migration Service (AWS DMS).

</details>

<details>
    <summary>Answer</summary>

-   [ ] D. Modify the Auto Scaling group to use EC2 instances across three Availability Zones.
    Migrate the embedded NoSQL database to Amazon DynamoDB by using AWS Database Migration Service (AWS DMS).

Why these are the correct answers:

D. Modify the Auto Scaling group to use EC2 instances across three Availability Zones. Migrate the embedded NoSQL database to Amazon DynamoDB by using AWS Database Migration Service (AWS DMS).

-   [ ] Auto Scaling across AZs improves availability.
-   [ ] DynamoDB is a managed NoSQL database that provides high availability and eventual consistency.
-   [ ] AWS DMS simplifies database migration.

<hr> Why are the other answers wrong? <hr>

-   [ ] A and B. Network Load Balancers do not provide the same level of flexibility as ALBs for web applications.
-   [ ] A and C. Maintaining the embedded database increases operational overhead.

Therefore, Option D is the correct solution for high availability and managed services.
</details>
<details>
    <summary>Question 694</summary>

A company is building a shopping application on AWS. The application offers a catalog that changes once each month and needs to scale with traffic volume. The company wants the lowest possible latency from the application. Data from each user's shopping cart needs to be highly available. User session data must be available even if the user is disconnected and reconnects. What should a solutions architect do to ensure that the shopping cart data is preserved at all times?

-   [ ] A. Configure an Application Load Balancer to enable the sticky sessions feature (session affinity) for access to the catalog in Amazon Aurora.
-   [ ] B. Configure Amazon ElastiCache for Redis to cache catalog data from Amazon DynamoDB and shopping cart data from the user's session.
-   [ ] C. Configure Amazon OpenSearch Service to cache catalog data from Amazon DynamoDB and shopping cart data from the user's session.
-   [ ] D. Configure an Amazon EC2 instance with Amazon Elastic Block Store (Amazon EBS) storage for the catalog and shopping cart.
    Configure automated snapshots.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Configure Amazon ElastiCache for Redis to cache catalog data from Amazon DynamoDB and shopping cart data from the user's session.

Why these are the correct answers:

B. Configure Amazon ElastiCache for Redis to cache catalog data from Amazon DynamoDB and shopping cart data from the user's session.

-   [ ] ElastiCache for Redis provides low-latency caching.
-   [ ] It can store session data and shopping cart data, ensuring availability.
-   [ ] Caching improves performance and preserves data across disconnections.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Sticky sessions do not guarantee data preservation across disconnections.
-   [ ] C. OpenSearch Service is for search and analytics, not caching session data.
-   [ ] D. EC2 instances with EBS storage are not highly available and do not provide caching.

Therefore, Option B is the correct solution for preserving shopping cart data.
</details>
<details>
    <summary>Question 695</summary>

A company is building a microservices-based application that will be deployed on Amazon Elastic Kubernetes Service (Amazon EKS). The microservices will interact with each other. The company wants to ensure that the application is observable to identify performance issues in the future. Which solution will meet these requirements?

-   [ ] A. Configure the application to use Amazon ElastiCache to reduce the number of requests that are sent to the microservices.
-   [ ] B. Configure Amazon CloudWatch Container Insights to collect metrics from the EKS clusters.
    Configure AWS X-Ray to trace the requests between the microservices.
-   [ ] C. Configure AWS CloudTrail to review the API calls.
    Build an Amazon QuickSight dashboard to observe the microservice interactions.
-   [ ] D. Use AWS Trusted Advisor to understand the performance of the application.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Configure Amazon CloudWatch Container Insights to collect metrics from the EKS clusters.
    Configure AWS X-Ray to trace the requests between the microservices.

Why these are the correct answers:

B. Configure Amazon CloudWatch Container Insights to collect metrics from the EKS clusters. Configure AWS X-Ray to trace the requests between the microservices.

-   [ ] CloudWatch Container Insights provides metrics and logs from EKS clusters.
-   [ ] AWS X-Ray traces requests between microservices.
-   [ ] This combination provides comprehensive observability.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. ElastiCache is for caching, not observability.
-   [ ] C. CloudTrail is for API calls, not microservice interactions. QuickSight is for business intelligence.
-   [ ] D. Trusted Advisor provides best practices, not observability.

Therefore, Option B is the correct solution for monitoring microservices.
</details>
<details>
    <summary>Question 696</summary>

A company needs to provide customers with secure access to its data. The company processes customer data and stores the results in an Amazon S3 bucket. All the data is subject to strong regulations and security requirements. The data must be encrypted at rest. Each customer must be able to access only their data from their AWS account. Company employees must not be able to access the data. Which solution will meet these requirements?

-   [ ] A. Provision an AWS Certificate Manager (ACM) certificate for each customer.
    Encrypt the data client-side.
    In the private certificate policy, deny access to the certificate for all principals except an IAM role that the customer provides.
-   [ ] B. Provision a separate AWS Key Management Service (AWS KMS) key for each customer.
    Encrypt the data server-side.
    In the S3 bucket policy, deny decryption of data for all principals except an IAM role that the customer provides.
-   [ ] C. Provision a separate AWS Key Management Service (AWS KMS) key for each customer.
    Encrypt the data server-side.
    In each KMS key policy, deny decryption of data for all principals except an IAM role that the customer provides.
-   [ ] D. Provision an AWS Certificate Manager (ACM) certificate for each customer.
    Encrypt the data client-side.
    In the public certificate policy, deny access to the certificate for all principals except an IAM role that the customer provides.

</details>

<details>
    <summary>Answer</summary>

-   [ ] C. Provision a separate AWS Key Management Service (AWS KMS) key for each customer.
    Encrypt the data server-side.
    In each KMS key policy, deny decryption of data for all principals except an IAM role that the customer provides.

Why these are the correct answers:

C. Provision a separate AWS Key Management Service (AWS KMS) key for each customer. Encrypt the data server-side. In each KMS key policy, deny decryption of data for all principals except an IAM role that the customer provides.

-   [ ] KMS keys encrypt data at rest.
-   [ ] Separate keys isolate customer data.
-   [ ] KMS key policies control access to decryption.
-   [ ] This solution ensures data is secure and accessible only to authorized customers.

<hr> Why are the other answers wrong? <hr>

-   [ ] A and D. ACM certificates are for encrypting data in transit, not at rest.
-   [ ] B. S3 bucket policies control access to the bucket, not decryption.

Therefore, Option C is the correct solution for secure data access.
</details>
<details>
    <summary>Question 697</summary>

A solutions architect creates a VPC that includes two public subnets and two private subnets. A corporate security mandate requires the solutions architect to launch all Amazon EC2 instances in a private subnet. However, when the solutions architect launches an EC2 instance that runs a web server on ports 80 and 443 in a private subnet, no external internet traffic can connect to the server. What should the solutions architect do to resolve this issue?

-   [ ] A. Attach the EC2 instance to an Auto Scaling group in a private subnet.
    Ensure that the DNS record for the website resolves to the Auto Scaling group identifier.
-   [ ] B. Provision an internet-facing Application Load Balancer (ALB) in a public subnet.
    Add the EC2 instance to the target group that is associated with the ALEnsure that the DNS record for the website resolves to the ALB.
-   [ ] C. Launch a NAT gateway in a private subnet.
    Update the route table for the private subnets to add a default route to the NAT gateway.
    Attach a public Elastic IP address to the NAT gateway.
-   [ ] D. Ensure that the security group that is attached to the EC2 instance allows HTTP traffic on port 80 and HTTPS traffic on port 443.
    Ensure that the DNS record for the website resolves to the public IP address of the EC2 instance.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Provision an internet-facing Application Load Balancer (ALB) in a public subnet.
    Add the EC2 instance to the target group that is associated with the ALEnsure that the DNS record for the website resolves to the ALB.

Why these are the correct answers:

B. Provision an internet-facing Application Load Balancer (ALB) in a public subnet. Add the EC2 instance to the target group that is associated with the ALEnsure that the DNS record for the website resolves to the ALB.

-   [ ] An ALB in a public subnet can receive internet traffic.
-   [ ] It can forward traffic to EC2 instances in private subnets.
-   [ ] This allows the web server to be accessible from the internet.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Auto Scaling groups do not make instances accessible from the internet.
-   [ ] C. NAT gateways allow private instances to access the internet, not the other way around.
-   [ ] D. EC2 instances in private subnets are not directly accessible from the internet.

Therefore, Option B is the correct solution for accessing the web server.
</details>
<details>
    <summary>Question 698</summary>

A company is deploying a new application to Amazon Elastic Kubernetes Service (Amazon EKS) with an AWS Fargate cluster. The application needs a storage solution for data persistence. The solution must be highly available and fault tolerant. The solution also must be shared between multiple application containers. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Create Amazon Elastic Block Store (Amazon EBS) volumes in the same Availability Zones where EKS worker nodes are placed.
    Register the volumes in a StorageClass object on an EKS cluster.
    Use EBS Multi-Attach to share the data between containers.
-   [ ] B. Create an Amazon Elastic File System (Amazon EFS) file system.
    Register the file system in a StorageClass object on an EKS cluster.
    Use the same file system for all containers.
-   [ ] C. Create an Amazon Elastic Block Store (Amazon EBS) volume.
    Register the volume in a StorageClass object on an EKS cluster.
    Use the same volume for all containers.
-   [ ] D. Create Amazon Elastic File System (Amazon EFS) file systems in the same Availability Zones where EKS worker nodes are placed.
    Register the file systems in a StorageClass object on an EKS cluster.
    Create an AWS Lambda function to synchronize the data between file systems.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Create an Amazon Elastic File System (Amazon EFS) file system.
    Register the file system in a StorageClass object on an EKS cluster.
    Use the same file system for all containers.

Why these are the correct answers:

B. Create an Amazon Elastic File System (Amazon EFS) file system. Register the file system in a StorageClass object on an EKS cluster. Use the same file system for all containers.

-   [ ] Amazon EFS provides shared, scalable storage for containers.
-   [ ] It is highly available and fault tolerant.
-   [ ] EKS can use EFS via a StorageClass.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. EBS Multi-Attach has limitations and is not ideal for Fargate.
-   [ ] C. EBS volumes are not inherently shared.
-   [ ] D. Multiple EFS file systems and Lambda synchronization add complexity.

Therefore, Option B is the most efficient solution for persistent storage in EKS Fargate.
</details>
<details>
    <summary>Question 699</summary>

A company has an application that uses Docker containers in its local data center. The application runs on a container host that stores persistent data in a volume on the host. The container instances use the stored persistent data. The company wants to move the application to a fully managed service because the company does not want to manage any servers or storage infrastructure. Which solution will meet these requirements?

-   [ ] A. Use Amazon Elastic Kubernetes Service (Amazon EKS) with self-managed nodes.
    Create an Amazon Elastic Block Store (Amazon EBS) volume attached to an Amazon EC2 instance.
    Use the EBS volume as a persistent volume mounted in the containers.
-   [ ] B. Use Amazon Elastic Container Service (Amazon ECS) with an AWS Fargate launch type.
    Create an Amazon Elastic File System (Amazon EFS) volume.
    Add the EFS volume as a persistent storage volume mounted in the containers.
-   [ ] C. Use Amazon Elastic Container Service (Amazon ECS) with an AWS Fargate launch type.
    Create an Amazon S3 bucket.
    Map the S3 bucket as a persistent storage volume mounted in the containers.
-   [ ] D. Use Amazon Elastic Container Service (Amazon ECS) with an Amazon EC2 launch type.
    Create an Amazon Elastic File System (Amazon EFS) volume.
    Add the EFS volume as a persistent storage volume mounted in the containers.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Use Amazon Elastic Container Service (Amazon ECS) with an AWS Fargate launch type.
    Create an Amazon Elastic File System (Amazon EFS) volume.
    Add the EFS volume as a persistent storage volume mounted in the containers.

Why these are the correct answers:

B. Use Amazon Elastic Container Service (Amazon ECS) with an AWS Fargate launch type. Create an Amazon Elastic File System (Amazon EFS) volume. Add the EFS volume as a persistent storage volume mounted in the containers.

-   [ ] ECS Fargate is a fully managed container orchestration service.
-   [ ] EFS provides shared, persistent storage.
-   [ ] This solution avoids managing servers or storage.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. EKS requires managing nodes.
-   [ ] C. S3 is object storage, not a file system for containers.
-   [ ] D. ECS with EC2 launch type requires managing EC2 instances.

Therefore, Option B is the correct solution for a fully managed container environment.
</details>

<details>
    <summary>Question 700</summary>

A gaming company wants to launch a new internet-facing application in multiple AWS Regions. The application will use the TCP and UDP protocols for communication. The company needs to provide high availability and minimum latency for global users. Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)

-   [ ] A. Create internal Network Load Balancers in front of the application in each Region.
-   [ ] B. Create external Application Load Balancers in front of the application in each Region.
-   [ ] C. Create an AWS Global Accelerator accelerator to route traffic to the load balancers in each Region.
-   [ ] D. Configure Amazon Route 53 to use a geolocation routing policy to distribute the traffic.
-   [ ] E. Configure Amazon CloudFront to handle the traffic and route requests to the application in each Region

</details>

<details>
    <summary>Answer</summary>

-   [ ] A. Create internal Network Load Balancers in front of the application in each Region.
-   [ ] C. Create an AWS Global Accelerator accelerator to route traffic to the load balancers in each Region.

Why these are the correct answers:

A. Create internal Network Load Balancers in front of the application in each Region.

-   [ ] Network Load Balancers (NLBs) can handle both TCP and UDP traffic.
-   [ ] Internal NLBs provide load balancing within each Region.

C. Create an AWS Global Accelerator accelerator to route traffic to the load balancers in each Region.

-   [ ] AWS Global Accelerator minimizes latency for global users.
-   [ ] It routes traffic to the nearest healthy endpoint.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. Application Load Balancers (ALBs) only support HTTP and HTTPS, not UDP.
-   [ ] D. Route 53 geolocation routing routes traffic based on user location but does not inherently minimize latency.
-   [ ] E. CloudFront is for caching content, not for load balancing or handling UDP traffic.

Therefore, Options A and C are the correct actions to meet the requirements.
</details>




































