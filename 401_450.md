# AWS-SAA-PRACTICE-EXAM Questions 401-410

<details>
  <summary>Question 401</summary>

A company wants to use the AWS Cloud to make an existing application highly available and resilient. The current version of the application resides in the company's data center. The application recently experienced data loss after a database server crashed because of an unexpected power outage. The company needs a solution that avoids any single points of failure. The solution must give the application the ability to scale to meet user demand.

Which solution will meet these requirements?

-   [ ] A. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance in a Multi-AZ configuration.
-   [ ] B. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group in a single Availability Zone. Deploy the database on an EC2 instance. Enable EC2 Auto Recovery.
-   [ ] C. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance with a read replica in a single Availability Zone. Promote the read replica to replace the primary DB instance if the primary DB instance fails.
-   [ ] D. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Deploy the primary and secondary database servers on EC2 instances across multiple Availability Zones. Use Amazon Elastic Block Store (Amazon EBS) Multi-Attach to create shared storage between the instances.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance in a Multi-AZ configuration.

Why these are the correct answers:

A. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance in a Multi-AZ configuration.

-   [ ]   Auto Scaling groups across multiple Availability Zones provide high availability and scalability for the application servers.
-   [ ]   Amazon RDS Multi-AZ configuration ensures database availability and failover capability, avoiding a single point of failure.

<hr> Why are the other answers wrong? <hr>

-   [ ]   B. Using Auto Scaling in a single Availability Zone does not provide high availability. EC2 Auto Recovery only recovers individual instances, not Availability Zone failures.
-   [ ]   C. A single read replica does not provide automatic failover for write operations. Manual promotion adds complexity and potential downtime.
-   [ ]   D. Deploying the database on EC2 instances increases management overhead. EBS Multi-Attach has limitations and is not suitable for all database workloads.

Therefore, using Auto Scaling across Availability Zones and Amazon RDS Multi-AZ is the most appropriate solution.
</details>
<details>
  <summary>Question 402</summary>

A company needs to ingest and handle large amounts of streaming data that its application generates. The application runs on Amazon EC2 instances and sends data to Amazon Kinesis Data Streams, which is configured with default settings. Every other day, the application consumes the data and writes the data to an Amazon S3 bucket for business intelligence (BI) processing. The company observes that Amazon S3 is not receiving all the data that the application sends to Kinesis Data Streams. What should a solutions architect do to resolve this issue?

-   [ ] A. Update the Kinesis Data Streams default settings by modifying the data retention period.
-   [ ] B. Update the application to use the Kinesis Producer Library (KPL) to send the data to Kinesis Data Streams.
-   [ ] C. Update the number of Kinesis shards to handle the throughput of the data that is sent to Kinesis Data Streams.
-   [ ] D. Turn on S3 Versioning within the S3 bucket to preserve every version of every object that is ingested in the S3 bucket.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Update the Kinesis Data Streams default settings by modifying the data retention period.

Why these are the correct answers:

A. Update the Kinesis Data Streams default settings by modifying the data retention period.

-   [ ]   Kinesis Data Streams has a default data retention period of 24 hours.
-   [ ]   If the application consumes the data every other day (every 48 hours), the default retention period is insufficient.
-   [ ]   Increasing the retention period ensures that data is available for processing.

<hr> Why are the other answers wrong? <hr>

-   [ ]   B. The Kinesis Producer Library (KPL) improves throughput but does not directly address data loss due to retention period.
-   [ ]   C. Increasing the number of shards helps with throughput but does not solve the issue of data expiring before it is consumed.
-   [ ]   D. S3 Versioning preserves different versions of objects but does not address data loss in Kinesis Data Streams.

Therefore, modifying the data retention period is the most appropriate solution.
</details>
<details>
  <summary>Question 403</summary>

A developer has an application that uses an AWS Lambda function to upload files to Amazon S3 and needs the required permissions to perform the task. The developer already has an IAM user with valid IAM credentials required for Amazon S3. What should a solutions architect do to grant the permissions?

-   [ ] A. Add required IAM permissions in the resource policy of the Lambda function.
-   [ ] B. Create a signed request using the existing IAM credentials in the Lambda function.
-   [ ] C. Create a new IAM user and use the existing IAM credentials in the Lambda function.
-   [ ] D. Create an IAM execution role with the required permissions and attach the IAM role to the Lambda function.
</details>
<details>
  <summary>Answer</summary>

-   [ ] D. Create an IAM execution role with the required permissions and attach the IAM role to the Lambda function.

Why these are the correct answers:

D. Create an IAM execution role with the required permissions and attach the IAM role to the Lambda function.

-   [ ]   IAM roles provide temporary security credentials to AWS services, such as Lambda functions.
-   [ ]   Attaching an IAM role to the Lambda function grants it the necessary permissions to access S3.
-   [ ]   This is the most secure and recommended way to grant permissions to Lambda functions.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Resource policies are not used to grant permissions to Lambda functions to access other AWS services.
-   [ ]   B. Signed requests use IAM user credentials, which is less secure than using roles.
-   [ ]   C. Creating a new IAM user is unnecessary and does not solve the problem of securely granting permissions.

Therefore, using an IAM execution role is the best practice for granting permissions to Lambda functions.
</details>
<details>
  <summary>Question 404</summary>

A company has deployed a serverless application that invokes an AWS Lambda function when new documents are uploaded to an Amazon S3 bucket. The application uses the Lambda function to process the documents. After a recent marketing campaign, the company noticed that the application did not process many of the documents. What should a solutions architect do to improve the architecture of this application?

-   [ ] A. Set the Lambda function's runtime timeout value to 15 minutes.
-   [ ] B. Configure an S3 bucket replication policy. Stage the documents in the S3 bucket for later processing.
-   [ ] C. Deploy an additional Lambda function. Load balance the processing of the documents across the two Lambda functions.
-   [ ] D. Create an Amazon Simple Queue Service (Amazon SQS) message queue. Send the requests to the queue. Configure the queue as an event source for Lambda.
</details>
<details>
  <summary>Answer</summary>

-   [ ] D. Create an Amazon Simple Queue Service (Amazon SQS) message queue. Send the requests to the queue. Configure the queue as an event source for Lambda.

Why these are the correct answers:

D. Create an Amazon Simple Queue Service (Amazon SQS) message queue. Send the requests to the queue. Configure the queue as an event source for Lambda.

-   [ ]   Amazon SQS decouples the S3 upload process from the Lambda processing.
-   [ ]   SQS queues can buffer incoming requests, ensuring that no documents are lost during high traffic.
-   [ ]   Configuring the queue as an event source for Lambda allows Lambda to process messages from the queue reliably.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Increasing the Lambda timeout might help with individual processing times but does not address the issue of lost documents.
-   [ ]   B. S3 replication is for copying objects between buckets, not for ensuring reliable processing.
-   [ ]   C. Deploying additional Lambda functions without a queue does not prevent document loss during high traffic.

Therefore, using an SQS queue is the most appropriate solution for reliable document processing.
</details>
<details>
  <summary>Question 405</summary>

A solutions architect is designing the architecture for a software demonstration environment. The environment will run on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). The system will experience significant increases in traffic during working hours but is not required to operate on weekends. Which combination of actions should the solutions architect take to ensure that the system can scale to meet demand? (Choose two.)

-   [ ] A. Use AWS Auto Scaling to adjust the ALB capacity based on request rate.
-   [ ] B. Use AWS Auto Scaling to scale the capacity of the VPC internet gateway.
-   [ ] C. Launch the EC2 instances in multiple AWS Regions to distribute the load across Regions.
-   [ ] D. Use a target tracking scaling policy to scale the Auto Scaling group based on instance CPU utilization.
-   [ ] E. Use scheduled scaling to change the Auto Scaling group minimum, maximum, and desired capacity to zero for weekends. Revert to the default values at the start of the week.
</details>
<details>
  <summary>Answer</summary>

-   [ ] D. Use a target tracking scaling policy to scale the Auto Scaling group based on instance CPU utilization.
-   [ ] E. Use scheduled scaling to change the Auto Scaling group minimum, maximum, and desired capacity to zero for weekends. Revert to the default values at the start of the week.

Why these are the correct answers:

D. Use a target tracking scaling policy to scale the Auto Scaling group based on instance CPU utilization.

-   [ ]   Target tracking scaling policies automatically adjust the number of EC2 instances in response to changes in CPU utilization, ensuring the system can handle increased traffic during working hours.

E. Use scheduled scaling to change the Auto Scaling group minimum, maximum, and desired capacity to zero for weekends. Revert to the default values at the start of the week.

-   [ ]   Scheduled scaling allows you to set the capacity of the Auto Scaling group based on a predictable schedule, such as reducing it to zero on weekends when the system is not in use.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Auto Scaling does not directly adjust ALB capacity. Auto Scaling adjusts the number of EC2 instances behind the ALB.
-   [ ]   B. You cannot scale the capacity of a VPC internet gateway.
-   [ ]   C. Launching EC2 instances in multiple AWS Regions is unnecessary and costly for scaling within working hours.

Therefore, using target tracking and scheduled scaling is the most appropriate solution for this scenario.
</details>
<details>
  <summary>Question 406</summary>

A solutions architect is designing a two-tiered architecture that includes a public subnet and a database subnet. The web servers in the public subnet must be open to the internet on port 443. The Amazon RDS for MySQL DB instance in the database subnet must be accessible only to the web servers on port 3306.

Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)

-   [ ] A. Create a network ACL for the public subnet. Add a rule to deny outbound traffic to 0.0.0.0/0 on port 3306.
-   [ ] B. Create a security group for the DB instance. Add a rule to allow traffic from the public subnet CIDR block on port 3306.
-   [ ] C. Create a security group for the web servers in the public subnet. Add a rule to allow traffic from 0.0.0.0/0 on port 443.
-   [ ] D. Create a security group for the DB instance. Add a rule to allow traffic from the web servers' security group on port 3306.
-   [ ] E. Create a security group for the DB instance. Add a rule to deny all traffic except traffic from the web servers' security group on port 3306.
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. Create a security group for the web servers in the public subnet. Add a rule to allow traffic from 0.0.0.0/0 on port 443.
-   [ ] D. Create a security group for the DB instance. Add a rule to allow traffic from the web servers' security group on port 3306.

Why these are the correct answers:

C. Create a security group for the web servers in the public subnet. Add a rule to allow traffic from 0.0.0.0/0 on port 443.

-   [ ]   Security groups act as a virtual firewall for EC2 instances.
-   [ ]   Allowing inbound traffic from 0.0.0.0/0 on port 443 makes the web servers accessible from the internet.

D. Create a security group for the DB instance. Add a rule to allow traffic from the web servers' security group on port 3306.

-   [ ]   This ensures that only the web servers can access the database, enhancing security.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Network ACLs operate at the subnet level and are less granular than security groups for controlling traffic to specific instances. Denying outbound traffic from the public subnet is not the correct approach.
-   [ ]   B. Allowing traffic from the entire public subnet CIDR block is less secure than allowing traffic only from the web servers' security group.
-   [ ]   E. Denying all traffic except from the web servers' security group is redundant and less clear than simply allowing traffic from that security group.

Therefore, using security groups with appropriate rules is the most secure and effective solution.
</details>
<details>
  <summary>Question 407</summary>

A company is implementing a shared storage solution for a gaming application that is hosted in the AWS Cloud. The company needs the ability to use Lustre clients to access data. The solution must be fully managed. Which solution meets these requirements?

-   [ ] A. Create an AWS DataSync task that shares the data as a mountable file system. Mount the file system to the application server.
-   [ ] B. Create an AWS Storage Gateway file gateway. Create a file share that uses the required client protocol. Connect the application server to the file share.
-   [ ] C. Create an Amazon Elastic File System (Amazon EFS) file system, and configure it to support Lustre. Attach the file system to the origin server. Connect the application server to the file system.
-   [ ] D. Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect the application server to the file system.
</details>
<details>
  <summary>Answer</summary>

-   [ ] D. Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect the application server to the file system.

Why these are the correct answers:

D. Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect the application server to the file system.

-   [ ]   Amazon FSx for Lustre is a fully managed file system optimized for compute-intensive workloads that require high performance and POSIX compliance.
-   [ ]   It supports the Lustre file system, meeting the requirement for Lustre clients.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. AWS DataSync is a data transfer service, not a file system.
-   [ ]   B. AWS Storage Gateway provides file interfaces to cloud storage but does not support Lustre.
-   [ ]   C. Amazon EFS does not support the Lustre protocol.

Therefore, Amazon FSx for Lustre is the most appropriate solution for a fully managed, high-performance file system with Lustre client support.
</details>
<details>
  <summary>Question 408</summary>

A company runs an application that receives data from thousands of geographically dispersed remote devices that use UDP. The application processes the data immediately and sends a message back to the device if necessary. No data is stored. The company needs a solution that minimizes latency for the data transmission from the devices. The solution also must provide rapid failover to another AWS Region.

Which solution will meet these requirements?

-   [ ] A. Configure an Amazon Route 53 failover routing policy. Create a Network Load Balancer (NLB) in each of the two Regions. Configure the NLB to invoke an AWS Lambda function to process the data.
-   [ ] B. Use AWS Global Accelerator. Create a Network Load Balancer (NLB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the NLProcess the data in Amazon ECS.
-   [ ] C. Use AWS Global Accelerator. Create an Application Load Balancer (ALB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the ALB. Process the data in Amazon ECS.
-   [ ] D. Configure an Amazon Route 53 failover routing policy. Create an Application Load Balancer (ALB) in each of the two Regions. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the ALB. Process the data in Amazon ECS.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Use AWS Global Accelerator. Create a Network Load Balancer (NLB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the NLProcess the data in Amazon ECS.

Why these are the correct answers:

B. Use AWS Global Accelerator. Create a Network Load Balancer (NLB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the NLProcess the data in Amazon ECS.

-   [ ]   AWS Global Accelerator minimizes latency by using the AWS global network to optimize the path from the remote devices to the application.
-   [ ]   Network Load Balancers (NLBs) are designed for high-performance, low-latency traffic and support UDP.
-   [ ]   Amazon ECS with Fargate provides a scalable and serverless container orchestration platform.
-   [ ]   This combination provides low latency and rapid failover.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Route 53 failover routing is for DNS-level failover, which is slower than Global Accelerator for real-time applications. Lambda is not suitable for handling high-volume, continuous UDP traffic.
-   [ ]   C and D. Application Load Balancers (ALBs) are designed for HTTP/HTTPS traffic, not UDP.

Therefore, AWS Global Accelerator with NLBs and ECS Fargate is the most appropriate solution.
</details>
<details>
  <summary>Question 409</summary>

A solutions architect must migrate a Windows Internet Information Services (IIS) web application to AWS. The application currently relies on a file share hosted in the user's on-premises network-attached storage (NAS). The solutions architect has proposed migrating the IIS web servers to Amazon EC2 instances in multiple Availability Zones that are connected to the storage solution, and configuring an Elastic Load Balancer attached to the instances. Which replacement to the on-premises file share is MOST resilient and durable?

-   [ ] A. Migrate the file share to Amazon RDS.
-   [ ] B. Migrate the file share to AWS Storage Gateway.
-   [ ] C. Migrate the file share to Amazon FSx for Windows File Server.
-   [ ] D. Migrate the file share to Amazon Elastic File System (Amazon EFS).
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. Migrate the file share to Amazon FSx for Windows File Server.

Why these are the correct answers:

C. Migrate the file share to Amazon FSx for Windows File Server.

-   [ ]   Amazon FSx for Windows File Server provides a fully managed, highly available, and scalable file storage solution that supports the SMB protocol, which is native to Windows.
-   [ ]   It is designed to be compatible with Windows-based applications and provides the necessary resilience and durability.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Amazon RDS is a database service and not suitable for replacing a file share.
-   [ ]   B. AWS Storage Gateway connects on-premises storage to AWS cloud storage but is not a fully managed file system in the cloud.
-   [ ]   D. Amazon EFS is a file storage service for Linux-based instances and does not natively support the SMB protocol used by Windows applications.

Therefore, Amazon FSx for Windows File Server is the most appropriate replacement for an on-premises Windows file share.
</details>
<details>
  <summary>Question 410</summary>

A company is deploying a new application on Amazon EC2 instances. The application writes data to Amazon Elastic Block Store (Amazon EBS) volumes. The company needs to ensure that all data that is written to the EBS volumes is encrypted at rest. Which solution will meet this requirement?

-   [ ] A. Create an IAM role that specifies EBS encryption. Attach the role to the EC2 instances.
-   [ ] B. Create the EBS volumes as encrypted volumes. Attach the EBS volumes to the EC2 instances.
-   [ ] C. Create an EC2 instance tag that has a key of Encrypt and a value of True. Tag all instances that require encryption at the EBS level.
-   [ ] D. Create an AWS Key Management Service (AWS KMS) key policy that enforces EBS encryption in the account. Ensure that the key policy is active.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Create the EBS volumes as encrypted volumes. Attach the EBS volumes to the EC2 instances.

Why these are the correct answers:

B. Create the EBS volumes as encrypted volumes. Attach the EBS volumes to the EC2 instances.

-   [ ]   Encrypting EBS volumes at creation ensures that all data written to the volume is encrypted at rest.
-   [ ]   This is the most direct and effective way to meet the requirement.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. IAM roles control access to AWS services but do not enforce EBS encryption.
-   [ ]   C. EC2 instance tags are metadata and do not enforce encryption.
-   [ ]   D. KMS key policies control access to KMS keys but do not enforce EBS encryption.

Therefore, creating encrypted EBS volumes is the correct solution.
</details>

# AWS-SAA-PRACTICE-EXAM Questions 411-420

<details>
  <summary>Question 411</summary>

A company has a web application with sporadic usage patterns. There is heavy usage at the beginning of each month, moderate usage at the start of each week, and unpredictable usage during the week. The application consists of a web server and a MySQL database server running inside the data center. The company would like to move the application to the AWS Cloud, and needs to select a cost-effective database platform that will not require database modifications. Which solution will meet these requirements?

-   [ ] A. Amazon DynamoDB
-   [ ] B. Amazon RDS for MySQL
-   [ ] C. MySQL-compatible Amazon Aurora Serverless
-   [ ] D. MySQL deployed on Amazon EC2 in an Auto Scaling group
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. MySQL-compatible Amazon Aurora Serverless

Why these are the correct answers:

C. MySQL-compatible Amazon Aurora Serverless

-   [ ]   Amazon Aurora Serverless automatically starts up, shuts down, and scales database capacity based on application needs.
-   [ ]   It is cost-effective for applications with sporadic usage patterns.
-   [ ]   It is compatible with MySQL, requiring minimal to no database modifications.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Amazon DynamoDB is a NoSQL database and would require significant application modifications.
-   [ ]   B. Amazon RDS for MySQL requires provisioning a specific instance size, which may not be cost-effective for sporadic usage.
-   [ ]   D. Deploying MySQL on Amazon EC2 with Auto Scaling involves managing the database and scaling, increasing operational overhead.

Therefore, Amazon Aurora Serverless is the most suitable and cost-effective solution.
</details>
<details>
  <summary>Question 412</summary>

An image-hosting company stores its objects in Amazon S3 buckets. The company wants to avoid accidental exposure of the objects in the S3 buckets to the public. All S3 objects in the entire AWS account need to remain private. Which solution will meet these requirements?

-   [ ] A. Use Amazon GuardDuty to monitor S3 bucket policies. Create an automatic remediation action rule that uses an AWS Lambda function to remediate any change that makes the objects public.
-   [ ] B. Use AWS Trusted Advisor to find publicly accessible S3 buckets. Configure email notifications in Trusted Advisor when a change is detected. Manually change the S3 bucket policy if it allows public access.
-   [ ] C. Use AWS Resource Access Manager to find publicly accessible S3 buckets. Use Amazon Simple Notification Service (Amazon SNS) to invoke an AWS Lambda function when a change is detected. Deploy a Lambda function that programmatically remediates the change.
-   [ ] D. Use the S3 Block Public Access feature on the account level. Use AWS Organizations to create a service control policy (SCP) that prevents IAM users from changing the setting. Apply the SCP to the account.
</details>
<details>
  <summary>Answer</summary>

-   [ ] D. Use the S3 Block Public Access feature on the account level. Use AWS Organizations to create a service control policy (SCP) that prevents IAM users from changing the setting. Apply the SCP to the account.

Why these are the correct answers:

D. Use the S3 Block Public Access feature on the account level. Use AWS Organizations to create a service control policy (SCP) that prevents IAM users from changing the setting. Apply the SCP to the account.

-   [ ]   S3 Block Public Access at the account level prevents any public access to S3 buckets and objects within that account.
-   [ ]   AWS Organizations service control policies (SCPs) can enforce this setting across all accounts in the organization, preventing users from changing it.
-   [ ]   This solution provides the strongest and most centralized control.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A, B, and C. These solutions involve monitoring and remediation, which are reactive rather than preventative. They also introduce operational overhead.

Therefore, using S3 Block Public Access with an SCP is the most secure and effective solution to prevent public access.
</details>
<details>
  <summary>Question 413</summary>

An ecommerce company is experiencing an increase in user traffic. The company's store is deployed on Amazon EC2 instances as a two-tier web application consisting of a web tier and a separate database tier. As traffic increases, the company notices that the architecture is causing significant delays in sending timely marketing and order confirmation email to users. The company wants to reduce the time it spends resolving complex email delivery issues and minimize operational overhead. What should a solutions architect do to meet these requirements?

-   [ ] A. Create a separate application tier using EC2 instances dedicated to email processing.
-   [ ] B. Configure the web instance to send email through Amazon Simple Email Service (Amazon SES).
-   [ ] C. Configure the web instance to send email through Amazon Simple Notification Service (Amazon SNS).
-   [ ] D. Create a separate application tier using EC2 instances dedicated to email processing. Place the instances in an Auto Scaling group.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Configure the web instance to send email through Amazon Simple Email Service (Amazon SES).

Why these are the correct answers:

B. Configure the web instance to send email through Amazon Simple Email Service (Amazon SES).

-   [ ]   Amazon SES is a cloud-based email sending service that is designed to help digital marketers and application developers send marketing, notification, and transactional emails.
-   [ ]   Offloading email sending to SES reduces the load on the web instances and simplifies email delivery issues.
-   [ ]   This solution minimizes operational overhead by using a managed service.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A and D. Creating separate EC2 instances or Auto Scaling groups for email processing adds complexity and operational overhead.
-   [ ]   C. Amazon SNS is for sending notifications, not for transactional or marketing emails.

Therefore, using Amazon SES is the most efficient and appropriate solution.
</details>
<details>
  <summary>Question 414</summary>

A company has a business system that generates hundreds of reports each day. The business system saves the reports to a network share in CSV format. The company needs to store this data in the AWS Cloud in near-real time for analysis. Which solution will meet these requirements with the LEAST administrative overhead?

-   [ ] A. Use AWS DataSync to transfer the files to Amazon S3. Create a scheduled task that runs at the end of each day.
-   [ ] B. Create an Amazon S3 File Gateway. Update the business system to use a new network share from the S3 File Gateway.
-   [ ] C. Use AWS DataSync to transfer the files to Amazon S3. Create an application that uses the DataSync API in the automation workflow.
-   [ ] D. Deploy an AWS Transfer for SFTP endpoint. Create a script that checks for new files on the network share and uploads the new files by using SFTP.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Create an Amazon S3 File Gateway. Update the business system to use a new network share from the S3 File Gateway.

Why these are the correct answers:

B. Create an Amazon S3 File Gateway. Update the business system to use a new network share from the S3 File Gateway.

-   [ ]   Amazon S3 File Gateway provides a seamless way to access S3 objects as files on a local file system.
-   [ ]   Updating the business system to use this new network share requires minimal changes and reduces administrative overhead.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A and C. Using AWS DataSync requires additional scheduling or API integration, increasing overhead.
-   [ ]   D. Deploying an SFTP endpoint and creating scripts adds complexity and management overhead.

Therefore, Amazon S3 File Gateway is the simplest and most efficient solution for near-real-time data transfer.
</details>
<details>
  <summary>Question 415</summary>

A company is storing petabytes of data in Amazon S3 Standard. The data is stored in multiple S3 buckets and is accessed with varying frequency. The company does not know access patterns for all the data. The company needs to implement a solution for each S3 bucket to optimize the cost of S3 usage. Which solution will meet these requirements with the MOST operational efficiency?

-   [ ] A. Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Intelligent-Tiering.
-   [ ] B. Use the S3 storage class analysis tool to determine the correct tier for each object in the S3 bucket. Move each object to the identified storage tier.
-   [ ] C. Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Glacier Instant Retrieval.
-   [ ] D. Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 One Zone-Infrequent Access (S3 One Zone-IA).
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Intelligent-Tiering.

Why these are the correct answers:

A. Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Intelligent-Tiering.

-   [ ]   S3 Intelligent-Tiering automatically optimizes storage costs by moving data to the most cost-effective tier based on access patterns.
-   [ ]   It handles objects with varying access frequencies, and using a lifecycle configuration automates this process.

<hr> Why are the other answers wrong? <hr>

-   [ ]   B. Using the S3 storage class analysis tool and manually moving objects is operationally intensive.
-   [ ]   C. S3 Glacier Instant Retrieval is for archive data with infrequent access, not for data with varying access patterns.
-   [ ]   D. S3 One Zone-IA reduces availability and is not suitable for all data.

Therefore, S3 Intelligent-Tiering is the most efficient solution for cost optimization with unknown access patterns.
</details>
<details>
  <summary>Question 416</summary>

A rapidly growing global ecommerce company is hosting its web application on AWS. The web application includes static content and dynamic content. The website stores online transaction processing (OLTP) data in an Amazon RDS database The website's users are experiencing slow page loads. Which combination of actions should a solutions architect take to resolve this issue? (Choose two.)

-   [ ] A. Configure an Amazon Redshift cluster.
-   [ ] B. Set up an Amazon CloudFront distribution.
-   [ ] C. Host the dynamic web content in Amazon S3.
-   [ ] D. Create a read replica for the RDS DB instance.
-   [ ] E. Configure a Multi-AZ deployment for the RDS DB instance.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Set up an Amazon CloudFront distribution.
-   [ ] D. Create a read replica for the RDS DB instance.

Why these are the correct answers:

B. Set up an Amazon CloudFront distribution.

-   [ ]   Amazon CloudFront is a content delivery network (CDN) that can cache static content closer to users, improving load times.

D. Create a read replica for the RDS DB instance.

-   [ ]   A read replica offloads read operations from the primary database, improving database performance.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Amazon Redshift is for data warehousing and analytics, not for improving web application performance.
-   [ ]   C. Hosting dynamic content in S3 is not practical. S3 is for static content.
-   [ ]   E. Multi-AZ deployments improve database availability, not read performance.

Therefore, CloudFront and RDS read replicas are the most appropriate solutions.
</details>
<details>
  <summary>Question 417</summary>

A company uses Amazon EC2 instances and AWS Lambda functions to run its application. The company has VPCs with public subnets and private subnets in its AWS account. The EC2 instances run in a private subnet in one of the VPCs. The Lambda functions need direct network access to the EC2 instances for the application to work. The application will run for at least 1 year. The company expects the number of Lambda functions that the application uses to increase during that time. The company wants to maximize its savings on all application resources and to keep network latency between the services low. Which solution will meet these requirements?

-   [ ] A. Purchase an EC2 Instance Savings Plan Optimize the Lambda functions' duration and memory usage and the number of invocations. Connect the Lambda functions to the private subnet that contains the EC2 instances.
-   [ ] B. Purchase an EC2 Instance Savings Plan Optimize the Lambda functions' duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to a public subnet in the same VPC where the EC2 instances run.
-   [ ] C. Purchase a Compute Savings Plan. Optimize the Lambda functions' duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to the private subnet that contains the EC2 instances.
-   [ ] D. Purchase a Compute Savings Plan. Optimize the Lambda functions' duration and memory usage, the number of invocations, and the amount of data that is transferred. Keep the Lambda functions in the Lambda service VPC.
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. Purchase a Compute Savings Plan. Optimize the Lambda functions' duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to the private subnet that contains the EC2 instances.

Why these are the correct answers:

C. Purchase a Compute Savings Plan. Optimize the Lambda functions' duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to the private subnet that contains the EC2 instances.

-   [ ]   Compute Savings Plans offer cost savings for both EC2 and Lambda usage, which is beneficial as the number of Lambda functions increases.
-   [ ]   Optimizing Lambda functions reduces costs.
-   [ ]   Connecting Lambda functions to the private subnet ensures low latency.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A and B. EC2 Instance Savings Plans are less flexible than Compute Savings Plans, as they do not apply to Lambda costs. Using a public subnet increases security risks.
-   [ ]   D. Keeping Lambda functions in the Lambda service VPC would require a VPC peering or other network connection, increasing latency.

Therefore, Compute Savings Plans and connecting Lambda functions to the private subnet are the most cost-effective and efficient solution.
</details>
<details>
  <summary>Question 418</summary>

A solutions architect needs to allow team members to access Amazon S3 buckets in two different AWS accounts: a development account and a production account. The team currently has access to S3 buckets in the development account by using unique IAM users that are assigned to an IAM group that has appropriate permissions in the account. The solutions architect has created an IAM role in the production account. The role has a policy that grants access to an S3 bucket in the production account. Which solution will meet these requirements while complying with the principle of least privilege?

-   [ ] A. Attach the Administrator Access policy to the development account users.
-   [ ] B. Add the development account as a principal in the trust policy of the role in the production account.
-   [ ] C. Turn off the S3 Block Public Access feature on the S3 bucket in the production account.
-   [ ] D. Create a user in the production account with unique credentials for each team member.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Add the development account as a principal in the trust policy of the role in the production account.

Why these are the correct answers:

B. Add the development account as a principal in the trust policy of the role in the production account.

-   [ ]   IAM roles allow users in one AWS account to assume the role in another account, granting them temporary access to resources in that account.
-   [ ]   Adding the development account as a trusted principal allows the team members to assume the role.
-   [ ]   This follows the principle of least privilege by granting only the necessary permissions.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Administrator Access policy grants excessive permissions, violating least privilege.
-   [ ]   C. Turning off S3 Block Public Access increases security risks.
-   [ ]   D. Creating separate users in the production account adds management overhead and does not leverage IAM roles for secure cross-account access.

Therefore, using IAM roles for cross-account access is the most secure and efficient solution.
</details>
<details>
  <summary>Question 419</summary>

A company uses AWS Organizations with all features enabled and runs multiple Amazon EC2 workloads in the ap-southeast-2 Region. The company has a service control policy (SCP) that prevents any resources from being created in any other Region. A security policy requires the company to encrypt all data at rest. An audit discovers that employees have created Amazon Elastic Block Store (Amazon EBS) volumes for EC2 instances without encrypting the volumes. The company wants any new EC2 instances that any IAM user or root user launches in ap-southeast-2 to use encrypted EBS volumes. The company wants a solution that will have minimal effect on employees who create EBS volumes. Which combination of steps will meet these requirements? (Choose two.)

-   [ ] A. In the Amazon EC2 console, select the EBS encryption account attribute and define a default encryption key.
-   [ ] B. Create an IAM permission boundary. Attach the permission boundary to the root organizational unit (OU). Define the boundary to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.
-   [ ] C. Create an SCP. Attach the SCP to the root organizational unit (OU). Define the SCP to deny the ec2:CreateVolume action whenthe ec2:Encrypted condition equals false.
-   [ ] D. Update the IAM policies for each account to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.
-   [ ] E. In the Organizations management account, specify the Default EBS volume encryption setting.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. In the Amazon EC2 console, select the EBS encryption account attribute and define a default encryption key.
-   [ ] C. Create an SCP. Attach the SCP to the root organizational unit (OU). Define the SCP to deny the ec2:CreateVolume action whenthe ec2:Encrypted condition equals false.

Why these are the correct answers:

A. In the Amazon EC2 console, select the EBS encryption account attribute and define a default encryption key.

-   [ ]   Setting the default EBS encryption ensures that all new EBS volumes are encrypted by default, minimizing the impact on employees.

C. Create an SCP. Attach the SCP to the root organizational unit (OU). Define the SCP to deny the ec2:CreateVolume action whenthe ec2:Encrypted condition equals false.

-   [ ]   SCPs enforce organizational policies across all accounts in AWS Organizations.
-   [ ]   This prevents IAM users and root users from creating unencrypted EBS volumes.

<hr> Why are the other answers wrong? <hr>

-   [ ]   B. IAM permission boundaries do not enforce policies across AWS Organizations.
-   [ ]   D. Updating IAM policies in each account is cumbersome and does not provide centralized control.
-   [ ]   E. While setting the Default EBS volume encryption setting is correct, it does not prevent users from disabling encryption, so an SCP is still needed for enforcement.

Therefore, setting the default encryption and using an SCP is the most effective solution.
</details>
<details>
  <summary>Question 420</summary>

A company wants to use an Amazon RDS for PostgreSQL DB cluster to simplify time-consuming database administrative tasks for production database workloads. The company wants to ensure that its database is highly available and will provide automatic failover support in most scenarios in less than 40 seconds. The company wants to offload reads off of the primary instance and keep costs as low as possible. Which solution will meet these requirements?

-   [ ] A. Use an Amazon RDS Multi-AZ DB instance deployment. Create one read replica and point the read workload to the read replica.
-   [ ] B. Use an Amazon RDS Multi-AZ DB duster deployment Create two read replicas and point the read workload to the read replicas.
-   [ ] C. Use an Amazon RDS Multi-AZ DB instance deployment. Point the read workload to the secondary instances in the Multi-AZ pair.
-   [ ] D. Use an Amazon RDS Multi-AZ DB cluster deployment Point the read workload to the reader endpoint.
</details>
<details>
  <summary>Answer</summary>

-   [ ] D. Use an Amazon RDS Multi-AZ DB cluster deployment Point the read workload to the reader endpoint.

Why these are the correct answers:

D. Use an Amazon RDS Multi-AZ DB cluster deployment Point the read workload to the reader endpoint.

-   [ ]   Amazon Aurora PostgreSQL Multi-AZ DB clusters provide high availability and automatic failover.
-   [ ]   Aurora clusters have a reader endpoint that distributes read traffic across Aurora Replicas, offloading the primary instance.
-   [ ]   Aurora's architecture is designed for fast failover.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A, B, and C. Amazon RDS Multi-AZ DB instances are for high availability, not read scaling. Read replicas are separate instances, and Multi-AZ secondary instances are for failover, not general read traffic.

Therefore, Amazon Aurora PostgreSQL Multi-AZ DB clusters with the reader endpoint are the most appropriate solution.

</details>

# AWS-SAA-PRACTICE-EXAM Questions 421-430

<details>
  <summary>Question 421</summary>

A company runs a highly available SFTP service.
The SFTP service uses two Amazon EC2 Linux instances that run with elastic IP addresses to accept traffic from trusted IP sources on the internet.
The SFTP service is backed by shared storage that is attached to the instances.
User accounts are created and managed as Linux users in the SFTP servers.
The company wants a serverless option that provides high IOPS performance and highly configurable security.
The company also wants to maintain control over user permissions.

Which solution will meet these requirements?

- [ ] A. Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume.
Create an AWS Transfer Family SFTP service with a public endpoint that allows only trusted IP addresses.
Attach the EBS volume to the SFTP service endpoint.
Grant users access to the SFTP service.
- [ ] B. Create an encrypted Amazon Elastic File System (Amazon EFS) volume.
Create an AWS Transfer Family SFTP service with elastic IP addresses and a VPC endpoint that has internet-facing access.
Attach a security group to the endpoint that allows only trusted IP addresses.
Attach the EFS volume to the SFTP service endpoint.
Grant users access to the SFTP service.
- [ ] C. Create an Amazon S3 bucket with default encryption enabled.
Create an AWS Transfer Family SFTP service with a public endpoint that allows only trusted IP addresses.
Attach the S3 bucket to the SFTP service endpoint.
Grant users access to the SFTP service.
- [ ] D. Create an Amazon S3 bucket with default encryption enabled.
Create an AWS Transfer Family SFTP service with a VPC endpoint that has internal access in a private subnet.
Attach a security group that allows only trusted IP addresses.
Attach the S3 bucket to the SFTP service endpoint.
Grant users access to the SFTP service.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Create an encrypted Amazon Elastic File System (Amazon EFS) volume.
Create an AWS Transfer Family SFTP service with elastic IP addresses and a VPC endpoint that has internet-facing access.
Attach a security group to the endpoint that allows only trusted IP addresses.
Attach the EFS volume to the SFTP service endpoint.
Grant users access to the SFTP service.

Why these are the correct answers:

B. Create an encrypted Amazon Elastic File System (Amazon EFS) volume.
Create an AWS Transfer Family SFTP service with elastic IP addresses and a VPC endpoint that has internet-facing access.
Attach a security group to the endpoint that allows only trusted IP addresses.
Attach the EFS volume to the SFTP service endpoint.
Grant users access to the SFTP service.

- [ ] AWS Transfer Family provides a serverless SFTP service.
- [ ] Amazon EFS provides shared file storage with high IOPS performance.
- [ ] Using a VPC endpoint with a security group allows for highly configurable security by controlling access.
- [ ] EFS allows for maintaining control over user permissions.

<hr> Why are the other answers wrong? <hr>

- [ ] A. Using EBS volumes with AWS Transfer Family is not a valid configuration, as EBS is block storage and not designed for shared file access in this context.
- [ ] C and D. Amazon S3 is object storage, not a file system, and while Transfer Family can use S3, it doesn't provide the same file system semantics or performance as EFS for an SFTP service requiring high IOPS.

Therefore, Option B is the most suitable solution as it leverages AWS Transfer Family for serverless SFTP, EFS for shared storage with high IOPS, and VPC endpoints with security groups for secure access control.

</details>

<details>
  <summary>Question 422</summary>

A company is developing a new machine learning (ML) model solution on AWS.
The models are developed as independent microservices that fetch approximately 1 GB of model data from Amazon S3 at startup and load the data into memory.
Users access the models through an asynchronous API.
Users can send a request or a batch of requests and specify where the results should be sent.
The company provides models to hundreds of users.
The usage patterns for the models are irregular.
Some models could be unused for days or weeks.
Other models could receive batches of thousands of requests at a time.
Which design should a solutions architect recommend to meet these requirements?

- [ ] A. Direct the requests from the API to a Network Load Balancer (NLB).
Deploy the models as AWS Lambda functions that are invoked by the NLB.
- [ ] B. Direct the requests from the API to an Application Load Balancer (ALB).
Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from an Amazon Simple Queue Service (Amazon SQS) queue.
Use AWS App Mesh to scale the instances of the ECS cluster based on the SQS queue size.
- [ ] C. Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue.
Deploy the models as AWS Lambda functions that are invoked by SQS events.
Use AWS Auto Scaling to increase the number of vCPUs for the Lambda functions based on the SQS queue size.
- [ ] D. Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue.
Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from the queue.
Enable AWS Auto Scaling on Amazon ECS for both the cluster and copies of the service based on the queue size.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue.
Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from the queue.
Enable AWS Auto Scaling on Amazon ECS for both the cluster and copies of the service based on the queue size.

Why these are the correct answers:

D. Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue.
Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from the queue.
Enable AWS Auto Scaling on Amazon ECS for both the cluster and copies of the service based on the queue size.

- [ ] Amazon SQS helps decouple the API requests from the model processing, accommodating irregular usage patterns.
- [ ] Amazon ECS allows deploying the models as scalable microservices.
- [ ] AWS Auto Scaling on ECS ensures that the number of service instances adjusts dynamically based on the queue load, optimizing resource utilization and cost.

<hr> Why are the other answers wrong? <hr>

- [ ] A. Lambda functions have limitations on memory and execution time, and loading 1GB of model data might exceed those limits. NLB is for load balancing traffic, not for invoking Lambda functions directly in this manner.
- [ ] B. While ECS with App Mesh can scale, it adds complexity compared to using ECS with Auto Scaling directly. App Mesh is more suitable for complex microservice architectures with many inter-service communications.
- [ ] C. Lambda functions are not designed to have their vCPU count scaled; they scale by increasing the number of concurrent executions. Loading 1GB of model data into Lambda for each invocation can lead to performance and cost inefficiencies.

Therefore, Option D provides the most scalable, cost-effective, and operationally efficient solution for handling the described machine learning model deployment scenario.

</details>

<details>
  <summary>Question 423</summary>

A solutions architect wants to use the following JSON text as an identity-based policy to grant specific permissions:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "1",
      "Effect": "Allow",
      "Action": [
        "ssm:ListDocuments",
        "ssm:GetDocument"
      ],
      "Resource": "*",
      "Condition": {
        "StringEquals": {
          "ec2:Region": "us-east-1"
        }
      }
    },
    {
      "Sid": "2",
      "Effect": "Deny",
      "Action": [
        "ec2:StopInstances",
        "ec2:TerminateInstances"
      ],
      "Resource": "*",
      "Condition": {
        "BoolIfExists": {
          "aws:MultiFactorAuthPresent": "false"
        }
      }
    }
  ]
}
```

Which IAM principals can the solutions architect attach this policy to? (Choose two.)

- [ ] A. Role
- [ ] B. Group
- [ ] C. Organization
- [ ] D. Amazon Elastic Container Service (Amazon ECS) resource
- [ ] E. Amazon EC2 resource

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Role
- [ ] B. Group

Why these are the correct answers:

- [ ] IAM policies are designed to be attached to IAM principals, which include users, groups, and roles.
- [ ] Roles are used to grant permissions to AWS services or other AWS accounts.
- [ ] Groups are used to manage permissions for multiple IAM users.

<hr> Why are the other answers wrong? <hr>

- [ ] C. Organization: Policies that apply at the organization level are Service Control Policies (SCPs), not IAM policies.
- [ ] D and E. Amazon ECS resources and Amazon EC2 resources are not IAM principals to which identity-based policies are attached. Instead, roles can be assumed by these resources to gain permissions.

Therefore, Options A and B are the correct choices as IAM policies can be directly attached to IAM roles and groups.

</details>

<details>
  <summary>Question 424</summary>

A company is running a custom application on Amazon EC2 On-Demand Instances.
The application has frontend nodes that need to run 24 hours a day, 7 days a week and backend nodes that need to run only for a short time based on workload.
The number of backend nodes varies during the day.

The company needs to scale out and scale in more instances based on workload.
Which solution will meet these requirements MOST cost-effectively?

-   [ ] A. Use Reserved Instances for the frontend nodes.
    Use AWS Fargate for the backend nodes.
-   [ ] B. Use Reserved Instances for the frontend nodes.
    Use Spot Instances for the backend nodes.
-   [ ] C. Use Spot Instances for the frontend nodes.
    Use Reserved Instances for the backend nodes.
-   [ ] D. Use Spot Instances for the frontend nodes.
    Use AWS Fargate for the backend nodes.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Use Reserved Instances for the frontend nodes.
    Use Spot Instances for the backend nodes.

Why these are the correct answers:

B. Use Reserved Instances for the frontend nodes.
Use Spot Instances for the backend nodes.

-   [ ] Reserved Instances offer cost savings for long-running, predictable workloads like the frontend nodes.
-   [ ] Spot Instances provide cost savings for short-term, flexible workloads that can tolerate interruptions, such as the backend nodes.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. AWS Fargate is a serverless compute service and is generally more expensive than EC2 Spot Instances for comparable compute capacity.
    It's suitable for containerized applications, not general EC2 instances.
-   [ ] C. Using Spot Instances for the frontend, which requires continuous availability, is not ideal because Spot Instances can be interrupted.
    Reserved Instances are better suited for this purpose.
-   [ ] D. Combining Spot Instances for the frontend (for which they are unsuitable) and Fargate for the backend (which is less cost-effective than Spot) is not the best cost optimization strategy.

Therefore, Option B is the most cost-effective solution because it pairs Reserved Instances for the stable frontend and Spot Instances for the scalable backend.

</details>
<details>
  <summary>Question 425</summary>

A company uses high block storage capacity to runs its workloads on premises.
The company's daily peak input and output transactions per second are not more than 15,000 IOPS.
The company wants to migrate the workloads to Amazon EC2 and to provision disk performance independent of storage capacity.
Which Amazon Elastic Block Store (Amazon EBS) volume type will meet these requirements MOST cost-effectively?

-   [ ] A. GP2 volume type
-   [ ] B. io2 volume type
-   [ ] C. GP3 volume type
-   [ ] D. io1 volume type

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. GP3 volume type

Why these are the correct answers:

C. GP3 volume type

-   [ ] GP3 volumes allow you to provision IOPS and throughput independently of storage capacity, providing cost efficiency when you need high performance without necessarily needing a lot of storage.
-   [ ] GP3 volumes offer a balance of performance and cost for workloads that require specific IOPS and throughput.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. GP2 volumes tie IOPS to storage capacity, which can be less cost-effective if you need high IOPS with less storage.
-   [ ] B and D. io1 and io2 volumes are provisioned IOPS volumes designed for the highest performance and are more expensive than GP3.
    They are typically used for very I/O-intensive applications that require consistent, high IOPS, which is not the primary concern in this scenario.

Therefore, Option C, GP3, is the most cost-effective choice because it meets the requirement of provisioning disk performance independently of storage capacity.

</details>
<details>
  <summary>Question 426</summary>

A company needs to store data from its healthcare application.
The application's data frequently changes.
A new regulation requires audit access at all levels of the stored data.
The company hosts the application on an on-premises infrastructure that is running out of storage capacity.
A solutions architect must securely migrate the existing data to AWS while satisfying the new regulation.
Which solution will meet these requirements?

-   [ ] A. Use AWS DataSync to move the existing data to Amazon S3.
    Use AWS CloudTrail to log data events.
-   [ ] B. Use AWS Snowcone to move the existing data to Amazon S3.
    Use AWS CloudTrail to log management events.
-   [ ] C. Use Amazon S3 Transfer Acceleration to move the existing data to Amazon S3.
    Use AWS CloudTrail to log data events.
-   [ ] D. Use AWS Storage Gateway to move the existing data to Amazon S3.
    Use AWS CloudTrail to log management events.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Use AWS DataSync to move the existing data to Amazon S3.
    Use AWS CloudTrail to log data events.

Why these are the correct answers:

A. Use AWS DataSync to move the existing data to Amazon S3.
Use AWS CloudTrail to log data events.

-   [ ] AWS DataSync is designed for efficient and secure online data transfer to AWS storage services like Amazon S3.
-   [ ] AWS CloudTrail can log data events for S3, providing a detailed audit trail of data access and changes, which satisfies the regulatory requirement.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. AWS Snowcone is used for edge computing and data transfer where network connectivity is limited.
    It's less efficient for continuous, online data migration compared to DataSync.
    CloudTrail logs management events, not data events in S3.
-   [ ] C. Amazon S3 Transfer Acceleration speeds up transfers over long distances but does not provide a mechanism for auditing data access.
    CloudTrail logs are needed for auditing.
-   [ ] D. AWS Storage Gateway is a hybrid cloud storage service that connects on-premises applications to AWS cloud storage.
    It does not directly migrate data in the same way as DataSync, and CloudTrail logs management events, not data events.

Therefore, Option A provides the best solution for migrating the data to S3 and meeting the audit requirements by using DataSync for efficient transfer and CloudTrail for logging data events.

</details>
<details>
  <summary>Question 427</summary>

A solutions architect is implementing a complex Java application with a MySQL database.
The Java application must be deployed on Apache Tomcat and must be highly available.
What should the solutions architect do to meet these requirements?

-   [ ] A. Deploy the application in AWS Lambda.
    Configure an Amazon API Gateway API to connect with the Lambda functions.
-   [ ] B. Deploy the application by using AWS Elastic Beanstalk.
    Configure a load-balanced environment and a rolling deployment policy.
-   [ ] C. Migrate the database to Amazon ElastiCache.
    Configure the ElastiCache security group to allow access from the application.
-   [ ] D. Launch an Amazon EC2 instance.
    Install a MySQL server on the EC2 instance.
    Configure the application on the server.
    Create an AMI.
    Use the AMI to create a launch template with an Auto Scaling group.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Deploy the application by using AWS Elastic Beanstalk.
    Configure a load-balanced environment and a rolling deployment policy.

Why these are the correct answers:

B. Deploy the application by using AWS Elastic Beanstalk.
Configure a load-balanced environment and a rolling deployment policy.

-   [ ] AWS Elastic Beanstalk simplifies the deployment and management of Java applications on Apache Tomcat.
-   [ ] Elastic Beanstalk can automatically provision and manage the underlying infrastructure, including EC2 instances, load balancers, and Auto Scaling groups, to ensure high availability.
-   [ ] Rolling deployment policies minimize downtime during updates by gradually replacing instances.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. AWS Lambda is designed for serverless functions, not for running long-running applications like a Java application on Tomcat.
-   [ ] C. Amazon ElastiCache is a caching service, not a database for running a MySQL database.
-   [ ] D. Manually launching EC2 instances, creating AMIs, and setting up Auto Scaling groups is more complex and time-consuming than using Elastic Beanstalk.
    It also requires more operational overhead.

Therefore, Option B is the most efficient and straightforward way to deploy the Java application on Tomcat with high availability.

</details>
<details>
  <summary>Question 428</summary>

A serverless application uses Amazon API Gateway, AWS Lambda, and Amazon DynamoDB.
The Lambda function needs permissions to read and write to the DynamoDB table.
Which solution will give the Lambda function access to the DynamoDB table MOST securely?

-   [ ] A. Create an IAM user with programmatic access to the Lambda function.
    Attach a policy to the user that allows read and write access to the DynamoDB table.
    Store the access\_key\_id and secret\_access\_key parameters as part of the Lambda environment variables.
    Ensure that other AWS users do not have read and write access to the Lambda function configuration.
-   [ ] B. Create an IAM role that includes Lambda as a trusted service.
    Attach a policy to the role that allows read and write access to the DynamoDB table.
    Update the configuration of the Lambda function to use the new role as the execution role.
-   [ ] C. Create an IAM user with programmatic access to the Lambda function.
    Attach a policy to the user that allows read and write access to the DynamoDB table.
    Store the access\_key\_id and secret\_access\_key parameters in AWS Systems Manager Parameter Store as secure string parameters.
    Update the Lambda function code to retrieve the secure string parameters before connecting to the DynamoDB table.
-   [ ] D. Create an IAM role that includes DynamoDB as a trusted service.
    Attach a policy to the role that allows read and write access from the Lambda function.
    Update the code of the Lambda function to attach to the new role as an execution role.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Create an IAM role that includes Lambda as a trusted service.
    Attach a policy to the role that allows read and write access to the DynamoDB table.
    Update the configuration of the Lambda function to use the new role as the execution role.

Why these are the correct answers:

B. Create an IAM role that includes Lambda as a trusted service.
Attach a policy to the role that allows read and write access to the DynamoDB table.
Update the configuration of the Lambda function to use the new role as the execution role.

-   [ ] IAM roles provide secure permissions to AWS services without needing to manage long-term credentials.
-   [ ] By assigning a role to the Lambda function, you grant it permissions to access DynamoDB in a secure and managed way.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Storing IAM user credentials (access keys) in Lambda environment variables is insecure.
    If the Lambda function is compromised, the credentials could be exposed.
-   [ ] C. While using Systems Manager Parameter Store is more secure than environment variables, it's still less secure and more complex than using IAM roles directly.
    It also involves additional code to retrieve the credentials.
-   [ ] D. IAM roles are designed to be assumed by AWS services (like Lambda), not the other way around.
    DynamoDB cannot assume a role to grant permissions to Lambda.

Therefore, Option B is the most secure and recommended method for granting Lambda function access to DynamoDB.

</details>

<details>
  <summary>Question 429</summary>

The following IAM policy is attached to an IAM group. This is the only policy applied to the group.

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "1",
      "Effect": "Allow",
      "Action": "ec2:\*",
      "Resource": "\*",
      "Condition": {
        "StringEquals": {
          "ec2:Region": "us-east-1"
        }
      }
    },
    {
      "Sid": "2",
      "Effect": "Deny",
      "Action": [
        "ec2:StopInstances",
        "ec2:TerminateInstances"
      ],
      "Resource": "\*",
      "Condition": {
        "BoolIfExists": {
          "aws:MultiFactorAuthPresent": "false"
        }
      }
    }
  ]
}
```

What are the effective IAM permissions of this policy for group members?

- [ ] A. Group members are permitted any Amazon EC2 action within the us-east-1 Region. Statements after the Allow permission are not applied.
- [ ] B. Group members are denied any Amazon EC2 permissions in the us-east-1 Region unless they are logged in with multi-factor authentication (MFA).
- [ ] C. Group members are allowed the ec2:StopInstances and ec2:TerminateInstances permissions for all Regions when logged in with multi-factor authentication (MFA). Group members are permitted any other Amazon EC2 action.
- [ ] D. Group members are allowed the ec2:StopInstances and ec2:TerminateInstances permissions for the us-east-1 Region only when logged in with multi-factor authentication (MFA). Group members are permitted any other Amazon EC2 action within the us-east-1 Region.
</details>

<details>
  <summary>Answer</summary>

- [ ] D. Group members are allowed the ec2:StopInstances and ec2:TerminateInstances permissions for the us-east-1 Region only when logged in with multi-factor authentication (MFA). Group members are permitted any other Amazon EC2 action within the us-east-1 Region.
Why these are the correct answers:

D. Group members are allowed the ec2:StopInstances and ec2:TerminateInstances permissions for the us-east-1 Region only when logged in with multi-factor authentication (MFA).
Group members are permitted any other Amazon EC2 action within the us-east-1 Region.

- [ ] The first statement allows all EC2 actions in the us-east-1 Region.
- [ ] The second statement denies the StopInstances and TerminateInstances actions unless MFA is present.
- [ ] Because both conditions apply, the effective permissions are a combination of both statements.

<hr> Why are the other answers wrong? <hr>

- [ ] A. The deny statement does affect the permissions.
- [ ] B. The allow statement grants general permissions, and the deny statement restricts specific actions.
- [ ] C. The deny statement is conditional and only applies to StopInstances and TerminateInstances. It is also limited to the us-east-1 Region due to the allow statement's condition.

Therefore, Option D accurately reflects the combined effect of both the allow and deny statements in the IAM policy.

</details>

<details>
  <summary>Question 430</summary>

A manufacturing company has machine sensors that upload .csv files to an Amazon S3 bucket.
These .csv files must be converted into images and must be made available as soon as possible for the automatic generation of graphical reports.
The images become irrelevant after 1 month, but the .csv files must be kept to train machine learning (ML) models twice a year.
The ML trainings and audits are planned weeks in advance.

Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)

- [ ] A. Launch an Amazon EC2 Spot Instance that downloads the .csv files every hour, generates the image files, and uploads the images to the S3 bucket.
- [ ] B. Design an AWS Lambda function that converts the .csv files into images and stores the images in the S3 bucket.
    Invoke the Lambda function when a .csv file is uploaded.
- [ ] C. Create S3 Lifecycle rules for .csv files and image files in the S3 bucket.
    Transition the .csv files from S3 Standard to S3 Glacier 1 day after they are uploaded.
    Expire the image files after 30 days.
- [ ] D. Create S3 Lifecycle rules for .csv files and image files in the S3 bucket.
    Transition the .csv files from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) 1 day after they are uploaded.
    Expire the image files after 30 days.
- [ ] E. Create S3 Lifecycle rules for .csv files and image files in the S3 bucket.
    Transition the .csv files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 1 day after they are uploaded.
    Keep the image files in Reduced Redundancy Storage (RRS).

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Design an AWS Lambda function that converts the .csv files into images and stores the images in the S3 bucket.
    Invoke the Lambda function when a .csv file is uploaded.
- [ ] C. Create S3 Lifecycle rules for .csv files and image files in the S3 bucket.
    Transition the .csv files from S3 Standard to S3 Glacier 1 day after they are uploaded.
    Expire the image files after 30 days.

Why these are the correct answers:

B. Design an AWS Lambda function that converts the .csv files into images and stores the images in the S3 bucket.
Invoke the Lambda function when a .csv file is uploaded.

- [ ] Lambda functions can be triggered by S3 events, providing a serverless and efficient way to process files immediately after upload.

C. Create S3 Lifecycle rules for .csv files and image files in the S3 bucket.
Transition the .csv files from S3 Standard to S3 Glacier 1 day after they are uploaded.
Expire the image files after 30 days.

- [ ] S3 Lifecycle rules automate the management of objects, reducing storage costs by moving .csv files to Glacier for long-term storage and deleting images after 30 days.

<hr> Why are the other answers wrong? <hr>

- [ ] A. Launching an EC2 Spot Instance adds operational overhead for managing the instance and introduces potential interruptions.
    It is also less cost-effective for event-driven processing compared to Lambda.
- [ ] D. S3 One Zone-IA is cheaper than S3 Standard but is less durable and available than S3 Glacier for long-term archival.
- [ ] E. S3 Standard-IA is more expensive than S3 Glacier, and Reduced Redundancy Storage (RRS) is deprecated and not recommended for new applications.

Therefore, Options B and C provide the most cost-effective and efficient solution by using Lambda for immediate processing and S3 Lifecycle rules for storage management.

</details>

# AWS-SAA-PRACTICE-EXAM Questions 431-440

<details>
  <summary>Question 431</summary>

A company has developed a new video game as a web application.
The application is in a three-tier architecture in a VPC with Amazon RDS for MySQL in the database layer.
Several players will compete concurrently online.
The game's developers want to display a top-10 scoreboard in near-real time and offer the ability to stop and restore the game while preserving the current scores.
What should a solutions architect do to meet these requirements?

-   [ ] A. Set up an Amazon ElastiCache for Memcached cluster to cache the scores for the web application to display.
-   [ ] B. Set up an Amazon ElastiCache for Redis cluster to compute and cache the scores for the web application to display.
-   [ ] C. Place an Amazon CloudFront distribution in front of the web application to cache the scoreboard in a section of the application.
-   [ ] D. Create a read replica on Amazon RDS for MySQL to run queries to compute the scoreboard and serve the read traffic to the web application.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Set up an Amazon ElastiCache for Redis cluster to compute and cache the scores for the web application to display.

Why these are the correct answers:

B. Set up an Amazon ElastiCache for Redis cluster to compute and cache the scores for the web application to display.

-   [ ] Amazon ElastiCache for Redis supports more complex data structures and operations than Memcached, making it suitable for real-time score computations and caching.
-   [ ] Redis can also be used to persist game state, allowing for stopping and restoring the game.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Memcached is a simpler caching solution that does not offer the data structures or persistence capabilities needed for this scenario.
-   [ ] C. CloudFront is designed for caching static content and is not suitable for real-time, frequently updated data like a scoreboard.
-   [ ] D. A read replica is used for offloading read queries but does not provide the in-memory, low-latency capabilities needed for real-time score updates and persistence.

Therefore, Option B is the most appropriate solution for providing a real-time scoreboard and game state management.

</details>
<details>
  <summary>Question 432</summary>

An ecommerce company wants to use machine learning (ML) algorithms to build and train models.
The company will use the models to visualize complex scenarios and to detect trends in customer data.
The architecture team wants to integrate its ML models with a reporting platform to analyze the augmented data and use the data directly in its business intelligence dashboards.
Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Use AWS Glue to create an ML transform to build and train models.
    Use Amazon OpenSearch Service to visualize the data.
-   [ ] B. Use Amazon SageMaker to build and train models.
    Use Amazon QuickSight to visualize the data.
-   [ ] C. Use a pre-built ML Amazon Machine Image (AMI) from the AWS Marketplace to build and train models.
    Use Amazon OpenSearch Service to visualize the data.
-   [ ] D. Use Amazon QuickSight to build and train models by using calculated fields.
    Use Amazon QuickSight to visualize the data.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Use Amazon SageMaker to build and train models.
    Use Amazon QuickSight to visualize the data.

Why these are the correct answers:

B. Use Amazon SageMaker to build and train models.
Use Amazon QuickSight to visualize the data.

-   [ ] Amazon SageMaker provides a fully managed environment for building, training, and deploying ML models, reducing operational overhead.
-   [ ] Amazon QuickSight is a fully managed BI service that integrates well with SageMaker, allowing for easy visualization and analysis of ML model outputs.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. AWS Glue is primarily an ETL service, not an ML platform.
    OpenSearch Service is for search and analytics, not for direct integration with BI dashboards for ML model visualization.
-   [ ] C. Using pre-built ML AMIs from the AWS Marketplace requires more operational overhead for managing EC2 instances and integrating with visualization tools.
-   [ ] D. Amazon QuickSight is a BI tool and is not designed for building and training complex ML models.
    Calculated fields are for data manipulation within dashboards, not for creating ML models.

Therefore, Option B provides the most streamlined and managed solution for building, training, and visualizing ML models.

</details>
<details>
  <summary>Question 433</summary>

A company is running its production and nonproduction environment workloads in multiple AWS accounts.
The accounts are in an organization in AWS Organizations.
The company needs to design a solution that will prevent the modification of cost usage tags.
Which solution will meet these requirements?

-   [ ] A. Create a custom AWS Config rule to prevent tag modification except by authorized principals.
-   [ ] B. Create a custom trail in AWS CloudTrail to prevent tag modification.
-   [ ] C. Create a service control policy (SCP) to prevent tag modification except by authorized principals.
-   [ ] D. Create custom Amazon CloudWatch logs to prevent tag modification.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Create a service control policy (SCP) to prevent tag modification except by authorized principals.

Why these are the correct answers:

C. Create a service control policy (SCP) to prevent tag modification except by authorized principals.

-   [ ] Service Control Policies (SCPs) allow you to centrally control AWS service permissions for all accounts in your AWS Organization.
-   [ ] SCPs can be used to enforce that cost usage tags cannot be modified, ensuring consistency and accuracy in cost allocation.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. AWS Config rules monitor resource configurations but do not prevent actions from being taken.
    They can alert on non-compliant tag modifications but cannot enforce prevention.
-   [ ] B. AWS CloudTrail records API calls but does not prevent actions.
    It can be used to audit tag modifications but not to prevent them.
-   [ ] D. Amazon CloudWatch Logs collects and monitors log data but does not have the capability to prevent tag modifications.

Therefore, Option C is the only solution that effectively prevents the modification of cost usage tags across multiple AWS accounts in an organization.

</details>
<details>
  <summary>Question 434</summary>

A company hosts its application in the AWS Cloud.
The application runs on Amazon EC2 instances behind an Elastic Load Balancer in an Auto Scaling group and with an Amazon DynamoDB table.
The company wants to ensure the application can be made available in another AWS Region with minimal downtime.
What should a solutions architect do to meet these requirements with the LEAST amount of downtime?

-   [ ] A. Create an Auto Scaling group and a load balancer in the disaster recovery Region.
    Configure the DynamoDB table as a global table.
    Configure DNS failover to point to the new disaster recovery Region's load balancer.
-   [ ] B. Create an AWS CloudFormation template to create EC2 instances, load balancers, and DynamoDB tables to be launched when needed Configure DNS failover to point to the new disaster recovery Region's load balancer.
-   [ ] C. Create an AWS CloudFormation template to create EC2 instances and a load balancer to be launched when needed.
    Configure the DynamoDB table as a global table.
    Configure DNS failover to point to the new disaster recovery Region's load balancer.
-   [ ] D. Create an Auto Scaling group and load balancer in the disaster recovery Region.
    Configure the DynamoDB table as a global table.
    Create an Amazon CloudWatch alarm to trigger an AWS Lambda function that updates Amazon Route 53 pointing to the disaster recovery load balancer.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Create an Auto Scaling group and a load balancer in the disaster recovery Region.
    Configure the DynamoDB table as a global table.
    Configure DNS failover to point to the new disaster recovery Region's load balancer.

Why these are the correct answers:

A. Create an Auto Scaling group and a load balancer in the disaster recovery Region.
Configure the DynamoDB table as a global table.
Configure DNS failover to point to the new disaster recovery Region's load balancer.

-   [ ] Creating pre-configured resources in the disaster recovery Region minimizes the time it takes to fail over.
-   [ ] DynamoDB global tables provide automatic replication across Regions, ensuring data availability.
-   [ ] DNS failover allows for quick redirection of traffic to the disaster recovery Region.

<hr> Why are the other answers wrong? <hr>

-   [ ] B and C. Using CloudFormation to create resources on demand introduces a delay in the failover process, increasing downtime.
-   [ ] D. Using a CloudWatch alarm and Lambda function to update Route 53 adds complexity and potential latency compared to a direct DNS failover configuration.

Therefore, Option A provides the fastest and most efficient failover with the least downtime.

</details>
<details>
  <summary>Question 435</summary>

A company needs to migrate a MySQL database from its on-premises data center to AWS within 2 weeks.
The database is 20 TB in size.
The company wants to complete the migration with minimal downtime.
Which solution will migrate the database MOST cost-effectively?

-   [ ] A. Order an AWS Snowball Edge Storage Optimized device.
    Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with replication of ongoing changes.
    Send the Snowball Edge device to AWS to finish the migration and continue the ongoing replication.
-   [ ] B. Order an AWS Snowmobile vehicle.
    Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with ongoing changes.
    Send the Snowmobile vehicle back to AWS to finish the migration and continue the ongoing replication.
-   [ ] C. Order an AWS Snowball Edge Compute Optimized with GPU device.
    Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with ongoing changes.
    Send the Snowball device to AWS to finish the migration and continue the ongoing replication
-   [ ] D. Order a 1 GB dedicated AWS Direct Connect connection to establish a connection with the data center.
    Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with replication of ongoing changes.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Order an AWS Snowball Edge Storage Optimized device.
    Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with replication of ongoing changes.
    Send the Snowball Edge device to AWS to finish the migration and continue the ongoing replication.

Why these are the correct answers:

A. Order an AWS Snowball Edge Storage Optimized device.
Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with replication of ongoing changes.
Send the Snowball Edge device to AWS to finish the migration and continue the ongoing replication.

-   [ ] AWS Snowball Edge Storage Optimized devices are designed for large-scale data transfers and are cost-effective for migrating 20 TB of data.
-   [ ] AWS DMS minimizes downtime by replicating ongoing changes while the bulk of the data is transferred using Snowball.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. AWS Snowmobile is designed for exabyte-scale data transfers and is more expensive than Snowball Edge for 20 TB.
-   [ ] C. Snowball Edge Compute Optimized is designed for edge computing and is not cost-effective for simple data migration.
-   [ ] D. A 1 GB Direct Connect connection might not be fast enough to transfer 20 TB within 2 weeks, and it does not address the initial bulk transfer.

Therefore, Option A provides the most cost-effective and efficient solution for migrating the database with minimal downtime.

</details>
<details>
  <summary>Question 436</summary>

A company moved its on-premises PostgreSQL database to an Amazon RDS for PostgreSQL DB instance.
The company successfully launched a new product.
The workload on the database has increased.
The company wants to accommodate the larger workload without adding infrastructure.
Which solution will meet these requirements MOST cost-effectively?

-   [ ] A. Buy reserved DB instances for the total workload.
    Make the Amazon RDS for PostgreSQL DB instance larger.
-   [ ] B. Make the Amazon RDS for PostgreSQL DB instance a Multi-AZ DB instance.
-   [ ] C. Buy reserved DB instances for the total workload.
    Add another Amazon RDS for PostgreSQL DB instance.
-   [ ] D. Make the Amazon RDS for PostgreSQL DB instance an on-demand DB instance.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Buy reserved DB instances for the total workload.
    Make the Amazon RDS for PostgreSQL DB instance larger.

Why these are the correct answers:

A. Buy reserved DB instances for the total workload.
Make the Amazon RDS for PostgreSQL DB instance larger.

-   [ ] Scaling up the existing RDS instance (vertical scaling) is generally more cost-effective than adding new instances.
-   [ ] Reserved Instances provide cost savings for long-term, stable workloads.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. Multi-AZ is for high availability, not for scaling performance.
-   [ ] C. Adding another RDS instance (horizontal scaling) increases costs and complexity.
-   [ ] D. On-Demand instances are more expensive than Reserved Instances for sustained workloads.

Therefore, Option A is the most cost-effective way to handle the increased workload by scaling up the existing instance and using Reserved Instances.

</details>
<details>
  <summary>Question 437</summary>

A company operates an ecommerce website on Amazon EC2 instances behind an Application Load Balancer (ALB) in an Auto Scaling group.
The site is experiencing performance issues related to a high request rate from illegitimate external systems with changing IP addresses.
The security team is worried about potential DDoS attacks against the website.
The company must block the illegitimate incoming requests in a way that has a minimal impact on legitimate users.
What should a solutions architect recommend?

-   [ ] A. Deploy Amazon Inspector and associate it with the ALB.
-   [ ] B. Deploy AWS WAF, associate it with the ALB, and configure a rate-limiting rule.
-   [ ] C. Deploy rules to the network ACLs associated with the ALB to block the incoming traffic.
-   [ ] D. Deploy Amazon GuardDuty and enable rate-limiting protection when configuring GuardDuty.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Deploy AWS WAF, associate it with the ALB, and configure a rate-limiting rule.

Why these are the correct answers:

B. Deploy AWS WAF, associate it with the ALB, and configure a rate-limiting rule.

-   [ ] AWS WAF is designed to protect web applications from common web exploits and DDoS attacks.
-   [ ] WAF allows for rate-limiting rules to control the number of requests from an IP address, mitigating attacks while minimizing impact on legitimate users.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Amazon Inspector is a vulnerability management service that does not provide protection against DDoS attacks or rate limiting.
-   [ ] C. Network ACLs operate at the subnet level and are not suitable for fine-grained filtering of web requests.
    They also lack the ability to perform rate limiting.
-   [ ] D. Amazon GuardDuty is a threat detection service that monitors for malicious activity but does not provide rate-limiting or web application firewall capabilities.

Therefore, Option B is the most appropriate solution for protecting the website from DDoS attacks and illegitimate requests.

</details>
<details>
  <summary>Question 438</summary>

A company wants to share accounting data with an external auditor.
The data is stored in an Amazon RDS DB instance that resides in a private subnet.
The auditor has its own AWS account and requires its own copy of the database.
What is the MOST secure way for the company to share the database with the auditor?

-   [ ] A. Create a read replica of the database.
    Configure IAM standard database authentication to grant the auditor access.
-   [ ] B. Export the database contents to text files.
    Store the files in an Amazon S3 bucket.
    Create a new IAM user for the auditor.
    Grant the user access to the S3 bucket.
-   [ ] C. Copy a snapshot of the database to an Amazon S3 bucket.
    Create an IAM user.
    Share the user's keys with the auditor to grant access to the object in the S3 bucket.
-   [ ] D. Create an encrypted snapshot of the database.
    Share the snapshot with the auditor.
    Allow access to the AWS Key Management Service (AWS KMS) encryption key.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Create an encrypted snapshot of the database.
    Share the snapshot with the auditor.
    Allow access to the AWS Key Management Service (AWS KMS) encryption key.

Why these are the correct answers:

D. Create an encrypted snapshot of the database.
Share the snapshot with the auditor.
Allow access to the AWS Key Management Service (AWS KMS) encryption key.

-   [ ] Creating an encrypted snapshot ensures that the data is protected during transfer and storage.
-   [ ] Sharing the snapshot with the auditor's AWS account is secure and provides them with their own copy.
-   [ ] Granting access to the KMS key allows the auditor to decrypt the snapshot.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Sharing a read replica grants the auditor direct access to the company's database environment, which is less secure than providing a copy.
    IAM database authentication is complex to manage for external access.
-   [ ] B. Exporting to text files and storing in S3 is less secure than using encrypted snapshots.
    Managing IAM users for external auditors adds overhead.
-   [ ] C. Sharing IAM user keys is a security risk.
    Copying snapshots to S3 is less secure than sharing encrypted snapshots directly.

Therefore, Option D is the most secure method for sharing the database with an external auditor.

</details>
<details>
  <summary>Question 439</summary>

A solutions architect configured a VPC that has a small range of IP addresses.
The number of Amazon EC2 instances that are in the VPC is increasing, and there is an insufficient number of IP addresses for future workloads.
Which solution resolves this issue with the LEAST operational overhead?

-   [ ] A. Add an additional IPv4 CIDR block to increase the number of IP addresses and create additional subnets in the VPC.
    Create new resources in the new subnets by using the new CIDR.
-   [ ] B. Create a second VPC with additional subnets.
    Use a peering connection to connect the second VPC with the first VPC Update the routes and create new resources in the subnets of the second VPC.
-   [ ] C. Use AWS Transit Gateway to add a transit gateway and connect a second VPC with the first VPUpdate the routes of the transit gateway and VPCs.
    Create new resources in the subnets of the second VPC.
-   [ ] D. Create a second VPC.
    Create a Site-to-Site VPN connection between the first VPC and the second VPC by using a VPN-hosted solution on Amazon EC2 and a virtual private gateway.
    Update the route between VPCs to the traffic through the VPN.
    Create new resources in the subnets of the second VPC.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Add an additional IPv4 CIDR block to increase the number of IP addresses and create additional subnets in the VPC.
    Create new resources in the new subnets by using the new CIDR.

Why these are the correct answers:

A. Add an additional IPv4 CIDR block to increase the number of IP addresses and create additional subnets in the VPC.
Create new resources in the new subnets by using the new CIDR.

-   [ ] Adding a CIDR block is the simplest and most direct way to increase the number of IP addresses in a VPC.
-   [ ] It avoids the complexity of managing multiple VPCs or connections between them.

<hr> Why are the other answers wrong? <hr>

-   [ ] B, C, and D. Creating a second VPC and connecting it via peering, Transit Gateway, or VPN adds significant operational overhead.
    These solutions require managing multiple VPCs, routing configurations, and potentially more complex network architectures.

Therefore, Option A is the solution with the least operational overhead.

</details>
<details>
  <summary>Question 440</summary>

A company used an Amazon RDS for MySQL DB instance during application testing.
Before terminating the DB instance at the end of the test cycle, a solutions architect created two backups.
The solutions architect created the first backup by using the mysqldump utility to create a database dump.
The solutions architect created the second backup by enabling the final DB snapshot option on RDS termination.
The company is now planning for a new test cycle and wants to create a new DB instance from the most recent backup.
The company has chosen a MySQL-compatible edition of Amazon Aurora to host the DB instance.
Which solutions will create the new DB instance? (Choose two.)

-   [ ] A. Import the RDS snapshot directly into Aurora.
-   [ ] B. Upload the RDS snapshot to Amazon S3.
    Then import the RDS snapshot into Aurora.
-   [ ] C. Upload the database dump to Amazon S3.
    Then import the database dump into Aurora.
-   [ ] D. Use AWS Database Migration Service (AWS DMS) to import the RDS snapshot into Aurora.
-   [ ] E. Upload the database dump to Amazon S3.
    Then use AWS Database Migration Service (AWS DMS) to import the database dump into Aurora.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Import the RDS snapshot directly into Aurora.
-   [ ] C. Upload the database dump to Amazon S3.
    Then import the database dump into Aurora.

Why these are the correct answers:

A. Import the RDS snapshot directly into Aurora.

-   [ ] RDS snapshots can be directly imported into Aurora, providing a quick and easy way to restore a database.

C. Upload the database dump to Amazon S3.
Then import the database dump into Aurora.

-   [ ] Database dumps created with mysqldump can be imported into Aurora, although this process might be slower than using a snapshot.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. It is not necessary to upload the RDS snapshot to Amazon S3 before importing it into Aurora.
    Snapshots can be imported directly.
-   [ ] D and E. AWS DMS is typically used for migrating databases between different database engines or across platforms.
    While it can be used to import data, it's not the most efficient or direct method for restoring a MySQL backup to Aurora.

Therefore, Options A and C are the most appropriate solutions for creating a new Aurora DB instance from the available backups.

</details>

# AWS-SAA-PRACTICE-EXAM Questions 441-450

<details>
  <summary>Question 441</summary>

A company hosts a multi-tier web application on Amazon Linux Amazon EC2 instances behind an Application Load Balancer.
The instances run in an Auto Scaling group across multiple Availability Zones.
The company observes that the Auto Scaling group launches more On-Demand Instances when the application's end users access high volumes of static web content.
The company wants to optimize cost.

What should a solutions architect do to redesign the application MOST cost-effectively?

-   [ ] A. Update the Auto Scaling group to use Reserved Instances instead of On-Demand Instances.
-   [ ] B. Update the Auto Scaling group to scale by launching Spot Instances instead of On-Demand Instances.
-   [ ] C. Create an Amazon CloudFront distribution to host the static web contents from an Amazon S3 bucket.
-   [ ] D. Create an AWS Lambda function behind an Amazon API Gateway API to host the static website contents.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Create an Amazon CloudFront distribution to host the static web contents from an Amazon S3 bucket.

Why these are the correct answers:

C. Create an Amazon CloudFront distribution to host the static web contents from an Amazon S3 bucket.

-   [ ] Amazon CloudFront is a content delivery network (CDN) that efficiently serves static content, reducing the load on the EC2 instances and the need to scale them.
-   [ ] S3 provides cost-effective storage for static content.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Reserved Instances reduce costs for long-running instances but do not address the issue of serving static content efficiently.
-   [ ] B. Spot Instances can reduce costs but are not suitable for serving critical web application traffic due to potential interruptions.
-   [ ] D. Using Lambda and API Gateway to serve static content is more complex and expensive than using CloudFront and S3.

Therefore, Option C is the most cost-effective way to offload static content and reduce the scaling of On-Demand Instances.

</details>
<details>
  <summary>Question 442</summary>

A company stores several petabytes of data across multiple AWS accounts.
The company uses AWS Lake Formation to manage its data lake.
The company's data science team wants to securely share selective data from its accounts with the company's engineering team for analytical purposes.
Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Copy the required data to a common account.
    Create an IAM access role in that account.
    Grant access by specifying a permission policy that includes users from the engineering team accounts as trusted entities.
-   [ ] B. Use the Lake Formation permissions Grant command in each account where the data is stored to allow the required engineering team users to access the data.
-   [ ] C. Use AWS Data Exchange to privately publish the required data to the required engineering team accounts.
-   [ ] D. Use Lake Formation tag-based access control to authorize and grant cross-account permissions for the required data to the engineering team accounts.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Use Lake Formation tag-based access control to authorize and grant cross-account permissions for the required data to the engineering team accounts.

Why these are the correct answers:

D. Use Lake Formation tag-based access control to authorize and grant cross-account permissions for the required data to the engineering team accounts.

-   [ ] Lake Formation tag-based access control simplifies the management of permissions in a data lake environment.
-   [ ] It allows for fine-grained control over data access across accounts with minimal operational overhead.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Copying data to a common account introduces data duplication and increases storage costs and management overhead.
    Managing IAM roles across accounts is also complex.
-   [ ] B. Using the Lake Formation Grant command in each account is more complex and time-consuming than using tag-based access control.
-   [ ] C. AWS Data Exchange is designed for sharing data with external parties, not for internal data sharing within an organization.
    It also adds unnecessary cost and complexity.

Therefore, Option D is the most efficient and least operationally intensive solution for sharing data within the organization's data lake.

</details>
<details>
  <summary>Question 443</summary>

A company wants to host a scalable web application on AWS.
The application will be accessed by users from different geographic regions of the world.
Application users will be able to download and upload unique data up to gigabytes in size.
The development team wants a cost-effective solution to minimize upload and download latency and maximize performance.
What should a solutions architect do to accomplish this?

-   [ ] A. Use Amazon S3 with Transfer Acceleration to host the application.
-   [ ] B. Use Amazon S3 with CacheControl headers to host the application.
-   [ ] C. Use Amazon EC2 with Auto Scaling and Amazon CloudFront to host the application.
-   [ ] D. Use Amazon EC2 with Auto Scaling and Amazon ElastiCache to host the application.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Use Amazon S3 with Transfer Acceleration to host the application.

Why these are the correct answers:

A. Use Amazon S3 with Transfer Acceleration to host the application.

-   [ ] Amazon S3 Transfer Acceleration optimizes data transfers to and from S3 over long distances, reducing latency.
-   [ ] S3 provides scalability and cost-effectiveness for storing and retrieving large files.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. CacheControl headers in S3 are used for caching content in browsers and CDNs but do not accelerate the actual upload and download process.
-   [ ] C. While EC2 with Auto Scaling and CloudFront can host a web application, it is more complex and expensive than using S3 with Transfer Acceleration for large file transfers.
-   [ ] D. ElastiCache is an in-memory caching service and is not designed for storing and transferring large files.

Therefore, Option A is the most suitable solution for minimizing latency and maximizing performance for large file transfers.

</details>
<details>
  <summary>Question 444</summary>

A company has hired a solutions architect to design a reliable architecture for its application.
The application consists of one Amazon RDS DB instance and two manually provisioned Amazon EC2 instances that run web servers.
The EC2 instances are located in a single Availability Zone.
An employee recently deleted the DB instance, and the application was unavailable for 24 hours as a result.
The company is concerned with the overall reliability of its environment.
What should the solutions architect do to maximize reliability of the application's infrastructure?

-   [ ] A. Delete one EC2 instance and enable termination protection on the other EC2 instance.
    Update the DB instance to be Multi-AZ, and enable deletion protection.
-   [ ] B. Update the DB instance to be Multi-AZ, and enable deletion protection.
    Place the EC2 instances behind an Application Load Balancer, and run them in an EC2 Auto Scaling group across multiple Availability Zones.
-   [ ] C. Create an additional DB instance along with an Amazon API Gateway and an AWS Lambda function.
    Configure the application to invoke the Lambda function through API Gateway.
    Have the Lambda function write the data to the two DB instances.
-   [ ] D. Place the EC2 instances in an EC2 Auto Scaling group that has multiple subnets located in multiple Availability Zones.
    Use Spot Instances instead of On-Demand Instances.
    Set up Amazon CloudWatch alarms to monitor the health of the instances Update the DB instance to be Multi-AZ, and enable deletion protection.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Update the DB instance to be Multi-AZ, and enable deletion protection.
    Place the EC2 instances behind an Application Load Balancer, and run them in an EC2 Auto Scaling group across multiple Availability Zones.

Why these are the correct answers:

B. Update the DB instance to be Multi-AZ, and enable deletion protection.
Place the EC2 instances behind an Application Load Balancer, and run them in an EC2 Auto Scaling group across multiple Availability Zones.

-   [ ] Multi-AZ RDS provides high availability for the database.
-   [ ] Enabling deletion protection prevents accidental deletion of the DB instance.
-   [ ] Auto Scaling groups and Application Load Balancers distribute traffic and ensure EC2 instances are resilient to failures across Availability Zones.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Deleting an EC2 instance reduces availability.
    Termination protection only prevents termination, not other failures.
-   [ ] C. Adding API Gateway and Lambda functions adds complexity without directly addressing the reliability of the EC2 instances or the database.
-   [ ] D. Spot Instances can be interrupted, reducing reliability.
    While Auto Scaling and Multi-AZ improve reliability, using Spot Instances undermines it.

Therefore, Option B provides the most comprehensive solution for maximizing application reliability.

</details>
<details>
  <summary>Question 445</summary>

A company is storing 700 terabytes of data on a large network-attached storage (NAS) system in its corporate data center.
The company has a hybrid environment with a 10 Gbps AWS Direct Connect connection.
After an audit from a regulator, the company has 90 days to move the data to the cloud.
The company needs to move the data efficiently and without disruption.
The company still needs to be able to access and update the data during the transfer window.
Which solution will meet these requirements?

-   [ ] A. Create an AWS DataSync agent in the corporate data center.
    Create a data transfer task Start the transfer to an Amazon S3 bucket.
-   [ ] B. Back up the data to AWS Snowball Edge Storage Optimized devices.
    Ship the devices to an AWS data center.
    Mount a target Amazon S3 bucket on the on-premises file system.
-   [ ] C. Use rsync to copy the data directly from local storage to a designated Amazon S3 bucket over the Direct Connect connection.
-   [ ] D. Back up the data on tapes.
    Ship the tapes to an AWS data center.
    Mount a target Amazon S3 bucket on the on-premises file system.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Create an AWS DataSync agent in the corporate data center.
    Create a data transfer task Start the transfer to an Amazon S3 bucket.

Why these are the correct answers:

A. Create an AWS DataSync agent in the corporate data center.
Create a data transfer task Start the transfer to an Amazon S3 bucket.

-   [ ] AWS DataSync is designed for efficient and secure online data transfer.
-   [ ] It can transfer data while it is being accessed and updated, minimizing disruption.
-   [ ] Using Direct Connect optimizes the transfer speed.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. Snowball Edge is suitable for large data transfers but involves physical shipping, which is disruptive and does not allow for continuous access and updates during the transfer.
-   [ ] C. While rsync can transfer data, it is not as efficient or optimized for large-scale transfers as DataSync, and it may not handle concurrent access and updates as gracefully.
-   [ ] D. Transferring data via tapes is slow, disruptive, and does not allow for online access and updates.

Therefore, Option A is the most appropriate solution for migrating the data efficiently and without disruption.

</details>
<details>
  <summary>Question 446</summary>

A company stores data in PDF format in an Amazon S3 bucket.
The company must follow a legal requirement to retain all new and existing data in Amazon S3 for 7 years.
Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Turn on the S3 Versioning feature for the S3 bucket.
    Configure S3 Lifecycle to delete the data after 7 years.
    Configure multi-factor authentication (MFA) delete for all S3 objects.
-   [ ] B. Turn on S3 Object Lock with governance retention mode for the S3 bucket.
    Set the retention period to expire after 7 years.
    Recopy all existing objects to bring the existing data into compliance.
-   [ ] C. Turn on S3 Object Lock with compliance retention mode for the S3 bucket.
    Set the retention period to expire after 7 years.
    Recopy all existing objects to bring the existing data into compliance.
-   [ ] D. Turn on S3 Object Lock with compliance retention mode for the S3 bucket.
    Set the retention period to expire after 7 years.
    Use S3 Batch Operations to bring the existing data into compliance.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Turn on S3 Object Lock with compliance retention mode for the S3 bucket.
    Set the retention period to expire after 7 years.
    Use S3 Batch Operations to bring the existing data into compliance.

Why these are the correct answers:

D. Turn on S3 Object Lock with compliance retention mode for the S3 bucket.
Set the retention period to expire after 7 years.
Use S3 Batch Operations to bring the existing data into compliance.

-   [ ] S3 Object Lock with compliance mode ensures that objects cannot be deleted or overwritten for the specified retention period.
-   [ ] S3 Batch Operations allows for efficient application of Object Lock to existing objects, minimizing operational overhead.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. S3 Versioning protects against accidental deletion but does not prevent intentional deletion by authorized users.
    MFA Delete adds an extra layer of security but does not guarantee retention.
-   [ ] B. Governance retention mode allows privileged users to delete objects, which does not meet the strict compliance requirement.
    Recopying all objects adds operational overhead.
-   [ ] C. While compliance mode is correct, using S3 Batch Operations is more efficient than recopying all objects.

Therefore, Option D provides the most compliant and operationally efficient solution for retaining data for 7 years.

</details>
<details>
  <summary>Question 447</summary>

A company has a stateless web application that runs on AWS Lambda functions that are invoked by Amazon API Gateway.
The company wants to deploy the application across multiple AWS Regions to provide Regional failover capabilities.
What should a solutions architect do to route traffic to multiple Regions?

-   [ ] A. Create Amazon Route 53 health checks for each Region.
    Use an active-active failover configuration.
-   [ ] B. Create an Amazon CloudFront distribution with an origin for each Region.
    Use CloudFront health checks to route traffic.
-   [ ] C. Create a transit gateway.
    Attach the transit gateway to the API Gateway endpoint in each Region.
    Configure the transit gateway to route requests.
-   [ ] D. Create an Application Load Balancer in the primary Region.
    Set the target group to point to the API Gateway endpoint hostnames in each Region.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Create Amazon Route 53 health checks for each Region.
    Use an active-active failover configuration.

Why these are the correct answers:

A. Create Amazon Route 53 health checks for each Region.
Use an active-active failover configuration.

-   [ ] Amazon Route 53 allows for DNS-based routing of traffic to different Regions.
-   [ ] Health checks ensure that traffic is routed only to healthy Regions.
-   [ ] An active-active failover configuration provides high availability and low latency by distributing traffic across multiple Regions.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. CloudFront is primarily for caching content and is not ideal for routing all application traffic based on Regional health.
-   [ ] C. Transit Gateway connects VPCs and on-premises networks, not API Gateway endpoints across Regions.
-   [ ] D. Application Load Balancers cannot route traffic to API Gateway endpoints in different Regions as target groups.

Therefore, Option A is the most appropriate solution for routing traffic to multiple Regions for failover.

</details>
<details>
  <summary>Question 448</summary>

A company has two VPCs named Management and Production.
The Management VPC uses VPNs through a customer gateway to connect to a single device in the data center.
The Production VPC uses a virtual private gateway with two attached AWS Direct Connect connections.
The Management and Production VPCs both use a single VPC peering connection to allow communication between the applications.
What should a solutions architect do to mitigate any single point of failure in this architecture?

-   [ ] A. Add a set of VPNs between the Management and Production VPCs.
-   [ ] B. Add a second virtual private gateway and attach it to the Management VPC.
-   [ ] C. Add a second set of VPNs to the Management VPC from a second customer gateway device.
-   [ ] D. Add a second VPC peering connection between the Management VPC and the Production VPC.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Add a second set of VPNs to the Management VPC from a second customer gateway device.

Why these are the correct answers:

C. Add a second set of VPNs to the Management VPC from a second customer gateway device.

-   [ ] The current architecture has a single point of failure in the connection between the Management VPC and the data center.
-   [ ] Adding a second set of VPNs from a second customer gateway device provides redundancy and mitigates this risk.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Adding VPNs between the Management and Production VPCs does not address the single point of failure in the connection to the data center.
-   [ ] B. Adding a second virtual private gateway to the Management VPC provides redundancy on the AWS side but not for the connection to the data center.
-   [ ] D. Adding a second VPC peering connection only provides redundancy for the connection between the VPCs, not for the connection to the data center.

Therefore, Option C is the most effective solution to eliminate the single point of failure.

</details>
<details>
  <summary>Question 449</summary>

A company runs its application on an Oracle database.
The company plans to quickly migrate to AWS because of limited resources for the database, backup administration, and data center maintenance.
The application uses third-party database features that require privileged access.
Which solution will help the company migrate the database to AWS MOST cost-effectively?

-   [ ] A. Migrate the database to Amazon RDS for Oracle.
    Replace third-party features with cloud services.
-   [ ] B. Migrate the database to Amazon RDS Custom for Oracle.
    Customize the database settings to support third-party features.
-   [ ] C. Migrate the database to an Amazon EC2 Amazon Machine Image (AMI) for Oracle.
    Customize the database settings to support third-party features.
-   [ ] D. Migrate the database to Amazon RDS for PostgreSQL by rewriting the application code to remove dependency on Oracle APEX.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Migrate the database to Amazon RDS Custom for Oracle.
    Customize the database settings to support third-party features.

Why these are the correct answers:

B. Migrate the database to Amazon RDS Custom for Oracle.
Customize the database settings to support third-party features.

-   [ ] Amazon RDS Custom for Oracle allows for customization of the underlying operating system and database environment, supporting applications that require privileged access.
-   [ ] It provides a managed database service, reducing operational overhead.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Amazon RDS for Oracle does not allow for the necessary level of customization for applications that require privileged access to the database.
    Replacing third-party features adds complexity and cost.
-   [ ] C. Migrating to EC2 provides flexibility but increases operational overhead, as the company is responsible for managing the database and infrastructure.
-   [ ] D. Migrating to PostgreSQL requires significant application code changes, which is time-consuming and costly.

Therefore, Option B is the most cost-effective solution for migrating the Oracle database with privileged access requirements.

</details>

<details>
  <summary>Question 450</summary>

A company has a three-tier web application that is in a single server.
The company wants to migrate the application to the AWS Cloud.
The company also wants the application to align with the AWS Well-Architected Framework and to be consistent with AWS recommended best practices for security, scalability, and resiliency.
Which combination of solutions will meet these requirements? (Choose three.)

-   [ ] A. Create a VPC across two Availability Zones with the application's existing architecture.
    Host the application with existing architecture on an Amazon EC2 instance in a private subnet in each Availability Zone with EC2 Auto Scaling groups.
    Secure the EC2 instance with security groups and network access control lists (network ACLs).
-   [ ] B. Set up security groups and network access control lists (network ACLs) to control access to the database layer.
    Set up a single Amazon RDS database in a private subnet.
-   [ ] C. Create a VPC across two Availability Zones.
    Refactor the application to host the web tier, application tier, and database tier.
    Host each tier on its own private subnet with Auto Scaling groups for the web tier and application tier.
-   [ ] D. Use a single Amazon RDS database.
    Allow database access only from the application tier security group.
-   [ ] E. Use Elastic Load Balancers in front of the web tier.
    Control access by using security groups containing references to each layer's security groups.
-   [ ] F. Use an Amazon RDS database Multi-AZ cluster deployment in private subnets.
    Allow database access only from application tier security groups.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Create a VPC across two Availability Zones.
    Refactor the application to host the web tier, application tier, and database tier.
    Host each tier on its own private subnet with Auto Scaling groups for the web tier and application tier.
-   [ ] E. Use Elastic Load Balancers in front of the web tier.
    Control access by using security groups containing references to each layer's security groups.
-   [ ] F. Use an Amazon RDS database Multi-AZ cluster deployment in private subnets.
    Allow database access only from application tier security groups.

Why these are the correct answers:

C. Create a VPC across two Availability Zones.
Refactor the application to host the web tier, application tier, and database tier.
Host each tier on its own private subnet with Auto Scaling groups for the web tier and application tier.

-   [ ] Creating a VPC across multiple Availability Zones enhances availability and resiliency.
-   [ ] Refactoring into tiers and hosting each in its own subnet improves security and scalability.
-   [ ] Auto Scaling groups ensure scalability and fault tolerance for the web and application tiers.

E. Use Elastic Load Balancers in front of the web tier.
Control access by using security groups containing references to each layer's security groups.

-   [ ] Elastic Load Balancers distribute traffic and improve availability.
-   [ ] Security groups provide network security by controlling traffic between tiers.

F. Use an Amazon RDS database Multi-AZ cluster deployment in private subnets.
Allow database access only from application tier security groups.

-   [ ] Multi-AZ RDS enhances database availability.
-   [ ] Private subnets and security groups secure the database layer.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Hosting the application with its existing architecture on a single EC2 instance in each AZ does not fully leverage the benefits of a tiered architecture for scalability and security.
-   [ ] B and D. Using a single RDS database, even in a private subnet, introduces a single point of failure.

Therefore, Options C, E, and F provide the most robust, scalable, and secure solution, aligning with AWS best practices.

</details>






















































