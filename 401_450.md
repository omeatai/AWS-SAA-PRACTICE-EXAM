<details>
  <summary>Question 401</summary>

A company wants to use the AWS Cloud to make an existing application highly available and resilient. The current version of the application resides in the company's data center. The application recently experienced data loss after a database server crashed because of an unexpected power outage. The company needs a solution that avoids any single points of failure. The solution must give the application the ability to scale to meet user demand.

Which solution will meet these requirements?

-   [ ] A. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance in a Multi-AZ configuration.
-   [ ] B. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group in a single Availability Zone. Deploy the database on an EC2 instance. Enable EC2 Auto Recovery.
-   [ ] C. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance with a read replica in a single Availability Zone. Promote the read replica to replace the primary DB instance if the primary DB instance fails.
-   [ ] D. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Deploy the primary and secondary database servers on EC2 instances across multiple Availability Zones. Use Amazon Elastic Block Store (Amazon EBS) Multi-Attach to create shared storage between the instances.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance in a Multi-AZ configuration.

Why these are the correct answers:

A. Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance in a Multi-AZ configuration.

-   [ ]   Auto Scaling groups across multiple Availability Zones provide high availability and scalability for the application servers.
-   [ ]   Amazon RDS Multi-AZ configuration ensures database availability and failover capability, avoiding a single point of failure.

Why are the other answers wrong?

-   [ ]   B. Using Auto Scaling in a single Availability Zone does not provide high availability. EC2 Auto Recovery only recovers individual instances, not Availability Zone failures.
-   [ ]   C. A single read replica does not provide automatic failover for write operations. Manual promotion adds complexity and potential downtime.
-   [ ]   D. Deploying the database on EC2 instances increases management overhead. EBS Multi-Attach has limitations and is not suitable for all database workloads.

Therefore, using Auto Scaling across Availability Zones and Amazon RDS Multi-AZ is the most appropriate solution.
</details>
<details>
  <summary>Question 402</summary>

A company needs to ingest and handle large amounts of streaming data that its application generates. The application runs on Amazon EC2 instances and sends data to Amazon Kinesis Data Streams, which is configured with default settings. Every other day, the application consumes the data and writes the data to an Amazon S3 bucket for business intelligence (BI) processing. The company observes that Amazon S3 is not receiving all the data that the application sends to Kinesis Data Streams. What should a solutions architect do to resolve this issue?

-   [ ] A. Update the Kinesis Data Streams default settings by modifying the data retention period.
-   [ ] B. Update the application to use the Kinesis Producer Library (KPL) to send the data to Kinesis Data Streams.
-   [ ] C. Update the number of Kinesis shards to handle the throughput of the data that is sent to Kinesis Data Streams.
-   [ ] D. Turn on S3 Versioning within the S3 bucket to preserve every version of every object that is ingested in the S3 bucket.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Update the Kinesis Data Streams default settings by modifying the data retention period.

Why these are the correct answers:

A. Update the Kinesis Data Streams default settings by modifying the data retention period.

-   [ ]   Kinesis Data Streams has a default data retention period of 24 hours.
-   [ ]   If the application consumes the data every other day (every 48 hours), the default retention period is insufficient.
-   [ ]   Increasing the retention period ensures that data is available for processing.

Why are the other answers wrong?

-   [ ]   B. The Kinesis Producer Library (KPL) improves throughput but does not directly address data loss due to retention period.
-   [ ]   C. Increasing the number of shards helps with throughput but does not solve the issue of data expiring before it is consumed.
-   [ ]   D. S3 Versioning preserves different versions of objects but does not address data loss in Kinesis Data Streams.

Therefore, modifying the data retention period is the most appropriate solution.
</details>
<details>
  <summary>Question 403</summary>

A developer has an application that uses an AWS Lambda function to upload files to Amazon S3 and needs the required permissions to perform the task. The developer already has an IAM user with valid IAM credentials required for Amazon S3. What should a solutions architect do to grant the permissions?

-   [ ] A. Add required IAM permissions in the resource policy of the Lambda function.
-   [ ] B. Create a signed request using the existing IAM credentials in the Lambda function.
-   [ ] C. Create a new IAM user and use the existing IAM credentials in the Lambda function.
-   [ ] D. Create an IAM execution role with the required permissions and attach the IAM role to the Lambda function.
</details>
<details>
  <summary>Answer</summary>

-   [ ] D. Create an IAM execution role with the required permissions and attach the IAM role to the Lambda function.

Why these are the correct answers:

D. Create an IAM execution role with the required permissions and attach the IAM role to the Lambda function.

-   [ ]   IAM roles provide temporary security credentials to AWS services, such as Lambda functions.
-   [ ]   Attaching an IAM role to the Lambda function grants it the necessary permissions to access S3.
-   [ ]   This is the most secure and recommended way to grant permissions to Lambda functions.

Why are the other answers wrong?

-   [ ]   A. Resource policies are not used to grant permissions to Lambda functions to access other AWS services.
-   [ ]   B. Signed requests use IAM user credentials, which is less secure than using roles.
-   [ ]   C. Creating a new IAM user is unnecessary and does not solve the problem of securely granting permissions.

Therefore, using an IAM execution role is the best practice for granting permissions to Lambda functions.
</details>
<details>
  <summary>Question 404</summary>

A company has deployed a serverless application that invokes an AWS Lambda function when new documents are uploaded to an Amazon S3 bucket. The application uses the Lambda function to process the documents. After a recent marketing campaign, the company noticed that the application did not process many of the documents. What should a solutions architect do to improve the architecture of this application?

-   [ ] A. Set the Lambda function's runtime timeout value to 15 minutes.
-   [ ] B. Configure an S3 bucket replication policy. Stage the documents in the S3 bucket for later processing.
-   [ ] C. Deploy an additional Lambda function. Load balance the processing of the documents across the two Lambda functions.
-   [ ] D. Create an Amazon Simple Queue Service (Amazon SQS) message queue. Send the requests to the queue. Configure the queue as an event source for Lambda.
</details>
<details>
  <summary>Answer</summary>

-   [ ] D. Create an Amazon Simple Queue Service (Amazon SQS) message queue. Send the requests to the queue. Configure the queue as an event source for Lambda.

Why these are the correct answers:

D. Create an Amazon Simple Queue Service (Amazon SQS) message queue. Send the requests to the queue. Configure the queue as an event source for Lambda.

-   [ ]   Amazon SQS decouples the S3 upload process from the Lambda processing.
-   [ ]   SQS queues can buffer incoming requests, ensuring that no documents are lost during high traffic.
-   [ ]   Configuring the queue as an event source for Lambda allows Lambda to process messages from the queue reliably.

Why are the other answers wrong?

-   [ ]   A. Increasing the Lambda timeout might help with individual processing times but does not address the issue of lost documents.
-   [ ]   B. S3 replication is for copying objects between buckets, not for ensuring reliable processing.
-   [ ]   C. Deploying additional Lambda functions without a queue does not prevent document loss during high traffic.

Therefore, using an SQS queue is the most appropriate solution for reliable document processing.
</details>
<details>
  <summary>Question 405</summary>

A solutions architect is designing the architecture for a software demonstration environment. The environment will run on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). The system will experience significant increases in traffic during working hours but is not required to operate on weekends. Which combination of actions should the solutions architect take to ensure that the system can scale to meet demand? (Choose two.)

-   [ ] A. Use AWS Auto Scaling to adjust the ALB capacity based on request rate.
-   [ ] B. Use AWS Auto Scaling to scale the capacity of the VPC internet gateway.
-   [ ] C. Launch the EC2 instances in multiple AWS Regions to distribute the load across Regions.
-   [ ] D. Use a target tracking scaling policy to scale the Auto Scaling group based on instance CPU utilization.
-   [ ] E. Use scheduled scaling to change the Auto Scaling group minimum, maximum, and desired capacity to zero for weekends. Revert to the default values at the start of the week.
</details>
<details>
  <summary>Answer</summary>

-   [ ] D. Use a target tracking scaling policy to scale the Auto Scaling group based on instance CPU utilization.
-   [ ] E. Use scheduled scaling to change the Auto Scaling group minimum, maximum, and desired capacity to zero for weekends. Revert to the default values at the start of the week.

Why these are the correct answers:

D. Use a target tracking scaling policy to scale the Auto Scaling group based on instance CPU utilization.

-   [ ]   Target tracking scaling policies automatically adjust the number of EC2 instances in response to changes in CPU utilization, ensuring the system can handle increased traffic during working hours.

E. Use scheduled scaling to change the Auto Scaling group minimum, maximum, and desired capacity to zero for weekends. Revert to the default values at the start of the week.

-   [ ]   Scheduled scaling allows you to set the capacity of the Auto Scaling group based on a predictable schedule, such as reducing it to zero on weekends when the system is not in use.

Why are the other answers wrong?

-   [ ]   A. Auto Scaling does not directly adjust ALB capacity. Auto Scaling adjusts the number of EC2 instances behind the ALB.
-   [ ]   B. You cannot scale the capacity of a VPC internet gateway.
-   [ ]   C. Launching EC2 instances in multiple AWS Regions is unnecessary and costly for scaling within working hours.

Therefore, using target tracking and scheduled scaling is the most appropriate solution for this scenario.
</details>
<details>
  <summary>Question 406</summary>

A solutions architect is designing a two-tiered architecture that includes a public subnet and a database subnet. The web servers in the public subnet must be open to the internet on port 443. The Amazon RDS for MySQL DB instance in the database subnet must be accessible only to the web servers on port 3306.

Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)

-   [ ] A. Create a network ACL for the public subnet. Add a rule to deny outbound traffic to 0.0.0.0/0 on port 3306.
-   [ ] B. Create a security group for the DB instance. Add a rule to allow traffic from the public subnet CIDR block on port 3306.
-   [ ] C. Create a security group for the web servers in the public subnet. Add a rule to allow traffic from 0.0.0.0/0 on port 443.
-   [ ] D. Create a security group for the DB instance. Add a rule to allow traffic from the web servers' security group on port 3306.
-   [ ] E. Create a security group for the DB instance. Add a rule to deny all traffic except traffic from the web servers' security group on port 3306.
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. Create a security group for the web servers in the public subnet. Add a rule to allow traffic from 0.0.0.0/0 on port 443.
-   [ ] D. Create a security group for the DB instance. Add a rule to allow traffic from the web servers' security group on port 3306.

Why these are the correct answers:

C. Create a security group for the web servers in the public subnet. Add a rule to allow traffic from 0.0.0.0/0 on port 443.

-   [ ]   Security groups act as a virtual firewall for EC2 instances.
-   [ ]   Allowing inbound traffic from 0.0.0.0/0 on port 443 makes the web servers accessible from the internet.

D. Create a security group for the DB instance. Add a rule to allow traffic from the web servers' security group on port 3306.

-   [ ]   This ensures that only the web servers can access the database, enhancing security.

Why are the other answers wrong?

-   [ ]   A. Network ACLs operate at the subnet level and are less granular than security groups for controlling traffic to specific instances. Denying outbound traffic from the public subnet is not the correct approach.
-   [ ]   B. Allowing traffic from the entire public subnet CIDR block is less secure than allowing traffic only from the web servers' security group.
-   [ ]   E. Denying all traffic except from the web servers' security group is redundant and less clear than simply allowing traffic from that security group.

Therefore, using security groups with appropriate rules is the most secure and effective solution.
</details>
<details>
  <summary>Question 407</summary>

A company is implementing a shared storage solution for a gaming application that is hosted in the AWS Cloud. The company needs the ability to use Lustre clients to access data. The solution must be fully managed. Which solution meets these requirements?

-   [ ] A. Create an AWS DataSync task that shares the data as a mountable file system. Mount the file system to the application server.
-   [ ] B. Create an AWS Storage Gateway file gateway. Create a file share that uses the required client protocol. Connect the application server to the file share.
-   [ ] C. Create an Amazon Elastic File System (Amazon EFS) file system, and configure it to support Lustre. Attach the file system to the origin server. Connect the application server to the file system.
-   [ ] D. Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect the application server to the file system.
</details>
<details>
  <summary>Answer</summary>

-   [ ] D. Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect the application server to the file system.

Why these are the correct answers:

D. Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect the application server to the file system.

-   [ ]   Amazon FSx for Lustre is a fully managed file system optimized for compute-intensive workloads that require high performance and POSIX compliance.
-   [ ]   It supports the Lustre file system, meeting the requirement for Lustre clients.

Why are the other answers wrong?

-   [ ]   A. AWS DataSync is a data transfer service, not a file system.
-   [ ]   B. AWS Storage Gateway provides file interfaces to cloud storage but does not support Lustre.
-   [ ]   C. Amazon EFS does not support the Lustre protocol.

Therefore, Amazon FSx for Lustre is the most appropriate solution for a fully managed, high-performance file system with Lustre client support.
</details>
<details>
  <summary>Question 408</summary>

A company runs an application that receives data from thousands of geographically dispersed remote devices that use UDP. The application processes the data immediately and sends a message back to the device if necessary. No data is stored. The company needs a solution that minimizes latency for the data transmission from the devices. The solution also must provide rapid failover to another AWS Region.

Which solution will meet these requirements?

-   [ ] A. Configure an Amazon Route 53 failover routing policy. Create a Network Load Balancer (NLB) in each of the two Regions. Configure the NLB to invoke an AWS Lambda function to process the data.
-   [ ] B. Use AWS Global Accelerator. Create a Network Load Balancer (NLB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the NLProcess the data in Amazon ECS.
-   [ ] C. Use AWS Global Accelerator. Create an Application Load Balancer (ALB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the ALB. Process the data in Amazon ECS.
-   [ ] D. Configure an Amazon Route 53 failover routing policy. Create an Application Load Balancer (ALB) in each of the two Regions. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the ALB. Process the data in Amazon ECS.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Use AWS Global Accelerator. Create a Network Load Balancer (NLB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the NLProcess the data in Amazon ECS.

Why these are the correct answers:

B. Use AWS Global Accelerator. Create a Network Load Balancer (NLB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the NLProcess the data in Amazon ECS.

-   [ ]   AWS Global Accelerator minimizes latency by using the AWS global network to optimize the path from the remote devices to the application.
-   [ ]   Network Load Balancers (NLBs) are designed for high-performance, low-latency traffic and support UDP.
-   [ ]   Amazon ECS with Fargate provides a scalable and serverless container orchestration platform.
-   [ ]   This combination provides low latency and rapid failover.

Why are the other answers wrong?

-   [ ]   A. Route 53 failover routing is for DNS-level failover, which is slower than Global Accelerator for real-time applications. Lambda is not suitable for handling high-volume, continuous UDP traffic.
-   [ ]   C and D. Application Load Balancers (ALBs) are designed for HTTP/HTTPS traffic, not UDP.

Therefore, AWS Global Accelerator with NLBs and ECS Fargate is the most appropriate solution.
</details>
<details>
  <summary>Question 409</summary>

A solutions architect must migrate a Windows Internet Information Services (IIS) web application to AWS. The application currently relies on a file share hosted in the user's on-premises network-attached storage (NAS). The solutions architect has proposed migrating the IIS web servers to Amazon EC2 instances in multiple Availability Zones that are connected to the storage solution, and configuring an Elastic Load Balancer attached to the instances. Which replacement to the on-premises file share is MOST resilient and durable?

-   [ ] A. Migrate the file share to Amazon RDS.
-   [ ] B. Migrate the file share to AWS Storage Gateway.
-   [ ] C. Migrate the file share to Amazon FSx for Windows File Server.
-   [ ] D. Migrate the file share to Amazon Elastic File System (Amazon EFS).
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. Migrate the file share to Amazon FSx for Windows File Server.

Why these are the correct answers:

C. Migrate the file share to Amazon FSx for Windows File Server.

-   [ ]   Amazon FSx for Windows File Server provides a fully managed, highly available, and scalable file storage solution that supports the SMB protocol, which is native to Windows.
-   [ ]   It is designed to be compatible with Windows-based applications and provides the necessary resilience and durability.

Why are the other answers wrong?

-   [ ]   A. Amazon RDS is a database service and not suitable for replacing a file share.
-   [ ]   B. AWS Storage Gateway connects on-premises storage to AWS cloud storage but is not a fully managed file system in the cloud.
-   [ ]   D. Amazon EFS is a file storage service for Linux-based instances and does not natively support the SMB protocol used by Windows applications.

Therefore, Amazon FSx for Windows File Server is the most appropriate replacement for an on-premises Windows file share.
</details>
<details>
  <summary>Question 410</summary>

A company is deploying a new application on Amazon EC2 instances. The application writes data to Amazon Elastic Block Store (Amazon EBS) volumes. The company needs to ensure that all data that is written to the EBS volumes is encrypted at rest. Which solution will meet this requirement?

-   [ ] A. Create an IAM role that specifies EBS encryption. Attach the role to the EC2 instances.
-   [ ] B. Create the EBS volumes as encrypted volumes. Attach the EBS volumes to the EC2 instances.
-   [ ] C. Create an EC2 instance tag that has a key of Encrypt and a value of True. Tag all instances that require encryption at the EBS level.
-   [ ] D. Create an AWS Key Management Service (AWS KMS) key policy that enforces EBS encryption in the account. Ensure that the key policy is active.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Create the EBS volumes as encrypted volumes. Attach the EBS volumes to the EC2 instances.

Why these are the correct answers:

B. Create the EBS volumes as encrypted volumes. Attach the EBS volumes to the EC2 instances.

-   [ ]   Encrypting EBS volumes at creation ensures that all data written to the volume is encrypted at rest.
-   [ ]   This is the most direct and effective way to meet the requirement.

Why are the other answers wrong?

-   [ ]   A. IAM roles control access to AWS services but do not enforce EBS encryption.
-   [ ]   C. EC2 instance tags are metadata and do not enforce encryption.
-   [ ]   D. KMS key policies control access to KMS keys but do not enforce EBS encryption.

Therefore, creating encrypted EBS volumes is the correct solution.
</details>

<details>
  <summary>Question 411</summary>

A company has a web application with sporadic usage patterns. There is heavy usage at the beginning of each month, moderate usage at the start of each week, and unpredictable usage during the week. The application consists of a web server and a MySQL database server running inside the data center. The company would like to move the application to the AWS Cloud, and needs to select a cost-effective database platform that will not require database modifications. Which solution will meet these requirements?

-   [ ] A. Amazon DynamoDB
-   [ ] B. Amazon RDS for MySQL
-   [ ] C. MySQL-compatible Amazon Aurora Serverless
-   [ ] D. MySQL deployed on Amazon EC2 in an Auto Scaling group
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. MySQL-compatible Amazon Aurora Serverless

Why these are the correct answers:

C. MySQL-compatible Amazon Aurora Serverless

-   [ ]   Amazon Aurora Serverless automatically starts up, shuts down, and scales database capacity based on application needs.
-   [ ]   It is cost-effective for applications with sporadic usage patterns.
-   [ ]   It is compatible with MySQL, requiring minimal to no database modifications.

Why are the other answers wrong?

-   [ ]   A. Amazon DynamoDB is a NoSQL database and would require significant application modifications.
-   [ ]   B. Amazon RDS for MySQL requires provisioning a specific instance size, which may not be cost-effective for sporadic usage.
-   [ ]   D. Deploying MySQL on Amazon EC2 with Auto Scaling involves managing the database and scaling, increasing operational overhead.

Therefore, Amazon Aurora Serverless is the most suitable and cost-effective solution.
</details>
<details>
  <summary>Question 412</summary>

An image-hosting company stores its objects in Amazon S3 buckets. The company wants to avoid accidental exposure of the objects in the S3 buckets to the public. All S3 objects in the entire AWS account need to remain private. Which solution will meet these requirements?

-   [ ] A. Use Amazon GuardDuty to monitor S3 bucket policies. Create an automatic remediation action rule that uses an AWS Lambda function to remediate any change that makes the objects public.
-   [ ] B. Use AWS Trusted Advisor to find publicly accessible S3 buckets. Configure email notifications in Trusted Advisor when a change is detected. Manually change the S3 bucket policy if it allows public access.
-   [ ] C. Use AWS Resource Access Manager to find publicly accessible S3 buckets. Use Amazon Simple Notification Service (Amazon SNS) to invoke an AWS Lambda function when a change is detected. Deploy a Lambda function that programmatically remediates the change.
-   [ ] D. Use the S3 Block Public Access feature on the account level. Use AWS Organizations to create a service control policy (SCP) that prevents IAM users from changing the setting. Apply the SCP to the account.
</details>
<details>
  <summary>Answer</summary>

-   [ ] D. Use the S3 Block Public Access feature on the account level. Use AWS Organizations to create a service control policy (SCP) that prevents IAM users from changing the setting. Apply the SCP to the account.

Why these are the correct answers:

D. Use the S3 Block Public Access feature on the account level. Use AWS Organizations to create a service control policy (SCP) that prevents IAM users from changing the setting. Apply the SCP to the account.

-   [ ]   S3 Block Public Access at the account level prevents any public access to S3 buckets and objects within that account.
-   [ ]   AWS Organizations service control policies (SCPs) can enforce this setting across all accounts in the organization, preventing users from changing it.
-   [ ]   This solution provides the strongest and most centralized control.

Why are the other answers wrong?

-   [ ]   A, B, and C. These solutions involve monitoring and remediation, which are reactive rather than preventative. They also introduce operational overhead.

Therefore, using S3 Block Public Access with an SCP is the most secure and effective solution to prevent public access.
</details>
<details>
  <summary>Question 413</summary>

An ecommerce company is experiencing an increase in user traffic. The company's store is deployed on Amazon EC2 instances as a two-tier web application consisting of a web tier and a separate database tier. As traffic increases, the company notices that the architecture is causing significant delays in sending timely marketing and order confirmation email to users. The company wants to reduce the time it spends resolving complex email delivery issues and minimize operational overhead. What should a solutions architect do to meet these requirements?

-   [ ] A. Create a separate application tier using EC2 instances dedicated to email processing.
-   [ ] B. Configure the web instance to send email through Amazon Simple Email Service (Amazon SES).
-   [ ] C. Configure the web instance to send email through Amazon Simple Notification Service (Amazon SNS).
-   [ ] D. Create a separate application tier using EC2 instances dedicated to email processing. Place the instances in an Auto Scaling group.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Configure the web instance to send email through Amazon Simple Email Service (Amazon SES).

Why these are the correct answers:

B. Configure the web instance to send email through Amazon Simple Email Service (Amazon SES).

-   [ ]   Amazon SES is a cloud-based email sending service that is designed to help digital marketers and application developers send marketing, notification, and transactional emails.
-   [ ]   Offloading email sending to SES reduces the load on the web instances and simplifies email delivery issues.
-   [ ]   This solution minimizes operational overhead by using a managed service.

Why are the other answers wrong?

-   [ ]   A and D. Creating separate EC2 instances or Auto Scaling groups for email processing adds complexity and operational overhead.
-   [ ]   C. Amazon SNS is for sending notifications, not for transactional or marketing emails.

Therefore, using Amazon SES is the most efficient and appropriate solution.
</details>
<details>
  <summary>Question 414</summary>

A company has a business system that generates hundreds of reports each day. The business system saves the reports to a network share in CSV format. The company needs to store this data in the AWS Cloud in near-real time for analysis. Which solution will meet these requirements with the LEAST administrative overhead?

-   [ ] A. Use AWS DataSync to transfer the files to Amazon S3. Create a scheduled task that runs at the end of each day.
-   [ ] B. Create an Amazon S3 File Gateway. Update the business system to use a new network share from the S3 File Gateway.
-   [ ] C. Use AWS DataSync to transfer the files to Amazon S3. Create an application that uses the DataSync API in the automation workflow.
-   [ ] D. Deploy an AWS Transfer for SFTP endpoint. Create a script that checks for new files on the network share and uploads the new files by using SFTP.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Create an Amazon S3 File Gateway. Update the business system to use a new network share from the S3 File Gateway.

Why these are the correct answers:

B. Create an Amazon S3 File Gateway. Update the business system to use a new network share from the S3 File Gateway.

-   [ ]   Amazon S3 File Gateway provides a seamless way to access S3 objects as files on a local file system.
-   [ ]   Updating the business system to use this new network share requires minimal changes and reduces administrative overhead.

Why are the other answers wrong?

-   [ ]   A and C. Using AWS DataSync requires additional scheduling or API integration, increasing overhead.
-   [ ]   D. Deploying an SFTP endpoint and creating scripts adds complexity and management overhead.

Therefore, Amazon S3 File Gateway is the simplest and most efficient solution for near-real-time data transfer.
</details>
<details>
  <summary>Question 415</summary>

A company is storing petabytes of data in Amazon S3 Standard. The data is stored in multiple S3 buckets and is accessed with varying frequency. The company does not know access patterns for all the data. The company needs to implement a solution for each S3 bucket to optimize the cost of S3 usage. Which solution will meet these requirements with the MOST operational efficiency?

-   [ ] A. Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Intelligent-Tiering.
-   [ ] B. Use the S3 storage class analysis tool to determine the correct tier for each object in the S3 bucket. Move each object to the identified storage tier.
-   [ ] C. Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Glacier Instant Retrieval.
-   [ ] D. Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 One Zone-Infrequent Access (S3 One Zone-IA).
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Intelligent-Tiering.

Why these are the correct answers:

A. Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Intelligent-Tiering.

-   [ ]   S3 Intelligent-Tiering automatically optimizes storage costs by moving data to the most cost-effective tier based on access patterns.
-   [ ]   It handles objects with varying access frequencies, and using a lifecycle configuration automates this process.

Why are the other answers wrong?

-   [ ]   B. Using the S3 storage class analysis tool and manually moving objects is operationally intensive.
-   [ ]   C. S3 Glacier Instant Retrieval is for archive data with infrequent access, not for data with varying access patterns.
-   [ ]   D. S3 One Zone-IA reduces availability and is not suitable for all data.

Therefore, S3 Intelligent-Tiering is the most efficient solution for cost optimization with unknown access patterns.
</details>
<details>
  <summary>Question 416</summary>

A rapidly growing global ecommerce company is hosting its web application on AWS. The web application includes static content and dynamic content. The website stores online transaction processing (OLTP) data in an Amazon RDS database The website's users are experiencing slow page loads. Which combination of actions should a solutions architect take to resolve this issue? (Choose two.)

-   [ ] A. Configure an Amazon Redshift cluster.
-   [ ] B. Set up an Amazon CloudFront distribution.
-   [ ] C. Host the dynamic web content in Amazon S3.
-   [ ] D. Create a read replica for the RDS DB instance.
-   [ ] E. Configure a Multi-AZ deployment for the RDS DB instance.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Set up an Amazon CloudFront distribution.
-   [ ] D. Create a read replica for the RDS DB instance.

Why these are the correct answers:

B. Set up an Amazon CloudFront distribution.

-   [ ]   Amazon CloudFront is a content delivery network (CDN) that can cache static content closer to users, improving load times.

D. Create a read replica for the RDS DB instance.

-   [ ]   A read replica offloads read operations from the primary database, improving database performance.

Why are the other answers wrong?

-   [ ]   A. Amazon Redshift is for data warehousing and analytics, not for improving web application performance.
-   [ ]   C. Hosting dynamic content in S3 is not practical. S3 is for static content.
-   [ ]   E. Multi-AZ deployments improve database availability, not read performance.

Therefore, CloudFront and RDS read replicas are the most appropriate solutions.
</details>
<details>
  <summary>Question 417</summary>

A company uses Amazon EC2 instances and AWS Lambda functions to run its application. The company has VPCs with public subnets and private subnets in its AWS account. The EC2 instances run in a private subnet in one of the VPCs. The Lambda functions need direct network access to the EC2 instances for the application to work. The application will run for at least 1 year. The company expects the number of Lambda functions that the application uses to increase during that time. The company wants to maximize its savings on all application resources and to keep network latency between the services low. Which solution will meet these requirements?

-   [ ] A. Purchase an EC2 Instance Savings Plan Optimize the Lambda functions' duration and memory usage and the number of invocations. Connect the Lambda functions to the private subnet that contains the EC2 instances.
-   [ ] B. Purchase an EC2 Instance Savings Plan Optimize the Lambda functions' duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to a public subnet in the same VPC where the EC2 instances run.
-   [ ] C. Purchase a Compute Savings Plan. Optimize the Lambda functions' duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to the private subnet that contains the EC2 instances.
-   [ ] D. Purchase a Compute Savings Plan. Optimize the Lambda functions' duration and memory usage, the number of invocations, and the amount of data that is transferred. Keep the Lambda functions in the Lambda service VPC.
</details>
<details>
  <summary>Answer</summary>

-   [ ] C. Purchase a Compute Savings Plan. Optimize the Lambda functions' duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to the private subnet that contains the EC2 instances.

Why these are the correct answers:

C. Purchase a Compute Savings Plan. Optimize the Lambda functions' duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to the private subnet that contains the EC2 instances.

-   [ ]   Compute Savings Plans offer cost savings for both EC2 and Lambda usage, which is beneficial as the number of Lambda functions increases.
-   [ ]   Optimizing Lambda functions reduces costs.
-   [ ]   Connecting Lambda functions to the private subnet ensures low latency.

Why are the other answers wrong?

-   [ ]   A and B. EC2 Instance Savings Plans are less flexible than Compute Savings Plans, as they do not apply to Lambda costs. Using a public subnet increases security risks.
-   [ ]   D. Keeping Lambda functions in the Lambda service VPC would require a VPC peering or other network connection, increasing latency.

Therefore, Compute Savings Plans and connecting Lambda functions to the private subnet are the most cost-effective and efficient solution.
</details>
<details>
  <summary>Question 418</summary>

A solutions architect needs to allow team members to access Amazon S3 buckets in two different AWS accounts: a development account and a production account. The team currently has access to S3 buckets in the development account by using unique IAM users that are assigned to an IAM group that has appropriate permissions in the account. The solutions architect has created an IAM role in the production account. The role has a policy that grants access to an S3 bucket in the production account. Which solution will meet these requirements while complying with the principle of least privilege?

-   [ ] A. Attach the Administrator Access policy to the development account users.
-   [ ] B. Add the development account as a principal in the trust policy of the role in the production account.
-   [ ] C. Turn off the S3 Block Public Access feature on the S3 bucket in the production account.
-   [ ] D. Create a user in the production account with unique credentials for each team member.
</details>
<details>
  <summary>Answer</summary>

-   [ ] B. Add the development account as a principal in the trust policy of the role in the production account.

Why these are the correct answers:

B. Add the development account as a principal in the trust policy of the role in the production account.

-   [ ]   IAM roles allow users in one AWS account to assume the role in another account, granting them temporary access to resources in that account.
-   [ ]   Adding the development account as a trusted principal allows the team members to assume the role.
-   [ ]   This follows the principle of least privilege by granting only the necessary permissions.

Why are the other answers wrong?

-   [ ]   A. Administrator Access policy grants excessive permissions, violating least privilege.
-   [ ]   C. Turning off S3 Block Public Access increases security risks.
-   [ ]   D. Creating separate users in the production account adds management overhead and does not leverage IAM roles for secure cross-account access.

Therefore, using IAM roles for cross-account access is the most secure and efficient solution.
</details>
<details>
  <summary>Question 419</summary>

A company uses AWS Organizations with all features enabled and runs multiple Amazon EC2 workloads in the ap-southeast-2 Region. The company has a service control policy (SCP) that prevents any resources from being created in any other Region. A security policy requires the company to encrypt all data at rest. An audit discovers that employees have created Amazon Elastic Block Store (Amazon EBS) volumes for EC2 instances without encrypting the volumes. The company wants any new EC2 instances that any IAM user or root user launches in ap-southeast-2 to use encrypted EBS volumes. The company wants a solution that will have minimal effect on employees who create EBS volumes. Which combination of steps will meet these requirements? (Choose two.)

-   [ ] A. In the Amazon EC2 console, select the EBS encryption account attribute and define a default encryption key.
-   [ ] B. Create an IAM permission boundary. Attach the permission boundary to the root organizational unit (OU). Define the boundary to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.
-   [ ] C. Create an SCP. Attach the SCP to the root organizational unit (OU). Define the SCP to deny the ec2:CreateVolume action whenthe ec2:Encrypted condition equals false.
-   [ ] D. Update the IAM policies for each account to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.
-   [ ] E. In the Organizations management account, specify the Default EBS volume encryption setting.
</details>
<details>
  <summary>Answer</summary>

-   [ ] A. In the Amazon EC2 console, select the EBS encryption account attribute and define a default encryption key.
-   [ ] C. Create an SCP. Attach the SCP to the root organizational unit (OU). Define the SCP to deny the ec2:CreateVolume action whenthe ec2:Encrypted condition equals false.

Why these are the correct answers:

A. In the Amazon EC2 console, select the EBS encryption account attribute and define a default encryption key.

-   [ ]   Setting the default EBS encryption ensures that all new EBS volumes are encrypted by default, minimizing the impact on employees.

C. Create an SCP. Attach the SCP to the root organizational unit (OU). Define the SCP to deny the ec2:CreateVolume action whenthe ec2:Encrypted condition equals false.

-   [ ]   SCPs enforce organizational policies across all accounts in AWS Organizations.
-   [ ]   This prevents IAM users and root users from creating unencrypted EBS volumes.

Why are the other answers wrong?

-   [ ]   B. IAM permission boundaries do not enforce policies across AWS Organizations.
-   [ ]   D. Updating IAM policies in each account is cumbersome and does not provide centralized control.
-   [ ]   E. While setting the Default EBS volume encryption setting is correct, it does not prevent users from disabling encryption, so an SCP is still needed for enforcement.

Therefore, setting the default encryption and using an SCP is the most effective solution.
</details>
<details>
  <summary>Question 420</summary>

A company wants to use an Amazon RDS for PostgreSQL DB cluster to simplify time-consuming database administrative tasks for production database workloads. The company wants to ensure that its database is highly available and will provide automatic failover support in most scenarios in less than 40 seconds. The company wants to offload reads off of the primary instance and keep costs as low as possible. Which solution will meet these requirements?

-   [ ] A. Use an Amazon RDS Multi-AZ DB instance deployment. Create one read replica and point the read workload to the read replica.
-   [ ] B. Use an Amazon RDS Multi-AZ DB duster deployment Create two read replicas and point the read workload to the read replicas.
-   [ ] C. Use an Amazon RDS Multi-AZ DB instance deployment. Point the read workload to the secondary instances in the Multi-AZ pair.
-   [ ] D. Use an Amazon RDS Multi-AZ DB cluster deployment Point the read workload to the reader endpoint.
</details>
<details>
  <summary>Answer</summary>

-   [ ] D. Use an Amazon RDS Multi-AZ DB cluster deployment Point the read workload to the reader endpoint.

Why these are the correct answers:

D. Use an Amazon RDS Multi-AZ DB cluster deployment Point the read workload to the reader endpoint.

-   [ ]   Amazon Aurora PostgreSQL Multi-AZ DB clusters provide high availability and automatic failover.
-   [ ]   Aurora clusters have a reader endpoint that distributes read traffic across Aurora Replicas, offloading the primary instance.
-   [ ]   Aurora's architecture is designed for fast failover.

Why are the other answers wrong?

-   [ ]   A, B, and C. Amazon RDS Multi-AZ DB instances are for high availability, not read scaling. Read replicas are separate instances, and Multi-AZ secondary instances are for failover, not general read traffic.

Therefore, Amazon Aurora PostgreSQL Multi-AZ DB clusters with the reader endpoint are the most appropriate solution.
</details>

<details>
  <summary>Question 421</summary>

A company runs a highly available SFTP service.
The SFTP service uses two Amazon EC2 Linux instances that run with elastic IP addresses to accept traffic from trusted IP sources on the internet.
The SFTP service is backed by shared storage that is attached to the instances.
User accounts are created and managed as Linux users in the SFTP servers.
The company wants a serverless option that provides high IOPS performance and highly configurable security.
The company also wants to maintain control over user permissions.

Which solution will meet these requirements?

- [ ] A. Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume.
Create an AWS Transfer Family SFTP service with a public endpoint that allows only trusted IP addresses.
Attach the EBS volume to the SFTP service endpoint.
Grant users access to the SFTP service.
- [ ] B. Create an encrypted Amazon Elastic File System (Amazon EFS) volume.
Create an AWS Transfer Family SFTP service with elastic IP addresses and a VPC endpoint that has internet-facing access.
Attach a security group to the endpoint that allows only trusted IP addresses.
Attach the EFS volume to the SFTP service endpoint.
Grant users access to the SFTP service.
- [ ] C. Create an Amazon S3 bucket with default encryption enabled.
Create an AWS Transfer Family SFTP service with a public endpoint that allows only trusted IP addresses.
Attach the S3 bucket to the SFTP service endpoint.
Grant users access to the SFTP service.
- [ ] D. Create an Amazon S3 bucket with default encryption enabled.
Create an AWS Transfer Family SFTP service with a VPC endpoint that has internal access in a private subnet.
Attach a security group that allows only trusted IP addresses.
Attach the S3 bucket to the SFTP service endpoint.
Grant users access to the SFTP service.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Create an encrypted Amazon Elastic File System (Amazon EFS) volume.
Create an AWS Transfer Family SFTP service with elastic IP addresses and a VPC endpoint that has internet-facing access.
Attach a security group to the endpoint that allows only trusted IP addresses.
Attach the EFS volume to the SFTP service endpoint.
Grant users access to the SFTP service.

Why these are the correct answers:

B. Create an encrypted Amazon Elastic File System (Amazon EFS) volume.
Create an AWS Transfer Family SFTP service with elastic IP addresses and a VPC endpoint that has internet-facing access.
Attach a security group to the endpoint that allows only trusted IP addresses.
Attach the EFS volume to the SFTP service endpoint.
Grant users access to the SFTP service.

- [ ] AWS Transfer Family provides a serverless SFTP service.
- [ ] Amazon EFS provides shared file storage with high IOPS performance.
- [ ] Using a VPC endpoint with a security group allows for highly configurable security by controlling access.
- [ ] EFS allows for maintaining control over user permissions.

Why are the other answers wrong?

- [ ] A. Using EBS volumes with AWS Transfer Family is not a valid configuration, as EBS is block storage and not designed for shared file access in this context.
- [ ] C and D. Amazon S3 is object storage, not a file system, and while Transfer Family can use S3, it doesn't provide the same file system semantics or performance as EFS for an SFTP service requiring high IOPS.

Therefore, Option B is the most suitable solution as it leverages AWS Transfer Family for serverless SFTP, EFS for shared storage with high IOPS, and VPC endpoints with security groups for secure access control.

</details>

<details>
  <summary>Question 422</summary>

A company is developing a new machine learning (ML) model solution on AWS.
The models are developed as independent microservices that fetch approximately 1 GB of model data from Amazon S3 at startup and load the data into memory.
Users access the models through an asynchronous API.
Users can send a request or a batch of requests and specify where the results should be sent.
The company provides models to hundreds of users.
The usage patterns for the models are irregular.
Some models could be unused for days or weeks.
Other models could receive batches of thousands of requests at a time.
Which design should a solutions architect recommend to meet these requirements?

- [ ] A. Direct the requests from the API to a Network Load Balancer (NLB).
Deploy the models as AWS Lambda functions that are invoked by the NLB.
- [ ] B. Direct the requests from the API to an Application Load Balancer (ALB).
Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from an Amazon Simple Queue Service (Amazon SQS) queue.
Use AWS App Mesh to scale the instances of the ECS cluster based on the SQS queue size.
- [ ] C. Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue.
Deploy the models as AWS Lambda functions that are invoked by SQS events.
Use AWS Auto Scaling to increase the number of vCPUs for the Lambda functions based on the SQS queue size.
- [ ] D. Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue.
Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from the queue.
Enable AWS Auto Scaling on Amazon ECS for both the cluster and copies of the service based on the queue size.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue.
Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from the queue.
Enable AWS Auto Scaling on Amazon ECS for both the cluster and copies of the service based on the queue size.

Why these are the correct answers:

D. Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue.
Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from the queue.
Enable AWS Auto Scaling on Amazon ECS for both the cluster and copies of the service based on the queue size.

- [ ] Amazon SQS helps decouple the API requests from the model processing, accommodating irregular usage patterns.
- [ ] Amazon ECS allows deploying the models as scalable microservices.
- [ ] AWS Auto Scaling on ECS ensures that the number of service instances adjusts dynamically based on the queue load, optimizing resource utilization and cost.

Why are the other answers wrong?

- [ ] A. Lambda functions have limitations on memory and execution time, and loading 1GB of model data might exceed those limits. NLB is for load balancing traffic, not for invoking Lambda functions directly in this manner.
- [ ] B. While ECS with App Mesh can scale, it adds complexity compared to using ECS with Auto Scaling directly. App Mesh is more suitable for complex microservice architectures with many inter-service communications.
- [ ] C. Lambda functions are not designed to have their vCPU count scaled; they scale by increasing the number of concurrent executions. Loading 1GB of model data into Lambda for each invocation can lead to performance and cost inefficiencies.

Therefore, Option D provides the most scalable, cost-effective, and operationally efficient solution for handling the described machine learning model deployment scenario.

</details>

<details>
  <summary>Question 423</summary>

A solutions architect wants to use the following JSON text as an identity-based policy to grant specific permissions:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "1",
      "Effect": "Allow",
      "Action": [
        "ssm:ListDocuments",
        "ssm:GetDocument"
      ],
      "Resource": "*",
      "Condition": {
        "StringEquals": {
          "ec2:Region": "us-east-1"
        }
      }
    },
    {
      "Sid": "2",
      "Effect": "Deny",
      "Action": [
        "ec2:StopInstances",
        "ec2:TerminateInstances"
      ],
      "Resource": "*",
      "Condition": {
        "BoolIfExists": {
          "aws:MultiFactorAuthPresent": "false"
        }
      }
    }
  ]
}
```

Which IAM principals can the solutions architect attach this policy to? (Choose two.)

- [ ] A. Role
- [ ] B. Group
- [ ] C. Organization
- [ ] D. Amazon Elastic Container Service (Amazon ECS) resource
- [ ] E. Amazon EC2 resource

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Role
- [ ] B. Group

Why these are the correct answers:

- [ ] IAM policies are designed to be attached to IAM principals, which include users, groups, and roles.
- [ ] Roles are used to grant permissions to AWS services or other AWS accounts.
- [ ] Groups are used to manage permissions for multiple IAM users.

Why are the other answers wrong?

- [ ] C. Organization: Policies that apply at the organization level are Service Control Policies (SCPs), not IAM policies.
- [ ] D and E. Amazon ECS resources and Amazon EC2 resources are not IAM principals to which identity-based policies are attached. Instead, roles can be assumed by these resources to gain permissions.

Therefore, Options A and B are the correct choices as IAM policies can be directly attached to IAM roles and groups.

</details>

































































