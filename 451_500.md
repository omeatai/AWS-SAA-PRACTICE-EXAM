# AWS-SAA-PRACTICE-EXAM Questions 451-460

<details>
  <summary>Question 451</summary>

A company is migrating its applications and databases to the AWS Cloud.
The company will use Amazon Elastic Container Service (Amazon ECS), AWS Direct Connect, and Amazon RDS.
Which activities will be managed by the company's operational team?
(Choose three.)

-   [ ] A. Management of the Amazon RDS infrastructure layer, operating system, and platforms
-   [ ] B. Creation of an Amazon RDS DB instance and configuring the scheduled maintenance window
-   [ ] C. Configuration of additional software components on Amazon ECS for monitoring, patch management, log management, and host intrusion detection
-   [ ] D. Installation of patches for all minor and major database versions for Amazon RDS
-   [ ] E. Ensure the physical security of the Amazon RDS infrastructure in the data center
-   [ ] F. Encryption of the data that moves in transit through Direct Connect

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Creation of an Amazon RDS DB instance and configuring the scheduled maintenance window
-   [ ] C. Configuration of additional software components on Amazon ECS for monitoring, patch management, log management, and host intrusion detection
-   [ ] F. Encryption of the data that moves in transit through Direct Connect

Why these are the correct answers:

B. Creation of an Amazon RDS DB instance and configuring the scheduled maintenance window

-   [ ] The company is responsible for creating and configuring RDS instances, including setting maintenance windows.

C. Configuration of additional software components on Amazon ECS for monitoring, patch management, log management, and host intrusion detection

-   [ ] The company manages the software and tools within their ECS environment.

F. Encryption of the data that moves in transit through Direct Connect

-   [ ] The company is responsible for securing data in transit, including encryption over Direct Connect.

Why are the other answers wrong?

-   [ ] A and D. AWS manages the underlying infrastructure, OS, and patching for RDS.
-   [ ] E. AWS is responsible for the physical security of its data centers.

Therefore, Options B, C, and F are the activities managed by the company.

</details>
<details>
  <summary>Question 452</summary>

A company runs a Java-based job on an Amazon EC2 instance.
The job runs every hour and takes 10 seconds to run.
The job runs on a scheduled interval and consumes 1 GB of memory.
The CPU utilization of the instance is low except for short surges during which the job uses the maximum CPU available.
The company wants to optimize the costs to run the job.

Which solution will meet these requirements?

-   [ ] A. Use AWS App2Container (A2C) to containerize the job.
    Run the job as an Amazon Elastic Container Service (Amazon ECS) task on AWS Fargate with 0.5 virtual CPU (vCPU) and 1 GB of memory.
-   [ ] B. Copy the code into an AWS Lambda function that has 1 GB of memory.
    Create an Amazon EventBridge scheduled rule to run the code each hour.
-   [ ] C. Use AWS App2Container (A2C) to containerize the job.
    Install the container in the existing Amazon Machine Image (AMI).
    Ensure that the schedule stops the container when the task finishes.
-   [ ] D. Configure the existing schedule to stop the EC2 instance at the completion of the job and restart the EC2 instance when the next job starts.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Copy the code into an AWS Lambda function that has 1 GB of memory.
    Create an Amazon EventBridge scheduled rule to run the code each hour.

Why these are the correct answers:

B. Copy the code into an AWS Lambda function that has 1 GB of memory.
Create an Amazon EventBridge scheduled rule to run the code each hour.

-   [ ] AWS Lambda is cost-effective for short-running, event-driven tasks.
-   [ ] EventBridge allows for scheduling Lambda functions, replacing the need for a continuously running EC2 instance.
-   [ ] Lambda's pay-per-use model aligns well with the job's short execution time and intermittent usage.

Why are the other answers wrong?

-   [ ] A. Running the job on Fargate is more expensive than Lambda for short, infrequent tasks.
-   [ ] C. Containerizing the job and running it on the existing EC2 instance does not optimize costs since the instance remains running.
-   [ ] D. Starting and stopping an EC2 instance adds overhead and is less efficient than using Lambda.

Therefore, Option B is the most cost-effective solution.

</details>
<details>
  <summary>Question 453</summary>

A company wants to implement a backup strategy for Amazon EC2 data and multiple Amazon S3 buckets.
Because of regulatory requirements, the company must retain backup files for a specific time period.
The company must not alter the files for the duration of the retention period.

Which solution will meet these requirements?

-   [ ] A. Use AWS Backup to create a backup vault that has a vault lock in governance mode.
    Create the required backup plan.
-   [ ] B. Use Amazon Data Lifecycle Manager to create the required automated snapshot policy.
-   [ ] C. Use Amazon S3 File Gateway to create the backup.
    Configure the appropriate S3 Lifecycle management.
-   [ ] D. Use AWS Backup to create a backup vault that has a vault lock in compliance mode.
    Create the required backup plan.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Use AWS Backup to create a backup vault that has a vault lock in compliance mode.
    Create the required backup plan.

Why these are the correct answers:

D. Use AWS Backup to create a backup vault that has a vault lock in compliance mode.
Create the required backup plan.

-   [ ] AWS Backup centralizes backup management across AWS services.
-   [ ] Vault Lock in compliance mode prevents anyone, including the root user, from deleting or altering backups during the retention period, meeting the regulatory requirement.

Why are the other answers wrong?

-   [ ] A. Governance mode allows privileged users to delete backups, which does not meet the requirement of immutability.
-   [ ] B. Data Lifecycle Manager automates snapshot management for EBS volumes but does not cover S3 backups or provide immutable storage.
-   [ ] C. S3 File Gateway is for integrating on-premises applications with S3 and does not provide backup and retention management like AWS Backup.

Therefore, Option D is the only solution that ensures immutable backups with AWS Backup and Vault Lock in compliance mode.

</details>
<details>
  <summary>Question 454</summary>

A company has resources across multiple AWS Regions and accounts.
A newly hired solutions architect discovers a previous employee did not provide details about the resources inventory.
The solutions architect needs to build and map the relationship details of the various workloads across all accounts.
Which solution will meet these requirements in the MOST operationally efficient way?

-   [ ] A. Use AWS Systems Manager Inventory to generate a map view from the detailed view report.
-   [ ] B. Use AWS Step Functions to collect workload details.
    Build architecture diagrams of the workloads manually.
-   [ ] C. Use Workload Discovery on AWS to generate architecture diagrams of the workloads.
-   [ ] D. Use AWS X-Ray to view the workload details.
    Build architecture diagrams with relationships.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Use Workload Discovery on AWS to generate architecture diagrams of the workloads.

Why these are the correct answers:

C. Use Workload Discovery on AWS to generate architecture diagrams of the workloads.

-   [ ] Workload Discovery on AWS automatically discovers and maps dependencies between applications and infrastructure, providing a visual representation of workloads.
-   [ ] It reduces the manual effort involved in documenting and understanding complex environments.

Why are the other answers wrong?

-   [ ] A. Systems Manager Inventory collects software and configuration data from EC2 instances but does not provide automated mapping of workload relationships.
-   [ ] B. Using Step Functions to collect details and manually building diagrams is time-consuming and inefficient.
-   [ ] D. AWS X-Ray is for tracing and analyzing distributed applications, not for discovering and mapping infrastructure.

Therefore, Option C is the most operationally efficient solution.

</details>
<details>
  <summary>Question 455</summary>

A company uses AWS Organizations.
The company wants to operate some of its AWS accounts with different budgets.
The company wants to receive alerts and automatically prevent provisioning of additional resources on AWS accounts when the allocated budget threshold is met during a specific period.
Which combination of solutions will meet these requirements? (Choose three.)

-   [ ] A. Use AWS Budgets to create a budget.
    Set the budget amount under the Cost and Usage Reports section of the required AWS accounts.
-   [ ] B. Use AWS Budgets to create a budget.
    Set the budget amount under the Billing dashboards of the required AWS accounts.
-   [ ] C. Create an IAM user for AWS Budgets to run budget actions with the required permissions.
-   [ ] D. Create an IAM role for AWS Budgets to run budget actions with the required permissions.
-   [ ] E. Add an alert to notify the company when each account meets its budget threshold.
    Add a budget action that selects the IAM identity created with the appropriate config rule to prevent provisioning of additional resources.
-   [ ] F. Add an alert to notify the company when each account meets its budget threshold.
    Add a budget action that selects the IAM identity created with the appropriate service control policy (SCP) to prevent provisioning of additional resources.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Use AWS Budgets to create a budget.
    Set the budget amount under the Billing dashboards of the required AWS accounts.
-   [ ] D. Create an IAM role for AWS Budgets to run budget actions with the required permissions.
-   [ ] F. Add an alert to notify the company when each account meets its budget threshold.
    Add a budget action that selects the IAM identity created with the appropriate service control policy (SCP) to prevent provisioning of additional resources.

Why these are the correct answers:

B. Use AWS Budgets to create a budget.
Set the budget amount under the Billing dashboards of the required AWS accounts.

-   [ ] AWS Budgets allows you to set custom budgets to track costs.

D. Create an IAM role for AWS Budgets to run budget actions with the required permissions.

-   [ ] An IAM role provides secure permissions for AWS Budgets to take actions.

F. Add an alert to notify the company when each account meets its budget threshold.
Add a budget action that selects the IAM identity created with the appropriate service control policy (SCP) to prevent provisioning of additional resources.

-   [ ] Budget actions can be configured to prevent resource provisioning, and SCPs can enforce these restrictions across accounts.

Why are the other answers wrong?

-   [ ] A. Budget amounts are not set under Cost and Usage Reports.
-   [ ] C. Using an IAM user for budget actions is less secure than using an IAM role.
-   [ ] E. Config rules do not prevent resource provisioning in the same way that SCPs do.

Therefore, Options B, D, and F are the correct solutions.

</details>
<details>
  <summary>Question 456</summary>

A company runs applications on Amazon EC2 instances in one AWS Region.
The company wants to back up the EC2 instances to a second Region.
The company also wants to provision EC2 resources in the second Region and manage the EC2 instances centrally from one AWS account.
Which solution will meet these requirements MOST cost-effectively?

-   [ ] A. Create a disaster recovery (DR) plan that has a similar number of EC2 instances in the second Region.
    Configure data replication.
-   [ ] B. Create point-in-time Amazon Elastic Block Store (Amazon EBS) snapshots of the EC2 instances.
    Copy the snapshots to the second Region periodically.
-   [ ] C. Create a backup plan by using AWS Backup.
    Configure cross-Region backup to the second Region for the EC2 instances.
-   [ ] D. Deploy a similar number of EC2 instances in the second Region.
    Use AWS DataSync to transfer the data from the source Region to the second Region.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Create a backup plan by using AWS Backup.
    Configure cross-Region backup to the second Region for the EC2 instances.

Why these are the correct answers:

C. Create a backup plan by using AWS Backup.
Configure cross-Region backup to the second Region for the EC2 instances.

-   [ ] AWS Backup centralizes backup management and supports cross-Region backups.
-   [ ] It provides a cost-effective way to manage backups across Regions.

Why are the other answers wrong?

-   [ ] A. Maintaining a similar number of EC2 instances in the DR Region increases costs.
-   [ ] B. Manually copying EBS snapshots is complex and does not provide centralized management.
-   [ ] D. Using DataSync is for data migration, not for backup and recovery.
    It also involves managing EC2 instances in both Regions.

Therefore, Option C is the most cost-effective and efficient solution.

</details>
<details>
  <summary>Question 457</summary>

A company that uses AWS is building an application to transfer data to a product manufacturer.
The company has its own identity provider (IdP).
The company wants the IdP to authenticate application users while the users use the application to transfer data.
The company must use Applicability Statement 2 (AS2) protocol.

Which solution will meet these requirements?

-   [ ] A. Use AWS DataSync to transfer the data.
    Create an AWS Lambda function for IdP authentication.
-   [ ] B. Use Amazon AppFlow flows to transfer the data.
    Create an Amazon Elastic Container Service (Amazon ECS) task for IdP authentication.
-   [ ] C. Use AWS Transfer Family to transfer the data.
    Create an AWS Lambda function for IdP authentication.
-   [ ] D. Use AWS Storage Gateway to transfer the data.
    Create an Amazon Cognito identity pool for IdP authentication.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Use AWS Transfer Family to transfer the data.
    Create an AWS Lambda function for IdP authentication.

Why these are the correct answers:

C. Use AWS Transfer Family to transfer the data.
Create an AWS Lambda function for IdP authentication.

-   [ ] AWS Transfer Family supports the AS2 protocol for secure data transfer.
-   [ ] Lambda functions can be used for custom authentication logic with the company's IdP.

Why are the other answers wrong?

-   [ ] A. AWS DataSync is for large-scale data migration, not for AS2 transfers.
-   [ ] B. Amazon AppFlow is for data transfer between SaaS applications, not for AS2.
-   [ ] D. AWS Storage Gateway is for hybrid cloud storage, not for AS2.
    Amazon Cognito is for user authentication, not IdP integration in this context.

Therefore, Option C is the correct solution.

</details>
<details>
  <summary>Question 458</summary>

A solutions architect is designing a REST API in Amazon API Gateway for a cash payback service.
The application requires 1 GB of memory and 2 GB of storage for its computation resources.
The application will require that the data is in a relational format.
Which additional combination of AWS services will meet these requirements with the LEAST administrative effort? (Choose two.)

-   [ ] A. Amazon EC2
-   [ ] B. AWS Lambda
-   [ ] C. Amazon RDS
-   [ ] D. Amazon DynamoDB
-   [ ] E. Amazon Elastic Kubernetes Services (Amazon EKS)

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. AWS Lambda
-   [ ] C. Amazon RDS

Why these are the correct answers:

B. AWS Lambda

-   [ ] Lambda provides a serverless compute environment that can meet the application's memory and storage requirements with minimal administration.

C. Amazon RDS

-   [ ] Amazon RDS is a managed relational database service that meets the requirement for relational data storage and reduces administrative overhead.

Why are the other answers wrong?

-   [ ] A and E. Amazon EC2 and Amazon EKS require more administrative effort for managing infrastructure.
-   [ ] D. Amazon DynamoDB is a NoSQL database and does not meet the relational data requirement.

Therefore, Options B and C are the most suitable choices.

</details>
<details>
  <summary>Question 459</summary>

A company uses AWS Organizations to run workloads within multiple AWS accounts.
A tagging policy adds department tags to AWS resources when the company creates tags.
An accounting team needs to determine spending on Amazon EC2 consumption.
The accounting team must determine which departments are responsible for the costs regardless of AWS account.
The accounting team has access to AWS Cost Explorer for all AWS accounts within the organization and needs to access all reports from Cost Explorer.
Which solution meets these requirements in the MOST operationally efficient way?

-   [ ] A. From the Organizations management account billing console, activate a user-defined cost allocation tag named department.
    Create one cost report in Cost Explorer grouping by tag name, and filter by EC2.
-   [ ] B. From the Organizations management account billing console, activate an AWS-defined cost allocation tag named department.
    Create one cost report in Cost Explorer grouping by tag name, and filter by EC2.
-   [ ] C. From the Organizations member account billing console, activate a user-defined cost allocation tag named department.
    Create one cost report in Cost Explorer grouping by the tag name, and filter by EC2.
-   [ ] D. From the Organizations member account billing console, activate an AWS-defined cost allocation tag named department.
    Create one cost report in Cost Explorer grouping by tag name, and filter by EC2.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. From the Organizations management account billing console, activate a user-defined cost allocation tag named department.
    Create one cost report in Cost Explorer grouping by tag name, and filter by EC2.

Why these are the correct answers:

A. From the Organizations management account billing console, activate a user-defined cost allocation tag named department.
Create one cost report in Cost Explorer grouping by tag name, and filter by EC2.

-   [ ] Activating the tag in the management account ensures that the cost allocation tag is applied across all accounts in the organization.
-   [ ] Cost Explorer can then be used to generate a report grouped by the department tag, providing a consolidated view of EC2 spending.

Why are the other answers wrong?

-   [ ] B. AWS-defined cost allocation tags cannot be created or activated by users.
-   [ ] C and D. Activating the tag in a member account does not provide a consolidated view across the organization.

Therefore, Option A is the most operationally efficient solution.

</details>
<details>
  <summary>Question 460</summary>

A company wants to securely exchange data between its software as a service (SaaS) application Salesforce account and Amazon S3.
The company must encrypt the data at rest by using AWS Key Management Service (AWS KMS) customer managed keys (CMKs).
The company must also encrypt the data in transit.
The company has enabled API access for the Salesforce account.

-   [ ] A. Create AWS Lambda functions to transfer the data securely from Salesforce to Amazon S3.
-   [ ] B. Create an AWS Step Functions workflow.
    Define the task to transfer the data securely from Salesforce to Amazon S3.
-   [ ] C. Create Amazon AppFlow flows to transfer the data securely from Salesforce to Amazon S3.
-   [ ] D. Create a custom connector for Salesforce to transfer the data securely from Salesforce to Amazon S3.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Create Amazon AppFlow flows to transfer the data securely from Salesforce to Amazon S3.

Why these are the correct answers:

C. Create Amazon AppFlow flows to transfer the data securely from Salesforce to Amazon S3.

-   [ ] Amazon AppFlow is designed for secure data transfer between SaaS applications and AWS services like S3.
-   [ ] AppFlow supports encryption at rest with KMS and encryption in transit.

Why are the other answers wrong?

-   [ ] A. Lambda functions can transfer data but require more custom code and management for security and encryption.
-   [ ] B. Step Functions orchestrate workflows but do not directly transfer data.
    They would need to integrate with other services, adding complexity.
-   [ ] D. Creating a custom connector is more complex and time-consuming than using AppFlow.

Therefore, Option C provides the most straightforward and secure solution.

</details>

# AWS-SAA-PRACTICE-EXAM Questions 461-470

<details>
  <summary>Question 461</summary>

A company is developing a mobile gaming app in a single AWS Region.
The app runs on multiple Amazon EC2 instances in an Auto Scaling group.
The company stores the app data in Amazon DynamoDB.
The app communicates by using TCP traffic and UDP traffic between the users and the servers.
The application will be used globally.
The company wants to ensure the lowest possible latency for all users.
Which solution will meet these requirements?

-   [ ] A. Use AWS Global Accelerator to create an accelerator.
    Create an Application Load Balancer (ALB) behind an accelerator endpoint that uses Global Accelerator integration and listening on the TCP and UDP ports.
    Update the Auto Scaling group to register instances on the ALB.
-   [ ] B. Use AWS Global Accelerator to create an accelerator.
    Create a Network Load Balancer (NLB) behind an accelerator endpoint that uses Global Accelerator integration and listening on the TCP and UDP ports.
    Update the Auto Scaling group to register instances on the NLB.
-   [ ] C. Create an Amazon CloudFront content delivery network (CDN) endpoint.
    Create a Network Load Balancer (NLB) behind the endpoint and listening on the TCP and UDP ports.
    Update the Auto Scaling group to register instances on the NLB.
    Update CloudFront to use the NLB as the origin.
-   [ ] D. Create an Amazon CloudFront content delivery network (CDN) endpoint.
    Create an Application Load Balancer (ALB) behind the endpoint and listening on the TCP and UDP ports.
    Update the Auto Scaling group to register instances on the ALB.
    Update CloudFront to use the ALB as the origin.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Use AWS Global Accelerator to create an accelerator.
    Create a Network Load Balancer (NLB) behind an accelerator endpoint that uses Global Accelerator integration and listening on the TCP and UDP ports.
    Update the Auto Scaling group to register instances on the NLB.

Why these are the correct answers:

B. Use AWS Global Accelerator to create an accelerator.
Create a Network Load Balancer (NLB) behind an accelerator endpoint that uses Global Accelerator integration and listening on the TCP and UDP ports.
Update the Auto Scaling group to register instances on the NLB.

-   [ ] AWS Global Accelerator improves global application performance by routing traffic through AWS's global network, reducing latency.
-   [ ] Network Load Balancers (NLBs) can handle both TCP and UDP traffic, which is necessary for the gaming app.

Why are the other answers wrong?

-   [ ] A. Application Load Balancers (ALBs) primarily handle HTTP/HTTPS traffic, not general TCP/UDP traffic.
-   [ ] C and D. CloudFront is a CDN designed for caching content, which is not the primary need for a low-latency, real-time gaming application.

Therefore, Option B is the most suitable solution for minimizing latency for global users.

</details>
<details>
  <summary>Question 462</summary>

A company has an application that processes customer orders.
The company hosts the application on an Amazon EC2 instance that saves the orders to an Amazon Aurora database.
Occasionally when traffic is high the workload does not process orders fast enough.
What should a solutions architect do to write the orders reliably to the database as quickly as possible?

-   [ ] A. Increase the instance size of the EC2 instance when traffic is high.
    Write orders to Amazon Simple Notification Service (Amazon SNS).
    Subscribe the database endpoint to the SNS topic.
-   [ ] B. Write orders to an Amazon Simple Queue Service (Amazon SQS) queue.
    Use EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SQS queue and process orders into the database.
-   [ ] C. Write orders to Amazon Simple Notification Service (Amazon SNS).
    Subscribe the database endpoint to the SNS topic.
    Use EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SNS topic.
-   [ ] D. Write orders to an Amazon Simple Queue Service (Amazon SQS) queue when the EC2 instance reaches CPU threshold limits.
    Use scheduled scaling of EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SQS queue and process orders into the database.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Write orders to an Amazon Simple Queue Service (Amazon SQS) queue.
    Use EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SQS queue and process orders into the database.

Why these are the correct answers:

B. Write orders to an Amazon Simple Queue Service (Amazon SQS) queue.
Use EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SQS queue and process orders into the database.

-   [ ] Amazon SQS decouples the order processing from the application, allowing orders to be queued during high traffic.
-   [ ] Auto Scaling groups ensure that there are enough EC2 instances to process the orders from the queue, and the Application Load Balancer distributes the workload.

Why are the other answers wrong?

-   [ ] A. Increasing the EC2 instance size might help temporarily but does not address the queuing of orders during traffic spikes.
    SNS is for pub/sub messaging, not for reliable queuing of order data.
-   [ ] C. Similar to A, SNS is not suitable for reliable order queuing.
-   [ ] D. Scheduled scaling does not react to immediate traffic spikes.
    It also adds complexity compared to Auto Scaling based on queue depth.

Therefore, Option B is the most reliable and scalable solution.

</details>
<details>
  <summary>Question 463</summary>

An IoT company is releasing a mattress that has sensors to collect data about a user's sleep.
The sensors will send data to an Amazon S3 bucket.
The sensors collect approximately 2 MB of data every night for each mattress.
The company must process and summarize the data for each mattress.
The results need to be available as soon as possible.
Data processing will require 1 GB of memory and will finish within 30 seconds.
Which solution will meet these requirements MOST cost-effectively?

-   [ ] A. Use AWS Glue with a Scala job
-   [ ] B. Use Amazon EMR with an Apache Spark script
-   [ ] C. Use AWS Lambda with a Python script
-   [ ] D. Use AWS Glue with a PySpark job

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Use AWS Lambda with a Python script

Why these are the correct answers:

C. Use AWS Lambda with a Python script

-   [ ] AWS Lambda is cost-effective for event-driven processing of small data sizes.
-   [ ] It can be triggered by S3 events when new data is uploaded.
-   [ ] Lambda supports up to 10 GB of memory and execution times up to 15 minutes, which meets the requirements.

Why are the other answers wrong?

-   [ ] A, B, and D. AWS Glue and Amazon EMR are more suitable for large-scale data processing and analytics.
    They are more expensive and have more overhead for this small-scale, event-driven processing.

Therefore, Option C is the most cost-effective solution.

</details>
<details>
  <summary>Question 464</summary>

A company hosts an online shopping application that stores all orders in an Amazon RDS for PostgreSQL Single-AZ DB instance.
Management wants to eliminate single points of failure and has asked a solutions architect to recommend an approach to minimize database downtime without requiring any changes to the application code.
Which solution meets these requirements?

-   [ ] A. Convert the existing database instance to a Multi-AZ deployment by modifying the database instance and specifying the Multi-AZ option.
-   [ ] B. Create a new RDS Multi-AZ deployment.
    Take a snapshot of the current RDS instance and restore the new Multi-AZ deployment with the snapshot.
-   [ ] C. Create a read-only replica of the PostgreSQL database in another Availability Zone.
    Use Amazon Route 53 weighted record sets to distribute requests across the databases.
-   [ ] D. Place the RDS for PostgreSQL database in an Amazon EC2 Auto Scaling group with a minimum group size of two.
    Use Amazon Route 53 weighted record sets to distribute requests across instances.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Convert the existing database instance to a Multi-AZ deployment by modifying the database instance and specifying the Multi-AZ option.

Why these are the correct answers:

A. Convert the existing database instance to a Multi-AZ deployment by modifying the database instance and specifying the Multi-AZ option.

-   [ ] Amazon RDS Multi-AZ provides high availability by synchronously replicating data to a standby instance in a different Availability Zone.
-   [ ] Converting the existing instance is the simplest way to achieve this without application code changes.

Why are the other answers wrong?

-   [ ] B. Creating a new Multi-AZ deployment and restoring from a snapshot involves downtime during the restoration process.
-   [ ] C. Read replicas are for scaling read operations, not for high availability.
    Route 53 weighted record sets do not provide automatic failover.
-   [ ] D. RDS databases cannot run in EC2 Auto Scaling groups.

Therefore, Option A is the best solution for minimizing downtime without application changes.

</details>
<details>
  <summary>Question 465</summary>

A company is developing an application to support customer demands.
The company wants to deploy the application on multiple Amazon EC2 Nitro-based instances within the same Availability Zone.
The company also wants to give the application the ability to write to multiple block storage volumes in multiple EC2 Nitro-based instances simultaneously to achieve higher application availability.
Which solution will meet these requirements?

-   [ ] A. Use General Purpose SSD (gp3) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach
-   [ ] B. Use Throughput Optimized HDD (st1) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach
-   [ ] C. Use Provisioned IOPS SSD (io2) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach
-   [ ] D. Use General Purpose SSD (gp2) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Use Provisioned IOPS SSD (io2) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach

Why these are the correct answers:

C. Use Provisioned IOPS SSD (io2) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach

-   [ ] Amazon EBS Multi-Attach allows a Provisioned IOPS SSD (io2) volume to be attached to multiple EC2 instances simultaneously.
-   [ ] io2 volumes provide high performance and are suitable for applications requiring concurrent writes.

Why are the other answers wrong?

-   [ ] A, B, and D. General Purpose SSD (gp3 and gp2) and Throughput Optimized HDD (st1) volumes do not support EBS Multi-Attach for concurrent writes.

Therefore, Option C is the only solution that meets the requirements.

</details>
<details>
  <summary>Question 466</summary>

A company designed a stateless two-tier application that uses Amazon EC2 in a single Availability Zone and an Amazon RDS Multi-AZ DB instance.
New company management wants to ensure the application is highly available.
What should a solutions architect do to meet this requirement?

-   [ ] A. Configure the application to use Multi-AZ EC2 Auto Scaling and create an Application Load Balancer
-   [ ] B. Configure the application to take snapshots of the EC2 instances and send them to a different AWS Region
-   [ ] C. Configure the application to use Amazon Route 53 latency-based routing to feed requests to the application
-   [ ] D. Configure Amazon Route 53 rules to handle incoming requests and create a Multi-AZ Application Load Balancer

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Configure the application to use Multi-AZ EC2 Auto Scaling and create an Application Load Balancer

Why these are the correct answers:

A. Configure the application to use Multi-AZ EC2 Auto Scaling and create an Application Load Balancer

-   [ ] Multi-AZ EC2 Auto Scaling ensures that the application is resilient to AZ failures.
-   [ ] An Application Load Balancer distributes traffic across instances, improving availability.

Why are the other answers wrong?

-   [ ] B. Snapshots are for backup and recovery, not for high availability.
-   [ ] C. Route 53 latency-based routing improves performance but does not provide automatic failover within a single Region.
-   [ ] D. Route 53 rules do not create infrastructure, and ALBs are not inherently Multi-AZ.

Therefore, Option A is the correct solution.

</details>
<details>
  <summary>Question 467</summary>

A company uses AWS Organizations.
A member account has purchased a Compute Savings Plan.
Because of changes in the workloads inside the member account, the account no longer receives the full benefit of the Compute Savings Plan commitment.
The company uses less than 50% of its purchased compute power.
What should the company do?

-   [ ] A. Turn on discount sharing from the Billing Preferences section of the account console in the member account that purchased the Compute Savings Plan.
-   [ ] B. Turn on discount sharing from the Billing Preferences section of the account console in the company's Organizations management account.
-   [ ] C. Migrate additional compute workloads from another AWS account to the account that has the Compute Savings Plan.
-   [ ] D. Sell the excess Savings Plan commitment in the Reserved Instance Marketplace.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Turn on discount sharing from the Billing Preferences section of the account console in the company's Organizations management account.

Why these are the correct answers:

B. Turn on discount sharing from the Billing Preferences section of the account console in the company's Organizations management account.

-   [ ] Discount sharing in AWS Organizations allows other accounts to benefit from the Savings Plan, maximizing its utilization.

Why are the other answers wrong?

-   [ ] A. Discount sharing is enabled in the management account, not the member account.
-   [ ] C. Migrating workloads might not be feasible or cost-effective.
-   [ ] D. Savings Plans cannot be sold in the Reserved Instance Marketplace.

Therefore, Option B is the correct solution.

</details>
<details>
  <summary>Question 468</summary>

A company is developing a microservices application that will provide a search catalog for customers.
The company must use REST APIs to present the frontend of the application to users.
The REST APIs must access the backend services that the company hosts in containers in private VPC subnets.
Which solution will meet these requirements?

-   [ ] A. Design a WebSocket API by using Amazon API Gateway.
    Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet.
    Create a private VPC link for API Gateway to access Amazon ECS.
-   [ ] B. Design a REST API by using Amazon API Gateway.
    Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet.
    Create a private VPC link for API Gateway to access Amazon ECS.
-   [ ] C. Design a WebSocket API by using Amazon API Gateway.
    Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet.
    Create a security group for API Gateway to access Amazon ECS.
-   [ ] D. Design a REST API by using Amazon API Gateway.
    Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet.
    Create a security group for API Gateway to access Amazon ECS.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Design a REST API by using Amazon API Gateway.
    Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet.
    Create a private VPC link for API Gateway to access Amazon ECS.

Why these are the correct answers:

B. Design a REST API by using Amazon API Gateway.
Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet.
Create a private VPC link for API Gateway to access Amazon ECS.

-   [ ] Amazon API Gateway is used to create REST APIs.
-   [ ] Amazon ECS in a private subnet provides a secure environment for the microservices.
-   [ ] A private VPC link allows API Gateway to access ECS within the VPC without exposing traffic to the internet.

Why are the other answers wrong?

-   [ ] A and C. WebSocket APIs are for real-time, bidirectional communication, not for typical REST API use cases.
-   [ ] D. Security groups provide security but do not enable API Gateway to access ECS in a private subnet in the same way as a VPC link.

Therefore, Option B is the correct solution.

</details>
<details>
  <summary>Question 469</summary>

A company stores raw collected data in an Amazon S3 bucket.
The data is used for several types of analytics on behalf of the company's customers.
The type of analytics requested determines the access pattern on the S3 objects.
The company cannot predict or control the access pattern.
The company wants to reduce its S3 costs.
Which solution will meet these requirements?

-   [ ] A. Use S3 replication to transition infrequently accessed objects to S3 Standard-Infrequent Access (S3 Standard-IA)
-   [ ] B. Use S3 Lifecycle rules to transition objects from S3 Standard to Standard-Infrequent Access (S3 Standard-IA)
-   [ ] C. Use S3 Lifecycle rules to transition objects from S3 Standard to S3 Intelligent-Tiering
-   [ ] D. Use S3 Inventory to identify and transition objects that have not been accessed from S3 Standard to S3 Intelligent-Tiering

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Use S3 Lifecycle rules to transition objects from S3 Standard to S3 Intelligent-Tiering

Why these are the correct answers:

C. Use S3 Lifecycle rules to transition objects from S3 Standard to S3 Intelligent-Tiering

-   [ ] S3 Intelligent-Tiering automatically optimizes storage costs by moving data to the most cost-effective tier based on access patterns.
-   [ ] S3 Lifecycle rules automate this process.

Why are the other answers wrong?

-   [ ] A and B. S3 Standard-IA is suitable for infrequently accessed data, but since access patterns are unpredictable, Intelligent-Tiering is more efficient.
-   [ ] D. S3 Inventory helps identify object metadata but does not automate the tiering process like Lifecycle rules with Intelligent-Tiering.

Therefore, Option C is the most suitable solution.

</details>
<details>
  <summary>Question 470</summary>

A company has applications hosted on Amazon EC2 instances with IPv6 addresses.
The applications must initiate communications with other external applications using the internet.
However, the company's security policy states that any external service cannot initiate a connection to the EC2 instances.
What should a solutions architect recommend to resolve this issue?

-   [ ] A. Create a NAT gateway and make it the destination of the subnet's route table
-   [ ] B. Create an internet gateway and make it the destination of the subnet's route table
-   [ ] C. Create a virtual private gateway and make it the destination of the subnet's route table
-   [ ] D. Create an egress-only internet gateway and make it the destination of the subnet's route table

</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Create an egress-only internet gateway and make it the destination of the subnet's route table

Why these are the correct answers:

D. Create an egress-only internet gateway and make it the destination of the subnet's route table

-   [ ] An egress-only internet gateway allows EC2 instances with IPv6 addresses to initiate outbound traffic to the internet but prevents inbound traffic.

Why are the other answers wrong?

-   [ ] A. A NAT gateway is for IPv4, not IPv6.
-   [ ] B. An internet gateway allows both inbound and outbound traffic.
-   [ ] C. A virtual private gateway is for VPN connections, not general internet access.

Therefore, Option D is the correct solution.

</details>

# AWS-SAA-PRACTICE-EXAM Questions 471-480

<details>
  <summary>Question 471</summary>

A company is creating an application that runs on containers in a VPC.
The application stores and accesses data in an Amazon S3 bucket.
During the development phase, the application will store and access 1 TB of data in Amazon S3 each day.
The company wants to minimize costs and wants to prevent traffic from traversing the internet whenever possible.
Which solution will meet these requirements?

-   [ ] A. Enable S3 Intelligent-Tiering for the S3 bucket
-   [ ] B. Enable S3 Transfer Acceleration for the S3 bucket
-   [ ] C. Create a gateway VPC endpoint for Amazon S3.
    Associate this endpoint with all route tables in the VPC
-   [ ] D. Create an interface endpoint for Amazon S3 in the VPC.
    Associate this endpoint with all route tables in the VPC

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Create a gateway VPC endpoint for Amazon S3.
    Associate this endpoint with all route tables in the VPC

Why these are the correct answers:

C. Create a gateway VPC endpoint for Amazon S3.
Associate this endpoint with all route tables in the VPC

-   [ ] Gateway VPC endpoints are cost-effective and allow traffic to stay within the AWS network.
-   [ ] They support high throughput and are suitable for large data transfers.

Why are the other answers wrong?

-   [ ] A. S3 Intelligent-Tiering optimizes storage costs but does not prevent traffic from traversing the internet.
-   [ ] B. S3 Transfer Acceleration speeds up transfers over the internet but does not keep traffic within the AWS network.
-   [ ] D. Interface VPC endpoints use PrivateLink and are more expensive than gateway endpoints for S3.

Therefore, Option C is the most cost-effective solution that keeps traffic within the AWS network.

</details>
<details>
  <summary>Question 472</summary>

A company has a mobile chat application with a data store based in Amazon DynamoDB.
Users would like new messages to be read with as little latency as possible.
A solutions architect needs to design an optimal solution that requires minimal application changes.
Which method should the solutions architect select?

-   [ ] A. Configure Amazon DynamoDB Accelerator (DAX) for the new messages table.
    Update the code to use the DAX endpoint.
-   [ ] B. Add DynamoDB read replicas to handle the increased read load.
    Update the application to point to the read endpoint for the read replicas.
-   [ ] C. Double the number of read capacity units for the new messages table in DynamoDB.
    Continue to use the existing DynamoDB endpoint.
-   [ ] D. Add an Amazon ElastiCache for Redis cache to the application stack.
    Update the application to point to the Redis cache endpoint instead of DynamoDB.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Configure Amazon DynamoDB Accelerator (DAX) for the new messages table.
    Update the code to use the DAX endpoint.

Why these are the correct answers:

A. Configure Amazon DynamoDB Accelerator (DAX) for the new messages table.
Update the code to use the DAX endpoint.

-   [ ] DAX is a fully managed, highly available, in-memory cache for DynamoDB that reduces read latency.
-   [ ] It requires minimal application changes to point to the DAX endpoint.

Why are the other answers wrong?

-   [ ] B. Read replicas are not available for DynamoDB.
-   [ ] C. Increasing read capacity units increases costs but does not provide the same low latency as DAX.
-   [ ] D. ElastiCache requires significant application changes to implement caching logic.

Therefore, Option A is the most suitable solution.

</details>
<details>
  <summary>Question 473</summary>

A company hosts a website on Amazon EC2 instances behind an Application Load Balancer (ALB).
The website serves static content.
Website traffic is increasing, and the company is concerned about a potential increase in cost.
Which solution is the most cost-effective?

-   [ ] A. Create an Amazon CloudFront distribution to cache state files at edge locations
-   [ ] B. Create an Amazon ElastiCache cluster.
    Connect the ALB to the ElastiCache cluster to serve cached files
-   [ ] C. Create an AWS WAF web ACL and associate it with the ALB.
    Add a rule to the web ACL to cache static files
-   [ ] D. Create a second ALB in an alternative AWS Region.
    Route user traffic to the closest Region to minimize data transfer costs

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Create an Amazon CloudFront distribution to cache state files at edge locations

Why these are the correct answers:

A. Create an Amazon CloudFront distribution to cache state files at edge locations

-   [ ] Amazon CloudFront is a content delivery network (CDN) that caches static content at edge locations, reducing the load on EC2 instances and lowering costs.

Why are the other answers wrong?

-   [ ] B. ElastiCache is for caching dynamic data, not static files.
    It is also more complex and expensive than CloudFront for this purpose.
-   [ ] C. AWS WAF is a web application firewall and does not cache static files.
-   [ ] D. Creating a second ALB in another Region increases costs and is not necessary for serving static content.

Therefore, Option A is the most cost-effective solution.

</details>
<details>
  <summary>Question 474</summary>

A company has multiple VPCs across AWS Regions to support and run workloads that are isolated from workloads in other Regions.
Because of a recent application launch requirement, the company's VPCs must communicate with all other VPCs across all Regions.
Which solution will meet these requirements with the LEAST amount of administrative effort?

-   [ ] A. Use VPC peering to manage VPC communication in a single Region.
    Use VPC peering across Regions to manage VPC communications.
-   [ ] B. Use AWS Direct Connect gateways across all Regions to connect VPCs across regions and manage VPC communications.
-   [ ] C. Use AWS Transit Gateway to manage VPC communication in a single Region and Transit Gateway peering across Regions to manage VPC communications.
-   [ ] D. Use AWS PrivateLink across all Regions to connect VPCs across Regions and manage VPC communications

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Use AWS Transit Gateway to manage VPC communication in a single Region and Transit Gateway peering across Regions to manage VPC communications.

Why these are the correct answers:

C. Use AWS Transit Gateway to manage VPC communication in a single Region and Transit Gateway peering across Regions to manage VPC communications.

-   [ ] AWS Transit Gateway simplifies the management of connections between multiple VPCs and across Regions.
-   [ ] It reduces the complexity of managing numerous peering connections.

Why are the other answers wrong?

-   [ ] A. VPC peering becomes complex and difficult to manage as the number of VPCs increases.
-   [ ] B. Direct Connect gateways are for connecting to on-premises networks, not for VPC-to-VPC communication.
-   [ ] D. AWS PrivateLink is for accessing AWS services privately, not for connecting VPCs.

Therefore, Option C is the most efficient solution.

</details>
<details>
  <summary>Question 475</summary>

A company is designing a containerized application that will use Amazon Elastic Container Service (Amazon ECS).
The application needs to access a shared file system that is highly durable and can recover data to another AWS Region with a recovery point objective (RPO) of 8 hours.
The file system needs to provide a mount target in each Availability Zone within a Region.
A solutions architect wants to use AWS Backup to manage the replication to another Region.
Which solution will meet these requirements?

-   [ ] A. Amazon FSx for Windows File Server with a Multi-AZ deployment
-   [ ] B. Amazon FSx for NetApp ONTAP with a Multi-AZ deployment
-   [ ] C. Amazon Elastic File System (Amazon EFS) with the Standard storage class
-   [ ] D. Amazon FSx for OpenZFS

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Amazon Elastic File System (Amazon EFS) with the Standard storage class

Why these are the correct answers:

C. Amazon Elastic File System (Amazon EFS) with the Standard storage class

-   [ ] Amazon EFS provides a scalable, durable file system that can be mounted by multiple ECS tasks across Availability Zones.
-   [ ] AWS Backup can be used to back up EFS file systems for disaster recovery.

Why are the other answers wrong?

-   [ ] A, B, and D. Amazon FSx for Windows File Server, FSx for NetApp ONTAP, and FSx for OpenZFS are not designed to be mounted by multiple ECS tasks across Availability Zones in the same way as EFS.

Therefore, Option C is the correct solution.

</details>
<details>
  <summary>Question 476</summary>

A company is expecting rapid growth in the near future.
A solutions architect needs to configure existing users and grant permissions to new users on AWS.
The solutions architect has decided to create IAM groups.
Which additional action is the MOST secure way to grant permissions to the new users?

-   [ ] A. Apply service control policies (SCPs) to manage access permissions
-   [ ] B. Create IAM roles that have least privilege permission.
    Attach the roles to the IAM groups
-   [ ] C. Create an IAM policy that grants least privilege permission.
    Attach the policy to the IAM groups
-   [ ] D. Create IAM roles.
    Associate the roles with a permissions boundary that defines the maximum permissions

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Create an IAM policy that grants least privilege permission.
    Attach the policy to the IAM groups

Why these are the correct answers:

C. Create an IAM policy that grants least privilege permission.
Attach the policy to the IAM groups

-   [ ] IAM policies are the standard way to grant permissions to IAM users, groups, and roles.
-   [ ] Attaching a policy to a group allows you to manage permissions for multiple users efficiently.
-   [ ] Least privilege ensures that users have only the necessary permissions.

Why are the other answers wrong?

-   [ ] A. SCPs are used to manage permissions at the AWS Organizations level, not for granting permissions to individual users or groups.
-   [ ] B. Roles are typically assumed by AWS services or applications, not attached to IAM groups.
-   [ ] D. Permission boundaries set the maximum permissions that a role can have but do not grant permissions directly to users.

Therefore, Option C is the most secure and appropriate way to grant permissions.

</details>

<details>
  <summary>Question 477</summary>

A group requires permissions to list an Amazon S3 bucket and delete objects from that bucket.
An administrator has created the following IAM policy to provide access to the bucket and applied that policy to the group.
The group is not able to delete objects in the bucket.
The company follows least-privilege access rules.

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": [
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::bucket-name"
      ],
      "Effect": "Allow"
    },
    {
      "Action": [
        "s3:DeleteObject"
      ],
      "Resource": [],
      "Effect": "Allow"
    }
  ]
}
```

Which statement should a solutions architect add to the policy to correct bucket access?

-   [ ] A.

```json
{
  "Action": [
    "s3:\*Object"
  ],
  "Resource": [
    "arn:aws:s3:::bucket-name/\*"
  ],
  "Effect": "Allow"
}
```

-   [ ] B.

```json
{
  "Action": [
    "s3:\*"
  ],
  "Resource": [
    "arn:aws:s3:::bucket-name/\*"
  ],
  "Effect": "Allow"
}
```

-   [ ] C.

```json
{
  "Action": [
    "s3:DeleteObject"
  ],
  "Resource": [
    "arn:aws:s3:::bucket-name\*"
  ],
  "Effect": "Allow"
}
```

-   [ ] D.

```json
{
  "Action": [
    "s3:DeleteObject"
  ],
  "Resource": [
    "arn:aws:s3:::bucket-name/\*"
  ],
  "Effect": "Allow"
}
```

</details>

<details>
  <summary>Answer</summary>

- [ ] D.

```json
{
  "Action": [
    "s3:DeleteObject"
  ],
  "Resource": [
    "arn:aws:s3:::bucket-name/\*"
  ],
  "Effect": "Allow"
}
```

Why D is the correct answer:

- [ ] The original policy was missing the Resource for the s3:DeleteObject action.
- [ ] The correct Resource should specify the objects within the bucket using arn:aws:s3:::bucket-name/*.

Why are the other answers wrong?

- [ ] A and B. Using s3:*Object or s3:* grants more permissions than necessary, violating the least-privilege principle.
- [ ] C. arn:aws:s3:::bucket-name* is an invalid ARN format.

Therefore, Option D is the correct and most secure solution.

</details>

<details>
  <summary>Question 478</summary>

A law firm needs to share information with the public.
The information includes hundreds of files that must be publicly readable.
Modifications or deletions of the files by anyone before a designated future date are prohibited.
Which solution will meet these requirements in the MOST secure way?

-   [ ] A. Upload all files to an Amazon S3 bucket that is configured for static website hosting.
    Grant read-only IAM permissions to any AWS principals that access the S3 bucket until the designated date.
-   [ ] B. Create a new Amazon S3 bucket with S3 Versioning enabled.
    Use S3 Object Lock with a retention period in accordance with the designated date.
    Configure the S3 bucket for static website hosting.
    Set an S3 bucket policy to allow read-only access to the objects.
-   [ ] C. Create a new Amazon S3 bucket with S3 Versioning enabled.
    Configure an event trigger to run an AWS Lambda function in case of object modification or deletion.
    Configure the Lambda function to replace the objects with the original versions from a private S3 bucket.
-   [ ] D. Upload all files to an Amazon S3 bucket that is configured for static website hosting.
    Select the folder that contains the files.
    Use S3 Object Lock with a retention period in accordance with the designated date.
    Grant read-only IAM permissions to any AWS principals that access the S3 bucket.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Create a new Amazon S3 bucket with S3 Versioning enabled.
    Use S3 Object Lock with a retention period in accordance with the designated date.
    Configure the S3 bucket for static website hosting.
    Set an S3 bucket policy to allow read-only access to the objects.

Why these are the correct answers:

B. Create a new Amazon S3 bucket with S3 Versioning enabled.
Use S3 Object Lock with a retention period in accordance with the designated date.
Configure the S3 bucket for static website hosting.
Set an S3 bucket policy to allow read-only access to the objects.

-   [ ] S3 Object Lock prevents objects from being deleted or modified for a specified retention period.
-   [ ] S3 Versioning protects against accidental deletions.
-   [ ] Static website hosting allows public access to the files.
-   [ ] An S3 bucket policy grants read-only access.

Why are the other answers wrong?

-   [ ] A. IAM permissions do not prevent authorized users from modifying or deleting objects.
-   [ ] C. Using Lambda functions to restore objects is complex and less secure than S3 Object Lock.
-   [ ] D. S3 Object Lock cannot be applied to a folder in S3.

Therefore, Option B is the most secure and efficient solution.

</details>
<details>
  <summary>Question 479</summary>

A company is making a prototype of the infrastructure for its new website by manually provisioning the necessary infrastructure.
This infrastructure includes an Auto Scaling group, an Application Load Balancer and an Amazon RDS database.
After the configuration has been thoroughly validated, the company wants the capability to immediately deploy the infrastructure for development and production use in two Availability Zones in an automated fashion.
What should a solutions architect recommend to meet these requirements?

-   [ ] A. Use AWS Systems Manager to replicate and provision the prototype infrastructure in two Availability Zones
-   [ ] B. Define the infrastructure as a template by using the prototype infrastructure as a guide.
    Deploy the infrastructure with AWS CloudFormation.
-   [ ] C. Use AWS Config to record the inventory of resources that are used in the prototype infrastructure.
    Use AWS Config to deploy the prototype infrastructure into two Availability Zones.
-   [ ] D. Use AWS Elastic Beanstalk and configure it to use an automated reference to the prototype infrastructure to automatically deploy new environments in two Availability Zones.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Define the infrastructure as a template by using the prototype infrastructure as a guide.
    Deploy the infrastructure with AWS CloudFormation.

Why these are the correct answers:

B. Define the infrastructure as a template by using the prototype infrastructure as a guide.
Deploy the infrastructure with AWS CloudFormation.

-   [ ] AWS CloudFormation allows you to define infrastructure as code, enabling automated and repeatable deployments.
-   [ ] It ensures consistency and reduces manual errors.

Why are the other answers wrong?

-   [ ] A. AWS Systems Manager is for managing EC2 instances, not for provisioning infrastructure.
-   [ ] C. AWS Config records configuration changes but does not deploy infrastructure.
-   [ ] D. Elastic Beanstalk is for deploying applications, not for defining and deploying infrastructure components like Auto Scaling groups and load balancers.

Therefore, Option B is the most appropriate solution.

</details>
<details>
  <summary>Question 480</summary>

A business application is hosted on Amazon EC2 and uses Amazon S3 for encrypted object storage.
The chief information security officer has directed that no application traffic between the two services should traverse the public internet.
Which capability should the solutions architect use to meet the compliance requirements?

-   [ ] A. AWS Key Management Service (AWS KMS)
-   [ ] B. VPC endpoint
-   [ ] C. Private subnet
-   [ ] D. Virtual private gateway

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. VPC endpoint

Why these are the correct answers:

B. VPC endpoint

-   [ ] VPC endpoints enable private connections to AWS services like S3 without using the internet.
-   [ ] They keep traffic within the AWS network.

Why are the other answers wrong?

-   [ ] A. AWS KMS is for managing encryption keys, not for network traffic routing.
-   [ ] C. Private subnets provide network isolation within a VPC but do not prevent traffic from leaving the VPC to access S3.
-   [ ] D. A virtual private gateway is for connecting to on-premises networks, not for S3 access.

Therefore, Option B is the correct solution.

</details>

# AWS-SAA-PRACTICE-EXAM Questions 481-490

<details>
  <summary>Question 481</summary>

A company hosts a three-tier web application in the AWS Cloud.
A Multi-AZ Amazon RDS for MySQL server forms the database layer Amazon ElastiCache forms the cache layer.
The company wants a caching strategy that adds or updates data in the cache when a customer adds an item to the database.
The data in the cache must always match the data in the database.

Which solution will meet these requirements?

-   [ ] A. Implement the lazy loading caching strategy
-   [ ] B. Implement the write-through caching strategy
-   [ ] C. Implement the adding TTL caching strategy
-   [ ] D. Implement the AWS AppConfig caching strategy

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Implement the write-through caching strategy

Why these are the correct answers:

B. Implement the write-through caching strategy

-   [ ] Write-through caching ensures that data is written to both the cache and the database simultaneously, maintaining data consistency.

Why are the other answers wrong?

-   [ ] A. Lazy loading only loads data into the cache when it is first requested, which does not guarantee that the cache is updated when a customer adds an item.
-   [ ] C. Adding TTL (Time To Live) involves setting an expiration time for cached data, which does not ensure immediate cache updates when the database is updated.
-   [ ] D. AWS AppConfig is for application configuration management, not for caching strategies.

Therefore, Option B is the correct solution.

</details>
<details>
  <summary>Question 482</summary>

A company wants to migrate 100 GB of historical data from an on-premises location to an Amazon S3 bucket.
The company has a 100 megabits per second (Mbps) internet connection on premises.
The company needs to encrypt the data in transit to the S3 bucket.
The company will store new data directly in Amazon S3.
Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Use the s3 sync command in the AWS CLI to move the data directly to an S3 bucket
-   [ ] B. Use AWS DataSync to migrate the data from the on-premises location to an S3 bucket
-   [ ] C. Use AWS Snowball to move the data to an S3 bucket
-   [ ] D. Set up an IPsec VPN from the on-premises location to AWS.
    Use the s3 cp command in the AWS CLI to move the data directly to an S3 bucket

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Use AWS DataSync to migrate the data from the on-premises location to an S3 bucket

Why these are the correct answers:

B. Use AWS DataSync to migrate the data from the on-premises location to an S3 bucket

-   [ ] AWS DataSync automates and accelerates data transfer between on-premises storage and Amazon S3.
-   [ ] It handles encryption in transit and minimizes operational overhead.

Why are the other answers wrong?

-   [ ] A and D. Using the AWS CLI or setting up an IPsec VPN requires more manual configuration and management.
-   [ ] C. AWS Snowball is for large-scale data transfers and is not suitable for only 100 GB of data.

Therefore, Option B is the most efficient solution.

</details>
<details>
  <summary>Question 483</summary>

A company containerized a Windows job that runs on .NET 6 Framework under a Windows container.
The company wants to run this job in the AWS Cloud.
The job runs every 10 minutes.
The job's runtime varies between 1 minute and 3 minutes.
Which solution will meet these requirements MOST cost-effectively?

-   [ ] A. Create an AWS Lambda function based on the container image of the job.
    Configure Amazon EventBridge to invoke the function every 10 minutes.
-   [ ] B. Use AWS Batch to create a job that uses AWS Fargate resources.
    Configure the job scheduling to run every 10 minutes.
-   [ ] C. Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate to run the job.
    Create a scheduled task based on the container image of the job to run every 10 minutes.
-   [ ] D. Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate to run the job.
    Create a standalone task based on the container image of the job.
    Use Windows task scheduler to run the job every 10 minutes.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate to run the job.
    Create a scheduled task based on the container image of the job to run every 10 minutes.

Why these are the correct answers:

C. Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate to run the job.
Create a scheduled task based on the container image of the job to run every 10 minutes.

-   [ ] Amazon ECS on Fargate is cost-effective for running containerized applications without managing the underlying infrastructure.
-   [ ] Scheduled tasks in ECS allow for automated execution of the job at regular intervals.

Why are the other answers wrong?

-   [ ] A. Lambda has limitations on runtime for container images, and it may not be suitable for jobs that can run up to 3 minutes.
-   [ ] B. AWS Batch is designed for batch processing and is more suitable for large-scale, long-running jobs, not for short, frequent tasks.
-   [ ] D. Using Windows task scheduler adds complexity and requires managing a separate scheduling mechanism.

Therefore, Option C is the most cost-effective and efficient solution.

</details>
<details>
  <summary>Question 484</summary>

A company wants to move from many standalone AWS accounts to a consolidated, multi-account architecture.
The company plans to create many new AWS accounts for different business units.
The company needs to authenticate access to these AWS accounts by using a centralized corporate directory service.
Which combination of actions should a solutions architect recommend to meet these requirements? (Choose two.)

-   [ ] A. Create a new organization in AWS Organizations with all features turned on.
    Create the new AWS accounts in the organization.
-   [ ] B. Set up an Amazon Cognito identity pool.
    Configure AWS IAM Identity Center (AWS Single Sign-On) to accept Amazon Cognito authentication.
-   [ ] C. Configure a service control policy (SCP) to manage the AWS accounts.
    Add AWS IAM Identity Center (AWS Single Sign-On) to AWS Directory Service.
-   [ ] D. Create a new organization in AWS Organizations.
    Configure the organization's authentication mechanism to use AWS Directory Service directly.
-   [ ] E. Set up AWS IAM Identity Center (AWS Single Sign-On) in the organization.
    Configure IAM Identity Center, and integrate it with the company's corporate directory service.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Create a new organization in AWS Organizations with all features turned on.
    Create the new AWS accounts in the organization.
-   [ ] E. Set up AWS IAM Identity Center (AWS Single Sign-On) in the organization.
    Configure IAM Identity Center, and integrate it with the company's corporate directory service.

Why these are the correct answers:

A. Create a new organization in AWS Organizations with all features turned on.
Create the new AWS accounts in the organization.

-   [ ] AWS Organizations provides centralized management of multiple AWS accounts.

E. Set up AWS IAM Identity Center (AWS Single Sign-On) in the organization.
Configure IAM Identity Center, and integrate it with the company's corporate directory service.

-   [ ] AWS IAM Identity Center (successor to AWS SSO) enables centralized authentication using a corporate directory.

Why are the other answers wrong?

-   [ ] B. Amazon Cognito is for customer identity and access management, not for internal corporate directory integration.
-   [ ] C. SCPs manage permissions, not authentication.
-   [ ] D. AWS Organizations does not directly integrate with AWS Directory Service for authentication.
    IAM Identity Center is needed.

Therefore, Options A and E are the correct solutions.

</details>
<details>
  <summary>Question 485</summary>

A company is looking for a solution that can store video archives in AWS from old news footage.
The company needs to minimize costs and will rarely need to restore these files.
When the files are needed, they must be available in a maximum of five minutes.
What is the MOST cost-effective solution?

-   [ ] A. Store the video archives in Amazon S3 Glacier and use Expedited retrievals.
-   [ ] B. Store the video archives in Amazon S3 Glacier and use Standard retrievals.
-   [ ] C. Store the video archives in Amazon S3 Standard-Infrequent Access (S3 Standard-IA).
-   [ ] D. Store the video archives in Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA).

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Store the video archives in Amazon S3 Glacier and use Expedited retrievals.

Why these are the correct answers:

A. Store the video archives in Amazon S3 Glacier and use Expedited retrievals.

-   [ ] Amazon S3 Glacier is the most cost-effective storage for archiving data.
-   [ ] Expedited retrievals allow for access to data within minutes.

Why are the other answers wrong?

-   [ ] B. Standard retrievals from Glacier take several hours, which does not meet the 5-minute requirement.
-   [ ] C and D. S3 Standard-IA and S3 One Zone-IA are more expensive than Glacier and are designed for more frequent access.

Therefore, Option A is the most cost-effective solution that meets the retrieval time requirement.

</details>
<details>
  <summary>Question 486</summary>

A company is building a three-tier application on AWS.
The presentation tier will serve a static website The logic tier is a containerized application.
This application will store data in a relational database.
The company wants to simplify deployment and to reduce operational costs.
Which solution will meet these requirements?

-   [ ] A. Use Amazon S3 to host static content.
    Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute power.
    Use a managed Amazon RDS cluster for the database.
-   [ ] B. Use Amazon CloudFront to host static content.
    Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 for compute power.
    Use a managed Amazon RDS cluster for the database.
-   [ ] C. Use Amazon S3 to host static content.
    Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for compute power.
    Use a managed Amazon RDS cluster for the database.
-   [ ] D. Use Amazon EC2 Reserved Instances to host static content.
    Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 for compute power.
    Use a managed Amazon RDS cluster for the database.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Use Amazon S3 to host static content.
    Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute power.
    Use a managed Amazon RDS cluster for the database.

Why these are the correct answers:

A. Use Amazon S3 to host static content.
Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute power.
Use a managed Amazon RDS cluster for the database.

-   [ ] Amazon S3 is cost-effective for hosting static content.
-   [ ] Amazon ECS with Fargate simplifies container deployment and management.
-   [ ] Managed RDS clusters reduce operational overhead for databases.

Why are the other answers wrong?

-   [ ] B. CloudFront is a CDN and is not necessary for hosting all static content.
    EC2 requires more management than Fargate.
-   [ ] C. EKS is more complex than ECS and is not needed for a simple containerized application.
-   [ ] D. EC2 Reserved Instances are for long-running workloads, not for hosting static content.
    EKS is more complex than ECS.

Therefore, Option A is the most straightforward and cost-effective solution.

</details>
<details>
  <summary>Question 487</summary>

A company seeks a storage solution for its application. The solution must be highly available and scalable.
The solution also must function as a file system be mountable by multiple Linux instances in AWS and on premises through native protocols, and have no minimum size requirements.
The company has set up a Site-to-Site VPN for access from its on-premises network to its VPC.
Which storage solution meets these requirements?

-   [ ] A. Amazon FSx Multi-AZ deployments
-   [ ] B. Amazon Elastic Block Store (Amazon EBS) Multi-Attach volumes
-   [ ] C. Amazon Elastic File System (Amazon EFS) with multiple mount targets
-   [ ] D. Amazon Elastic File System (Amazon EFS) with a single mount target and multiple access points

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Amazon Elastic File System (Amazon EFS) with multiple mount targets

Why these are the correct answers:

C. Amazon Elastic File System (Amazon EFS) with multiple mount targets

-   [ ] Amazon EFS provides a scalable, highly available file system that can be mounted by multiple Linux instances.
-   [ ] It supports access from both AWS and on-premises via Site-to-Site VPN.

Why are the other answers wrong?

-   [ ] A. Amazon FSx is not designed to be mounted by multiple Linux instances both in AWS and on-premises using native protocols.
-   [ ] B. EBS Multi-Attach allows attaching a volume to multiple EC2 instances but is not designed for on-premises access or as a file system.
-   [ ] D. EFS access points manage access but do not change the mount target behavior.

Therefore, Option C is the correct solution.

</details>
<details>
  <summary>Question 488</summary>

A 4-year-old media company is using the AWS Organizations all features feature set to organize its AWS accounts.
According to the company's finance team, the billing information on the member accounts must not be accessible to anyone, including the root user of the member accounts.
Which solution will meet these requirements?

-   [ ] A. Add all finance team users to an IAM group.
    Attach an AWS managed policy named Billing to the group.
-   [ ] B. Attach an identity-based policy to deny access to the billing information to all users, including the root user.
-   [ ] C. Create a service control policy (SCP) to deny access to the billing information.
    Attach the SCP to the root organizational unit (OU).
-   [ ] D. Convert from the Organizations all features feature set to the Organizations consolidated billing feature set.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Create a service control policy (SCP) to deny access to the billing information.
    Attach the SCP to the root organizational unit (OU).

Why these are the correct answers:

C. Create a service control policy (SCP) to deny access to the billing information.
Attach the SCP to the root organizational unit (OU).

-   [ ] Service Control Policies (SCPs) allow you to centrally manage permissions for all accounts in your AWS Organization.
-   [ ] SCPs can deny access to billing information, even to the root user.

Why are the other answers wrong?

-   [ ] A. IAM policies grant permissions but cannot override the root user's access to billing information.
-   [ ] B. Identity-based policies cannot deny access to the root user.
-   [ ] D. Consolidated billing simplifies billing but does not prevent access to billing information.

Therefore, Option C is the correct solution.

</details>
<details>
  <summary>Question 489</summary>

An ecommerce company runs an application in the AWS Cloud that is integrated with an on-premises warehouse solution.
The company uses Amazon Simple Notification Service (Amazon SNS) to send order messages to an on-premises HTTPS endpoint so the warehouse application can process the orders.
The local data center team has detected that some of the order messages were not received.
A solutions architect needs to retain messages that are not delivered and analyze the messages for up to 14 days.
Which solution will meet these requirements with the LEAST development effort?

-   [ ] A. Configure an Amazon SNS dead letter queue that has an Amazon Kinesis Data Stream target with a retention period of 14 days.
-   [ ] B. Add an Amazon Simple Queue Service (Amazon SQS) queue with a retention period of 14 days between the application and Amazon SNS.
-   [ ] C. Configure an Amazon SNS dead letter queue that has an Amazon Simple Queue Service (Amazon SQS) target with a retention period of 14 days.
-   [ ] D. Configure an Amazon SNS dead letter queue that has an Amazon DynamoDB target with a TTL attribute set for a retention period of 14 days.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Configure an Amazon SNS dead letter queue that has an Amazon Simple Queue Service (Amazon SQS) target with a retention period of 14 days.

Why these are the correct answers:

C. Configure an Amazon SNS dead letter queue that has an Amazon Simple Queue Service (Amazon SQS) target with a retention period of 14 days.

-   [ ] SNS dead letter queues (DLQs) allow you to retain undeliverable messages.
-   [ ] SQS provides a queue with a configurable retention period, making it suitable for storing and analyzing failed messages.

Why are the other answers wrong?

-   [ ] A. Kinesis Data Streams are for real-time data streaming, not for storing undelivered messages.
-   [ ] B. Adding an SQS queue between the application and SNS requires more development effort to modify the application.
-   [ ] D. DynamoDB is a NoSQL database and is not the most efficient way to store and analyze messages compared to SQS.

Therefore, Option C is the most efficient solution with the least development effort.

</details>
<details>
  <summary>Question 490</summary>

A gaming company uses Amazon DynamoDB to store user information such as geographic location, player data, and leaderboards.
The company needs to configure continuous backups to an Amazon S3 bucket with a minimal amount of coding.
The backups must not affect availability of the application and must not affect the read capacity units (RCUs) that are defined for the table.
Which solution meets these requirements?

-   [ ] A. Use an Amazon EMR cluster.
    Create an Apache Hive job to back up the data to Amazon S3.
-   [ ] B. Export the data directly from DynamoDB to Amazon S3 with continuous backups.
    Turn on point-in-time recovery for the table.
-   [ ] C. Configure Amazon DynamoDB Streams.
    Create an AWS Lambda function to consume the stream and export the data to an Amazon S3 bucket.
-   [ ] D. Create an AWS Lambda function to export the data from the database tables to Amazon S3 on a regular basis.
    Turn on point-in-time recovery for the table.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Export the data directly from DynamoDB to Amazon S3 with continuous backups.
    Turn on point-in-time recovery for the table.

Why these are the correct answers:

B. Export the data directly from DynamoDB to Amazon S3 with continuous backups.
Turn on point-in-time recovery for the table.

-   [ ] DynamoDB's point-in-time recovery (PITR) provides continuous backups without affecting application availability or RCUs.
-   [ ] It allows you to restore the table to any point in time within the retention period.

Why are the other answers wrong?

-   [ ] A. EMR and Apache Hive are complex and not cost-effective for simple backups.
-   [ ] C. DynamoDB Streams capture changes to the table but do not provide a full backup solution.
    Lambda functions would require more coding.
-   [ ] D. Lambda functions for regular exports require coding and can affect RCUs.

Therefore, Option B is the most efficient and least disruptive solution.

</details>

# AWS-SAA-PRACTICE-EXAM Questions 491-500

<details>
  <summary>Question 491</summary>

A solutions architect is designing an asynchronous application to process credit card data validation requests for a bank.
The application must be secure and be able to process each request at least once.
Which solution will meet these requirements MOST cost-effectively?

-   [ ] A. Use AWS Lambda event source mapping.
    Set Amazon Simple Queue Service (Amazon SQS) standard queues as the event source.
    Use AWS Key Management Service (SSE-KMS) for encryption.
    Add the kms:Decrypt permission for the Lambda execution role.
-   [ ] B. Use AWS Lambda event source mapping.
    Use Amazon Simple Queue Service (Amazon SQS) FIFO queues as the event source.
    Use SQS managed encryption keys (SSE-SQS) for encryption.
    Add the encryption key invocation permission for the Lambda function.
-   [ ] C. Use the AWS Lambda event source mapping.
    Set Amazon Simple Queue Service (Amazon SQS) FIFO queues as the event source.
    Use AWS KMS keys (SSE-KMS).
    Add the kms:Decrypt permission for the Lambda execution role.
-   [ ] D. Use the AWS Lambda event source mapping.
    Set Amazon Simple Queue Service (Amazon SQS) standard queues as the event source.
    Use AWS KMS keys (SSE-KMS) for encryption.
    Add the encryption key invocation permission for the Lambda function.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Use AWS Lambda event source mapping.
    Set Amazon Simple Queue Service (Amazon SQS) standard queues as the event source.
    Use AWS Key Management Service (SSE-KMS) for encryption.
    Add the kms:Decrypt permission for the Lambda execution role.

Why these are the correct answers:

A. Use AWS Lambda event source mapping.
Set Amazon Simple Queue Service (Amazon SQS) standard queues as the event source.
Use AWS Key Management Service (SSE-KMS) for encryption.
Add the kms:Decrypt permission for the Lambda execution role.

-   [ ] SQS standard queues provide at-least-once delivery, ensuring that each request is processed.
-   [ ] Lambda event source mapping simplifies processing items from SQS.
-   [ ] SSE-KMS encryption secures the data, and granting `kms:Decrypt` permission allows Lambda to decrypt the data.

Why are the other answers wrong?

-   [ ] B and C. FIFO queues are for ordered delivery, which is not a requirement here, and they are generally more expensive than standard queues.
-   [ ] D. The `kms:Decrypt` permission is correct for Lambda to decrypt data encrypted with SSE-KMS.
    The encryption key invocation permission is not the correct permission in this scenario.

Therefore, Option A is the most cost-effective and correct solution.

</details>
<details>
  <summary>Question 492</summary>

A company has multiple AWS accounts for development work.
Some staff consistently use oversized Amazon EC2 instances, which causes the company to exceed the yearly budget for the development accounts.
The company wants to centrally restrict the creation of AWS resources in these accounts.
Which solution will meet these requirements with the LEAST development effort?

-   [ ] A. Develop AWS Systems Manager templates that use an approved EC2 creation process.
    Use the approved Systems Manager templates to provision EC2 instances.
-   [ ] B. Use AWS Organizations to organize the accounts into organizational units (OUs).
    Define and attach a service control policy (SCP) to control the usage of EC2 instance types.
-   [ ] C. Configure an Amazon EventBridge rule that invokes an AWS Lambda function when an EC2 instance is created.
    Stop disallowed EC2 instance types.
-   [ ] D. Set up AWS Service Catalog products for the staff to create the allowed EC2 instance types.
    Ensure that staff can deploy EC2 instances only by using the Service Catalog products.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Use AWS Organizations to organize the accounts into organizational units (OUs).
    Define and attach a service control policy (SCP) to control the usage of EC2 instance types.

Why these are the correct answers:

B. Use AWS Organizations to organize the accounts into organizational units (OUs).
Define and attach a service control policy (SCP) to control the usage of EC2 instance types.

-   [ ] AWS Organizations allows you to centrally manage AWS accounts.
-   [ ] Service Control Policies (SCPs) enable you to control which AWS services and actions are available to users and roles in those accounts.
-   [ ] This approach requires minimal development effort.

Why are the other answers wrong?

-   [ ] A. Systems Manager templates do not prevent users from creating EC2 instances outside the templates.
-   [ ] C. EventBridge and Lambda can stop instances but do not prevent their creation, and this approach requires more development.
-   [ ] D. Service Catalog requires significant setup and does not strictly prevent users from using other methods to create EC2 instances.

Therefore, Option B is the most efficient solution.

</details>
<details>
  <summary>Question 493</summary>

A company wants to use artificial intelligence (AI) to determine the quality of its customer service calls.
The company currently manages calls in four different languages, including English.
The company will offer new languages in the future.
The company does not have the resources to regularly maintain machine learning (ML) models.
The company needs to create written sentiment analysis reports from the customer service call recordings.
The customer service call recording text must be translated into English.

Which combination of steps will meet these requirements? (Choose three.)

-   [ ] A. Use Amazon Comprehend to translate the audio recordings into English.
-   [ ] B. Use Amazon Lex to create the written sentiment analysis reports.
-   [ ] C. Use Amazon Polly to convert the audio recordings into text.
-   [ ] D. Use Amazon Transcribe to convert the audio recordings in any language into text.
-   [ ] E. Use Amazon Translate to translate text in any language to English.
-   [ ] F. Use Amazon Comprehend to create the sentiment analysis reports.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Use Amazon Transcribe to convert the audio recordings in any language into text.
-   [ ] E. Use Amazon Translate to translate text in any language to English.
-   [ ] F. Use Amazon Comprehend to create the sentiment analysis reports.

Why these are the correct answers:

D. Use Amazon Transcribe to convert the audio recordings in any language into text.

-   [ ] Amazon Transcribe converts audio into text, supporting multiple languages.

E. Use Amazon Translate to translate text in any language to English.

-   [ ] Amazon Translate translates text from various languages to English.

F. Use Amazon Comprehend to create the sentiment analysis reports.

-   [ ] Amazon Comprehend performs sentiment analysis on text.

Why are the other answers wrong?

-   [ ] A. Comprehend does not translate audio recordings.
-   [ ] B. Amazon Lex is for building conversational interfaces, not for sentiment analysis reports.
-   [ ] C. Amazon Polly converts text to speech, not the other way around.

Therefore, Options D, E, and F are the correct solutions.

</details>
<details>
  <summary>Question 494</summary>

A company uses Amazon EC2 instances to host its internal systems.
As part of a deployment operation, an administrator tries to use the AWS CLI to terminate an EC2 instance.
However, the administrator receives a 403 (Access Denied) error message.
The administrator is using an IAM role that has the following IAM policy attached:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "ec2:TerminateInstances"
      ],
      "Resource": [
        "\*"
      ]
    },
    {
      "Effect": "Deny",
      "Action": [
        "ec2:TerminateInstances"
      ],
      "Condition": {
        "NotIpAddress": {
          "aws:SourceIp": [
            "192.0.2.0/24",
            "203.0.113.0/24"
          ]
        }
      },
      "Resource": [
        "\*"
      ]
    }
  ]
}

```

What is the cause of the unsuccessful request?

-   [ ] A. The EC2 instance has a resource-based policy with a Deny statement.
-   [ ] B. The principal has not been specified in the policy statement.
-   [ ] C. The "Action" field does not grant the actions that are required to terminate the EC2 instance.
-   [ ] D. The request to terminate the EC2 instance does not originate from the CIDR blocks 192.0.2.0/24 or 203.0.113.0/24.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D. The request to terminate the EC2 instance does not originate from the CIDR blocks 192.0.2.0/24 or 203.0.113.0/24.

Why these are the correct answers:

D. The request to terminate the EC2 instance does not originate from the CIDR blocks 192.0.2.0/24 or 203.0.113.0/24.

-   [ ] The deny statement in the IAM policy explicitly denies the ec2:TerminateInstances action if the request does not come from the specified IP address ranges.

Why are the other answers wrong?

-   [ ] A. EC2 instances do not have resource-based policies that would cause this issue.
-   [ ] B. The principal (the IAM role) is implicitly specified when the policy is attached to it.
-   [ ] C. The "Action" field correctly specifies ec2:TerminateInstances.

Therefore, Option D is the correct reason for the access denial.

</details>

<details>
  <summary>Question 495</summary>

A company is conducting an internal audit.
The company wants to ensure that the data in an Amazon S3 bucket that is associated with the company's AWS Lake Formation data lake does not contain sensitive customer or employee data.
The company wants to discover personally identifiable information (PII) or financial information, including passport numbers and credit card numbers.
Which solution will meet these requirements?

-   [ ] A. Configure AWS Audit Manager on the account.
    Select the Payment Card Industry Data Security Standards (PCI DSS) for auditing.
-   [ ] B. Configure Amazon S3 Inventory on the S3 bucket Configure Amazon Athena to query the inventory.
-   [ ] C. Configure Amazon Macie to run a data discovery job that uses managed identifiers for the required data types.
-   [ ] D. Use Amazon S3 Select to run a report across the S3 bucket.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Configure Amazon Macie to run a data discovery job that uses managed identifiers for the required data types.

Why these are the correct answers:

C. Configure Amazon Macie to run a data discovery job that uses managed identifiers for the required data types.

-   [ ] Amazon Macie is designed to discover sensitive data, including PII and financial information, in S3 buckets.
-   [ ] It uses managed identifiers to identify specific data types like passport and credit card numbers.

Why are the other answers wrong?

-   [ ] A. AWS Audit Manager assesses compliance with industry standards but does not directly discover sensitive data within S3.
-   [ ] B. S3 Inventory and Athena can query object metadata but do not automatically identify sensitive data content.
-   [ ] D. S3 Select retrieves object content but does not provide automated discovery of sensitive data patterns.

Therefore, Option C is the correct solution.

</details>
<details>
  <summary>Question 496</summary>

A company uses on-premises servers to host its applications.
The company is running out of storage capacity.
The applications use both block storage and NFS storage.
The company needs a high-performing solution that supports local caching without re-architecting its existing applications.
Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)

-   [ ] A. Mount Amazon S3 as a file system to the on-premises servers.
-   [ ] B. Deploy an AWS Storage Gateway file gateway to replace NFS storage.
-   [ ] C. Deploy AWS Snowball Edge to provision NFS mounts to on-premises servers.
-   [ ] D. Deploy an AWS Storage Gateway volume gateway to replace the block storage.
-   [ ] E. Deploy Amazon Elastic File System (Amazon EFS) volumes and mount them to on-premises servers.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Deploy an AWS Storage Gateway file gateway to replace NFS storage.
-   [ ] D. Deploy an AWS Storage Gateway volume gateway to replace the block storage.

Why these are the correct answers:

B. Deploy an AWS Storage Gateway file gateway to replace NFS storage.

-   [ ] Storage Gateway file gateway provides a local cache and supports NFS, allowing on-premises applications to access AWS storage without re-architecting.

D. Deploy an AWS Storage Gateway volume gateway to replace the block storage.

-   [ ] Storage Gateway volume gateway provides a local cache and presents block storage to on-premises servers, allowing for seamless integration.

Why are the other answers wrong?

-   [ ] A. Amazon S3 is object storage and cannot be mounted as a file system in the same way as NFS.
-   [ ] C. Snowball Edge is for data migration and edge computing, not for providing ongoing storage access.
-   [ ] E. Amazon EFS is an AWS-native file system and cannot be directly mounted on-premises.

Therefore, Options B and D are the correct solutions.

</details>
<details>
  <summary>Question 497</summary>

A company has a service that reads and writes large amounts of data from an Amazon S3 bucket in the same AWS Region.
The service is deployed on Amazon EC2 instances within the private subnet of a VPC.
The service communicates with Amazon S3 over a NAT gateway in the public subnet.
However, the company wants a solution that will reduce the data output costs.
Which solution will meet these requirements MOST cost-effectively?

-   [ ] A. Provision a dedicated EC2 NAT instance in the public subnet.
    Configure the route table for the private subnet to use the elastic network interface of this instance as the destination for all S3 traffic.
-   [ ] B. Provision a dedicated EC2 NAT instance in the private subnet.
    Configure the route table for the public subnet to use the elastic network interface of this instance as the destination for all S3 traffic.
-   [ ] C. Provision a VPC gateway endpoint.
    Configure the route table for the private subnet to use the gateway endpoint as the route for all S3 traffic.
-   [ ] D. Provision a second NAT gateway.
    Configure the route table for the private subnet to use this NAT gateway as the destination for all S3 traffic.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Provision a VPC gateway endpoint.
    Configure the route table for the private subnet to use the gateway endpoint as the route for all S3 traffic.

Why these are the correct answers:

C. Provision a VPC gateway endpoint.
Configure the route table for the private subnet to use the gateway endpoint as the route for all S3 traffic.

-   [ ] VPC gateway endpoints allow direct access to S3 from within the VPC without incurring data transfer costs.
-   [ ] They are more cost-effective than using a NAT gateway.

Why are the other answers wrong?

-   [ ] A, B, and D. Using NAT gateways, whether dedicated or additional, still incurs data transfer costs.

Therefore, Option C is the most cost-effective solution.

</details>
<details>
  <summary>Question 498</summary>

A company uses Amazon S3 to store high-resolution pictures in an S3 bucket.
To minimize application changes, the company stores the pictures as the latest version of an S3 object.
The company needs to retain only the two most recent versions of the pictures.

The company wants to reduce costs.
The company has identified the S3 bucket as a large expense.
Which solution will reduce the S3 costs with the LEAST operational overhead?

-   [ ] A. Use S3 Lifecycle to delete expired object versions and retain the two most recent versions.
-   [ ] B. Use an AWS Lambda function to check for older versions and delete all but the two most recent versions.
-   [ ] C. Use S3 Batch Operations to delete noncurrent object versions and retain only the two most recent versions.
-   [ ] D. Deactivate versioning on the S3 bucket and retain the two most recent versions.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Use S3 Lifecycle to delete expired object versions and retain the two most recent versions.

Why these are the correct answers:

A. Use S3 Lifecycle to delete expired object versions and retain the two most recent versions.

-   [ ] S3 Lifecycle rules automate the management of object versions, reducing storage costs.
-   [ ] It is the most operationally efficient way to manage object retention.

Why are the other answers wrong?

-   [ ] B. Using a Lambda function adds complexity and operational overhead.
-   [ ] C. S3 Batch Operations are for large-scale operations and are not necessary for this simple version management task.
-   [ ] D. Deactivating versioning does not allow you to retain any previous versions.

Therefore, Option A is the most efficient solution.

</details>
<details>
  <summary>Question 499</summary>

A company needs to minimize the cost of its 1 Gbps AWS Direct Connect connection.
The company's average connection utilization is less than 10%.
A solutions architect must recommend a solution that will reduce the cost without compromising security.
Which solution will meet these requirements?

-   [ ] A. Set up a new 1 Gbps Direct Connect connection.
    Share the connection with another AWS account.
-   [ ] B. Set up a new 200 Mbps Direct Connect connection in the AWS Management Console.
-   [ ] C. Contact an AWS Direct Connect Partner to order a 1 Gbps connection.
    Share the connection with another AWS account.
-   [ ] D. Contact an AWS Direct Connect Partner to order a 200 Mbps hosted connection for an existing AWS account.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Contact an AWS Direct Connect Partner to order a 200 Mbps hosted connection for an existing AWS account.

Why these are the correct answers:

D. Contact an AWS Direct Connect Partner to order a 200 Mbps hosted connection for an existing AWS account.

-   [ ] A hosted connection from a Direct Connect Partner allows for smaller, more cost-effective connections.
-   [ ] Reducing the connection speed aligns with the low utilization.

Why are the other answers wrong?

-   [ ] A and C. Sharing a 1 Gbps connection does not address the underutilization issue.
-   [ ] B. You cannot directly set up a 200 Mbps Direct Connect connection in the AWS Management Console; you must use a Direct Connect Partner for hosted connections.

Therefore, Option D is the correct solution.

</details>

<details>
  <summary>Question 500</summary>

A company has multiple Windows file servers on premises.
The company wants to migrate and consolidate its files into an Amazon FSx for Windows File Server file system.
File permissions must be preserved to ensure that access rights do not change.

Which solutions will meet these requirements?
(Choose two.)

-   [ ] A. Deploy AWS DataSync agents on premises.
    Schedule DataSync tasks to transfer the data to the FSx for Windows File Server file system.
-   [ ] B. Copy the shares on each file server into Amazon S3 buckets by using the AWS CLI.
    Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.
-   [ ] C. Remove the drives from each file server.
    Ship the drives to AWS for import into Amazon S3.
    Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.
-   [ ] D. Order an AWS Snowcone device.
    Connect the device to the on-premises network.
    Launch AWS DataSync agents on the device.
    Schedule DataSync tasks to transfer the data to the FSx for Windows File Server file system.
-   [ ] E. Order an AWS Snowball Edge Storage Optimized device.
    Connect the device to the on-premises network.
    Copy data to the device by using the AWS CLI.
    Ship the device back to AWS for import into Amazon S3.
    Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Deploy AWS DataSync agents on premises.
    Schedule DataSync tasks to transfer the data to the FSx for Windows File Server file system.
-   [ ] D. Order an AWS Snowcone device.
    Connect the device to the on-premises network.
    Launch AWS DataSync agents on the device.
    Schedule DataSync tasks to transfer the data to the FSx for Windows File Server file system.

Why these are the correct answers:

A. Deploy AWS DataSync agents on premises.
Schedule DataSync tasks to transfer the data to the FSx for Windows File Server file system.

-   [ ] AWS DataSync is designed to efficiently and securely transfer file data between on-premises and AWS storage.
-   [ ] It preserves file metadata and permissions, which is crucial for maintaining access rights.

D. Order an AWS Snowcone device.
Connect the device to the on-premises network.
Launch AWS DataSync agents on the device.
Schedule DataSync tasks to transfer the data to the FSx for Windows File Server file system.

-   [ ] AWS Snowcone can be used for data transfer in situations where network bandwidth is limited.
-   [ ] Running DataSync agents on Snowcone allows for efficient data transfer with permission preservation.

Why are the other answers wrong?

-   [ ] B. Copying shares to S3 using the AWS CLI does not preserve Windows file permissions directly.
    While DataSync can move data from S3, the initial transfer loses important metadata.
-   [ ] C. Removing drives and shipping them to AWS is not a practical or efficient way to migrate data, especially when needing to preserve permissions.
-   [ ] E. Snowball Edge is designed for larger data transfers.
    Using the AWS CLI to copy data to it is less efficient than using DataSync, and the intermediate step of shipping the device adds complexity.

Therefore, Options A and D are the most appropriate solutions.

</details>








