# AWS-SAA-PRACTICE-EXAM Questions 801-810

<details>
  <summary>Question 801</summary>

A financial company needs to handle highly sensitive data. The company will store the data in an Amazon S3 bucket. The company needs to ensure that the data is encrypted in transit and at rest. The company must manage the encryption keys outside the AWS Cloud. Which solution will meet these requirements?

-   [ ] A.
    Encrypt the data in the S3 bucket with server-side encryption (SSE) that uses an AWS Key Management Service (AWS KMS) customer managed key.
-   [ ] B.
    Encrypt the data in the S3 bucket with server-side encryption (SSE) that uses an AWS Key Management Service (AWS KMS) AWS managed key.
-   [ ] C.
    Encrypt the data in the S3 bucket with the default server-side encryption (SSE).
-   [ ] D.
    Encrypt the data at the company's data center before storing the data in the S3 bucket.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D.
    Encrypt the data at the company's data center before storing the data in the S3 bucket.

Why this is the correct answer:

-   [ ]
    Encrypting the data before it reaches AWS allows the company to maintain full control over the encryption keys.
-   [ ]
    This satisfies the requirement that the company manage the encryption keys outside of the AWS Cloud.

Why are the other answers wrong?

-   [ ]
    A and B. AWS KMS, whether using customer managed or AWS managed keys, involves AWS managing part or all of the key management, which does not meet the requirement.
-   [ ]
    C. Default server-side encryption (SSE-S3) is managed by AWS and does not allow the company to manage the encryption keys.

</details>

<details>
  <summary>Question 802</summary>

A company wants to run its payment application on AWS. The application receives payment notifications from mobile devices. Payment notifications require a basic validation before they are sent for further processing. The backend processing application is long running and requires compute and memory to be adjusted. The company does not want to manage the infrastructure. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A.
    Create an Amazon Simple Queue Service (Amazon SQS) queue. Integrate the queue with an Amazon EventBridge rule to receive payment notifications from mobile devices. Configure the rule to validate payment notifications and send the notifications to the backend application. Deploy the backend application on Amazon Elastic Kubernetes Service (Amazon EKS) Anywhere. Create a standalone cluster.
-   [ ] B.
    Create an Amazon API Gateway API. Integrate the API with an AWS Step Functions state machine to receive payment notifications from mobile devices. Invoke the state machine to validate payment notifications and send the notifications to the backend application. Deploy the backend application on Amazon Elastic Kubernetes Service (Amazon EKS). Configure an EKS cluster with self-managed nodes.
-   [ ] C.
    Create an Amazon Simple Queue Service (Amazon SQS) queue. Integrate the queue with an Amazon EventBridge rule to receive payment notifications from mobile devices. Configure the rule to validate payment notifications and send the notifications to the backend application. Deploy the backend application on Amazon EC2 Spot Instances. Configure a Spot Fleet with a default allocation strategy.
-   [ ] D.
    Create an Amazon API Gateway API. Integrate the API with AWS Lambda to receive payment notifications from mobile devices. Invoke a Lambda function to validate payment notifications and send the notifications to the backend application. Deploy the backend application on Amazon Elastic Container Service (Amazon ECS). Configure Amazon ECS with an AWS Fargate launch type.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D.
    Create an Amazon API Gateway API. Integrate the API with AWS Lambda to receive payment notifications from mobile devices. Invoke a Lambda function to validate payment notifications and send the notifications to the backend application. Deploy the backend application on Amazon Elastic Container Service (Amazon ECS). Configure Amazon ECS with an AWS Fargate launch type.

Why this is the correct answer:

-   [ ]
    Amazon API Gateway and AWS Lambda are serverless services that handle the receiving and processing of payment notifications without infrastructure management.
-   [ ]
    Amazon ECS with AWS Fargate launch type also eliminates the need to manage the underlying EC2 instances for the backend application.
-   [ ]
    This combination provides a fully managed, scalable, and cost-effective solution.

Why are the other answers wrong?

-   [ ]
    A and B. Amazon EKS Anywhere and self-managed nodes in EKS require managing the Kubernetes infrastructure, which increases operational overhead.
-   [ ]
    C. Amazon EC2 Spot Instances require managing EC2 instances and dealing with potential interruptions, which increases operational overhead.

</details>

<details>
  <summary>Question 803</summary>

A solutions architect is designing a user authentication solution for a company. The solution must invoke two-factor authentication for users that log in from inconsistent geographical locations, IP addresses, or devices. The solution must also be able to scale up to accommodate millions of users. Which solution will meet these requirements?

-   [ ] A.
    Configure Amazon Cognito user pools for user authentication. Enable the risk-based adaptive authentication feature with multifactor authentication (MFA).
-   [ ] B.
    Configure Amazon Cognito identity pools for user authentication. Enable multi-factor authentication (MFA).
-   [ ] C.
    Configure AWS Identity and Access Management (IAM) users for user authentication. Attach an IAM policy that allows the AllowManage OwnUserMFA action.
-   [ ] D.
    Configure AWS IAM Identity Center (AWS Single Sign-On) authentication for user authentication. Configure the permission sets to require multi-factor authentication (MFA).

</details>

<details>
  <summary>Answer</summary>

-   [ ] A.
    Configure Amazon Cognito user pools for user authentication. Enable the risk-based adaptive authentication feature with multifactor authentication (MFA).

Why this is the correct answer:

-   [ ]
    Amazon Cognito user pools provide user directory services and support for MFA.
-   [ ]
    The risk-based adaptive authentication feature can dynamically require MFA based on contextual information like location, IP address, and device.
-   [ ]
    Cognito user pools are designed to scale to millions of users.

Why are the other answers wrong?

-   [ ]
    B. Amazon Cognito identity pools are for authorizing access to AWS resources, not for user authentication.
-   [ ]
    C. IAM users are for managing AWS resource access, not for authenticating application users. IAM does not provide risk-based adaptive authentication.
-   [ ]
    D. AWS IAM Identity Center (successor to AWS Single Sign-On) is for federated authentication to AWS accounts and applications, not for managing user authentication within an application.

</details>

<details>
  <summary>Question 804</summary>

A company has an Amazon S3 data lake. The company needs a solution that transforms the data from the data lake and loads the data into a data warehouse every day. The data warehouse must have massively parallel processing (MPP) capabilities. Data analysts then need to create and train machine learning (ML) models by using SQL commands on the data. The solution must use serverless AWS services wherever possible. Which solution will meet these requirements?

-   [ ] A.
    Run a daily Amazon EMR job to transform the data and load the data into Amazon Redshift. Use Amazon Redshift ML to create and train the ML models.
-   [ ] B.
    Run a daily Amazon EMR job to transform the data and load the data into Amazon Aurora Serverless. Use Amazon Aurora ML to create and train the ML models.
-   [ ] C.
    Run a daily AWS Glue job to transform the data and load the data into Amazon Redshift Serverless. Use Amazon Redshift ML to create and train the ML models.
-   [ ] D.
    Run a daily AWS Glue job to transform the data and load the data into Amazon Athena tables. Use Amazon Athena ML to create and train the ML models.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C.
    Run a daily AWS Glue job to transform the data and load the data into Amazon Redshift Serverless. Use Amazon Redshift ML to create and train the ML models.

Why this is the correct answer:

-   [ ]
    AWS Glue is a serverless data integration service that can transform data.
-   [ ]
    Amazon Redshift Serverless provides a serverless data warehouse with MPP capabilities.
-   [ ]
    Amazon Redshift ML enables creating and training ML models using SQL.
-   [ ]
    This combination meets all requirements using serverless services.

Why are the other answers wrong?

-   [ ]
    A and B. Amazon EMR is not serverless, and Amazon Aurora Serverless does not have the same MPP capabilities as Redshift, nor does it offer built-in ML capabilities like Redshift ML.
-   [ ]
    D. Amazon Athena is a serverless query service, not a data warehouse. While it can be used for analysis, it is not optimized for the same data warehousing and ML use cases as Redshift. Amazon Athena ML also has different capabilities than Redshift ML.

</details>

<details>
  <summary>Question 805</summary>

A company runs containers in a Kubernetes environment in the company's local data center. The company wants to use Amazon Elastic Kubernetes Service (Amazon EKS) and other AWS managed services. Data must remain locally in the company's data center and cannot be stored in any remote site or cloud to maintain compliance. Which solution will meet these requirements?

-   [ ] A.
    Deploy AWS Local Zones in the company's data center.
-   [ ] B.
    Use an AWS Snowmobile in the company's data center.
-   [ ] C.
    Install an AWS Outposts rack in the company's data center.
-   [ ] D.
    Install an AWS Snowball Edge Storage Optimized node in the data center.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C.
    Install an AWS Outposts rack in the company's data center.

Why this is the correct answer:

-   [ ]
    AWS Outposts brings AWS infrastructure and services to your on-premises data center.
-   [ ]
    This allows the company to run Amazon EKS and other AWS services on-premises, ensuring data remains local.

Why are the other answers wrong?

-   [ ]
    A. AWS Local Zones are an extension of an AWS Region and are not deployed in the company's data center.
-   [ ]
    B. AWS Snowmobile is a data transfer service for moving large amounts of data into AWS, not for running AWS services on-premises.
-   [ ]
    D. AWS Snowball Edge is a device for edge computing and data transfer, not for running EKS or other AWS services on-premises.

</details>

<details>
  <summary>Question 806</summary>

A social media company has workloads that collect and process data. The workloads store the data in on-premises NFS storage. The data store cannot scale fast enough to meet the company's expanding business needs. The company wants to migrate the current data store to AWS. Which solution will meet these requirements MOST cost-effectively?

-   [ ] A.
    Set up an AWS Storage Gateway Volume Gateway. Use an Amazon S3 Lifecycle policy to transition the data to the appropriate storage class.
-   [ ] B.
    Set up an AWS Storage Gateway Amazon S3 File Gateway. Use an Amazon S3 Lifecycle policy to transition the data to the appropriate storage class.
-   [ ] C.
    Use the Amazon Elastic File System (Amazon EFS) Standard-Infrequent Access (Standard-IA) storage class. Activate the infrequent access lifecycle policy.
-   [ ] D.
    Use the Amazon Elastic File System (Amazon EFS) One Zone-Infrequent Access (One Zone-IA) storage class. Activate the infrequent access lifecycle policy.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B.
    Set up an AWS Storage Gateway Amazon S3 File Gateway. Use an Amazon S3 Lifecycle policy to transition the data to the appropriate storage class.

Why this is the correct answer:

-   [ ]
    AWS Storage Gateway File Gateway allows you to store data in Amazon S3 while providing local access to applications via file protocols.
-   [ ]
    S3 Lifecycle policies automate the movement of data to lower-cost storage classes, optimizing costs.
-   [ ]
    This is cost-effective because you leverage S3's scalability and cost-efficiency.

Why are the other answers wrong?

-   [ ]
    A. Volume Gateway presents block storage to on-premises applications, which is not suitable for file-based workloads and is generally more expensive than S3.
-   [ ]
    C and D. Amazon EFS is a scalable file storage service for use with EC2 instances. While it provides file storage, it is generally more expensive than S3 for large-scale data storage, and using infrequent access tiers does not provide the same cost optimization as S3 Lifecycle policies.

</details>

<details>
  <summary>Question 807</summary>

A company uses high concurrency AWS Lambda functions to process a constantly increasing number of messages in a message queue during marketing events. The Lambda functions use CPU intensive code to process the messages. The company wants to reduce the compute costs and to maintain service latency for its customers. Which solution will meet these requirements?

-   [ ] A.
    Configure reserved concurrency for the Lambda functions. Decrease the memory allocated to the Lambda functions.
-   [ ] B.
    Configure reserved concurrency for the Lambda functions. Increase the memory according to AWS Compute Optimizer recommendations.
-   [ ] C.
    Configure provisioned concurrency for the Lambda functions. Decrease the memory allocated to the Lambda functions.
-   [ ] D.
    Configure provisioned concurrency for the Lambda functions. Increase the memory according to AWS Compute Optimizer recommendations.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D.
    Configure provisioned concurrency for the Lambda functions. Increase the memory according to AWS Compute Optimizer recommendations.

Why this is the correct answer:

-   [ ]
    Provisioned concurrency keeps Lambda functions initialized and ready to respond, minimizing cold starts and maintaining low latency.
-   [ ]
    Increasing memory can improve CPU performance for Lambda functions, as Lambda allocates CPU power proportionally to memory.
-   [ ]
    AWS Compute Optimizer provides recommendations for optimizing Lambda function memory allocation based on performance data.

Why are the other answers wrong?

-   [ ]
    A. Reserved concurrency helps to manage concurrency but does not eliminate cold starts or improve CPU performance. Decreasing memory would likely worsen CPU performance.
-   [ ]
    B. Reserved concurrency does not address cold starts. Increasing memory without considering Compute Optimizer recommendations might not be cost-effective.
-   [ ]
    C. Provisioned concurrency helps with cold starts, but decreasing memory would likely reduce CPU performance.

</details>

<details>
  <summary>Question 808</summary>

A company runs its workloads on Amazon Elastic Container Service (Amazon ECS). The container images that the ECS task definition uses need to be scanned for Common Vulnerabilities and Exposures (CVEs). New container images that are created also need to be scanned. Which solution will meet these requirements with the FEWEST changes to the workloads?

-   [ ] A.
    Use Amazon Elastic Container Registry (Amazon ECR) as a private image repository to store the container images. Specify scan on push filters for the ECR basic scan.
-   [ ] B.
    Store the container images in an Amazon S3 bucket. Use Amazon Macie to scan the images. Use an S3 Event Notification to initiate a Macie scan for every event with an s3:ObjectCreated:Put event type.
-   [ ] C.
    Deploy the workloads to Amazon Elastic Kubernetes Service (Amazon EKS). Use Amazon Elastic Container Registry (Amazon ECR) as a private image repository. Specify scan on push filters for the ECR enhanced scan.
-   [ ] D.
    Store the container images in an Amazon S3 bucket that has versioning enabled. Configure an S3 Event Notification for s3:ObjectCreated:\* events to invoke an AWS Lambda function. Configure the Lambda function to initiate an Amazon Inspector scan.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A.
    Use Amazon Elastic Container Registry (Amazon ECR) as a private image repository to store the container images. Specify scan on push filters for the ECR basic scan.

Why this is the correct answer:

-   [ ]
    Amazon ECR can scan container images for vulnerabilities when they are pushed to the repository.
-   [ ]
    This integrates directly with the container image storage and requires minimal changes to the workload deployment process.

Why are the other answers wrong?

-   [ ]
    B. Amazon Macie is a data security and privacy service that discovers sensitive data in S3, not for scanning container images for CVEs. This solution is not designed for this purpose.
-   [ ]
    C. Deploying workloads to Amazon EKS is not necessary for scanning container images. ECR can scan images regardless of where they are deployed.
-   [ ]
    D. Storing images in S3 and using Lambda and Inspector adds complexity and is not as efficient as using ECR's built-in scanning.

</details>

<details>
  <summary>Question 809</summary>

A company uses an AWS Batch job to run its end-of-day sales process. The company needs a serverless solution that will invoke a third-party reporting application when the AWS Batch job is successful. The reporting application has an HTTP API interface that uses username and password authentication. Which solution will meet these requirements?

-   [ ] A.
    Configure an Amazon EventBridge rule to match incoming AWS Batch job SUCCEEDED events. Configure the third-party API as an EventBridge API destination with a username and password. Set the API destination as the EventBridge rule target.
-   [ ] B.
    Configure Amazon EventBridge Scheduler to match incoming AWS Batch job SUCCEEDED events. Configure an AWS Lambda function to invoke the third-party API by using a username and password. Set the Lambda function as the EventBridge rule target.
-   [ ] C.
    Configure an AWS Batch job to publish job SUCCEEDED events to an Amazon API Gateway REST API. Configure an HTTP proxy integration on the API Gateway REST API to invoke the third-party API by using a username and password.
-   [ ] D.
    Configure an AWS Batch job to publish job SUCCEEDED events to an Amazon API Gateway REST API. Configure a proxy integration on the API Gateway REST API to an AWS Lambda function. Configure the Lambda function to invoke the third-party API by using a username and password.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A.
    Configure an Amazon EventBridge rule to match incoming AWS Batch job SUCCEEDED events. Configure the third-party API as an EventBridge API destination with a username and password. Set the API destination as the EventBridge rule target.

Why this is the correct answer:

-   [ ]
    Amazon EventBridge can directly invoke HTTP API endpoints using API destinations.
-   [ ]
    This simplifies the process of connecting AWS services to external APIs without needing additional services like Lambda or API Gateway.
-   [ ]
    EventBridge API destinations can handle authentication, making it suitable for invoking the third-party API.

Why are the other answers wrong?

-   [ ]
    B. EventBridge Scheduler is for scheduling events, not for reacting to events like a successful Batch job. Using Lambda adds complexity.
-   [ ]
    C and D. Using API Gateway to invoke the third-party API adds unnecessary complexity compared to EventBridge API destinations. It introduces another service that needs to be configured and managed.

</details>

<details>
  <summary>Question 810</summary>

A company collects and processes data from a vendor. The vendor stores its data in an Amazon RDS for MySQL database in the vendor's own AWS account. The company's VPC does not have an internet gateway, an AWS Direct Connect connection, or an AWS Site-to-Site VPN connection. The company needs to access the data that is in the vendor database. Which solution will meet this requirement?

-   [ ] A.
    Instruct the vendor to sign up for the AWS Hosted Connection Direct Connect Program. Use VPC peering to connect the company's VPC and the vendor's VPC.
-   [ ] B.
    Configure a client VPN connection between the company's VPC and the vendor's VPC. Use VPC peering to connect the company's VPC and the vendor's VPC.
-   [ ] C.
    Instruct the vendor to create a Network Load Balancer (NLB). Place the NLB in front of the Amazon RDS for MySQL database. Use AWS PrivateLink to integrate the company's VPC and the vendor's VPC.
-   [ ] D.
    Use AWS Transit Gateway to integrate the company's VPC and the vendor's VPC. Use VPC peering to connect the company's VPC and the vendor's VPC.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C.
    Instruct the vendor to create a Network Load Balancer (NLB). Place the NLB in front of the Amazon RDS for MySQL database. Use AWS PrivateLink to integrate the company's VPC and the vendor's VPC.

Why this is the correct answer:

-   [ ]
    AWS PrivateLink allows you to access services across VPCs and accounts privately, without traversing the internet.
-   [ ]
    An NLB in front of the RDS database enables PrivateLink to access the database.
-   [ ]
    This solution works even without an internet gateway, Direct Connect, or Site-to-Site VPN.

Why are the other answers wrong?

-   [ ]
    A. Direct Connect requires a physical connection and is not needed for this scenario. VPC peering alone does not provide access without a network connection.
-   [ ]
    B. Client VPN is for individual users to connect to a VPC, not for VPC-to-VPC communication. VPC peering alone is insufficient.
-   [ ]
    D. Transit Gateway is for connecting multiple VPCs and on-premises networks, not necessary for this point-to-point connection. VPC peering alone is insufficient.

</details>


































