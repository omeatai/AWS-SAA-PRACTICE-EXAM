# AWS-SAA-PRACTICE-EXAM Questions 201-210

<details>
  <summary>Question 201</summary>

A company is developing a marketing communications service that targets mobile app users. The company needs to send confirmation messages with Short Message Service (SMS) to its users. The users must be able to reply to the SMS messages. The company must store the responses for a year for analysis. What should a solutions architect do to meet these requirements?

-   [ ] A. Create an Amazon Connect contact flow to send the SMS messages. Use AWS Lambda to process the responses.
-   [ ] B. Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.
-   [ ] C. Use Amazon Simple Queue Service (Amazon SQS) to distribute the SMS messages. Use AWS Lambda to process the responses.
-   [ ] D. Create an Amazon Simple Notification Service (Amazon SNS) FIFO topic. Subscribe an Amazon Kinesis data stream to the SNS topic for analysis and archiving.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.
   

Why these are the correct answers:

B. Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.

-   [ ] Amazon Pinpoint is designed for marketing communication and supports sending and receiving SMS messages.
-   [ ] Amazon Pinpoint journeys allow you to create automated messaging campaigns.
-   [ ] Pinpoint can be configured to send events to a Kinesis data stream, enabling real-time analysis and archiving of responses.
-   [ ] This solution meets all the requirements: sending SMS, receiving replies, and storing responses for analysis.
   

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Amazon Connect is a cloud-based contact center service, not primarily designed for automated marketing SMS campaigns. While it can send SMS, it is not the most suitable service for this scenario.
-   [ ] C. Amazon SQS is a message queuing service and is not designed for sending SMS messages. It is used for queuing messages between applications.
-   [ ] D. Amazon SNS is a publish/subscribe messaging service, not designed for two-way SMS communication or managing conversations. FIFO topics provide ordering, which is not a primary requirement here, and SNS does not directly handle SMS replies in a conversational manner.

Therefore, Option B is the most appropriate solution for the marketing communications service.

</details>

<details>
  <summary>Question 202</summary>

A company is planning to move its data to an Amazon S3 bucket. The data must be encrypted when it is stored in the S3 bucket. Additionally, the encryption key must be automatically rotated every year. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Move the data to the S3 bucket. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use the built-in key rotation behavior of SSE-S3 encryption keys.
-   [ ] B. Create an AWS Key Management Service (AWS KMS) customer managed key. Enable automatic key rotation. Set the S3 bucket's default encryption behavior to use the customer managed KMS key. Move the data to the S3 bucket.
-   [ ] C. Create an AWS Key Management Service (AWS KMS) customer managed key. Set the S3 bucket's default encryption behavior to use the customer managed KMS key. Move the data to the S3 bucket. Manually rotate the KMS key every year.
-   [ ] D. Encrypt the data with customer key material before moving the data to the S3 bucket. Create an AWS Key Management Service (AWS KMS) key without key material. Import the customer key material into the KMS key. Enable automatic key rotation.

</details>

<details>
  <summary>Answer</summary>

A. Move the data to the S3 bucket. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use the built-in key rotation behavior of SSE-S3 encryption keys.

Why these are the correct answers:

A. Move the data to the S3 bucket. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use the built-in key rotation behavior of SSE-S3 encryption keys.

-   [ ] SSE-S3 is the simplest encryption method, as AWS manages the encryption keys.
-   [ ] SSE-S3 provides automatic encryption of data at rest.
-   [ ] SSE-S3 keys are automatically rotated by AWS, which satisfies the key rotation requirement with the least operational overhead.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. While using KMS customer managed keys with automatic key rotation meets the requirements, it involves more configuration than SSE-S3, increasing operational overhead.
-   [ ] C. Manually rotating KMS keys increases operational overhead, which contradicts the requirement for the least overhead.
-   [ ] D. Encrypting data before moving it to S3 and importing key material into KMS is the most complex option and has the highest operational overhead.

Therefore, Option A is the best solution as it meets the requirements with the least operational overhead.

</details>

<details>
  <summary>Question 203</summary>

The customers of a finance company request appointments with financial advisors by sending text messages. A web application that runs on Amazon EC2 instances accepts the appointment requests. The text messages are published to an Amazon Simple Queue Service (Amazon SQS) queue through the web application. Another application that runs on EC2 instances then sends meeting invitations and meeting confirmation email messages to the customers. After successful scheduling, this application stores the meeting information in an Amazon DynamoDB database. As the company expands, customers report that their meeting invitations are taking longer to arrive. What should a solutions architect recommend to resolve this issue?

-   [ ] A. Add a DynamoDB Accelerator (DAX) cluster in front of the DynamoDB database.
-   [ ] B. Add an Amazon API Gateway API in front of the web application that accepts the appointment requests.
-   [ ] C. Add an Amazon CloudFront distribution. Set the origin as the web application that accepts the appointment requests.
-   [ ] D. Add an Auto Scaling group for the application that sends meeting invitations. Configure the Auto Scaling group to scale based on the depth of the SOS queue.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Add an Auto Scaling group for the application that sends meeting invitations. Configure the Auto Scaling group to scale based on the depth of the SOS queue.

Why these are the correct answers:

D. Add an Auto Scaling group for the application that sends meeting invitations. Configure the Auto Scaling group to scale based on the depth of the SOS queue.

-   [ ] The problem is identified as meeting invitations taking longer to arrive, indicating a bottleneck in the application that sends these invitations.
-   [ ] Auto Scaling groups can automatically adjust the number of EC2 instances based on demand.
-   [ ] Scaling based on SQS queue depth ensures that the application that sends meeting invitations scales in response to the number of requests waiting to be processed.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. DAX is used to accelerate DynamoDB read operations, but the issue is with sending meeting invitations, not writing to the database.
-   [ ] B. API Gateway is used for managing APIs, not for processing messages or scaling the application that sends invitations.
-   [ ] C. CloudFront is a content delivery network (CDN) and does not address the processing bottleneck for sending invitations.

Therefore, Option D is the most appropriate solution to resolve the issue by scaling the application responsible for sending meeting invitations based on the queue depth.

</details>

<details>
  <summary>Question 204</summary>

An online retail company has more than 50 million active customers and receives more than 25,000 orders each day. The company collects purchase data for customers and stores this data in Amazon S3. Additional customer data is stored in Amazon RDS.

The company wants to make all the data available to various teams so that the teams can perform analytics. The solution must provide the ability to manage fine-grained permissions for the data and must minimize operational overhead. Which solution will meet these requirements?

-   [ ] A. Migrate the purchase data to write directly to Amazon RDS. Use RDS access controls to limit access.
-   [ ] B. Schedule an AWS Lambda function to periodically copy data from Amazon RDS to Amazon S3. Create an AWS Glue crawler. Use Amazon Athena to query the data. Use S3 policies to limit access.
-   [ ] C. Create a data lake by using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon RDS. Register the S3 bucket in Lake Formation. Use Lake Formation access controls to limit access.
-   [ ] D. Create an Amazon Redshift cluster. Schedule an AWS Lambda function to periodically copy data from Amazon S3 and Amazon RDS to Amazon Redshift. Use Amazon Redshift access controls to limit access.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Create a data lake by using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon RDS. Register the S3 bucket in Lake Formation. Use Lake Formation access controls to limit access.

Why these are the correct answers:

C. Create a data lake by using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon RDS. Register the S3 bucket in Lake Formation. Use Lake Formation access controls to limit access.

-   [ ] AWS Lake Formation simplifies the process of setting up a data lake.
-   [ ] It allows for fine-grained access control at the table and column level.
-   [ ] Using AWS Glue connections, data from Amazon RDS can be integrated into the data lake.
-   [ ] This solution minimizes operational overhead by providing a centralized and managed way to handle data access and permissions.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Migrating all purchase data directly into Amazon RDS might not be scalable or cost-effective for large volumes of data. RDS access controls are not as fine-grained as Lake Formation.
-   [ ] B. Using Lambda, Glue, Athena, and S3 policies requires more manual configuration and management of permissions across different services compared to Lake Formation.
-   [ ] D. Amazon Redshift is a data warehouse and might be overkill for providing data access to various teams for general analytics. It also involves more operational overhead than Lake Formation.

Therefore, Option C is the most suitable solution because it provides fine-grained permissions and minimizes operational overhead.

</details>

<details>
  <summary>Question 205</summary>

A company hosts a marketing website in an on-premises data center. The website consists of static documents and runs on a single server. An administrator updates the website content infrequently and uses an SFTP client to upload new documents. The company decides to host its website on AWS and to use Amazon CloudFront. The company's solutions architect creates a CloudFront distribution. The solutions architect must design the most cost-effective and resilient architecture for website hosting to serve as the CloudFront origin. Which solution will meet these requirements?

-   [ ] A. Create a virtual server by using Amazon Lightsail. Configure the web server in the Lightsail instance. Upload website content by using an SFTP client.
-   [ ] B. Create an AWS Auto Scaling group for Amazon EC2 instances. Use an Application Load Balancer. Upload website content by using an SFTP client.
-   [ ] C. Create a private Amazon S3 bucket. Use an S3 bucket policy to allow access from a CloudFront origin access identity (OAI). Upload website content by using the AWS CLI.
-   [ ] D. Create a public Amazon S3 bucket. Configure AWS Transfer for SFTP. Configure the S3 bucket for website hosting. Upload website content by using the SFTP client.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Create a private Amazon S3 bucket. Use an S3 bucket policy to allow access from a CloudFront origin access identity (OAI). Upload website content by using the AWS CLI.

Why these are the correct answers:

C. Create a private Amazon S3 bucket. Use an S3 bucket policy to allow access from a CloudFront origin access identity (OAI). Upload website content by using the AWS CLI.

-   [ ]   Amazon S3 is a cost-effective and highly available storage service for static website content.
-   [ ]   Using a private S3 bucket and an OAI ensures that the content is only accessible through CloudFront, enhancing security.
-   [ ]   This solution is resilient because S3 is inherently highly available and scalable.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Amazon Lightsail is a virtual private server and is not designed for highly scalable and resilient website hosting. It does not integrate well with CloudFront for origin access control.
-   [ ]   B. Using EC2 instances and Auto Scaling with an Application Load Balancer is more complex and expensive than using S3 for static content. It also involves more operational overhead.
-   [ ]   D. A public S3 bucket is less secure. Using AWS Transfer for SFTP is not necessary for serving static website content and adds complexity.

Therefore, Option C is the most cost-effective and resilient solution for hosting a static website with CloudFront.

</details>

<details>
  <summary>Question 206</summary>

A company wants to manage Amazon Machine Images (AMIs). The company currently copies AMIs to the same AWS Region where the AMIs were created. The company needs to design an application that captures AWS API calls and sends alerts whenever the Amazon EC2 Createlmage API operation is called within the company's account. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Create an AWS Lambda function to query AWS CloudTrail logs and to send an alert when a Createlmage API call is detected.
-   [ ] B. Configure AWS CloudTrail with an Amazon Simple Notification Service (Amazon SNS) notification that occurs when updated logs are sent to Amazon S3. Use Amazon Athena to create a new table and to query on Createlmage when an API call is detected.
-   [ ] C. Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the Createlmage API call. Configure the target as an Amazon Simple Notification Service (Amazon SNS) topic to send an alert when a Createlmage API call is detected.
-   [ ] D. Configure an Amazon Simple Queue Service (Amazon SQS) FIFO queue as a target for AWS CloudTrail logs. Create an AWS Lambda function to send an alert to an Amazon Simple Notification Service (Amazon SNS) topic when a Createlmage API call is detected.
  
</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the Createlmage API call. Configure the target as an Amazon Simple Notification Service (Amazon SNS) topic to send an alert when a Createlmage API call is detected.
  
Why these are the correct answers:

C. Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the Createlmage API call. Configure the target as an Amazon Simple Notification Service (Amazon SNS) topic to send an alert when a Createlmage API call is detected.

-   [ ] Amazon EventBridge allows you to create rules that react to events in your AWS environment, including API calls.
-   [ ] By creating a rule specifically for the `CreateImage` API call and setting an SNS topic as the target, you can efficiently trigger alerts with minimal setup and maintenance.
-   [ ] This solution avoids the need for custom code to query logs or manage queues, reducing operational overhead.
  
<hr> Why are the other answers wrong? <hr>

-   [ ] A. Using a Lambda function to query CloudTrail logs adds complexity and requires writing and maintaining code. It's also less efficient than EventBridge, which can directly capture the API event.
-   [ ] B. Configuring CloudTrail with SNS and then using Athena to query logs involves multiple services and more steps, increasing operational overhead. Athena is also better suited for analytical queries, not real-time alerting.
-   [ ] D. Using SQS to buffer CloudTrail logs and then processing them with Lambda adds unnecessary complexity and latency. EventBridge provides a more direct and efficient way to capture and react to API events.

Therefore, Option C is the most operationally efficient solution for capturing the `CreateImage` API call and sending alerts.

</details>

<details>
  <summary>Question 207</summary>

A company owns an asynchronous API that is used to ingest user requests and, based on the request type, dispatch requests to the appropriate microservice for processing. The company is using Amazon API Gateway to deploy the API front end, and an AWS Lambda function that invokes Amazon DynamoDB to store user requests before dispatching them to the processing microservices. The company provisioned as much DynamoDB throughput as its budget allows, but the company is still experiencing availability issues and is losing user requests. What should a solutions architect do to address this issue without impacting existing users?

-   [ ] A. Add throttling on the API Gateway with server-side throttling limits.
-   [ ] B. Use DynamoDB Accelerator (DAX) and Lambda to buffer writes to DynamoDB.
-   [ ] C. Create a secondary index in DynamoDB for the table with the user requests.
-   [ ] D. Use the Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer writes to DynamoDB.
  
</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Use the Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer writes to DynamoDB.
  
Why these are the correct answers:

D. Use the Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer writes to DynamoDB.

-   [ ]   Amazon SQS can act as a buffer between API Gateway/Lambda and DynamoDB, allowing requests to be queued during high traffic periods.
-   [ ]   This buffering prevents DynamoDB from being overwhelmed and helps avoid losing user requests.
-   [ ]   A Lambda function can then process the messages from the SQS queue and write them to DynamoDB at a controlled rate, ensuring that DynamoDB's throughput is not exceeded.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Adding throttling on API Gateway might help to control the rate of requests, but it doesn't address the underlying issue of DynamoDB being unable to handle the write load. It could also result in losing user requests if they are throttled.
-   [ ]   B. DAX is a cache for DynamoDB, designed to accelerate read operations. It does not help with write-heavy scenarios or buffer writes.
-   [ ]   C. Creating a secondary index in DynamoDB can improve read performance but does not directly address the issue of high write volume causing availability issues.

Therefore, Option D is the most suitable solution to buffer writes to DynamoDB and prevent losing user requests.

</details>

<details>
  <summary>Question 208</summary>

A company needs to move data from an Amazon EC2 instance to an Amazon S3 bucket. The company must ensure that no API calls and no data are routed through public internet routes. Only the EC2 instance can have access to upload data to the S3 bucket. Which solution will meet these requirements?

- [ ] A. Create an interface VPC endpoint for Amazon S3 in the subnet where the EC2 instance is located. Attach a resource policy to the S3 bucket to only allow the EC2 instance's IAM role for access.
- [ ] B. Create a gateway VPC endpoint for Amazon S3 in the Availability Zone where the EC2 instance is located. Attach appropriate security groups to the endpoint. Attach a resource policy to the S3 bucket to only allow the EC2 instance's IAM role for access.
- [ ] C. Run the nslookup tool from inside the EC2 instance to obtain the private IP address of the S3 bucket's service API endpoint. Create a route in the VPC route table to provide the EC2 instance with access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance's IAM role for access.
- [ ] D. Use the AWS provided, publicly available ip-ranges.json file to obtain the private IP address of the S3 bucket's service API endpoint. Create a route in the VPC route table to provide the EC2 instance with access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance's IAM role for access.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Create an interface VPC endpoint for Amazon S3 in the subnet where the EC2 instance is located. Attach a resource policy to the S3 bucket to only allow the EC2 instance's IAM role for access.

Why these are the correct answers:

A. Create an interface VPC endpoint for Amazon S3 in the subnet where the EC2 instance is located. Attach a resource policy to the S3 bucket to only allow the EC2 instance's IAM role for access.

- [ ] Interface VPC endpoints use private IP addresses to access S3, ensuring that traffic does not traverse the public internet.
- [ ] Attaching a resource policy to the S3 bucket restricts access to only the specified IAM role of the EC2 instance, enhancing security.
- [ ] This solution meets the requirements of keeping traffic private and restricting access.

<hr> Why are the other answers wrong? <hr>

- [ ] B. Gateway VPC endpoints for S3 are accessed via a gateway and do not use private IP addresses in the same way as interface endpoints. They are simpler but do not provide the same level of network isolation. Security groups are not directly attached to gateway endpoints.
- [ ] C. Using `nslookup` to obtain private IP addresses is not a reliable or scalable solution. IP addresses can change, and this approach does not guarantee that traffic will remain within the AWS network.
- [ ] D. Similar to option C, relying on the `ip-ranges.json` file is not a robust method for ensuring private connectivity. These IP ranges can change, and it does not provide the security and reliability of VPC endpoints.

Therefore, Option A is the most secure and reliable solution for ensuring that data transfer to S3 from an EC2 instance remains within the AWS network and is properly secured.

</details>

<details>
  <summary>Question 209</summary>

A solutions architect is designing the architecture of a new application being deployed to the AWS Cloud. The application will run on Amazon EC2 On-Demand Instances and will automatically scale across multiple Availability Zones. The EC2 instances will scale up and down frequently throughout the day. An Application Load Balancer (ALB) will handle the load distribution. The architecture needs to support distributed session data management. The company is willing to make changes to code if needed. What should the solutions architect do to ensure that the architecture supports distributed session data management?

- [ ] A. Use Amazon ElastiCache to manage and store session data.
- [ ] B. Use session affinity (sticky sessions) of the ALB to manage session data.
- [ ] C. Use Session Manager from AWS Systems Manager to manage the session.
- [ ] D. Use the GetSessionToken API operation in AWS Security Token Service (AWS STS) to manage the session.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Use Amazon ElastiCache to manage and store session data.

Why these are the correct answers:

A. Use Amazon ElastiCache to manage and store session data.

- [ ] Amazon ElastiCache (either Memcached or Redis) provides a distributed, in-memory cache that can be used to store session data.
- [ ] This allows any EC2 instance to retrieve session data, regardless of which instance originally handled the user's request.
- [ ] It supports the dynamic nature of Auto Scaling, as instances can come and go without losing session information.

<hr> Why are the other answers wrong? <hr>

- [ ] B. Session affinity (sticky sessions) can maintain user sessions to specific instances, but it doesn't work well with Auto Scaling. Instances can be terminated, and traffic can be unevenly distributed.
- [ ] C. AWS Systems Manager Session Manager is used for managing EC2 instances, not for storing application session data.
- [ ] D. AWS STS GetSessionToken is for obtaining temporary security credentials, not for session data management.

Therefore, Option A is the most appropriate choice for managing distributed session data in a dynamically scaling environment.

</details>

<details>
  <summary>Question 210</summary>

A company offers a food delivery service that is growing rapidly. Because of the growth, the company's order processing system is experiencing scaling problems during peak traffic hours. The current architecture includes the following:

• A group of Amazon EC2 instances that run in an Amazon EC2 Auto Scaling group to collect orders from the application

• Another group of EC2 instances that run in an Amazon EC2 Auto Scaling group to fulfill orders

The order collection process occurs quickly, but the order fulfillment process can take longer. Data must not be lost because of a scaling event. A solutions architect must ensure that the order collection process and the order fulfillment process can both scale properly during peak traffic hours. The solution must optimize utilization of the company's AWS resources. Which solution meets these requirements?

- [ ] A. Use Amazon CloudWatch metrics to monitor the CPU of each instance in the Auto Scaling groups. Configure each Auto Scaling group's minimum capacity according to peak workload values.
- [ ] B. Use Amazon CloudWatch metrics to monitor the CPU of each instance in the Auto Scaling groups. Configure a CloudWatch alarm to invoke an Amazon Simple Notification Service (Amazon SNS) topic that creates additional Auto Scaling groups on demand.
- [ ] C. Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection and another for order fulfillment. Configure the EC2 instances to poll their respective queue. Scale the Auto Scaling groups based on notifications that the queues send.
- [ ] D. Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection and another for order fulfillment. Configure the EC2 instances to poll their respective queue. Create a metric based on a backlog per instance calculation. Scale the Auto Scaling groups based on this metric.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection and another for order fulfillment. Configure the EC2 instances to poll their respective queue. Create a metric based on a backlog per instance calculation. Scale the Auto Scaling groups based on this metric.

Why these are the correct answers:

D. Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection and another for order fulfillment. Configure the EC2 instances to poll their respective queue. Create a metric based on a backlog per instance calculation. Scale the Auto Scaling groups based on this metric.

- [ ] Using SQS queues decouples the order collection and fulfillment processes, allowing them to scale independently.
- [ ] Scaling based on a backlog per instance calculation ensures that the Auto Scaling groups respond to the actual workload (number of orders waiting to be processed) rather than just CPU utilization.
- [ ] This approach optimizes resource utilization by scaling based on demand and prevents order loss by queuing them.

<hr> Why are the other answers wrong? <hr>

- [ ] A. Configuring minimum capacity based on peak workload values does not optimize resource utilization; it leads to over-provisioning during off-peak hours.
- [ ] B. Scaling based on CPU utilization might not accurately reflect the order processing load. High CPU can be due to other processes, while low CPU can still have a large order backlog. Using SNS to create Auto Scaling groups is not a standard or efficient practice.
- [ ] C. Scaling based on notifications from queues doesn't provide a granular way to manage scaling based on the actual backlog of orders. It might lead to scaling actions that are not finely tuned to the demand.

Therefore, Option D provides the most efficient and accurate solution for scaling the order processing system based on the actual order backlog.

</details>

# AWS-SAA-PRACTICE-EXAM Questions 211-220

<details>
  <summary>Question 211</summary>

A company hosts multiple production applications. One of the applications consists of resources from Amazon EC2, AWS Lambda, Amazon RDS, Amazon Simple Notification Service (Amazon SNS), and Amazon Simple Queue Service (Amazon SQS) across multiple AWS Regions. All company resources are tagged with a tag name of "application" and a value that corresponds to each application. A solutions architect must provide the quickest solution for identifying all of the tagged components. Which solution meets these requirements?

- [ ] A. Use AWS CloudTrail to generate a list of resources with the application tag.
- [ ] B. Use the AWS CLI to query each service across all Regions to report the tagged components.
- [ ] C. Run a query in Amazon CloudWatch Logs Insights to report on the components with the application tag.
- [ ] D. Run a query with the AWS Resource Groups Tag Editor to report on the resources globally with the application tag.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Run a query with the AWS Resource Groups Tag Editor to report on the resources globally with the application tag.

Why these are the correct answers:

D. Run a query with the AWS Resource Groups Tag Editor to report on the resources globally with the application tag.

- [ ] AWS Resource Groups Tag Editor allows you to search for and manage tags across multiple AWS services and Regions in a single view.
- [ ] It is designed to quickly identify resources based on tags, providing the fastest way to meet the requirements.

<hr> Why are the other answers wrong? <hr>

- [ ] A. AWS CloudTrail records API calls but does not provide a direct, efficient way to list resources based on tags.
- [ ] B. Using the AWS CLI to query each service across all Regions would be time-consuming and complex, requiring multiple commands and scripts.
- [ ] C. Amazon CloudWatch Logs Insights is used for querying log data, not for listing AWS resources based on tags.

Therefore, Option D is the most efficient and direct solution for identifying tagged components across multiple services and Regions.

</details>

<details>
  <summary>Question 212</summary>

A company needs to export its database once a day to Amazon S3 for other teams to access. The exported object size varies between 2 GB and 5 GB. The S3 access pattern for the data is variable and changes rapidly. The data must be immediately available and must remain accessible for up to 3 months. The company needs the most cost-effective solution that will not increase retrieval time. Which S3 storage class should the company use to meet these requirements?

- [ ] A. S3 Intelligent-Tiering
- [ ] B. S3 Glacier Instant Retrieval
- [ ] C. S3 Standard
- [ ] D. S3 Standard-Infrequent Access (S3 Standard-IA)

</details>

<details>
  <summary>Answer</summary>

- [ ] A. S3 Intelligent-Tiering

Why these are the correct answers:

A. S3 Intelligent-Tiering

- [ ] S3 Intelligent-Tiering automatically optimizes storage costs by moving data between frequent and infrequent access tiers based on usage patterns.
- [ ] It ensures data is immediately available with no retrieval fees, making it suitable for variable access patterns and the requirement for immediate availability.
- [ ] This provides a cost-effective solution by reducing storage costs when data is accessed less frequently, without sacrificing performance.

<hr> Why are the other answers wrong? <hr>

- [ ] B. S3 Glacier Instant Retrieval is designed for long-term archive with immediate retrieval, but it is generally more expensive for frequently accessed data.
- [ ] C. S3 Standard is suitable for frequently accessed data but does not optimize costs for data with variable access patterns.
- [ ] D. S3 Standard-IA is more cost-effective for infrequently accessed data but has retrieval fees and can be more expensive if data access patterns vary significantly.

Therefore, Option A is the most cost-effective solution that maintains immediate availability for data with variable access patterns.

</details>

<details>
  <summary>Question 213</summary>

A company is developing a new mobile app. The company must implement proper traffic filtering to protect its Application Load Balancer (ALB) against common application-level attacks, such as cross-site scripting or SQL injection. The company has minimal infrastructure and operational staff. The company needs to reduce its share of the responsibility in managing, updating, and securing servers for its AWS environment. What should a solutions architect recommend to meet these requirements?

- [ ] A. Configure AWS WAF rules and associate them with the ALB.
- [ ] B. Deploy the application using Amazon S3 with public hosting enabled.
- [ ] C. Deploy AWS Shield Advanced and add the ALB as a protected resource.
- [ ] D. Create a new ALB that directs traffic to an Amazon EC2 instance running a third-party firewall, which then passes the traffic to the current ALB.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Configure AWS WAF rules and associate them with the ALB.

Why these are the correct answers:

A. Configure AWS WAF rules and associate them with the ALB.

- [ ] AWS WAF is a web application firewall that protects web applications from common web exploits.
- [ ] It integrates directly with the ALB, providing application-level filtering of traffic.
- [ ] AWS WAF is a managed service, reducing the operational overhead for the company.

<hr> Why are the other answers wrong? <hr>

- [ ] B. Deploying the application using Amazon S3 with public hosting enabled does not provide protection against application-level attacks.
- [ ] C. AWS Shield Advanced provides protection against DDoS attacks but does not filter specific application-level attacks like SQL injection or cross-site scripting.
- [ ] D. Creating a new ALB with a third-party firewall on an EC2 instance increases operational overhead and complexity, contradicting the requirement to minimize management.

Therefore, Option A is the most suitable solution to protect against application-level attacks with minimal operational overhead.

</details>

<details>
  <summary>Question 214</summary>

A company's reporting system delivers hundreds of .csv files to an Amazon S3 bucket each day. The company must convert these files to Apache Parquet format and must store the files in a transformed data bucket. Which solution will meet these requirements with the LEAST development effort?

- [ ] A. Create an Amazon EMR cluster with Apache Spark installed. Write a Spark application to transform the data. Use EMR File System (EMRFS) to write files to the transformed data bucket.
- [ ] B. Create an AWS Glue crawler to discover the data. Create an AWS Glue extract, transform, and load (ETL) job to transform the data. Specify the transformed data bucket in the output step.
- [ ] C. Use AWS Batch to create a job definition with Bash syntax to transform the data and output the data to the transformed data bucket. Use the job definition to submit a job. Specify an array job as the job type.
- [ ] D. Create an AWS Lambda function to transform the data and output the data to the transformed data bucket. Configure an event notification for the S3 bucket. Specify the Lambda function as the destination for the event notification.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Create an AWS Glue crawler to discover the data. Create an AWS Glue extract, transform, and load (ETL) job to transform the data. Specify the transformed data bucket in the output step.

Why these are the correct answers:

B. Create an AWS Glue crawler to discover the data. Create an AWS Glue extract, transform, and load (ETL) job to transform the data. Specify the transformed data bucket in the output step.

- [ ] AWS Glue provides a serverless ETL service that simplifies the process of transforming data.
- [ ] AWS Glue crawler can automatically infer the schema of the .csv files.
- [ ] AWS Glue ETL jobs can efficiently transform the data into Parquet format with minimal coding effort.

<hr> Why are the other answers wrong? <hr>

- [ ] A. Creating an Amazon EMR cluster and writing a Spark application involves more development and operational overhead compared to using AWS Glue.
- [ ] C. Using AWS Batch requires defining job definitions and managing dependencies, which is more complex than using AWS Glue for simple ETL tasks.
- [ ] D. Using AWS Lambda for transforming large numbers of files can be complex and may hit execution time limits. It also requires more effort to manage dependencies and scaling.

Therefore, Option B is the most efficient solution with the least development effort for transforming .csv files to Parquet.

</details>

<details>
  <summary>Question 215</summary>

A company has 700 TB of backup data stored in network attached storage (NAS) in its data center. This backup data need to be accessible for infrequent regulatory requests and must be retained 7 years. The company has decided to migrate this backup data from its data center to AWS. The migration must be complete within 1 month. The company has 500 Mbps of dedicated bandwidth on its public internet connection available for data transfer. What should a solutions architect do to migrate and store the data at the LOWEST cost?

- [ ] A. Order AWS Snowball devices to transfer the data. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.
- [ ] B. Deploy a VPN connection between the data center and Amazon VPC. Use the AWS CLI to copy the data from on premises to Amazon S3 Glacier.
- [ ] C. Provision a 500 Mbps AWS Direct Connect connection and transfer the data to Amazon S3. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.
- [ ] D. Use AWS DataSync to transfer the data and deploy a DataSync agent on premises. Use the DataSync task to copy files from the on-premises NAS storage to Amazon S3 Glacier.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Order AWS Snowball devices to transfer the data. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.

Why these are the correct answers:

A. Order AWS Snowball devices to transfer the data. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.

- [ ] AWS Snowball devices are cost-effective for transferring large amounts of data, especially when network bandwidth is limited.
- [ ] Amazon S3 Glacier Deep Archive provides the lowest-cost storage for long-term retention of infrequently accessed data.
- [ ] Using a lifecycle policy automates the transition of data to Glacier Deep Archive, reducing storage costs.

<hr> Why are the other answers wrong? <hr>

- [ ] B. Transferring 700 TB of data over a 500 Mbps internet connection would take a very long time, exceeding the 1-month migration requirement.
- [ ] C. Provisioning an AWS Direct Connect connection is more expensive than using Snowball for a one-time migration of this size.
- [ ] D. AWS DataSync is designed for ongoing data synchronization and is more expensive than Snowball for a large, one-time data migration.

Therefore, Option A is the most cost-effective and time-efficient solution for migrating and storing the backup data.

</details>

<details>
  <summary>Question 216</summary>

A company has a serverless website with millions of objects in an Amazon S3 bucket. The company uses the S3 bucket as the origin for an Amazon CloudFront distribution. The company did not set encryption on the S3 bucket before the objects were loaded. A solutions architect needs to enable encryption for all existing objects and for all objects that are added to the S3 bucket in the future. Which solution will meet these requirements with the LEAST amount of effort?

- [ ] A. Create a new S3 bucket. Turn on the default encryption settings for the new S3 bucket. Download all existing objects to temporary local storage. Upload the objects to the new S3 bucket.
- [ ] B. Turn on the default encryption settings for the S3 bucket. Use the S3 Inventory feature to create a .csv file that lists the unencrypted objects. Run an S3 Batch Operations job that uses the copy command to encrypt those objects.
- [ ] C. Create a new encryption key by using AWS Key Management Service (AWS KMS). Change the settings on the S3 bucket to use server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Turn on versioning for the S3 bucket.
- [ ] D. Navigate to Amazon S3 in the AWS Management Console. Browse the S3 bucket's objects. Sort by the encryption field. Select each unencrypted object. Use the Modify button to apply default encryption settings to every unencrypted object in the S3 bucket.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Turn on the default encryption settings for the S3 bucket. Use the S3 Inventory feature to create a .csv file that lists the unencrypted objects. Run an S3 Batch Operations job that uses the copy command to encrypt those objects.

Why these are the correct answers:

B. Turn on the default encryption settings for the S3 bucket. Use the S3 Inventory feature to create a .csv file that lists the unencrypted objects. Run an S3 Batch Operations job that uses the copy command to encrypt those objects.

- [ ] S3 Batch Operations can efficiently encrypt existing objects at scale.
- [ ] Using S3 Inventory simplifies the process of identifying unencrypted objects.
- [ ] This approach minimizes the effort required to encrypt a large number of existing objects.
- [ ] Setting default encryption ensures that future objects are automatically encrypted.

<hr> Why are the other answers wrong? <hr>

- [ ] A. Creating a new bucket and copying all objects is time-consuming and involves more steps than using S3 Batch Operations.
- [ ] C. While creating a KMS key and setting default encryption is a valid approach for future objects, it does not efficiently encrypt existing objects.
- [ ] D. Manually encrypting objects through the AWS Management Console is impractical for millions of objects and requires significant manual effort.

Therefore, Option B is the most efficient solution for encrypting both existing and future objects with the least amount of effort.

</details>
<details>
  <summary>Question 217</summary>

A company runs a global web application on Amazon EC2 instances behind an Application Load Balancer. The application stores data in Amazon Aurora. The company needs to create a disaster recovery solution and can tolerate up to 30 minutes of downtime and potential data loss. The solution does not need to handle the load when the primary infrastructure is healthy. What should a solutions architect do to meet these requirements?

- [ ] A. Deploy the application with the required infrastructure elements in place. Use Amazon Route 53 to configure active-passive failover. Create an Aurora Replica in a second AWS Region.
- [ ] B. Host a scaled-down deployment of the application in a second AWS Region. Use Amazon Route 53 to configure active-active failover. Create an Aurora Replica in the second Region.
- [ ] C. Replicate the primary infrastructure in a second AWS Region. Use Amazon Route 53 to configure active-active failover. Create an Aurora database that is restored from the latest snapshot.
- [ ] D. Back up data with AWS Backup. Use the backup to create the required infrastructure in a second AWS Region. Use Amazon Route 53 to configure active-passive failover. Create an Aurora second primary instance in the second Region.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Deploy the application with the required infrastructure elements in place. Use Amazon Route 53 to configure active-passive failover. Create an Aurora Replica in a second AWS Region.

Why these are the correct answers:

A. Deploy the application with the required infrastructure elements in place. Use Amazon Route 53 to configure active-passive failover. Create an Aurora Replica in a second AWS Region.

- [ ] Active-passive failover with Route 53 allows for automatic redirection of traffic to a standby region in case of a failure.
- [ ] An Aurora Replica in a second region provides a ready-to-go database that can take over quickly.
- [ ] This solution meets the requirements for disaster recovery with a tolerance for some downtime and data loss.

<hr> Why are the other answers wrong? <hr>

- [ ] B. Active-active failover is designed for high availability, not disaster recovery, and involves running a scaled-down deployment, which is not required by the scenario.
- [ ] C. Replicating the entire primary infrastructure in a second region is more costly and complex than necessary for disaster recovery. Restoring from a snapshot can increase downtime.
- [ ] D. Using AWS Backup and restoring the infrastructure increases downtime and potential data loss compared to having a pre-configured replica. Aurora doesn't have a "second primary instance" in the same way as some other databases.

Therefore, Option A is the most suitable solution for a cost-effective disaster recovery plan with the specified recovery time objective (RTO).

</details>
<details>
  <summary>Question 218</summary>

A company has a web server running on an Amazon EC2 instance in a public subnet with an Elastic IP address. The default security group is assigned to the EC2 instance. The default network ACL has been modified to block all traffic. A solutions architect needs to make the web server accessible from everywhere on port 443. Which combination of steps will accomplish this task? (Choose two.)

- [ ] A. Create a security group with a rule to allow TCP port 443 from source 0.0.0.0/0.
- [ ] B. Create a security group with a rule to allow TCP port 443 to destination 0.0.0.0/0.
- [ ] C. Update the network ACL to allow TCP port 443 from source 0.0.0.0/0.
- [ ] D. Update the network ACL to allow inbound/outbound TCP port 443 from source 0.0.0.0/0 and to destination 0.0.0.0/0.
- [ ] E. Update the network ACL to allow inbound TCP port 443 from source 0.0.0.0/0 and outbound TCP port 32768-65535 to destination 0.0.0.0/0.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Create a security group with a rule to allow TCP port 443 from source 0.0.0.0/0.
- [ ] E. Update the network ACL to allow inbound TCP port 443 from source 0.0.0.0/0 and outbound TCP port 32768-65535 to destination 0.0.0.0/0.

Why these are the correct answers:

A. Create a security group with a rule to allow TCP port 443 from source 0.0.0.0/0.

- [ ] Security groups act as a virtual firewall for EC2 instances and must allow inbound traffic on port 443.
- [ ] 0.0.0.0/0 allows traffic from any IP address.

E. Update the network ACL to allow inbound TCP port 443 from source 0.0.0.0/0 and outbound TCP port 32768-65535 to destination 0.0.0.0/0.

- [ ] Network ACLs act as a firewall for the subnet and must also allow both inbound traffic on port 443 and outbound traffic on ephemeral ports (32768-65535) for the server's responses.

<hr> Why are the other answers wrong? <hr>

- [ ] B. Security group rules specify allowed traffic, not destination.
- [ ] C. Network ACLs need to allow both inbound and outbound traffic.
- [ ] D. Network ACLs require ephemeral port ranges to be open for responses.

Therefore, options A and E are necessary to allow inbound HTTPS traffic and the corresponding outbound responses.

</details>
<details>
  <summary>Question 219</summary>

A company's application is having performance issues. The application is stateful and needs to complete in-memory tasks on Amazon EC2 instances. The company used AWS CloudFormation to deploy infrastructure and used the M5 EC2 instance family. As traffic increased, the application performance degraded. Users are reporting delays when the users attempt to access the application. Which solution will resolve these issues in the MOST operationally efficient way?

- [ ] A. Replace the EC2 instances with T3 EC2 instances that run in an Auto Scaling group. Make the changes by using the AWS Management Console.
- [ ] B. Modify the CloudFormation templates to run the EC2 instances in an Auto Scaling group. Increase the desired capacity and the maximum capacity of the Auto Scaling group manually when an increase is necessary.
- [ ] C. Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Use Amazon CloudWatch built-in EC2 memory metrics to track the application performance for future capacity planning.
- [ ] D. Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Deploy the Amazon CloudWatch agent on the EC2 instances to generate custom application latency metrics for future capacity planning.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Deploy the Amazon CloudWatch agent on the EC2 instances to generate custom application latency metrics for future capacity planning.

Why these are the correct answers:

D. Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Deploy the Amazon CloudWatch agent on the EC2 instances to generate custom application latency metrics for future capacity planning.

- [ ] R5 instances are memory-optimized and suitable for in-memory tasks, addressing the application's needs.
- [ ] CloudFormation allows for infrastructure as code, enabling repeatable and manageable deployments.
- [ ] Custom latency metrics provide detailed insights into application performance, aiding in capacity planning.

<hr> Why are the other answers wrong? <hr>

- [ ] A. T3 instances are burstable and not ideal for consistent memory-intensive workloads. Making changes via the console is not operationally efficient.
- [ ] B. Manually adjusting Auto Scaling group capacity is not efficient and does not fully automate scaling.
- [ ] C. Built-in EC2 memory metrics might not provide the specific application-level latency metrics needed for detailed performance analysis.

Therefore, Option D provides the best combination of performance, scalability, and operational efficiency.

</details>

<details>
  <summary>Question 220</summary>

A solutions architect is designing a new API using Amazon API Gateway that will receive requests from users. The volume of requests is highly variable; several hours can pass without receiving a single request. The data processing will take place asynchronously but should be completed within a few seconds after a request is made. Which compute service should the solutions architect have the API invoke to deliver the requirements at the lowest cost?

- [ ] A. An AWS Glue job
- [ ] B. An AWS Lambda function
- [ ] C. A containerized service hosted in Amazon Elastic Kubernetes Service (Amazon EKS)
- [ ] D. A containerized service hosted in Amazon ECS with Amazon EC2

</details>

<details>
  <summary>Answer</summary>

- [ ] B. An AWS Lambda function

Why these are the correct answers:

B. An AWS Lambda function

- [ ] AWS Lambda is a serverless compute service that charges only for the compute time consumed.
- [ ] It scales automatically and can handle variable workloads, including periods of inactivity.
- [ ] Lambda functions can execute quickly and are suitable for asynchronous processing.

<hr> Why are the other answers wrong? <hr>

- [ ] A. AWS Glue jobs are designed for ETL (extract, transform, load) operations and are not suitable for real-time, low-latency processing.
- [ ] C. Amazon EKS involves running and managing container orchestration, which is more expensive and complex than Lambda for this use case.
- [ ] D. Amazon ECS with EC2 also requires managing infrastructure and is more expensive than Lambda for event-driven, variable workloads.

Therefore, Option B provides the most cost-effective solution for the given requirements.

</details>

# AWS-SAA-PRACTICE-EXAM Questions 221-230

<details>
  <summary>Question 221</summary>

A company runs an application on a group of Amazon Linux EC2 instances. For compliance reasons, the company must retain all application log files for 7 years. The log files will be analyzed by a reporting tool that must be able to access all the files concurrently. Which storage solution meets these requirements MOST cost-effectively?

- [ ] A. Amazon Elastic Block Store (Amazon EBS)
- [ ] B. Amazon Elastic File System (Amazon EFS)
- [ ] C. Amazon EC2 instance store
- [ ] D. Amazon S3

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Amazon S3

Why these are the correct answers:

D. Amazon S3

-   [ ] Amazon S3 provides highly durable and cost-effective storage for long-term retention.
-   [ ] It allows concurrent access to files by multiple tools and users.
-   [ ] S3's scalability and cost-effectiveness make it suitable for storing large volumes of log data for extended periods.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Amazon EBS volumes are block storage and are attached to a single EC2 instance, making concurrent access difficult and costly for long-term storage.
-   [ ] B. Amazon EFS is a file storage service for use with EC2 instances, but it is more expensive than S3 for long-term archival of log files.
-   [ ] C. Amazon EC2 instance store provides temporary block storage that is ephemeral and not suitable for long-term retention or concurrent access.

Therefore, Option D is the most cost-effective solution for long-term storage and concurrent access to log files.

</details>

<details>
  <summary>Question 222</summary>

A company has hired an external vendor to perform work in the company's AWS account. The vendor uses an automated tool that is hosted in an AWS account that the vendor owns. The vendor does not have IAM access to the company's AWS account. How should a solutions architect grant this access to the vendor?

- [ ] A. Create an IAM role in the company's account to delegate access to the vendor's IAM role. Attach the appropriate IAM policies to the role for the permissions that the vendor requires.
- [ ] B. Create an IAM user in the company's account with a password that meets the password complexity requirements. Attach the appropriate IAM policies to the user for the permissions that the vendor requires.
- [ ] C. Create an IAM group in the company's account. Add the tool's IAM user from the vendor account to the group. Attach the appropriate IAM policies to the group for the permissions that the vendor requires.
- [ ] D. Create a new identity provider by choosing "AWS account" as the provider type in the IAM console. Supply the vendor's AWS account ID and user name. Attach the appropriate IAM policies to the new provider for the permissions that the vendor requires.
   
</details>

<details>
  <summary>Answer</summary>

- [ ] A. Create an IAM role in the company's account to delegate access to the vendor's IAM role. Attach the appropriate IAM policies to the role for the permissions that the vendor requires.
   
Why these are the correct answers:

A. Create an IAM role in the company's account to delegate access to the vendor's IAM role. Attach the appropriate IAM policies to the role for the permissions that the vendor requires.

- [ ] Using IAM roles allows you to grant permissions to entities in other AWS accounts without sharing credentials.
- [ ] This approach follows the principle of least privilege by granting only the necessary permissions.
- [ ] It provides a secure and auditable way to manage external access.
   
<hr> Why are the other answers wrong? <hr>

- [ ] B. Creating an IAM user for the vendor is less secure and harder to manage, as it involves sharing credentials.
- [ ] C. IAM groups are used to manage permissions for multiple users within the same account, not for external access.
- [ ] D. Creating an identity provider for another AWS account is not the correct way to delegate access; it's typically used for external identity providers like SAML.

Therefore, Option A is the most secure and appropriate method for granting access to a vendor's tool in another AWS account.

</details>
<details>
  <summary>Question 223</summary>

A company has deployed a Java Spring Boot application as a pod that runs on Amazon Elastic Kubernetes Service (Amazon EKS) in private subnets. The application needs to write data to an Amazon DynamoDB table. A solutions architect must ensure that the application can interact with the DynamoDB table without exposing traffic to the internet. Which combination of steps should the solutions architect take to accomplish this goal? (Choose two.)

- [ ] A. Attach an IAM role that has sufficient privileges to the EKS pod.
- [ ] B. Attach an IAM user that has sufficient privileges to the EKS pod.
- [ ] C. Allow outbound connectivity to the DynamoDB table through the private subnets' network ACLs.
- [ ] D. Create a VPC endpoint for DynamoDB.
- [ ] E. Embed the access keys in the Java Spring Boot code.
   
</details>

<details>
  <summary>Answer</summary>

- [ ] A. Attach an IAM role that has sufficient privileges to the EKS pod.
- [ ] D. Create a VPC endpoint for DynamoDB.
   
Why these are the correct answers:

A. Attach an IAM role that has sufficient privileges to the EKS pod.

- [ ] IAM roles provide secure authentication and authorization for AWS services.
- [ ] Attaching an IAM role to the EKS pod allows the application to access DynamoDB without using access keys.
   
D. Create a VPC endpoint for DynamoDB.

- [ ] VPC endpoints enable private connectivity to DynamoDB without traversing the internet.
- [ ] This ensures that the traffic remains within the AWS network, enhancing security.
   
<hr> Why are the other answers wrong? <hr>

- [ ] B. Attaching an IAM user to the EKS pod is incorrect; IAM users are for people, not applications.
- [ ] C. Network ACLs control traffic at the subnet level but do not provide private connectivity like VPC endpoints.
- [ ] E. Embedding access keys in the code is a security risk and should be avoided.

Therefore, options A and D are the correct steps to securely access DynamoDB from an EKS application in private subnets.

</details>
<details>
  <summary>Question 224</summary>

A company recently migrated its web application to AWS by rehosting the application on Amazon EC2 instances in a single AWS Region. The company wants to redesign its application architecture to be highly available and fault tolerant. Traffic must reach all running EC2 instances randomly. Which combination of steps should the company take to meet these requirements? (Choose two.)

- [ ] A. Create an Amazon Route 53 failover routing policy.
- [ ] B. Create an Amazon Route 53 weighted routing policy.
- [ ] C. Create an Amazon Route 53 multivalue answer routing policy.
- [ ] D. Launch three EC2 instances: two instances in one Availability Zone and one instance in another Availability Zone.
- [ ] E. Launch four EC2 instances: two instances in one Availability Zone and two instances in another Availability Zone.
   
</details>

<details>
  <summary>Answer</summary>

- [ ] C. Create an Amazon Route 53 multivalue answer routing policy.
- [ ] E. Launch four EC2 instances: two instances in one Availability Zone and two instances in another Availability Zone.
   
Why these are the correct answers:

C. Create an Amazon Route 53 multivalue answer routing policy.

- [ ] Multivalue answer routing returns multiple IP addresses in response to DNS queries.
- [ ] This helps distribute traffic to multiple EC2 instances and improves availability.
   
E. Launch four EC2 instances: two instances in one Availability Zone and two instances in another Availability Zone.

- [ ] Distributing instances across multiple Availability Zones ensures fault tolerance.
- [ ] If one Availability Zone becomes unavailable, the application continues to run in the other zones.
   
<hr> Why are the other answers wrong? <hr>

- [ ] A. Failover routing is used for disaster recovery, not for distributing traffic to healthy instances.
- [ ] B. Weighted routing distributes traffic based on assigned weights, not randomly.
- [ ] D. Launching three instances does not provide even distribution across two Availability Zones for fault tolerance.

Therefore, options C and E are the correct steps to achieve high availability, fault tolerance, and random traffic distribution.

</details>
<details>
  <summary>Question 225</summary>

A media company collects and analyzes user activity data on premises. The company wants to migrate this capability to AWS. The user activity data store will continue to grow and will be petabytes in size. The company needs to build a highly available data ingestion solution that facilitates on-demand analytics of existing data and new data with SQL. Which solution will meet these requirements with the LEAST operational overhead?

- [ ] A. Send activity data to an Amazon Kinesis data stream. Configure the stream to deliver the data to an Amazon S3 bucket.
- [ ] B. Send activity data to an Amazon Kinesis Data Firehose delivery stream. Configure the stream to deliver the data to an Amazon Redshift cluster.
- [ ] C. Place activity data in an Amazon S3 bucket. Configure Amazon S3 to run an AWS Lambda function on the data as the data arrives in the S3 bucket.
- [ ] D. Create an ingestion service on Amazon EC2 instances that are spread across multiple Availability Zones. Configure the service to forward data to an Amazon RDS Multi-AZ database.
   
</details>

<details>
  <summary>Answer</summary>

- [ ] B. Send activity data to an Amazon Kinesis Data Firehose delivery stream. Configure the stream to deliver the data to an Amazon Redshift cluster.
   
Why these are the correct answers:

B. Send activity data to an Amazon Kinesis Data Firehose delivery stream. Configure the stream to deliver the data to an Amazon Redshift cluster.

- [ ] Kinesis Data Firehose can efficiently ingest large volumes of streaming data and deliver it to Amazon Redshift.
- [ ] Amazon Redshift is a data warehouse that supports SQL queries for analytics.
- [ ] This solution provides a managed, scalable, and cost-effective way to ingest and analyze petabytes of data.
   
<hr> Why are the other answers wrong? <hr>

- [ ] A. Kinesis Data Streams require more management for data processing and analysis compared to Firehose and Redshift.
- [ ] C. Using Lambda functions to process data from S3 as it arrives is less efficient for large-scale data ingestion and analytics.
- [ ] D. Managing an ingestion service on EC2 instances and using Amazon RDS is more operationally intensive and less scalable for petabytes of data.

Therefore, Option B is the most suitable solution for efficient, scalable, and managed data ingestion and analytics.

</details>
<details>
  <summary>Question 226</summary>

A company collects data from thousands of remote devices by using a RESTful web services application that runs on an Amazon EC2 instance. The EC2 instance receives the raw data, transforms the raw data, and stores all the data in an Amazon S3 bucket. The number of remote devices will increase into the millions soon. The company needs a highly scalable solution that minimizes operational overhead. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

- [ ] A. Use AWS Glue to process the raw data in Amazon S3.
- [ ] B. Use Amazon Route 53 to route traffic to different EC2 instances.
- [ ] C. Add more EC2 instances to accommodate the increasing amount of incoming data.
- [ ] D. Send the raw data to Amazon Simple Queue Service (Amazon SQS). Use EC2 instances to process the data.
- [ ] E. Use Amazon API Gateway to send the raw data to an Amazon Kinesis data stream. Configure Amazon Kinesis Data Firehose to use the data stream as a source to deliver the data to Amazon S3.
   
</details>

<details>
  <summary>Answer</summary>

- [ ] A. Use AWS Glue to process the raw data in Amazon S3.
- [ ] E. Use Amazon API Gateway to send the raw data to an Amazon Kinesis data stream. Configure Amazon Kinesis Data Firehose to use the data stream as a source to deliver the data to Amazon S3.
   
Why these are the correct answers:

A. Use AWS Glue to process the raw data in Amazon S3.

- [ ] AWS Glue is a serverless ETL service that can efficiently process and transform data in S3.
- [ ] It reduces operational overhead by managing the infrastructure for data processing.
   
E. Use Amazon API Gateway to send the raw data to an Amazon Kinesis data stream. Configure Amazon Kinesis Data Firehose to use the data stream as a source to deliver the data to Amazon S3.

- [ ] API Gateway can handle a large number of incoming requests and send data to Kinesis.
- [ ] Kinesis Data Firehose can efficiently ingest and deliver streaming data to S3, scaling automatically.
- [ ] This combination provides a scalable and managed solution for data ingestion and processing.
   
<hr> Why are the other answers wrong? <hr>

- [ ] B. Using Amazon Route 53 to route traffic to different EC2 instances does not address the data processing bottleneck.
- [ ] C. Adding more EC2 instances increases operational overhead and may not scale efficiently.
- [ ] D. Using Amazon SQS and EC2 instances for data processing still requires managing EC2 instances and scaling them.

Therefore, options A and E provide the most scalable and operationally efficient solution.

</details>
<details>
  <summary>Question 227</summary>

A company needs to retain its AWS CloudTrail logs for 3 years. The company is enforcing CloudTrail across a set of AWS accounts by using AWS Organizations from the parent account. The CloudTrail target S3 bucket is configured with S3 Versioning enabled. An S3 Lifecycle policy is in place to delete current objects after 3 years. After the fourth year of use of the S3 bucket, the S3 bucket metrics show that the number of objects has continued to rise. However, the number of new CloudTrail logs that are delivered to the S3 bucket has remained consistent. Which solution will delete objects that are older than 3 years in the MOST cost-effective manner?

- [ ] A. Configure the organization's centralized CloudTrail trail to expire objects after 3 years.
- [ ] B. Configure the S3 Lifecycle policy to delete previous versions as well as current versions.
- [ ] C. Create an AWS Lambda function to enumerate and delete objects from Amazon S3 that are older than 3 years.
- [ ] D. Configure the parent account as the owner of all objects that are delivered to the S3 bucket.
   
</details>

<details>
  <summary>Answer</summary>

- [ ] B. Configure the S3 Lifecycle policy to delete previous versions as well as current versions.
   
Why these are the correct answers:

B. Configure the S3 Lifecycle policy to delete previous versions as well as current versions.

- [ ] S3 Lifecycle policies can manage object expiration, including both current and previous versions.
- [ ] This is the most cost-effective way to automate the deletion of old log files.
   
<hr> Why are the other answers wrong? <hr>

- [ ] A. CloudTrail does not have a feature to directly expire objects in S3.
- [ ] C. Using a Lambda function is more complex and costly than using S3 Lifecycle policies.
- [ ] D. Configuring the parent account as the owner does not address the deletion of old objects.

Therefore, Option B is the most efficient and cost-effective solution.

</details>
<details>
  <summary>Question 228</summary>

A company has an API that receives real-time data from a fleet of monitoring devices. The API stores this data in an Amazon RDS DB instance for later analysis. The amount of data that the monitoring devices send to the API fluctuates. During periods of heavy traffic, the API often returns timeout errors. After an inspection of the logs, the company determines that the database is not capable of processing the volume of write traffic that comes from the API. A solutions architect must minimize the number of connections to the database and must ensure that data is not lost during periods of heavy traffic. Which solution will meet these requirements?

- [ ] A. Increase the size of the DB instance to an instance type that has more available memory.
- [ ] B. Modify the DB instance to be a Multi-AZ DB instance. Configure the application to write to all active RDS DB instances.
- [ ] C. Modify the API to write incoming data to an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function that Amazon SQS invokes to write data from the queue to the database.
- [ ] D. Modify the API to write incoming data to an Amazon Simple Notification Service (Amazon SNS) topic. Use an AWS Lambda function that Amazon SNS invokes to write data from the topic to the database.
   
</details>

<details>
  <summary>Answer</summary>

- [ ] C. Modify the API to write incoming data to an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function that Amazon SQS invokes to write data from the queue to the database.
   
Why these are the correct answers:

C. Modify the API to write incoming data to an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function that Amazon SQS invokes to write data from the queue to the database.

- [ ] Amazon SQS queues can buffer incoming data, preventing the database from being overwhelmed during peak traffic.
- [ ] This approach minimizes the number of connections to the database, as the Lambda function can process messages in batches.
- [ ] It also ensures that no data is lost, as SQS retains messages until they are successfully processed.
   
<hr> Why are the other answers wrong? <hr>

- [ ] A. Increasing the instance size might provide temporary relief but does not address the issue of fluctuating traffic and potential database overload.
- [ ] B. Multi-AZ is for high availability, not for scaling write capacity. Writing to all active instances is not a standard RDS configuration.
- [ ] D. Amazon SNS is for pub/sub messaging and does not provide the queuing and buffering capabilities of SQS.

Therefore, Option C is the most suitable solution for handling fluctuating write traffic and preventing database overload.

</details>
<details>
  <summary>Question 229</summary>

A company manages its own Amazon EC2 instances that run MySQL databases. The company is manually managing replication and scaling as demand increases or decreases. The company needs a new solution that simplifies the process of adding or removing compute capacity to or from its database tier as needed. The solution also must offer improved performance, scaling, and durability with minimal effort from operations. Which solution meets these requirements?

- [ ] A. Migrate the databases to Amazon Aurora Serverless for Aurora MySQL.
- [ ] B. Migrate the databases to Amazon Aurora Serverless for Aurora PostgreSQL.
- [ ] C. Combine the databases into one larger MySQL database. Run the larger database on larger EC2 instances.
- [ ] D. Create an EC2 Auto Scaling group for the database tier. Migrate the existing databases to the new environment.
   
</details>

<details>
  <summary>Answer</summary>

- [ ] A. Migrate the databases to Amazon Aurora Serverless for Aurora MySQL.
   
Why these are the correct answers:

A. Migrate the databases to Amazon Aurora Serverless for Aurora MySQL.

- [ ] Amazon Aurora Serverless automatically scales database capacity based on application needs.
- [ ] It simplifies database management and provides high performance and durability.
- [ ] Aurora Serverless minimizes operational effort by automating scaling and management tasks.
   
<hr> Why are the other answers wrong? <hr>

- [ ] B. While Aurora Serverless for PostgreSQL is also a valid option, the question specifies MySQL, making this less appropriate.
- [ ] C. Combining databases into a larger EC2 instance does not simplify scaling and requires manual management.
- [ ] D. Using EC2 Auto Scaling for databases is complex and does not provide the same level of automation as Aurora Serverless.

Therefore, Option A is the best solution for simplifying database scaling and management.

</details>
<details>
  <summary>Question 230</summary>

A company is concerned that two NAT instances in use will no longer be able to support the traffic needed for the company's application. A solutions architect wants to implement a solution that is highly available, fault tolerant, and automatically scalable. What should the solutions architect recommend?

- [ ] A. Remove the two NAT instances and replace them with two NAT gateways in the same Availability Zone.
- [ ] B. Use Auto Scaling groups with Network Load Balancers for the NAT instances in different Availability Zones.
- [ ] C. Remove the two NAT instances and replace them with two NAT gateways in different Availability Zones.
- [ ] D. Replace the two NAT instances with Spot Instances in different Availability Zones and deploy a Network Load Balancer.
   
</details>

<details>
  <summary>Answer</summary>

- [ ] C. Remove the two NAT instances and replace them with two NAT gateways in different Availability Zones.
   
Why these are the correct answers:

C. Remove the two NAT instances and replace them with two NAT gateways in different Availability Zones.

- [ ] NAT gateways are managed services that provide better availability and scalability than NAT instances.
- [ ] Deploying NAT gateways in different Availability Zones ensures fault tolerance.
- [ ] NAT gateways automatically scale to handle increased traffic.
   
<hr> Why are the other answers wrong? <hr>

- [ ] A. Deploying NAT gateways in the same Availability Zone does not provide fault tolerance.
- [ ] B. Using Auto Scaling groups with Network Load Balancers for NAT instances is more complex and less efficient than using NAT gateways.
- [ ] D. Replacing the two NAT instances with Spot Instances in different Availability Zones and deploying a Network Load Balancer.

Therefore, Option C is the most reliable and scalable solution.

</details>

# AWS-SAA-PRACTICE-EXAM Questions 231-240

<details>
  <summary>Question 231</summary>

An application runs on an Amazon EC2 instance that has an Elastic IP address in VPC A. The application requires access to a database in VPC B. Both VPCs are in the same AWS account. Which solution will provide the required access MOST securely?

-   [ ] A. Create a DB instance security group that allows all traffic from the public IP address of the application server in VPC A.
-   [ ] B. Configure a VPC peering connection between VPC A and VPC B.
-   [ ] C. Make the DB instance publicly accessible. Assign a public IP address to the DB instance.
-   [ ] D. Launch an EC2 instance with an Elastic IP address into VPC B. Proxy all requests through the new EC2 instance.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Configure a VPC peering connection between VPC A and VPC B.
   
Why these are the correct answers:

B. Configure a VPC peering connection between VPC A and VPC B.

-   [ ] VPC peering enables private connectivity between two VPCs.
-   [ ] It allows the application in VPC A to access the database in VPC B as if they were in the same network.
-   [ ] This solution provides secure and private access without exposing traffic to the internet.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] A. Allowing all traffic from the public IP address of the application server is less secure than using VPC peering.
-   [ ] C. Making the DB instance publicly accessible exposes it to the internet, which is a security risk.
-   [ ] D. Proxying requests through another EC2 instance adds complexity and does not provide the same level of security and simplicity as VPC peering.

Therefore, Option B is the most secure solution for enabling access between VPCs.

</details>
<details>
  <summary>Question 232</summary>

A company runs demonstration environments for its customers on Amazon EC2 instances. Each environment is isolated in its own VPC. The company's operations team needs to be notified when RDP or SSH access to an environment has been established.

-   [ ] A. Configure Amazon CloudWatch Application Insights to create AWS Systems Manager OpsItems when RDP or SSH access is detected.
-   [ ] B. Configure the EC2 instances with an IAM instance profile that has an IAM role with the AmazonSSMManagedInstanceCore policy attached.
-   [ ] C. Publish VPC flow logs to Amazon CloudWatch Logs. Create required metric filters. Create an Amazon CloudWatch metric alarm with a notification action for when the alarm is in the ALARM state.
-   [ ] D. Configure an Amazon EventBridge rule to listen for events of type EC2 Instance State-change Notification. Configure an Amazon Simple Notification Service (Amazon SNS) topic as a target. Subscribe the operations team to the topic.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Publish VPC flow logs to Amazon CloudWatch Logs. Create required metric filters. Create an Amazon CloudWatch metric alarm with a notification action for when the alarm is in the ALARM state.
   
Why these are the correct answers:

C. Publish VPC flow logs to Amazon CloudWatch Logs. Create required metric filters. Create an Amazon CloudWatch metric alarm with a notification action for when the alarm is in the ALARM state.

-   [ ] VPC flow logs capture information about the IP traffic going to and from network interfaces in your VPC.
-   [ ] By analyzing flow logs, you can detect when RDP (port 3389) or SSH (port 22) connections are established.
-   [ ] CloudWatch metric alarms can then notify the operations team.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] A. CloudWatch Application Insights is designed for monitoring application health, not for detecting specific access patterns like RDP or SSH.
-   [ ] B. IAM instance profiles with the AmazonSSMManagedInstanceCore policy are for AWS Systems Manager, not for monitoring network access.
-   [ ] D. EC2 Instance State-change Notification events do not provide information about the traffic to the instances.

Therefore, Option C is the most appropriate solution for monitoring and alerting on RDP and SSH access.

</details>
<details>
  <summary>Question 233</summary>

A solutions architect has created a new AWS account and must secure AWS account root user access. Which combination of actions will accomplish this? (Choose two.)

-   [ ] A. Ensure the root user uses a strong password.
-   [ ] B. Enable multi-factor authentication to the root user.
-   [ ] C. Store root user access keys in an encrypted Amazon S3 bucket.
-   [ ] D. Add the root user to a group containing administrative permissions.
-   [ ] E. Apply the required permissions to the root user with an inline policy document.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Ensure the root user uses a strong password.
-   [ ] B. Enable multi-factor authentication to the root user.
   
Why these are the correct answers:

A. Ensure the root user uses a strong password.

-   [ ] A strong password is a fundamental security practice.
   
B. Enable multi-factor authentication to the root user.

-   [ ] MFA adds an extra layer of security by requiring a second form of verification.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] C. Storing root user access keys is a security risk. Root user access keys should be avoided altogether.
-   [ ] D. Adding the root user to a group is unnecessary as the root user inherently has full administrative permissions.
-   [ ] E. Applying permissions with an inline policy is redundant for the root user, who already has full permissions.

Therefore, options A and B are the most important security measures for the root user.

</details>
<details>
  <summary>Question 234</summary>

A company is building a new web-based customer relationship management application. The application will use several Amazon EC2 instances that are backed by Amazon Elastic Block Store (Amazon EBS) volumes behind an Application Load Balancer (ALB). The application will also use an Amazon Aurora database. All data for the application must be encrypted at rest and in transit. Which solution will meet these requirements?

-   [ ] A. Use AWS Key Management Service (AWS KMS) certificates on the ALB to encrypt data in transit. Use AWS Certificate Manager (ACM) to encrypt the EBS volumes and Aurora database storage at rest.
-   [ ] B. Use the AWS root account to log in to the AWS Management Console. Upload the company's encryption certificates. While in the root account, select the option to turn on encryption for all data at rest and in transit for the account.
-   [ ] C. Use AWS Key Management Service (AWS KMS) to encrypt the EBS volumes and Aurora database storage at rest. Attach an AWS Certificate Manager (ACM) certificate to the ALB to encrypt data in transit.
-   [ ] D. Use BitLocker to encrypt all data at rest. Import the company's TLS certificate keys to AWS Key Management Service (AWS KMS) Attach the KMS keys to the ALB to encrypt data in transit.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Use AWS Key Management Service (AWS KMS) to encrypt the EBS volumes and Aurora database storage at rest. Attach an AWS Certificate Manager (ACM) certificate to the ALB to encrypt data in transit.
   
Why these are the correct answers:

C. Use AWS Key Management Service (AWS KMS) to encrypt the EBS volumes and Aurora database storage at rest. Attach an AWS Certificate Manager (ACM) certificate to the ALB to encrypt data in transit.

-   [ ] AWS KMS is used to encrypt data at rest for EBS volumes and Aurora.
-   [ ] AWS Certificate Manager (ACM) certificates are used to encrypt data in transit via the ALB.
-   [ ] This solution provides encryption for both data at rest and in transit.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] A. KMS is for encryption keys, not certificates. ACM is not used to encrypt EBS or Aurora storage.
-   [ ] B. Using the root account to manage encryption is a poor security practice. There is no single option to turn on encryption for all data at rest and in transit.
-   [ ] D. BitLocker is for Windows systems, not for EBS or Aurora. KMS is not used to attach TLS certificate keys directly to the ALB.

Therefore, Option C is the correct solution for encrypting data at rest and in transit.

</details>
<details>
  <summary>Question 235</summary>

A company is moving its on-premises Oracle database to Amazon Aurora PostgreSQL. The database has several applications that write to the same tables. The applications need to be migrated one by one with a month in between each migration. Management has expressed concerns that the database has a high number of reads and writes. The data must be kept in sync across both databases throughout the migration. What should a solutions architect recommend?

-   [ ] A. Use AWS DataSync for the initial migration. Use AWS Database Migration Service (AWS DMS) to create a change data capture (CDC) replication task and a table mapping to select all tables.
-   [ ] B. Use AWS DataSync for the initial migration. Use AWS Database Migration Service (AWS DMS) to create a full load plus change data capture (CDC) replication task and a table mapping to select all tables.
-   [ ] C. Use the AWS Schema Conversion Tool with AWS Database Migration Service (AWS DMS) using a memory optimized replication instance. Create a full load plus change data capture (CDC) replication task and a table mapping to select all tables.
-   [ ] D. Use the AWS Schema Conversion Tool with AWS Database Migration Service (AWS DMS) using a compute optimized replication instance. Create a full load plus change data capture (CDC) replication task and a table mapping to select the largest tables.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Use the AWS Schema Conversion Tool with AWS Database Migration Service (AWS DMS) using a memory optimized replication instance. Create a full load plus change data capture (CDC) replication task and a table mapping to select all tables.
   
Why these are the correct answers:

C. Use the AWS Schema Conversion Tool with AWS Database Migration Service (AWS DMS) using a memory optimized replication instance. Create a full load plus change data capture (CDC) replication task and a table mapping to select all tables.

-   [ ] AWS Schema Conversion Tool (SCT) helps convert the source database schema to the target.
-   [ ] AWS Database Migration Service (DMS) with Change Data Capture (CDC) keeps the databases in sync during migration.
-   [ ] A memory-optimized DMS instance is suitable for handling high read/write activity.
-   [ ] Mapping all tables ensures that all data is migrated and kept in sync.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] A. AWS DataSync is for data migration, not database migration and synchronization.
-   [ ] B. Using DataSync is incorrect for database migration.
-   [ ] D. A compute-optimized instance is less suitable for high memory workloads, and selecting only the largest tables is insufficient.

Therefore, Option C is the correct solution for migrating the database while keeping data in sync and handling high read/write activity.

</details>
<details>
  <summary>Question 236</summary>

A company has a three-tier application for image sharing. The application uses an Amazon EC2 instance for the front-end layer, another EC2 instance for the application layer, and a third EC2 instance for a MySQL database. A solutions architect must design a scalable and highly available solution that requires the least amount of change to the application. Which solution meets these requirements?

-   [ ] A. Use Amazon S3 to host the front-end layer. Use AWS Lambda functions for the application layer. Move the database to an Amazon DynamoDB table. Use Amazon S3 to store and serve users' images.
-   [ ] B. Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the application layer. Move the database to an Amazon RDS DB instance with multiple read replicas to serve users' images.
-   [ ] C. Use Amazon S3 to host the front-end layer. Use a fleet of EC2 instances in an Auto Scaling group for the application layer. Move the database to a memory optimized instance type to store and serve users' images.
-   [ ] D. Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the application layer. Move the database to an Amazon RDS Multi-AZ DB instance. Use Amazon S3 to store and serve users' images.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the application layer. Move the database to an Amazon RDS Multi-AZ DB instance. Use Amazon S3 to store and serve users' images.
   
Why these are the correct answers:

D. Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the application layer. Move the database to an Amazon RDS Multi-AZ DB instance. Use Amazon S3 to store and serve users' images.

-   [ ] Elastic Beanstalk simplifies deployment and provides scalability and high availability with minimal changes.
-   [ ] Amazon RDS Multi-AZ ensures high availability for the database.
-   [ ] Amazon S3 is used for scalable and cost-effective storage of images.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] A. Using Lambda and DynamoDB requires significant application changes.
-   [ ] B. RDS read replicas are for scaling reads, not for high availability in the same way as Multi-AZ.
-   [ ] C. Using S3 for the front-end and EC2 Auto Scaling still requires more management than Elastic Beanstalk, and a memory-optimized instance type alone does not provide high availability.

Therefore, Option D provides the best balance of scalability, high availability, and minimal application changes.

</details>
<details>
  <summary>Question 237</summary>

An application running on an Amazon EC2 instance in VPC-A needs to access files in another EC2 instance in VPC-B. Both VPCs are in separate AWS accounts. The network administrator needs to design a solution to configure secure access to EC2 instance in VPC-B from VPC-A. The connectivity should not have a single point of failure or bandwidth concerns. Which solution will meet these requirements?

-   [ ] A. Set up a VPC peering connection between VPC-A and VPC-B.
-   [ ] B. Set up VPC gateway endpoints for the EC2 instance running in VPC-B.
-   [ ] C. Attach a virtual private gateway to VPC-B and set up routing from VPC-A.
-   [ ] D. Create a private virtual interface (VIF) for the EC2 instance running in VPC-B and add appropriate routes from VPC-A.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Set up a VPC peering connection between VPC-A and VPC-B.
   
Why these are the correct answers:

A. Set up a VPC peering connection between VPC-A and VPC-B.

-   [ ] VPC peering connects two VPCs and enables resources in those VPCs to communicate with each other privately.
-   [ ] It does not have a single point of failure or bandwidth limitations.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] B. VPC gateway endpoints are for accessing AWS services like S3 or DynamoDB, not for EC2 instances in another VPC.
-   [ ] C. A virtual private gateway is used for VPN connections, not for connecting VPCs in different accounts.
-   [ ] D. A private virtual interface is used with AWS Direct Connect, which is not necessary for VPCs in different accounts.

Therefore, Option A is the most suitable solution for secure, scalable, and highly available connectivity between VPCs in different AWS accounts.

</details>
<details>
  <summary>Question 238</summary>

A company wants to experiment with individual AWS accounts for its engineer team. The company wants to be notified as soon as the Amazon EC2 instance usage for a given month exceeds a specific threshold for each account. What should a solutions architect do to meet this requirement MOST cost-effectively?

-   [ ] A. Use Cost Explorer to create a daily report of costs by service. Filter the report by EC2 instances. Configure Cost Explorer to send an Amazon Simple Email Service (Amazon SES) notification when a threshold is exceeded.
-   [ ] B. Use Cost Explorer to create a monthly report of costs by service. Filter the report by EC2 instances. Configure Cost Explorer to send an Amazon Simple Email Service (Amazon SES) notification when a threshold is exceeded.
-   [ ] C. Use AWS Budgets to create a cost budget for each account. Set the period to monthly. Set the scope to EC2 instances. Set an alert threshold for the budget. Configure an Amazon Simple Notification Service (Amazon SNS) topic to receive a notification when a threshold is exceeded.
-   [ ] D. Use AWS Cost and Usage Reports to create a report with hourly granularity. Integrate the report data with Amazon Athena. Use Amazon EventBridge to schedule an Athena query. Configure an Amazon Simple Notification Service (Amazon SNS) topic to receive a notification when a threshold is exceeded.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Use AWS Budgets to create a cost budget for each account. Set the period to monthly. Set the scope to EC2 instances. Set an alert threshold for the budget. Configure an Amazon Simple Notification Service (Amazon SNS) topic to receive a notification when a threshold is exceeded.
   
Why these are the correct answers:

C. Use AWS Budgets to create a cost budget for each account. Set the period to monthly. Set the scope to EC2 instances. Set an alert threshold for the budget. Configure an Amazon Simple Notification Service (Amazon SNS) topic to receive a notification when a threshold is exceeded.

-   [ ] AWS Budgets allows you to set custom budgets to track cost or usage.
-   [ ] You can configure budgets to monitor EC2 instance usage and receive alerts when thresholds are exceeded.
-   [ ] This is a cost-effective and straightforward way to monitor spending.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] A. Daily reports are more frequent than necessary and may lead to alert fatigue. Cost Explorer does not directly send notifications based on thresholds.
-   [ ] B. Similar to A, Cost Explorer alone does not provide threshold-based notifications.
-   [ ] D. Using Cost and Usage Reports with Athena and EventBridge is more complex and expensive than using AWS Budgets.

Therefore, Option C is the most cost-effective and efficient solution for monitoring EC2 usage and receiving notifications.

</details>
<details>
  <summary>Question 239</summary>

A solutions architect needs to design a new microservice for a company's application. Clients must be able to call an HTTPS endpoint to reach the microservice. The microservice also must use AWS Identity and Access Management (IAM) to authenticate calls. The solutions architect will write the logic for this microservice by using a single AWS Lambda function that is written in Go 1.x. Which solution will deploy the function in the MOST operationally efficient way?

-   [ ] A. Create an Amazon API Gateway REST API. Configure the method to use the Lambda function. Enable IAM authentication on the API.
-   [ ] B. Create a Lambda function URL for the function. Specify AWS_IAM as the authentication type.
-   [ ] C. Create an Amazon CloudFront distribution. Deploy the function to Lambda@Edge. Integrate IAM authentication logic into the Lambda@Edge function.
-   [ ] D. Create an Amazon CloudFront distribution. Deploy the function to CloudFront Functions. Specify AWS_IAM as the authentication type.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Create an Amazon API Gateway REST API. Configure the method to use the Lambda function. Enable IAM authentication on the API.
   
Why these are the correct answers:

A. Create an Amazon API Gateway REST API. Configure the method to use the Lambda function. Enable IAM authentication on the API.

-   [ ] API Gateway provides an HTTPS endpoint for invoking Lambda functions.
-   [ ] API Gateway supports IAM authentication to control access to the microservice.
-   [ ] This is a straightforward and managed way to expose a Lambda function as an API.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] B. Lambda function URLs do not directly support IAM authentication.
-   [ ] C. Lambda@Edge is for processing requests at CloudFront edge locations, adding complexity for a simple microservice.
-   [ ] D. CloudFront Functions do not support IAM authentication and are designed for lightweight tasks.

Therefore, Option A is the most operationally efficient solution for deploying the microservice with HTTPS and IAM authentication.

</details>
<details>
  <summary>Question 240</summary>

A company previously migrated its data warehouse solution to AWS. The company also has an AWS Direct Connect connection. Corporate office users query the data warehouse using a visualization tool. The average size of a query returned by the data warehouse is 50 MB and each webpage sent by the visualization tool is approximately 500 KB. Result sets returned by the data warehouse are not cached. Which solution provides the LOWEST data transfer egress cost for the company?

-   [ ] A. Host the visualization tool on premises and query the data warehouse directly over the internet.
-   [ ] B. Host the visualization tool in the same AWS Region as the data warehouse. Access it over the internet.
-   [ ] C. Host the visualization tool on premises and query the data warehouse directly over a Direct Connect connection at a location in the same AWS Region.
-   [ ] D. Host the visualization tool in the same AWS Region as the data warehouse and access it over a Direct Connect connection at a location in the same Region.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Host the visualization tool in the same AWS Region as the data warehouse and access it over a Direct Connect connection at a location in the same Region.
   
Why these are the correct answers:

D. Host the visualization tool in the same AWS Region as the data warehouse and access it over a Direct Connect connection at a location in the same Region.

-   [ ] Data transfer within the same AWS Region using Direct Connect is generally the most cost-effective.
-   [ ] Direct Connect provides lower costs than internet data transfer.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] A. Transferring data over the internet incurs higher data transfer costs.
-   [ ] B. Transferring data over the internet incurs higher data transfer costs.
-   [ ] C. While Direct Connect is used, hosting the visualization tool on premises incurs costs for transferring both queries and results.

Therefore, Option D minimizes data transfer egress costs by keeping both the data warehouse and visualization tool in the same AWS Region and using Direct Connect.

</details>

# AWS-SAA-PRACTICE-EXAM Questions 241-250

<details>
  <summary>Question 241</summary>

An online learning company is migrating to the AWS Cloud. The company maintains its student records in a PostgreSQL database. The company needs a solution in which its data is available and online across multiple AWS Regions at all times. Which solution will meet these requirements with the LEAST amount of operational overhead?

-   [ ] A. Migrate the PostgreSQL database to a PostgreSQL cluster on Amazon EC2 instances.
-   [ ] B. Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance with the Multi-AZ feature turned on.
-   [ ] C. Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. Create a read replica in another Region.
-   [ ] D. Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. Set up DB snapshots to be copied to another Region.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. Create a read replica in another Region.
   
Why these are the correct answers:

C. Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. Create a read replica in another Region.

-   [ ] Amazon RDS simplifies database management.
-   [ ] A read replica in another Region provides cross-region availability.
-   [ ] This setup minimizes operational overhead compared to managing a PostgreSQL cluster on EC2.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] A. Managing a PostgreSQL cluster on EC2 instances involves more operational overhead.
-   [ ] B. Multi-AZ provides high availability within a single Region, not across multiple Regions.
-   [ ] D. Relying on DB snapshots and copies increases recovery time and operational overhead.

Therefore, Option C is the most suitable solution for multi-region availability with the least operational overhead.

</details>
<details>
  <summary>Question 242</summary>

A company hosts its web application on AWS using seven Amazon EC2 instances. The company requires that the IP addresses of all healthy EC2 instances be returned in response to DNS queries. Which policy should be used to meet this requirement?

-   [ ] A. Simple routing policy
-   [ ] B. Latency routing policy
-   [ ] C. Multivalue answer routing policy
-   [ ] D. Geolocation routing policy
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Multivalue answer routing policy
   
Why these are the correct answers:

C. Multivalue answer routing policy

-   [ ] Multivalue answer routing returns multiple IP addresses for DNS queries.
-   [ ] It can be configured to return only the IP addresses of healthy instances.
-   [ ] This policy is designed for high availability and load distribution.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] A. Simple routing policy returns all records in no particular order and does not check health.
-   [ ] B. Latency routing policy routes traffic based on the lowest latency, not for returning all healthy IPs.
-   [ ] D. Geolocation routing policy routes traffic based on the location of the users.

Therefore, Option C is the correct policy to return IP addresses of all healthy EC2 instances.

</details>
<details>
  <summary>Question 243</summary>

A medical research lab produces data that is related to a new study. The lab wants to make the data available with minimum latency to clinics across the country for their on-premises, file-based applications. The data files are stored in an Amazon S3 bucket that has read-only permissions for each clinic. What should a solutions architect recommend to meet these requirements?

-   [ ] A. Deploy an AWS Storage Gateway file gateway as a virtual machine (VM) on premises at each clinic.
-   [ ] B. Migrate the files to each clinic's on-premises applications by using AWS DataSync for processing.
-   [ ] C. Deploy an AWS Storage Gateway volume gateway as a virtual machine (VM) on premises at each clinic.
-   [ ] D. Attach an Amazon Elastic File System (Amazon EFS) file system to each clinic's on-premises servers.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Deploy an AWS Storage Gateway file gateway as a virtual machine (VM) on premises at each clinic.
   
Why these are the correct answers:

A. Deploy an AWS Storage Gateway file gateway as a virtual machine (VM) on premises at each clinic.

-   [ ] AWS Storage Gateway file gateway provides low-latency access to data in S3 for on-premises applications.
-   [ ] It caches frequently accessed data locally, reducing the need to retrieve data from S3.
-   [ ] This solution allows clinics to access files in S3 with minimal latency.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] B. AWS DataSync is for data migration and synchronization, not for providing low-latency access to files.
-   [ ] C. AWS Storage Gateway volume gateway provides block storage, not file-based access.
-   [ ] D. Amazon EFS is a file storage service for EC2 instances, not for on-premises access.

Therefore, Option A is the most suitable solution for providing low-latency access to S3 data for on-premises file-based applications.

</details>
<details>
  <summary>Question 244</summary>

A company is using a content management system that runs on a single Amazon EC2 instance. The EC2 instance contains both the web server and the database software. The company must make its website platform highly available and must enable the website to scale to meet user demand. What should a solutions architect recommend to meet these requirements?

-   [ ] A. Move the database to Amazon RDS, and enable automatic backups. Manually launch another EC2 instance in the same Availability Zone. Configure an Application Load Balancer in the Availability Zone, and set the two instances as targets.
-   [ ] B. Migrate the database to an Amazon Aurora instance with a read replica in the same Availability Zone as the existing EC2 instance. Manually launch another EC2 instance in the same Availability Zone. Configure an Application Load Balancer, and set the two EC2 instances as targets.
-   [ ] C. Move the database to Amazon Aurora with a read replica in another Availability Zone. Create an Amazon Machine Image (AMI) from the EC2 instance. Configure an Application Load Balancer in two Availability Zones. Attach an Auto Scaling group that uses the AMI across two Availability Zones.
-   [ ] D. Move the database to a separate EC2 instance, and schedule backups to Amazon S3. Create an Amazon Machine Image (AMI) from the original EC2 instance. Configure an Application Load Balancer in two Availability Zones. Attach an Auto Scaling group that uses the AMI across two Availability Zones.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Move the database to Amazon Aurora with a read replica in another Availability Zone. Create an Amazon Machine Image (AMI) from the EC2 instance. Configure an Application Load Balancer in two Availability Zones. Attach an Auto Scaling group that uses the AMI across two Availability Zones.
   
Why these are the correct answers:

C. Move the database to Amazon Aurora with a read replica in another Availability Zone. Create an Amazon Machine Image (AMI) from the EC2 instance. Configure an Application Load Balancer in two Availability Zones. Attach an Auto Scaling group that uses the AMI across two Availability Zones.

-   [ ] Amazon Aurora with a read replica in another AZ provides high availability for the database.
-   [ ] An Auto Scaling group ensures the web tier can scale to meet demand.
-   [ ] An Application Load Balancer distributes traffic across multiple instances.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] A. Manually launching EC2 instances is not scalable or highly available. RDS with automatic backups does not provide the same level of HA as Aurora with a read replica.
-   [ ] B. Similar to A, manually launching instances is not scalable. A read replica in the same AZ does not provide cross-AZ HA.
-   [ ] D. Moving the database to a separate EC2 instance does not provide managed HA like Aurora.

Therefore, Option C provides the best solution for high availability and scalability.

</details>
<details>
  <summary>Question 245</summary>

A company is launching an application on AWS. The application uses an Application Load Balancer (ALB) to direct traffic to at least two Amazon EC2 instances in a single target group. The instances are in an Auto Scaling group for each environment. The company requires a development environment and a production environment. The production environment will have periods of high traffic. Which solution will configure the development environment MOST cost-effectively?

-   [ ] A. Reconfigure the target group in the development environment to have only one EC2 instance as a target.
-   [ ] B. Change the ALB balancing algorithm to least outstanding requests.
-   [ ] C. Reduce the size of the EC2 instances in both environments.
-   [ ] D. Reduce the maximum number of EC2 instances in the development environment's Auto Scaling group.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Reduce the maximum number of EC2 instances in the development environment's Auto Scaling group.
   
Why these are the correct answers:

D. Reduce the maximum number of EC2 instances in the development environment's Auto Scaling group.

-   [ ] Auto Scaling groups allow you to control the number of running instances.
-   [ ] Reducing the maximum size in the development environment lowers costs when there is less traffic.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] A. An ALB requires at least two instances for high availability, even in development.
-   [ ] B. Changing the ALB balancing algorithm does not reduce costs.
-   [ ] C. Reducing the size of EC2 instances affects performance in both environments.

Therefore, Option D is the most cost-effective solution for the development environment.

</details>
<details>
  <summary>Question 246</summary>

A company runs a web application on Amazon EC2 instances in multiple Availability Zones. The EC2 instances are in private subnets. A solutions architect implements an internet-facing Application Load Balancer (ALB) and specifies the EC2 instances as the target group. However, the internet traffic is not reaching the EC2 instances. How should the solutions architect reconfigure the architecture to resolve this issue?

-   [ ] A. Replace the ALB with a Network Load Balancer. Configure a NAT gateway in a public subnet to allow internet traffic.
-   [ ] B. Move the EC2 instances to public subnets. Add a rule to the EC2 instances' security groups to allow outbound traffic to 0.0.0.0/0.
-   [ ] C. Update the route tables for the EC2 instances' subnets to send 0.0.0.0/0 traffic through the internet gateway route. Add a rule to the EC2 instances' security groups to allow outbound traffic to 0.0.0.0/0.
-   [ ] D. Create public subnets in each Availability Zone. Associate the public subnets with the ALB. Update the route tables for the public subnets with a route to the private subnets.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Create public subnets in each Availability Zone. Associate the public subnets with the ALB. Update the route tables for the public subnets with a route to the private subnets.
   
Why these are the correct answers:

D. Create public subnets in each Availability Zone. Associate the public subnets with the ALB. Update the route tables for the public subnets with a route to the private subnets.

-   [ ] ALB must be in public subnets to receive internet traffic.
-   [ ] Public subnets need a route to the internet gateway.
-   [ ] Private subnets contain the EC2 instances.
-   [ ] Route tables direct traffic from the ALB to the instances.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] A. NLB is for TCP/UDP traffic, not HTTP/HTTPS. NAT gateway is for private subnets to access the internet, not the other way around.
-   [ ] B. Moving EC2 instances to public subnets is less secure.
-   [ ] C. Updating route tables for private subnets does not allow internet traffic to reach them directly.

Therefore, Option D correctly configures the subnets and routing for internet traffic to reach the EC2 instances.

</details>
<details>
  <summary>Question 247</summary>

A company has deployed a database in Amazon RDS for MySQL. Due to increased transactions, the database support team is reporting slow reads against the DB instance and recommends adding a read replica. Which combination of actions should a solutions architect take before implementing this change? (Choose two.)

-   [ ] A. Enable binlog replication on the RDS primary node.
-   [ ] B. Choose a failover priority for the source DB instance.
-   [ ] C. Allow long-running transactions to complete on the source DB instance.
-   [ ] D. Create a global table and specify the AWS Regions where the table will be available.
-   [ ] E. Enable automatic backups on the source instance by setting the backup retention period to a value other than 0.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Enable binlog replication on the RDS primary node.
-   [ ] E. Enable automatic backups on the source instance by setting the backup retention period to a value other than 0.
   
Why these are the correct answers:

A. Enable binlog replication on the RDS primary node.

-   [ ] Binary log (binlog) replication must be enabled for RDS to create a read replica.
   
E. Enable automatic backups on the source instance by setting the backup retention period to a value other than 0.

-   [ ] Automatic backups must be enabled to create a read replica.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] B. Failover priority is for Multi-AZ deployments, not read replicas.
-   [ ] C. While allowing long-running transactions to complete is a good practice, it is not a prerequisite for creating a read replica.
-   [ ] D. Global tables are for DynamoDB, not RDS.

Therefore, options A and E are the necessary actions before creating a read replica.

</details>
<details>
  <summary>Question 248</summary>

A company runs analytics software on Amazon EC2 instances. The software accepts job requests from users to process data that has been uploaded to Amazon S3. Users report that some submitted data is not being processed Amazon CloudWatch reveals that the EC2 instances have a consistent CPU utilization at or near 100%. The company wants to improve system performance and scale the system based on user load. What should a solutions architect do to meet these requirements?

-   [ ] A. Create a copy of the instance. Place all instances behind an Application Load Balancer.
-   [ ] B. Create an S3 VPC endpoint for Amazon S3. Update the software to reference the endpoint.
-   [ ] C. Stop the EC2 instances. Modify the instance type to one with a more powerful CPU and more memory. Restart the instances.
-   [ ] D. Route incoming requests to Amazon Simple Queue Service (Amazon SQS). Configure an EC2 Auto Scaling group based on queue size. Update the software to read from the queue.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Route incoming requests to Amazon Simple Queue Service (Amazon SQS). Configure an EC2 Auto Scaling group based on queue size. Update the software to read from the queue.
   
Why these are the correct answers:

D. Route incoming requests to Amazon Simple Queue Service (Amazon SQS). Configure an EC2 Auto Scaling group based on queue size. Update the software to read from the queue.

-   [ ] SQS queues decouple the request processing from the EC2 instances.
-   [ ] Auto Scaling groups scale the number of instances based on the queue size, which reflects user load.
-   [ ] This solution improves performance and scales the system efficiently.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] A. Creating a copy of the instance does not provide automatic scaling. An ALB alone does not address the processing bottleneck.
-   [ ] B. An S3 VPC endpoint improves S3 access but does not address the CPU bottleneck on the EC2 instances.
-   [ ] C. Stopping and resizing instances causes downtime and does not provide automatic scaling.

Therefore, Option D is the most suitable solution for improving performance and scaling based on user load.

</details>
<details>
  <summary>Question 249</summary>

A company is implementing a shared storage solution for a media application that is hosted in the AWS Cloud. The company needs the ability to use SMB clients to access data. The solution must be fully managed. Which AWS solution meets these requirements?

-   [ ] A. Create an AWS Storage Gateway volume gateway. Create a file share that uses the required client protocol. Connect the application server to the file share.
-   [ ] B. Create an AWS Storage Gateway tape gateway. Configure tapes to use Amazon S3. Connect the application server to the tape gateway.
-   [ ] C. Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the instance. Connect the application server to the file share.
-   [ ] D. Create an Amazon FSx for Windows File Server file system. Attach the file system to the origin server. Connect the application server to the file system.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Create an Amazon FSx for Windows File Server file system. Attach the file system to the origin server. Connect the application server to the file system.
   
Why these are the correct answers:

D. Create an Amazon FSx for Windows File Server file system. Attach the file system to the origin server. Connect the application server to the file system.

-   [ ] Amazon FSx for Windows File Server provides fully managed file storage that supports SMB access.
-   [ ] It eliminates the need to manage file servers and storage.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] A. AWS Storage Gateway volume gateway provides block storage, not file storage with SMB access.
-   [ ] B. AWS Storage Gateway tape gateway is for backup and archival, not for shared file storage.
-   [ ] C. Creating an EC2 Windows instance requires managing the file server, which is not a fully managed solution.

Therefore, Option D is the correct solution for a fully managed shared storage solution with SMB access.

</details>

<details>
  <summary>Question 250</summary>

A company's security team requests that network traffic be captured in VPC Flow Logs. The logs will be frequently accessed for 90 days and then accessed intermittently. What should a solutions architect do to meet these requirements when configuring the logs?

-   [ ] A. Use Amazon CloudWatch as the target. Set the CloudWatch log group with an expiration of 90 days
-   [ ] B. Use Amazon Kinesis as the target. Configure the Kinesis stream to always retain the logs for 90 days.
-   [ ] C. Use AWS CloudTrail as the target. Configure CloudTrail to save to an Amazon S3 bucket, and enable S3 Intelligent-Tiering.
-   [ ] D. Use Amazon S3 as the target. Enable an S3 Lifecycle policy to transition the logs to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Use Amazon S3 as the target. Enable an S3 Lifecycle policy to transition the logs to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days.
   
Why these are the correct answers:

D. Use Amazon S3 as the target. Enable an S3 Lifecycle policy to transition the logs to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days.

-   [ ] Amazon S3 is suitable for storing VPC Flow Logs.
-   [ ] S3 Lifecycle policies can automate the transition of logs to S3 Standard-IA for cost-effectiveness.
-   [ ] This meets the requirement for frequent access initially and cost-effective storage later.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] A. CloudWatch is better for monitoring and alerting, not for long-term storage of large log volumes.
-   [ ] B. Kinesis is for real-time streaming, not cost-effective long-term storage.
-   [ ] C. AWS CloudTrail records API calls, not network traffic.

Therefore, Option D is the most appropriate solution.

</details>

























































