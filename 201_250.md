
<details>
  <summary>Question 201</summary>

A company is developing a marketing communications service that targets mobile app users. The company needs to send confirmation messages with Short Message Service (SMS) to its users. The users must be able to reply to the SMS messages. The company must store the responses for a year for analysis. What should a solutions architect do to meet these requirements?

-   [ ] A. Create an Amazon Connect contact flow to send the SMS messages. Use AWS Lambda to process the responses.
-   [ ] B. Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.
-   [ ] C. Use Amazon Simple Queue Service (Amazon SQS) to distribute the SMS messages. Use AWS Lambda to process the responses.
-   [ ] D. Create an Amazon Simple Notification Service (Amazon SNS) FIFO topic. Subscribe an Amazon Kinesis data stream to the SNS topic for analysis and archiving.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.
   

Why these are the correct answers:

B. Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.

-   [ ] Amazon Pinpoint is designed for marketing communication and supports sending and receiving SMS messages.
-   [ ] Amazon Pinpoint journeys allow you to create automated messaging campaigns.
-   [ ] Pinpoint can be configured to send events to a Kinesis data stream, enabling real-time analysis and archiving of responses.
-   [ ] This solution meets all the requirements: sending SMS, receiving replies, and storing responses for analysis.
   

Why are the other answers wrong?

-   [ ] A. Amazon Connect is a cloud-based contact center service, not primarily designed for automated marketing SMS campaigns. While it can send SMS, it is not the most suitable service for this scenario.
-   [ ] C. Amazon SQS is a message queuing service and is not designed for sending SMS messages. It is used for queuing messages between applications.
-   [ ] D. Amazon SNS is a publish/subscribe messaging service, not designed for two-way SMS communication or managing conversations. FIFO topics provide ordering, which is not a primary requirement here, and SNS does not directly handle SMS replies in a conversational manner.

Therefore, Option B is the most appropriate solution for the marketing communications service.

</details>

<details>
  <summary>Question 202</summary>

A company is planning to move its data to an Amazon S3 bucket. The data must be encrypted when it is stored in the S3 bucket. Additionally, the encryption key must be automatically rotated every year. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Move the data to the S3 bucket. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use the built-in key rotation behavior of SSE-S3 encryption keys.
-   [ ] B. Create an AWS Key Management Service (AWS KMS) customer managed key. Enable automatic key rotation. Set the S3 bucket's default encryption behavior to use the customer managed KMS key. Move the data to the S3 bucket.
-   [ ] C. Create an AWS Key Management Service (AWS KMS) customer managed key. Set the S3 bucket's default encryption behavior to use the customer managed KMS key. Move the data to the S3 bucket. Manually rotate the KMS key every year.
-   [ ] D. Encrypt the data with customer key material before moving the data to the S3 bucket. Create an AWS Key Management Service (AWS KMS) key without key material. Import the customer key material into the KMS key. Enable automatic key rotation.

</details>

<details>
  <summary>Answer</summary>

A. Move the data to the S3 bucket. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use the built-in key rotation behavior of SSE-S3 encryption keys.

Why these are the correct answers:

A. Move the data to the S3 bucket. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use the built-in key rotation behavior of SSE-S3 encryption keys.

-   [ ] SSE-S3 is the simplest encryption method, as AWS manages the encryption keys.
-   [ ] SSE-S3 provides automatic encryption of data at rest.
-   [ ] SSE-S3 keys are automatically rotated by AWS, which satisfies the key rotation requirement with the least operational overhead.

Why are the other answers wrong?

-   [ ] B. While using KMS customer managed keys with automatic key rotation meets the requirements, it involves more configuration than SSE-S3, increasing operational overhead.
-   [ ] C. Manually rotating KMS keys increases operational overhead, which contradicts the requirement for the least overhead.
-   [ ] D. Encrypting data before moving it to S3 and importing key material into KMS is the most complex option and has the highest operational overhead.

Therefore, Option A is the best solution as it meets the requirements with the least operational overhead.

</details>

<details>
  <summary>Question 203</summary>

The customers of a finance company request appointments with financial advisors by sending text messages. A web application that runs on Amazon EC2 instances accepts the appointment requests. The text messages are published to an Amazon Simple Queue Service (Amazon SQS) queue through the web application. Another application that runs on EC2 instances then sends meeting invitations and meeting confirmation email messages to the customers. After successful scheduling, this application stores the meeting information in an Amazon DynamoDB database. As the company expands, customers report that their meeting invitations are taking longer to arrive. What should a solutions architect recommend to resolve this issue?

-   [ ] A. Add a DynamoDB Accelerator (DAX) cluster in front of the DynamoDB database.
-   [ ] B. Add an Amazon API Gateway API in front of the web application that accepts the appointment requests.
-   [ ] C. Add an Amazon CloudFront distribution. Set the origin as the web application that accepts the appointment requests.
-   [ ] D. Add an Auto Scaling group for the application that sends meeting invitations. Configure the Auto Scaling group to scale based on the depth of the SOS queue.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Add an Auto Scaling group for the application that sends meeting invitations. Configure the Auto Scaling group to scale based on the depth of the SOS queue.

Why these are the correct answers:

D. Add an Auto Scaling group for the application that sends meeting invitations. Configure the Auto Scaling group to scale based on the depth of the SOS queue.

-   [ ] The problem is identified as meeting invitations taking longer to arrive, indicating a bottleneck in the application that sends these invitations.
-   [ ] Auto Scaling groups can automatically adjust the number of EC2 instances based on demand.
-   [ ] Scaling based on SQS queue depth ensures that the application that sends meeting invitations scales in response to the number of requests waiting to be processed.

Why are the other answers wrong?

-   [ ] A. DAX is used to accelerate DynamoDB read operations, but the issue is with sending meeting invitations, not writing to the database.
-   [ ] B. API Gateway is used for managing APIs, not for processing messages or scaling the application that sends invitations.
-   [ ] C. CloudFront is a content delivery network (CDN) and does not address the processing bottleneck for sending invitations.

Therefore, Option D is the most appropriate solution to resolve the issue by scaling the application responsible for sending meeting invitations based on the queue depth.

</details>

<details>
  <summary>Question 204</summary>

An online retail company has more than 50 million active customers and receives more than 25,000 orders each day. The company collects purchase data for customers and stores this data in Amazon S3. Additional customer data is stored in Amazon RDS.

The company wants to make all the data available to various teams so that the teams can perform analytics. The solution must provide the ability to manage fine-grained permissions for the data and must minimize operational overhead. Which solution will meet these requirements?

-   [ ] A. Migrate the purchase data to write directly to Amazon RDS. Use RDS access controls to limit access.
-   [ ] B. Schedule an AWS Lambda function to periodically copy data from Amazon RDS to Amazon S3. Create an AWS Glue crawler. Use Amazon Athena to query the data. Use S3 policies to limit access.
-   [ ] C. Create a data lake by using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon RDS. Register the S3 bucket in Lake Formation. Use Lake Formation access controls to limit access.
-   [ ] D. Create an Amazon Redshift cluster. Schedule an AWS Lambda function to periodically copy data from Amazon S3 and Amazon RDS to Amazon Redshift. Use Amazon Redshift access controls to limit access.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Create a data lake by using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon RDS. Register the S3 bucket in Lake Formation. Use Lake Formation access controls to limit access.

Why these are the correct answers:

C. Create a data lake by using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon RDS. Register the S3 bucket in Lake Formation. Use Lake Formation access controls to limit access.

-   [ ] AWS Lake Formation simplifies the process of setting up a data lake.
-   [ ] It allows for fine-grained access control at the table and column level.
-   [ ] Using AWS Glue connections, data from Amazon RDS can be integrated into the data lake.
-   [ ] This solution minimizes operational overhead by providing a centralized and managed way to handle data access and permissions.

Why are the other answers wrong?

-   [ ] A. Migrating all purchase data directly into Amazon RDS might not be scalable or cost-effective for large volumes of data. RDS access controls are not as fine-grained as Lake Formation.
-   [ ] B. Using Lambda, Glue, Athena, and S3 policies requires more manual configuration and management of permissions across different services compared to Lake Formation.
-   [ ] D. Amazon Redshift is a data warehouse and might be overkill for providing data access to various teams for general analytics. It also involves more operational overhead than Lake Formation.

Therefore, Option C is the most suitable solution because it provides fine-grained permissions and minimizes operational overhead.

</details>

<details>
  <summary>Question 205</summary>

A company hosts a marketing website in an on-premises data center. The website consists of static documents and runs on a single server. An administrator updates the website content infrequently and uses an SFTP client to upload new documents. The company decides to host its website on AWS and to use Amazon CloudFront. The company's solutions architect creates a CloudFront distribution. The solutions architect must design the most cost-effective and resilient architecture for website hosting to serve as the CloudFront origin. Which solution will meet these requirements?

-   [ ] A. Create a virtual server by using Amazon Lightsail. Configure the web server in the Lightsail instance. Upload website content by using an SFTP client.
-   [ ] B. Create an AWS Auto Scaling group for Amazon EC2 instances. Use an Application Load Balancer. Upload website content by using an SFTP client.
-   [ ] C. Create a private Amazon S3 bucket. Use an S3 bucket policy to allow access from a CloudFront origin access identity (OAI). Upload website content by using the AWS CLI.
-   [ ] D. Create a public Amazon S3 bucket. Configure AWS Transfer for SFTP. Configure the S3 bucket for website hosting. Upload website content by using the SFTP client.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Create a private Amazon S3 bucket. Use an S3 bucket policy to allow access from a CloudFront origin access identity (OAI). Upload website content by using the AWS CLI.

Why these are the correct answers:

C. Create a private Amazon S3 bucket. Use an S3 bucket policy to allow access from a CloudFront origin access identity (OAI). Upload website content by using the AWS CLI.

-   [ ]   Amazon S3 is a cost-effective and highly available storage service for static website content.
-   [ ]   Using a private S3 bucket and an OAI ensures that the content is only accessible through CloudFront, enhancing security.
-   [ ]   This solution is resilient because S3 is inherently highly available and scalable.

Why are the other answers wrong?

-   [ ]   A. Amazon Lightsail is a virtual private server and is not designed for highly scalable and resilient website hosting. It does not integrate well with CloudFront for origin access control.
-   [ ]   B. Using EC2 instances and Auto Scaling with an Application Load Balancer is more complex and expensive than using S3 for static content. It also involves more operational overhead.
-   [ ]   D. A public S3 bucket is less secure. Using AWS Transfer for SFTP is not necessary for serving static website content and adds complexity.

Therefore, Option C is the most cost-effective and resilient solution for hosting a static website with CloudFront.

</details>

<details>
  <summary>Question 206</summary>

A company wants to manage Amazon Machine Images (AMIs). The company currently copies AMIs to the same AWS Region where the AMIs were created. The company needs to design an application that captures AWS API calls and sends alerts whenever the Amazon EC2 Createlmage API operation is called within the company's account. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Create an AWS Lambda function to query AWS CloudTrail logs and to send an alert when a Createlmage API call is detected.
-   [ ] B. Configure AWS CloudTrail with an Amazon Simple Notification Service (Amazon SNS) notification that occurs when updated logs are sent to Amazon S3. Use Amazon Athena to create a new table and to query on Createlmage when an API call is detected.
-   [ ] C. Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the Createlmage API call. Configure the target as an Amazon Simple Notification Service (Amazon SNS) topic to send an alert when a Createlmage API call is detected.
-   [ ] D. Configure an Amazon Simple Queue Service (Amazon SQS) FIFO queue as a target for AWS CloudTrail logs. Create an AWS Lambda function to send an alert to an Amazon Simple Notification Service (Amazon SNS) topic when a Createlmage API call is detected.
  
</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the Createlmage API call. Configure the target as an Amazon Simple Notification Service (Amazon SNS) topic to send an alert when a Createlmage API call is detected.
  
Why these are the correct answers:

C. Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the Createlmage API call. Configure the target as an Amazon Simple Notification Service (Amazon SNS) topic to send an alert when a Createlmage API call is detected.

-   [ ] Amazon EventBridge allows you to create rules that react to events in your AWS environment, including API calls.
-   [ ] By creating a rule specifically for the `CreateImage` API call and setting an SNS topic as the target, you can efficiently trigger alerts with minimal setup and maintenance.
-   [ ] This solution avoids the need for custom code to query logs or manage queues, reducing operational overhead.
  
Why are the other answers wrong?

-   [ ] A. Using a Lambda function to query CloudTrail logs adds complexity and requires writing and maintaining code. It's also less efficient than EventBridge, which can directly capture the API event.
-   [ ] B. Configuring CloudTrail with SNS and then using Athena to query logs involves multiple services and more steps, increasing operational overhead. Athena is also better suited for analytical queries, not real-time alerting.
-   [ ] D. Using SQS to buffer CloudTrail logs and then processing them with Lambda adds unnecessary complexity and latency. EventBridge provides a more direct and efficient way to capture and react to API events.

Therefore, Option C is the most operationally efficient solution for capturing the `CreateImage` API call and sending alerts.

</details>

<details>
  <summary>Question 207</summary>

A company owns an asynchronous API that is used to ingest user requests and, based on the request type, dispatch requests to the appropriate microservice for processing. The company is using Amazon API Gateway to deploy the API front end, and an AWS Lambda function that invokes Amazon DynamoDB to store user requests before dispatching them to the processing microservices. The company provisioned as much DynamoDB throughput as its budget allows, but the company is still experiencing availability issues and is losing user requests. What should a solutions architect do to address this issue without impacting existing users?

-   [ ] A. Add throttling on the API Gateway with server-side throttling limits.
-   [ ] B. Use DynamoDB Accelerator (DAX) and Lambda to buffer writes to DynamoDB.
-   [ ] C. Create a secondary index in DynamoDB for the table with the user requests.
-   [ ] D. Use the Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer writes to DynamoDB.
  
</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Use the Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer writes to DynamoDB.
  
Why these are the correct answers:

D. Use the Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer writes to DynamoDB.

-   [ ]   Amazon SQS can act as a buffer between API Gateway/Lambda and DynamoDB, allowing requests to be queued during high traffic periods.
-   [ ]   This buffering prevents DynamoDB from being overwhelmed and helps avoid losing user requests.
-   [ ]   A Lambda function can then process the messages from the SQS queue and write them to DynamoDB at a controlled rate, ensuring that DynamoDB's throughput is not exceeded.

Why are the other answers wrong?

-   [ ]   A. Adding throttling on API Gateway might help to control the rate of requests, but it doesn't address the underlying issue of DynamoDB being unable to handle the write load. It could also result in losing user requests if they are throttled.
-   [ ]   B. DAX is a cache for DynamoDB, designed to accelerate read operations. It does not help with write-heavy scenarios or buffer writes.
-   [ ]   C. Creating a secondary index in DynamoDB can improve read performance but does not directly address the issue of high write volume causing availability issues.

Therefore, Option D is the most suitable solution to buffer writes to DynamoDB and prevent losing user requests.

</details>

<details>
  <summary>Question 208</summary>

A company needs to move data from an Amazon EC2 instance to an Amazon S3 bucket. The company must ensure that no API calls and no data are routed through public internet routes. Only the EC2 instance can have access to upload data to the S3 bucket. Which solution will meet these requirements?

- [ ] A. Create an interface VPC endpoint for Amazon S3 in the subnet where the EC2 instance is located. Attach a resource policy to the S3 bucket to only allow the EC2 instance's IAM role for access.
- [ ] B. Create a gateway VPC endpoint for Amazon S3 in the Availability Zone where the EC2 instance is located. Attach appropriate security groups to the endpoint. Attach a resource policy to the S3 bucket to only allow the EC2 instance's IAM role for access.
- [ ] C. Run the nslookup tool from inside the EC2 instance to obtain the private IP address of the S3 bucket's service API endpoint. Create a route in the VPC route table to provide the EC2 instance with access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance's IAM role for access.
- [ ] D. Use the AWS provided, publicly available ip-ranges.json file to obtain the private IP address of the S3 bucket's service API endpoint. Create a route in the VPC route table to provide the EC2 instance with access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance's IAM role for access.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Create an interface VPC endpoint for Amazon S3 in the subnet where the EC2 instance is located. Attach a resource policy to the S3 bucket to only allow the EC2 instance's IAM role for access.

Why these are the correct answers:

A. Create an interface VPC endpoint for Amazon S3 in the subnet where the EC2 instance is located. Attach a resource policy to the S3 bucket to only allow the EC2 instance's IAM role for access.

- [ ] Interface VPC endpoints use private IP addresses to access S3, ensuring that traffic does not traverse the public internet.
- [ ] Attaching a resource policy to the S3 bucket restricts access to only the specified IAM role of the EC2 instance, enhancing security.
- [ ] This solution meets the requirements of keeping traffic private and restricting access.

Why are the other answers wrong?

- [ ] B. Gateway VPC endpoints for S3 are accessed via a gateway and do not use private IP addresses in the same way as interface endpoints. They are simpler but do not provide the same level of network isolation. Security groups are not directly attached to gateway endpoints.
- [ ] C. Using `nslookup` to obtain private IP addresses is not a reliable or scalable solution. IP addresses can change, and this approach does not guarantee that traffic will remain within the AWS network.
- [ ] D. Similar to option C, relying on the `ip-ranges.json` file is not a robust method for ensuring private connectivity. These IP ranges can change, and it does not provide the security and reliability of VPC endpoints.

Therefore, Option A is the most secure and reliable solution for ensuring that data transfer to S3 from an EC2 instance remains within the AWS network and is properly secured.

</details>
















































