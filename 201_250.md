
<details>
  <summary>Question 201</summary>

A company is developing a marketing communications service that targets mobile app users. The company needs to send confirmation messages with Short Message Service (SMS) to its users. The users must be able to reply to the SMS messages. The company must store the responses for a year for analysis. What should a solutions architect do to meet these requirements?

-   [ ] A. Create an Amazon Connect contact flow to send the SMS messages. Use AWS Lambda to process the responses.
-   [ ] B. Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.
-   [ ] C. Use Amazon Simple Queue Service (Amazon SQS) to distribute the SMS messages. Use AWS Lambda to process the responses.
-   [ ] D. Create an Amazon Simple Notification Service (Amazon SNS) FIFO topic. Subscribe an Amazon Kinesis data stream to the SNS topic for analysis and archiving.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.
   

Why these are the correct answers:

B. Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.

-   [ ] Amazon Pinpoint is designed for marketing communication and supports sending and receiving SMS messages.
-   [ ] Amazon Pinpoint journeys allow you to create automated messaging campaigns.
-   [ ] Pinpoint can be configured to send events to a Kinesis data stream, enabling real-time analysis and archiving of responses.
-   [ ] This solution meets all the requirements: sending SMS, receiving replies, and storing responses for analysis.
   

Why are the other answers wrong?

-   [ ] A. Amazon Connect is a cloud-based contact center service, not primarily designed for automated marketing SMS campaigns. While it can send SMS, it is not the most suitable service for this scenario.
-   [ ] C. Amazon SQS is a message queuing service and is not designed for sending SMS messages. It is used for queuing messages between applications.
-   [ ] D. Amazon SNS is a publish/subscribe messaging service, not designed for two-way SMS communication or managing conversations. FIFO topics provide ordering, which is not a primary requirement here, and SNS does not directly handle SMS replies in a conversational manner.

Therefore, Option B is the most appropriate solution for the marketing communications service.

</details>

<details>
  <summary>Question 202</summary>

A company is planning to move its data to an Amazon S3 bucket. The data must be encrypted when it is stored in the S3 bucket. Additionally, the encryption key must be automatically rotated every year. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Move the data to the S3 bucket. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use the built-in key rotation behavior of SSE-S3 encryption keys.
-   [ ] B. Create an AWS Key Management Service (AWS KMS) customer managed key. Enable automatic key rotation. Set the S3 bucket's default encryption behavior to use the customer managed KMS key. Move the data to the S3 bucket.
-   [ ] C. Create an AWS Key Management Service (AWS KMS) customer managed key. Set the S3 bucket's default encryption behavior to use the customer managed KMS key. Move the data to the S3 bucket. Manually rotate the KMS key every year.
-   [ ] D. Encrypt the data with customer key material before moving the data to the S3 bucket. Create an AWS Key Management Service (AWS KMS) key without key material. Import the customer key material into the KMS key. Enable automatic key rotation.

</details>

<details>
  <summary>Answer</summary>

A. Move the data to the S3 bucket. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use the built-in key rotation behavior of SSE-S3 encryption keys.

Why these are the correct answers:

A. Move the data to the S3 bucket. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use the built-in key rotation behavior of SSE-S3 encryption keys.

-   [ ] SSE-S3 is the simplest encryption method, as AWS manages the encryption keys.
-   [ ] SSE-S3 provides automatic encryption of data at rest.
-   [ ] SSE-S3 keys are automatically rotated by AWS, which satisfies the key rotation requirement with the least operational overhead.

Why are the other answers wrong?

-   [ ] B. While using KMS customer managed keys with automatic key rotation meets the requirements, it involves more configuration than SSE-S3, increasing operational overhead.
-   [ ] C. Manually rotating KMS keys increases operational overhead, which contradicts the requirement for the least overhead.
-   [ ] D. Encrypting data before moving it to S3 and importing key material into KMS is the most complex option and has the highest operational overhead.

Therefore, Option A is the best solution as it meets the requirements with the least operational overhead.

</details>

<details>
  <summary>Question 203</summary>

The customers of a finance company request appointments with financial advisors by sending text messages. A web application that runs on Amazon EC2 instances accepts the appointment requests. The text messages are published to an Amazon Simple Queue Service (Amazon SQS) queue through the web application. Another application that runs on EC2 instances then sends meeting invitations and meeting confirmation email messages to the customers. After successful scheduling, this application stores the meeting information in an Amazon DynamoDB database. As the company expands, customers report that their meeting invitations are taking longer to arrive. What should a solutions architect recommend to resolve this issue?

-   [ ] A. Add a DynamoDB Accelerator (DAX) cluster in front of the DynamoDB database.
-   [ ] B. Add an Amazon API Gateway API in front of the web application that accepts the appointment requests.
-   [ ] C. Add an Amazon CloudFront distribution. Set the origin as the web application that accepts the appointment requests.
-   [ ] D. Add an Auto Scaling group for the application that sends meeting invitations. Configure the Auto Scaling group to scale based on the depth of the SOS queue.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Add an Auto Scaling group for the application that sends meeting invitations. Configure the Auto Scaling group to scale based on the depth of the SOS queue.

Why these are the correct answers:

D. Add an Auto Scaling group for the application that sends meeting invitations. Configure the Auto Scaling group to scale based on the depth of the SOS queue.

-   [ ] The problem is identified as meeting invitations taking longer to arrive, indicating a bottleneck in the application that sends these invitations.
-   [ ] Auto Scaling groups can automatically adjust the number of EC2 instances based on demand.
-   [ ] Scaling based on SQS queue depth ensures that the application that sends meeting invitations scales in response to the number of requests waiting to be processed.

Why are the other answers wrong?

-   [ ] A. DAX is used to accelerate DynamoDB read operations, but the issue is with sending meeting invitations, not writing to the database.
-   [ ] B. API Gateway is used for managing APIs, not for processing messages or scaling the application that sends invitations.
-   [ ] C. CloudFront is a content delivery network (CDN) and does not address the processing bottleneck for sending invitations.

Therefore, Option D is the most appropriate solution to resolve the issue by scaling the application responsible for sending meeting invitations based on the queue depth.

</details>

<details>
  <summary>Question 204</summary>

An online retail company has more than 50 million active customers and receives more than 25,000 orders each day. The company collects purchase data for customers and stores this data in Amazon S3. Additional customer data is stored in Amazon RDS.

The company wants to make all the data available to various teams so that the teams can perform analytics. The solution must provide the ability to manage fine-grained permissions for the data and must minimize operational overhead. Which solution will meet these requirements?

A. Migrate the purchase data to write directly to Amazon RDS. Use RDS access controls to limit access.
B. Schedule an AWS Lambda function to periodically copy data from Amazon RDS to Amazon S3. Create an AWS Glue crawler. Use Amazon Athena to query the data. Use S3 policies to limit access.
C. Create a data lake by using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon RDS. Register the S3 bucket in Lake Formation. Use Lake Formation access controls to limit access.
D. Create an Amazon Redshift cluster. Schedule an AWS Lambda function to periodically copy data from Amazon S3 and Amazon RDS to Amazon Redshift. Use Amazon Redshift access controls to limit access.

</details>

<details>
  <summary>Answer</summary>

C. Create a data lake by using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon RDS. Register the S3 bucket in Lake Formation. Use Lake Formation access controls to limit access.

Why these are the correct answers:

C. Create a data lake by using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon RDS. Register the S3 bucket in Lake Formation. Use Lake Formation access controls to limit access.

-   [ ]   AWS Lake Formation simplifies the process of setting up a data lake.
-   [ ]   It allows for fine-grained access control at the table and column level.
-   [ ]   Using AWS Glue connections, data from Amazon RDS can be integrated into the data lake.
-   [ ]   This solution minimizes operational overhead by providing a centralized and managed way to handle data access and permissions.

Why are the other answers wrong?

-   [ ]   A. Migrating all purchase data directly into Amazon RDS might not be scalable or cost-effective for large volumes of data. RDS access controls are not as fine-grained as Lake Formation.
-   [ ]   B. Using Lambda, Glue, Athena, and S3 policies requires more manual configuration and management of permissions across different services compared to Lake Formation.
-   [ ]   D. Amazon Redshift is a data warehouse and might be overkill for providing data access to various teams for general analytics. It also involves more operational overhead than Lake Formation.

Therefore, Option C is the most suitable solution because it provides fine-grained permissions and minimizes operational overhead.

</details>

















