
<details>
  <summary>Question 201</summary>

A company is developing a marketing communications service that targets mobile app users. The company needs to send confirmation messages with Short Message Service (SMS) to its users. The users must be able to reply to the SMS messages. The company must store the responses for a year for analysis. What should a solutions architect do to meet these requirements?

-   [ ] A. Create an Amazon Connect contact flow to send the SMS messages. Use AWS Lambda to process the responses.
-   [ ] B. Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.
-   [ ] C. Use Amazon Simple Queue Service (Amazon SQS) to distribute the SMS messages. Use AWS Lambda to process the responses.
-   [ ] D. Create an Amazon Simple Notification Service (Amazon SNS) FIFO topic. Subscribe an Amazon Kinesis data stream to the SNS topic for analysis and archiving.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.
   

Why these are the correct answers:

B. Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.

-   [ ] Amazon Pinpoint is designed for marketing communication and supports sending and receiving SMS messages.
-   [ ] Amazon Pinpoint journeys allow you to create automated messaging campaigns.
-   [ ] Pinpoint can be configured to send events to a Kinesis data stream, enabling real-time analysis and archiving of responses.
-   [ ] This solution meets all the requirements: sending SMS, receiving replies, and storing responses for analysis.
   

Why are the other answers wrong?

-   [ ] A. Amazon Connect is a cloud-based contact center service, not primarily designed for automated marketing SMS campaigns. While it can send SMS, it is not the most suitable service for this scenario.
-   [ ] C. Amazon SQS is a message queuing service and is not designed for sending SMS messages. It is used for queuing messages between applications.
-   [ ] D. Amazon SNS is a publish/subscribe messaging service, not designed for two-way SMS communication or managing conversations. FIFO topics provide ordering, which is not a primary requirement here, and SNS does not directly handle SMS replies in a conversational manner.

Therefore, Option B is the most appropriate solution for the marketing communications service.

</details>

<details>
  <summary>Question 202</summary>

A company is planning to move its data to an Amazon S3 bucket. The data must be encrypted when it is stored in the S3 bucket. Additionally, the encryption key must be automatically rotated every year. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Move the data to the S3 bucket. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use the built-in key rotation behavior of SSE-S3 encryption keys.
-   [ ] B. Create an AWS Key Management Service (AWS KMS) customer managed key. Enable automatic key rotation. Set the S3 bucket's default encryption behavior to use the customer managed KMS key. Move the data to the S3 bucket.
-   [ ] C. Create an AWS Key Management Service (AWS KMS) customer managed key. Set the S3 bucket's default encryption behavior to use the customer managed KMS key. Move the data to the S3 bucket. Manually rotate the KMS key every year.
-   [ ] D. Encrypt the data with customer key material before moving the data to the S3 bucket. Create an AWS Key Management Service (AWS KMS) key without key material. Import the customer key material into the KMS key. Enable automatic key rotation.

</details>

<details>
  <summary>Answer</summary>

A. Move the data to the S3 bucket. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use the built-in key rotation behavior of SSE-S3 encryption keys.

Why these are the correct answers:

A. Move the data to the S3 bucket. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use the built-in key rotation behavior of SSE-S3 encryption keys.

-   [ ] SSE-S3 is the simplest encryption method, as AWS manages the encryption keys.
-   [ ] SSE-S3 provides automatic encryption of data at rest.
-   [ ] SSE-S3 keys are automatically rotated by AWS, which satisfies the key rotation requirement with the least operational overhead.

Why are the other answers wrong?

-   [ ] B. While using KMS customer managed keys with automatic key rotation meets the requirements, it involves more configuration than SSE-S3, increasing operational overhead.
-   [ ] C. Manually rotating KMS keys increases operational overhead, which contradicts the requirement for the least overhead.
-   [ ] D. Encrypting data before moving it to S3 and importing key material into KMS is the most complex option and has the highest operational overhead.

Therefore, Option A is the best solution as it meets the requirements with the least operational overhead.

</details>

<details>
  <summary>Question 203</summary>

The customers of a finance company request appointments with financial advisors by sending text messages. A web application that runs on Amazon EC2 instances accepts the appointment requests. The text messages are published to an Amazon Simple Queue Service (Amazon SQS) queue through the web application. Another application that runs on EC2 instances then sends meeting invitations and meeting confirmation email messages to the customers. After successful scheduling, this application stores the meeting information in an Amazon DynamoDB database. As the company expands, customers report that their meeting invitations are taking longer to arrive. What should a solutions architect recommend to resolve this issue?

-   [ ] A. Add a DynamoDB Accelerator (DAX) cluster in front of the DynamoDB database.
-   [ ] B. Add an Amazon API Gateway API in front of the web application that accepts the appointment requests.
-   [ ] C. Add an Amazon CloudFront distribution. Set the origin as the web application that accepts the appointment requests.
-   [ ] D. Add an Auto Scaling group for the application that sends meeting invitations. Configure the Auto Scaling group to scale based on the depth of the SOS queue.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Add an Auto Scaling group for the application that sends meeting invitations. Configure the Auto Scaling group to scale based on the depth of the SOS queue.

Why these are the correct answers:

D. Add an Auto Scaling group for the application that sends meeting invitations. Configure the Auto Scaling group to scale based on the depth of the SOS queue.

-   [ ] The problem is identified as meeting invitations taking longer to arrive, indicating a bottleneck in the application that sends these invitations.
-   [ ] Auto Scaling groups can automatically adjust the number of EC2 instances based on demand.
-   [ ] Scaling based on SQS queue depth ensures that the application that sends meeting invitations scales in response to the number of requests waiting to be processed.

Why are the other answers wrong?

-   [ ] A. DAX is used to accelerate DynamoDB read operations, but the issue is with sending meeting invitations, not writing to the database.
-   [ ] B. API Gateway is used for managing APIs, not for processing messages or scaling the application that sends invitations.
-   [ ] C. CloudFront is a content delivery network (CDN) and does not address the processing bottleneck for sending invitations.

Therefore, Option D is the most appropriate solution to resolve the issue by scaling the application responsible for sending meeting invitations based on the queue depth.

</details>

<details>
  <summary>Question 204</summary>

An online retail company has more than 50 million active customers and receives more than 25,000 orders each day. The company collects purchase data for customers and stores this data in Amazon S3. Additional customer data is stored in Amazon RDS.

The company wants to make all the data available to various teams so that the teams can perform analytics. The solution must provide the ability to manage fine-grained permissions for the data and must minimize operational overhead. Which solution will meet these requirements?

-   [ ] A. Migrate the purchase data to write directly to Amazon RDS. Use RDS access controls to limit access.
-   [ ] B. Schedule an AWS Lambda function to periodically copy data from Amazon RDS to Amazon S3. Create an AWS Glue crawler. Use Amazon Athena to query the data. Use S3 policies to limit access.
-   [ ] C. Create a data lake by using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon RDS. Register the S3 bucket in Lake Formation. Use Lake Formation access controls to limit access.
-   [ ] D. Create an Amazon Redshift cluster. Schedule an AWS Lambda function to periodically copy data from Amazon S3 and Amazon RDS to Amazon Redshift. Use Amazon Redshift access controls to limit access.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Create a data lake by using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon RDS. Register the S3 bucket in Lake Formation. Use Lake Formation access controls to limit access.

Why these are the correct answers:

C. Create a data lake by using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon RDS. Register the S3 bucket in Lake Formation. Use Lake Formation access controls to limit access.

-   [ ] AWS Lake Formation simplifies the process of setting up a data lake.
-   [ ] It allows for fine-grained access control at the table and column level.
-   [ ] Using AWS Glue connections, data from Amazon RDS can be integrated into the data lake.
-   [ ] This solution minimizes operational overhead by providing a centralized and managed way to handle data access and permissions.

Why are the other answers wrong?

-   [ ] A. Migrating all purchase data directly into Amazon RDS might not be scalable or cost-effective for large volumes of data. RDS access controls are not as fine-grained as Lake Formation.
-   [ ] B. Using Lambda, Glue, Athena, and S3 policies requires more manual configuration and management of permissions across different services compared to Lake Formation.
-   [ ] D. Amazon Redshift is a data warehouse and might be overkill for providing data access to various teams for general analytics. It also involves more operational overhead than Lake Formation.

Therefore, Option C is the most suitable solution because it provides fine-grained permissions and minimizes operational overhead.

</details>

<details>
  <summary>Question 205</summary>

A company hosts a marketing website in an on-premises data center. The website consists of static documents and runs on a single server. An administrator updates the website content infrequently and uses an SFTP client to upload new documents. The company decides to host its website on AWS and to use Amazon CloudFront. The company's solutions architect creates a CloudFront distribution. The solutions architect must design the most cost-effective and resilient architecture for website hosting to serve as the CloudFront origin. Which solution will meet these requirements?

-   [ ] A. Create a virtual server by using Amazon Lightsail. Configure the web server in the Lightsail instance. Upload website content by using an SFTP client.
-   [ ] B. Create an AWS Auto Scaling group for Amazon EC2 instances. Use an Application Load Balancer. Upload website content by using an SFTP client.
-   [ ] C. Create a private Amazon S3 bucket. Use an S3 bucket policy to allow access from a CloudFront origin access identity (OAI). Upload website content by using the AWS CLI.
-   [ ] D. Create a public Amazon S3 bucket. Configure AWS Transfer for SFTP. Configure the S3 bucket for website hosting. Upload website content by using the SFTP client.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Create a private Amazon S3 bucket. Use an S3 bucket policy to allow access from a CloudFront origin access identity (OAI). Upload website content by using the AWS CLI.

Why these are the correct answers:

C. Create a private Amazon S3 bucket. Use an S3 bucket policy to allow access from a CloudFront origin access identity (OAI). Upload website content by using the AWS CLI.

-   [ ]   Amazon S3 is a cost-effective and highly available storage service for static website content.
-   [ ]   Using a private S3 bucket and an OAI ensures that the content is only accessible through CloudFront, enhancing security.
-   [ ]   This solution is resilient because S3 is inherently highly available and scalable.

Why are the other answers wrong?

-   [ ]   A. Amazon Lightsail is a virtual private server and is not designed for highly scalable and resilient website hosting. It does not integrate well with CloudFront for origin access control.
-   [ ]   B. Using EC2 instances and Auto Scaling with an Application Load Balancer is more complex and expensive than using S3 for static content. It also involves more operational overhead.
-   [ ]   D. A public S3 bucket is less secure. Using AWS Transfer for SFTP is not necessary for serving static website content and adds complexity.

Therefore, Option C is the most cost-effective and resilient solution for hosting a static website with CloudFront.

</details>

<details>
  <summary>Question 206</summary>

A company wants to manage Amazon Machine Images (AMIs). The company currently copies AMIs to the same AWS Region where the AMIs were created. The company needs to design an application that captures AWS API calls and sends alerts whenever the Amazon EC2 Createlmage API operation is called within the company's account. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Create an AWS Lambda function to query AWS CloudTrail logs and to send an alert when a Createlmage API call is detected.
-   [ ] B. Configure AWS CloudTrail with an Amazon Simple Notification Service (Amazon SNS) notification that occurs when updated logs are sent to Amazon S3. Use Amazon Athena to create a new table and to query on Createlmage when an API call is detected.
-   [ ] C. Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the Createlmage API call. Configure the target as an Amazon Simple Notification Service (Amazon SNS) topic to send an alert when a Createlmage API call is detected.
-   [ ] D. Configure an Amazon Simple Queue Service (Amazon SQS) FIFO queue as a target for AWS CloudTrail logs. Create an AWS Lambda function to send an alert to an Amazon Simple Notification Service (Amazon SNS) topic when a Createlmage API call is detected.
  
</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the Createlmage API call. Configure the target as an Amazon Simple Notification Service (Amazon SNS) topic to send an alert when a Createlmage API call is detected.
  
Why these are the correct answers:

C. Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the Createlmage API call. Configure the target as an Amazon Simple Notification Service (Amazon SNS) topic to send an alert when a Createlmage API call is detected.

-   [ ] Amazon EventBridge allows you to create rules that react to events in your AWS environment, including API calls.
-   [ ] By creating a rule specifically for the `CreateImage` API call and setting an SNS topic as the target, you can efficiently trigger alerts with minimal setup and maintenance.
-   [ ] This solution avoids the need for custom code to query logs or manage queues, reducing operational overhead.
  
Why are the other answers wrong?

-   [ ] A. Using a Lambda function to query CloudTrail logs adds complexity and requires writing and maintaining code. It's also less efficient than EventBridge, which can directly capture the API event.
-   [ ] B. Configuring CloudTrail with SNS and then using Athena to query logs involves multiple services and more steps, increasing operational overhead. Athena is also better suited for analytical queries, not real-time alerting.
-   [ ] D. Using SQS to buffer CloudTrail logs and then processing them with Lambda adds unnecessary complexity and latency. EventBridge provides a more direct and efficient way to capture and react to API events.

Therefore, Option C is the most operationally efficient solution for capturing the `CreateImage` API call and sending alerts.

</details>

<details>
  <summary>Question 207</summary>

A company owns an asynchronous API that is used to ingest user requests and, based on the request type, dispatch requests to the appropriate microservice for processing. The company is using Amazon API Gateway to deploy the API front end, and an AWS Lambda function that invokes Amazon DynamoDB to store user requests before dispatching them to the processing microservices. The company provisioned as much DynamoDB throughput as its budget allows, but the company is still experiencing availability issues and is losing user requests. What should a solutions architect do to address this issue without impacting existing users?

-   [ ] A. Add throttling on the API Gateway with server-side throttling limits.
-   [ ] B. Use DynamoDB Accelerator (DAX) and Lambda to buffer writes to DynamoDB.
-   [ ] C. Create a secondary index in DynamoDB for the table with the user requests.
-   [ ] D. Use the Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer writes to DynamoDB.
  
</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Use the Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer writes to DynamoDB.
  
Why these are the correct answers:

D. Use the Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer writes to DynamoDB.

-   [ ]   Amazon SQS can act as a buffer between API Gateway/Lambda and DynamoDB, allowing requests to be queued during high traffic periods.
-   [ ]   This buffering prevents DynamoDB from being overwhelmed and helps avoid losing user requests.
-   [ ]   A Lambda function can then process the messages from the SQS queue and write them to DynamoDB at a controlled rate, ensuring that DynamoDB's throughput is not exceeded.

Why are the other answers wrong?

-   [ ]   A. Adding throttling on API Gateway might help to control the rate of requests, but it doesn't address the underlying issue of DynamoDB being unable to handle the write load. It could also result in losing user requests if they are throttled.
-   [ ]   B. DAX is a cache for DynamoDB, designed to accelerate read operations. It does not help with write-heavy scenarios or buffer writes.
-   [ ]   C. Creating a secondary index in DynamoDB can improve read performance but does not directly address the issue of high write volume causing availability issues.

Therefore, Option D is the most suitable solution to buffer writes to DynamoDB and prevent losing user requests.

</details>

<details>
  <summary>Question 208</summary>

A company needs to move data from an Amazon EC2 instance to an Amazon S3 bucket. The company must ensure that no API calls and no data are routed through public internet routes. Only the EC2 instance can have access to upload data to the S3 bucket. Which solution will meet these requirements?

- [ ] A. Create an interface VPC endpoint for Amazon S3 in the subnet where the EC2 instance is located. Attach a resource policy to the S3 bucket to only allow the EC2 instance's IAM role for access.
- [ ] B. Create a gateway VPC endpoint for Amazon S3 in the Availability Zone where the EC2 instance is located. Attach appropriate security groups to the endpoint. Attach a resource policy to the S3 bucket to only allow the EC2 instance's IAM role for access.
- [ ] C. Run the nslookup tool from inside the EC2 instance to obtain the private IP address of the S3 bucket's service API endpoint. Create a route in the VPC route table to provide the EC2 instance with access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance's IAM role for access.
- [ ] D. Use the AWS provided, publicly available ip-ranges.json file to obtain the private IP address of the S3 bucket's service API endpoint. Create a route in the VPC route table to provide the EC2 instance with access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance's IAM role for access.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Create an interface VPC endpoint for Amazon S3 in the subnet where the EC2 instance is located. Attach a resource policy to the S3 bucket to only allow the EC2 instance's IAM role for access.

Why these are the correct answers:

A. Create an interface VPC endpoint for Amazon S3 in the subnet where the EC2 instance is located. Attach a resource policy to the S3 bucket to only allow the EC2 instance's IAM role for access.

- [ ] Interface VPC endpoints use private IP addresses to access S3, ensuring that traffic does not traverse the public internet.
- [ ] Attaching a resource policy to the S3 bucket restricts access to only the specified IAM role of the EC2 instance, enhancing security.
- [ ] This solution meets the requirements of keeping traffic private and restricting access.

Why are the other answers wrong?

- [ ] B. Gateway VPC endpoints for S3 are accessed via a gateway and do not use private IP addresses in the same way as interface endpoints. They are simpler but do not provide the same level of network isolation. Security groups are not directly attached to gateway endpoints.
- [ ] C. Using `nslookup` to obtain private IP addresses is not a reliable or scalable solution. IP addresses can change, and this approach does not guarantee that traffic will remain within the AWS network.
- [ ] D. Similar to option C, relying on the `ip-ranges.json` file is not a robust method for ensuring private connectivity. These IP ranges can change, and it does not provide the security and reliability of VPC endpoints.

Therefore, Option A is the most secure and reliable solution for ensuring that data transfer to S3 from an EC2 instance remains within the AWS network and is properly secured.

</details>

<details>
  <summary>Question 209</summary>

A solutions architect is designing the architecture of a new application being deployed to the AWS Cloud. The application will run on Amazon EC2 On-Demand Instances and will automatically scale across multiple Availability Zones. The EC2 instances will scale up and down frequently throughout the day. An Application Load Balancer (ALB) will handle the load distribution. The architecture needs to support distributed session data management. The company is willing to make changes to code if needed. What should the solutions architect do to ensure that the architecture supports distributed session data management?

- [ ] A. Use Amazon ElastiCache to manage and store session data.
- [ ] B. Use session affinity (sticky sessions) of the ALB to manage session data.
- [ ] C. Use Session Manager from AWS Systems Manager to manage the session.
- [ ] D. Use the GetSessionToken API operation in AWS Security Token Service (AWS STS) to manage the session.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Use Amazon ElastiCache to manage and store session data.

Why these are the correct answers:

A. Use Amazon ElastiCache to manage and store session data.

- [ ] Amazon ElastiCache (either Memcached or Redis) provides a distributed, in-memory cache that can be used to store session data.
- [ ] This allows any EC2 instance to retrieve session data, regardless of which instance originally handled the user's request.
- [ ] It supports the dynamic nature of Auto Scaling, as instances can come and go without losing session information.

Why are the other answers wrong?

- [ ] B. Session affinity (sticky sessions) can maintain user sessions to specific instances, but it doesn't work well with Auto Scaling. Instances can be terminated, and traffic can be unevenly distributed.
- [ ] C. AWS Systems Manager Session Manager is used for managing EC2 instances, not for storing application session data.
- [ ] D. AWS STS GetSessionToken is for obtaining temporary security credentials, not for session data management.

Therefore, Option A is the most appropriate choice for managing distributed session data in a dynamically scaling environment.

</details>

<details>
  <summary>Question 210</summary>

A company offers a food delivery service that is growing rapidly. Because of the growth, the company's order processing system is experiencing scaling problems during peak traffic hours. The current architecture includes the following:

• A group of Amazon EC2 instances that run in an Amazon EC2 Auto Scaling group to collect orders from the application

• Another group of EC2 instances that run in an Amazon EC2 Auto Scaling group to fulfill orders

The order collection process occurs quickly, but the order fulfillment process can take longer. Data must not be lost because of a scaling event. A solutions architect must ensure that the order collection process and the order fulfillment process can both scale properly during peak traffic hours. The solution must optimize utilization of the company's AWS resources. Which solution meets these requirements?

- [ ] A. Use Amazon CloudWatch metrics to monitor the CPU of each instance in the Auto Scaling groups. Configure each Auto Scaling group's minimum capacity according to peak workload values.
- [ ] B. Use Amazon CloudWatch metrics to monitor the CPU of each instance in the Auto Scaling groups. Configure a CloudWatch alarm to invoke an Amazon Simple Notification Service (Amazon SNS) topic that creates additional Auto Scaling groups on demand.
- [ ] C. Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection and another for order fulfillment. Configure the EC2 instances to poll their respective queue. Scale the Auto Scaling groups based on notifications that the queues send.
- [ ] D. Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection and another for order fulfillment. Configure the EC2 instances to poll their respective queue. Create a metric based on a backlog per instance calculation. Scale the Auto Scaling groups based on this metric.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection and another for order fulfillment. Configure the EC2 instances to poll their respective queue. Create a metric based on a backlog per instance calculation. Scale the Auto Scaling groups based on this metric.

Why these are the correct answers:

D. Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection and another for order fulfillment. Configure the EC2 instances to poll their respective queue. Create a metric based on a backlog per instance calculation. Scale the Auto Scaling groups based on this metric.

- [ ] Using SQS queues decouples the order collection and fulfillment processes, allowing them to scale independently.
- [ ] Scaling based on a backlog per instance calculation ensures that the Auto Scaling groups respond to the actual workload (number of orders waiting to be processed) rather than just CPU utilization.
- [ ] This approach optimizes resource utilization by scaling based on demand and prevents order loss by queuing them.

Why are the other answers wrong?

- [ ] A. Configuring minimum capacity based on peak workload values does not optimize resource utilization; it leads to over-provisioning during off-peak hours.
- [ ] B. Scaling based on CPU utilization might not accurately reflect the order processing load. High CPU can be due to other processes, while low CPU can still have a large order backlog. Using SNS to create Auto Scaling groups is not a standard or efficient practice.
- [ ] C. Scaling based on notifications from queues doesn't provide a granular way to manage scaling based on the actual backlog of orders. It might lead to scaling actions that are not finely tuned to the demand.

Therefore, Option D provides the most efficient and accurate solution for scaling the order processing system based on the actual order backlog.

</details>

<details>
  <summary>Question 211</summary>

A company hosts multiple production applications. One of the applications consists of resources from Amazon EC2, AWS Lambda, Amazon RDS, Amazon Simple Notification Service (Amazon SNS), and Amazon Simple Queue Service (Amazon SQS) across multiple AWS Regions. All company resources are tagged with a tag name of "application" and a value that corresponds to each application. A solutions architect must provide the quickest solution for identifying all of the tagged components. Which solution meets these requirements?

- [ ] A. Use AWS CloudTrail to generate a list of resources with the application tag.
- [ ] B. Use the AWS CLI to query each service across all Regions to report the tagged components.
- [ ] C. Run a query in Amazon CloudWatch Logs Insights to report on the components with the application tag.
- [ ] D. Run a query with the AWS Resource Groups Tag Editor to report on the resources globally with the application tag.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Run a query with the AWS Resource Groups Tag Editor to report on the resources globally with the application tag.

Why these are the correct answers:

D. Run a query with the AWS Resource Groups Tag Editor to report on the resources globally with the application tag.

- [ ] AWS Resource Groups Tag Editor allows you to search for and manage tags across multiple AWS services and Regions in a single view.
- [ ] It is designed to quickly identify resources based on tags, providing the fastest way to meet the requirements.

Why are the other answers wrong?

- [ ] A. AWS CloudTrail records API calls but does not provide a direct, efficient way to list resources based on tags.
- [ ] B. Using the AWS CLI to query each service across all Regions would be time-consuming and complex, requiring multiple commands and scripts.
- [ ] C. Amazon CloudWatch Logs Insights is used for querying log data, not for listing AWS resources based on tags.

Therefore, Option D is the most efficient and direct solution for identifying tagged components across multiple services and Regions.

</details>

<details>
  <summary>Question 212</summary>

A company needs to export its database once a day to Amazon S3 for other teams to access. The exported object size varies between 2 GB and 5 GB. The S3 access pattern for the data is variable and changes rapidly. The data must be immediately available and must remain accessible for up to 3 months. The company needs the most cost-effective solution that will not increase retrieval time. Which S3 storage class should the company use to meet these requirements?

- [ ] A. S3 Intelligent-Tiering
- [ ] B. S3 Glacier Instant Retrieval
- [ ] C. S3 Standard
- [ ] D. S3 Standard-Infrequent Access (S3 Standard-IA)

</details>

<details>
  <summary>Answer</summary>

- [ ] A. S3 Intelligent-Tiering

Why these are the correct answers:

A. S3 Intelligent-Tiering

- [ ] S3 Intelligent-Tiering automatically optimizes storage costs by moving data between frequent and infrequent access tiers based on usage patterns.
- [ ] It ensures data is immediately available with no retrieval fees, making it suitable for variable access patterns and the requirement for immediate availability.
- [ ] This provides a cost-effective solution by reducing storage costs when data is accessed less frequently, without sacrificing performance.

Why are the other answers wrong?

- [ ] B. S3 Glacier Instant Retrieval is designed for long-term archive with immediate retrieval, but it is generally more expensive for frequently accessed data.
- [ ] C. S3 Standard is suitable for frequently accessed data but does not optimize costs for data with variable access patterns.
- [ ] D. S3 Standard-IA is more cost-effective for infrequently accessed data but has retrieval fees and can be more expensive if data access patterns vary significantly.

Therefore, Option A is the most cost-effective solution that maintains immediate availability for data with variable access patterns.

</details>

<details>
  <summary>Question 213</summary>

A company is developing a new mobile app. The company must implement proper traffic filtering to protect its Application Load Balancer (ALB) against common application-level attacks, such as cross-site scripting or SQL injection. The company has minimal infrastructure and operational staff. The company needs to reduce its share of the responsibility in managing, updating, and securing servers for its AWS environment. What should a solutions architect recommend to meet these requirements?

- [ ] A. Configure AWS WAF rules and associate them with the ALB.
- [ ] B. Deploy the application using Amazon S3 with public hosting enabled.
- [ ] C. Deploy AWS Shield Advanced and add the ALB as a protected resource.
- [ ] D. Create a new ALB that directs traffic to an Amazon EC2 instance running a third-party firewall, which then passes the traffic to the current ALB.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Configure AWS WAF rules and associate them with the ALB.

Why these are the correct answers:

A. Configure AWS WAF rules and associate them with the ALB.

- [ ] AWS WAF is a web application firewall that protects web applications from common web exploits.
- [ ] It integrates directly with the ALB, providing application-level filtering of traffic.
- [ ] AWS WAF is a managed service, reducing the operational overhead for the company.

Why are the other answers wrong?

- [ ] B. Deploying the application using Amazon S3 with public hosting enabled does not provide protection against application-level attacks.
- [ ] C. AWS Shield Advanced provides protection against DDoS attacks but does not filter specific application-level attacks like SQL injection or cross-site scripting.
- [ ] D. Creating a new ALB with a third-party firewall on an EC2 instance increases operational overhead and complexity, contradicting the requirement to minimize management.

Therefore, Option A is the most suitable solution to protect against application-level attacks with minimal operational overhead.

</details>

<details>
  <summary>Question 214</summary>

A company's reporting system delivers hundreds of .csv files to an Amazon S3 bucket each day. The company must convert these files to Apache Parquet format and must store the files in a transformed data bucket. Which solution will meet these requirements with the LEAST development effort?

- [ ] A. Create an Amazon EMR cluster with Apache Spark installed. Write a Spark application to transform the data. Use EMR File System (EMRFS) to write files to the transformed data bucket.
- [ ] B. Create an AWS Glue crawler to discover the data. Create an AWS Glue extract, transform, and load (ETL) job to transform the data. Specify the transformed data bucket in the output step.
- [ ] C. Use AWS Batch to create a job definition with Bash syntax to transform the data and output the data to the transformed data bucket. Use the job definition to submit a job. Specify an array job as the job type.
- [ ] D. Create an AWS Lambda function to transform the data and output the data to the transformed data bucket. Configure an event notification for the S3 bucket. Specify the Lambda function as the destination for the event notification.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Create an AWS Glue crawler to discover the data. Create an AWS Glue extract, transform, and load (ETL) job to transform the data. Specify the transformed data bucket in the output step.

Why these are the correct answers:

B. Create an AWS Glue crawler to discover the data. Create an AWS Glue extract, transform, and load (ETL) job to transform the data. Specify the transformed data bucket in the output step.

- [ ] AWS Glue provides a serverless ETL service that simplifies the process of transforming data.
- [ ] AWS Glue crawler can automatically infer the schema of the .csv files.
- [ ] AWS Glue ETL jobs can efficiently transform the data into Parquet format with minimal coding effort.

Why are the other answers wrong?

- [ ] A. Creating an Amazon EMR cluster and writing a Spark application involves more development and operational overhead compared to using AWS Glue.
- [ ] C. Using AWS Batch requires defining job definitions and managing dependencies, which is more complex than using AWS Glue for simple ETL tasks.
- [ ] D. Using AWS Lambda for transforming large numbers of files can be complex and may hit execution time limits. It also requires more effort to manage dependencies and scaling.

Therefore, Option B is the most efficient solution with the least development effort for transforming .csv files to Parquet.

</details>

<details>
  <summary>Question 215</summary>

A company has 700 TB of backup data stored in network attached storage (NAS) in its data center. This backup data need to be accessible for infrequent regulatory requests and must be retained 7 years. The company has decided to migrate this backup data from its data center to AWS. The migration must be complete within 1 month. The company has 500 Mbps of dedicated bandwidth on its public internet connection available for data transfer. What should a solutions architect do to migrate and store the data at the LOWEST cost?

- [ ] A. Order AWS Snowball devices to transfer the data. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.
- [ ] B. Deploy a VPN connection between the data center and Amazon VPC. Use the AWS CLI to copy the data from on premises to Amazon S3 Glacier.
- [ ] C. Provision a 500 Mbps AWS Direct Connect connection and transfer the data to Amazon S3. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.
- [ ] D. Use AWS DataSync to transfer the data and deploy a DataSync agent on premises. Use the DataSync task to copy files from the on-premises NAS storage to Amazon S3 Glacier.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Order AWS Snowball devices to transfer the data. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.

Why these are the correct answers:

A. Order AWS Snowball devices to transfer the data. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.

- [ ] AWS Snowball devices are cost-effective for transferring large amounts of data, especially when network bandwidth is limited.
- [ ] Amazon S3 Glacier Deep Archive provides the lowest-cost storage for long-term retention of infrequently accessed data.
- [ ] Using a lifecycle policy automates the transition of data to Glacier Deep Archive, reducing storage costs.

Why are the other answers wrong?

- [ ] B. Transferring 700 TB of data over a 500 Mbps internet connection would take a very long time, exceeding the 1-month migration requirement.
- [ ] C. Provisioning an AWS Direct Connect connection is more expensive than using Snowball for a one-time migration of this size.
- [ ] D. AWS DataSync is designed for ongoing data synchronization and is more expensive than Snowball for a large, one-time data migration.

Therefore, Option A is the most cost-effective and time-efficient solution for migrating and storing the backup data.

</details>

<details>
  <summary>Question 216</summary>

A company has a serverless website with millions of objects in an Amazon S3 bucket. The company uses the S3 bucket as the origin for an Amazon CloudFront distribution. The company did not set encryption on the S3 bucket before the objects were loaded. A solutions architect needs to enable encryption for all existing objects and for all objects that are added to the S3 bucket in the future. Which solution will meet these requirements with the LEAST amount of effort?

- [ ] A. Create a new S3 bucket. Turn on the default encryption settings for the new S3 bucket. Download all existing objects to temporary local storage. Upload the objects to the new S3 bucket.
- [ ] B. Turn on the default encryption settings for the S3 bucket. Use the S3 Inventory feature to create a .csv file that lists the unencrypted objects. Run an S3 Batch Operations job that uses the copy command to encrypt those objects.
- [ ] C. Create a new encryption key by using AWS Key Management Service (AWS KMS). Change the settings on the S3 bucket to use server-side encryption with AWS KMS managed encryption keys (SSE-KMS). Turn on versioning for the S3 bucket.
- [ ] D. Navigate to Amazon S3 in the AWS Management Console. Browse the S3 bucket's objects. Sort by the encryption field. Select each unencrypted object. Use the Modify button to apply default encryption settings to every unencrypted object in the S3 bucket.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Turn on the default encryption settings for the S3 bucket. Use the S3 Inventory feature to create a .csv file that lists the unencrypted objects. Run an S3 Batch Operations job that uses the copy command to encrypt those objects.

Why these are the correct answers:

B. Turn on the default encryption settings for the S3 bucket. Use the S3 Inventory feature to create a .csv file that lists the unencrypted objects. Run an S3 Batch Operations job that uses the copy command to encrypt those objects.

- [ ] S3 Batch Operations can efficiently encrypt existing objects at scale.
- [ ] Using S3 Inventory simplifies the process of identifying unencrypted objects.
- [ ] This approach minimizes the effort required to encrypt a large number of existing objects.
- [ ] Setting default encryption ensures that future objects are automatically encrypted.

Why are the other answers wrong?

- [ ] A. Creating a new bucket and copying all objects is time-consuming and involves more steps than using S3 Batch Operations.
- [ ] C. While creating a KMS key and setting default encryption is a valid approach for future objects, it does not efficiently encrypt existing objects.
- [ ] D. Manually encrypting objects through the AWS Management Console is impractical for millions of objects and requires significant manual effort.

Therefore, Option B is the most efficient solution for encrypting both existing and future objects with the least amount of effort.

</details>
<details>
  <summary>Question 217</summary>

A company runs a global web application on Amazon EC2 instances behind an Application Load Balancer. The application stores data in Amazon Aurora. The company needs to create a disaster recovery solution and can tolerate up to 30 minutes of downtime and potential data loss. The solution does not need to handle the load when the primary infrastructure is healthy. What should a solutions architect do to meet these requirements?

- [ ] A. Deploy the application with the required infrastructure elements in place. Use Amazon Route 53 to configure active-passive failover. Create an Aurora Replica in a second AWS Region.
- [ ] B. Host a scaled-down deployment of the application in a second AWS Region. Use Amazon Route 53 to configure active-active failover. Create an Aurora Replica in the second Region.
- [ ] C. Replicate the primary infrastructure in a second AWS Region. Use Amazon Route 53 to configure active-active failover. Create an Aurora database that is restored from the latest snapshot.
- [ ] D. Back up data with AWS Backup. Use the backup to create the required infrastructure in a second AWS Region. Use Amazon Route 53 to configure active-passive failover. Create an Aurora second primary instance in the second Region.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Deploy the application with the required infrastructure elements in place. Use Amazon Route 53 to configure active-passive failover. Create an Aurora Replica in a second AWS Region.

Why these are the correct answers:

A. Deploy the application with the required infrastructure elements in place. Use Amazon Route 53 to configure active-passive failover. Create an Aurora Replica in a second AWS Region.

- [ ] Active-passive failover with Route 53 allows for automatic redirection of traffic to a standby region in case of a failure.
- [ ] An Aurora Replica in a second region provides a ready-to-go database that can take over quickly.
- [ ] This solution meets the requirements for disaster recovery with a tolerance for some downtime and data loss.

Why are the other answers wrong?

- [ ] B. Active-active failover is designed for high availability, not disaster recovery, and involves running a scaled-down deployment, which is not required by the scenario.
- [ ] C. Replicating the entire primary infrastructure in a second region is more costly and complex than necessary for disaster recovery. Restoring from a snapshot can increase downtime.
- [ ] D. Using AWS Backup and restoring the infrastructure increases downtime and potential data loss compared to having a pre-configured replica. Aurora doesn't have a "second primary instance" in the same way as some other databases.

Therefore, Option A is the most suitable solution for a cost-effective disaster recovery plan with the specified recovery time objective (RTO).

</details>
<details>
  <summary>Question 218</summary>

A company has a web server running on an Amazon EC2 instance in a public subnet with an Elastic IP address. The default security group is assigned to the EC2 instance. The default network ACL has been modified to block all traffic. A solutions architect needs to make the web server accessible from everywhere on port 443. Which combination of steps will accomplish this task? (Choose two.)

- [ ] A. Create a security group with a rule to allow TCP port 443 from source 0.0.0.0/0.
- [ ] B. Create a security group with a rule to allow TCP port 443 to destination 0.0.0.0/0.
- [ ] C. Update the network ACL to allow TCP port 443 from source 0.0.0.0/0.
- [ ] D. Update the network ACL to allow inbound/outbound TCP port 443 from source 0.0.0.0/0 and to destination 0.0.0.0/0.
- [ ] E. Update the network ACL to allow inbound TCP port 443 from source 0.0.0.0/0 and outbound TCP port 32768-65535 to destination 0.0.0.0/0.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Create a security group with a rule to allow TCP port 443 from source 0.0.0.0/0.
- [ ] E. Update the network ACL to allow inbound TCP port 443 from source 0.0.0.0/0 and outbound TCP port 32768-65535 to destination 0.0.0.0/0.

Why these are the correct answers:

A. Create a security group with a rule to allow TCP port 443 from source 0.0.0.0/0.

- [ ] Security groups act as a virtual firewall for EC2 instances and must allow inbound traffic on port 443.
- [ ] 0.0.0.0/0 allows traffic from any IP address.

E. Update the network ACL to allow inbound TCP port 443 from source 0.0.0.0/0 and outbound TCP port 32768-65535 to destination 0.0.0.0/0.

- [ ] Network ACLs act as a firewall for the subnet and must also allow both inbound traffic on port 443 and outbound traffic on ephemeral ports (32768-65535) for the server's responses.

Why are the other answers wrong?

- [ ] B. Security group rules specify allowed traffic, not destination.
- [ ] C. Network ACLs need to allow both inbound and outbound traffic.
- [ ] D. Network ACLs require ephemeral port ranges to be open for responses.

Therefore, options A and E are necessary to allow inbound HTTPS traffic and the corresponding outbound responses.

</details>
<details>
  <summary>Question 219</summary>

A company's application is having performance issues. The application is stateful and needs to complete in-memory tasks on Amazon EC2 instances. The company used AWS CloudFormation to deploy infrastructure and used the M5 EC2 instance family. As traffic increased, the application performance degraded. Users are reporting delays when the users attempt to access the application. Which solution will resolve these issues in the MOST operationally efficient way?

- [ ] A. Replace the EC2 instances with T3 EC2 instances that run in an Auto Scaling group. Make the changes by using the AWS Management Console.
- [ ] B. Modify the CloudFormation templates to run the EC2 instances in an Auto Scaling group. Increase the desired capacity and the maximum capacity of the Auto Scaling group manually when an increase is necessary.
- [ ] C. Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Use Amazon CloudWatch built-in EC2 memory metrics to track the application performance for future capacity planning.
- [ ] D. Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Deploy the Amazon CloudWatch agent on the EC2 instances to generate custom application latency metrics for future capacity planning.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Deploy the Amazon CloudWatch agent on the EC2 instances to generate custom application latency metrics for future capacity planning.

Why these are the correct answers:

D. Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Deploy the Amazon CloudWatch agent on the EC2 instances to generate custom application latency metrics for future capacity planning.

- [ ] R5 instances are memory-optimized and suitable for in-memory tasks, addressing the application's needs.
- [ ] CloudFormation allows for infrastructure as code, enabling repeatable and manageable deployments.
- [ ] Custom latency metrics provide detailed insights into application performance, aiding in capacity planning.

Why are the other answers wrong?

- [ ] A. T3 instances are burstable and not ideal for consistent memory-intensive workloads. Making changes via the console is not operationally efficient.
- [ ] B. Manually adjusting Auto Scaling group capacity is not efficient and does not fully automate scaling.
- [ ] C. Built-in EC2 memory metrics might not provide the specific application-level latency metrics needed for detailed performance analysis.

Therefore, Option D provides the best combination of performance, scalability, and operational efficiency.

</details>
<details>
  <summary>Question 220</summary>

A solutions architect is designing a new API using Amazon API Gateway that will receive requests from users. The volume of requests is highly variable; several hours can pass without receiving a single request. The data processing will take place asynchronously but should be completed within a few seconds after a request is made. Which compute service should the solutions architect have the API invoke to deliver the requirements at the lowest cost?

- [ ] A. An AWS Glue job
- [ ] B. An AWS Lambda function
- [ ] C. A containerized service hosted in Amazon Elastic Kubernetes Service (Amazon EKS)
- [ ] D. A containerized service hosted in Amazon ECS with Amazon EC2

</details>

<details>
  <summary>Answer</summary>

- [ ] B. An AWS Lambda function

Why these are the correct answers:

B. An AWS Lambda function

- [ ] AWS Lambda is a serverless compute service that charges only for the compute time consumed.
- [ ] It scales automatically and can handle variable workloads, including periods of inactivity.
- [ ] Lambda functions can execute quickly and are suitable for asynchronous processing.

Why are the other answers wrong?

- [ ] A. AWS Glue jobs are designed for ETL (extract, transform, load) operations and are not suitable for real-time, low-latency processing.
- [ ] C. Amazon EKS involves running and managing container orchestration, which is more expensive and complex than Lambda for this use case.
- [ ] D. Amazon ECS with EC2 also requires managing infrastructure and is more expensive than Lambda for event-driven, variable workloads.

Therefore, Option B provides the most cost-effective solution for the given requirements.

</details>








































