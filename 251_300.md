<details>
  <summary>Question 251</summary>

An Amazon EC2 instance is located in a private subnet in a new VPC. This subnet does not have outbound internet access, but the EC2 instance needs the ability to download monthly security updates from an outside vendor. What should a solutions architect do to meet these requirements?

-   [ ] A. Create an internet gateway, and attach it to the VPC. Configure the private subnet route table to use the internet gateway as the default route.
-   [ ] B. Create a NAT gateway, and place it in a public subnet. Configure the private subnet route table to use the NAT gateway as the default route.
-   [ ] C. Create a NAT instance, and place it in the same subnet where the EC2 instance is located. Configure the private subnet route table to use the NAT instance as the default route.
-   [ ] D. Create an internet gateway, and attach it to the VPC. Create a NAT instance, and place it in the same subnet where the EC2 instance is located. Configure the private subnet route table to use the internet gateway as the default route.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Create a NAT gateway, and place it in a public subnet. Configure the private subnet route table to use the NAT gateway as the default route.
   
Why these are the correct answers:

B. Create a NAT gateway, and place it in a public subnet. Configure the private subnet route table to use the NAT gateway as the default route.

-   [ ] A NAT gateway in a public subnet allows instances in private subnets to access the internet.
-   [ ] The private subnet's route table directs outbound traffic to the NAT gateway.
-   [ ] This solution enables the EC2 instance to download updates securely.
   
Why are the other answers wrong?

-   [ ] A. An internet gateway allows public subnets to access the internet, but not private subnets.
-   [ ] C. A NAT instance requires more management than a NAT gateway. Placing it in the same subnet is incorrect.
-   [ ] D. Combining an internet gateway and a NAT instance is unnecessary and incorrect.

Therefore, Option B is the correct solution.

</details>
<details>
  <summary>Question 252</summary>

A solutions architect needs to design a system to store client case files. The files are core company assets and are important. The number of files will grow over time. The files must be simultaneously accessible from multiple application servers that run on Amazon EC2 instances. The solution must have built-in redundancy. Which solution meets these requirements?

-   [ ] A. Amazon Elastic File System (Amazon EFS)
-   [ ] B. Amazon Elastic Block Store (Amazon EBS)
-   [ ] C. Amazon S3 Glacier Deep Archive
-   [ ] D. AWS Backup
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Amazon Elastic File System (Amazon EFS)
   
Why these are the correct answers:

A. Amazon Elastic File System (Amazon EFS)

-   [ ] Amazon EFS provides scalable file storage for use with EC2 instances.
-   [ ] It supports concurrent access from multiple EC2 instances.
-   [ ] EFS is designed with built-in redundancy and scalability.
   
Why are the other answers wrong?

-   [ ] B. Amazon EBS is block storage and can only be attached to a single EC2 instance at a time.
-   [ ] C. Amazon S3 Glacier Deep Archive is for long-term archival, not concurrent access.
-   [ ] D. AWS Backup is for backup and recovery, not for providing shared file storage.

Therefore, Option A is the correct solution.

</details>

<details>
  <summary>Question 253</summary>

A solutions architect has created two IAM policies: Policy1 and Policy2. Both policies are attached to an IAM group.

![image](https://github.com/user-attachments/assets/33793b35-856c-4f09-be2a-7c4df3846bb2)

A cloud engineer is added as an IAM user to the IAM group. Which action will the cloud engineer be able to perform?

-   [ ] A. Deleting IAM users
-   [ ] B. Deleting directories
-   [ ] C. Deleting Amazon EC2 instances
-   [ ] D. Deleting logs from Amazon CloudWatch Logs
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Deleting Amazon EC2 instances
   
Why these are the correct answers:

C. Deleting Amazon EC2 instances

-   [ ] Policy 1 allows all EC2 actions.
-   [ ] Policy 2 denies "ds:Delete\*" actions.
-   [ ] Deny statements override allow statements, but the cloud engineer is still allowed to delete EC2 instances.

Why are the other answers wrong?

-   [ ] A. The engineer can't delete IAM users because it's not in Policy 1.
-   [ ] B. "ds:Delete\*" denies deleting directories.
-   [ ] D. "logs:Get\*" and "logs:Describe\*" do not allow deleting logs.

Therefore, Option C is the correct answer.

</details>

<details>
  <summary>Question 254</summary>

A company is reviewing a recent migration of a three-tier application to a VPC. The security team discovers that the principle of least privilege is not being applied to Amazon EC2 security group ingress and egress rules between the application tiers. What should a solutions architect do to correct this issue?

-   [ ] A. Create security group rules using the instance ID as the source or destination.
-   [ ] B. Create security group rules using the security group ID as the source or destination.
-   [ ] C. Create security group rules using the VPC CIDR blocks as the source or destination.
-   [ ] D. Create security group rules using the subnet CIDR blocks as the source or destination.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Create security group rules using the security group ID as the source or destination.
   
Why these are the correct answers:

B. Create security group rules using the security group ID as the source or destination.

-   [ ] Security groups can reference other security groups, applying least privilege.
-   [ ] This allows traffic only from instances in the specified security group.
   
Why are the other answers wrong?

-   [ ] A. Instance IDs are dynamic and not practical for security group rules.
-   [ ] C. VPC CIDR blocks are too broad and violate least privilege.
-   [ ] D. Subnet CIDR blocks are also too broad for tier-level security.

Therefore, Option B is the correct solution.

</details>
<details>
  <summary>Question 255</summary>

A company has an ecommerce checkout workflow that writes an order to a database and calls a service to process the payment. Users are experiencing timeouts during the checkout process. When users resubmit the checkout form, multiple unique orders are created for the same desired transaction. How should a solutions architect refactor this workflow to prevent the creation of multiple orders?

-   [ ] A. Configure the web application to send an order message to Amazon Kinesis Data Firehose. Set the payment service to retrieve the message from Kinesis Data Firehose and process the order.
-   [ ] B. Create a rule in AWS CloudTrail to invoke an AWS Lambda function based on the logged application path request. Use Lambda to query the database, call the payment service, and pass in the order information.
-   [ ] C. Store the order in the database. Send a message that includes the order number to Amazon Simple Notification Service (Amazon SNS). Set the payment service to poll Amazon SNS, retrieve the message, and process the order.
-   [ ] D. Store the order in the database. Send a message that includes the order number to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the payment service to retrieve the message and process the order. Delete the message from the queue.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Store the order in the database. Send a message that includes the order number to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the payment service to retrieve the message and process the order. Delete the message from the queue.
   
Why these are the correct answers:

D. Store the order in the database. Send a message that includes the order number to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the payment service to retrieve the message and process the order. Delete the message from the queue.

-   [ ] SQS FIFO queues ensure messages are processed exactly once and in order.
-   [ ] This prevents duplicate order creation by processing each order only once.
   
Why are the other answers wrong?

-   [ ] A. Kinesis Data Firehose is for streaming data to destinations, not for ensuring single processing.
-   [ ] B. CloudTrail is for API call logging, not for workflow management.
-   [ ] C. SNS is for pub/sub, not for ensuring single processing of messages.

Therefore, Option D is the correct solution.

</details>
<details>
  <summary>Question 256</summary>

A solutions architect is implementing a document review application using an Amazon S3 bucket for storage. The solution must prevent accidental deletion of the documents and ensure that all versions of the documents are available. Users must be able to download, modify, and upload documents. Which combination of actions should be taken to meet these requirements? (Choose two.)

-   [ ] A. Enable a read-only bucket ACL.
-   [ ] B. Enable versioning on the bucket.
-   [ ] C. Attach an IAM policy to the bucket.
-   [ ] D. Enable MFA Delete on the bucket.
-   [ ] E. Encrypt the bucket using AWS KMS.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Enable versioning on the bucket.
-   [ ] D. Enable MFA Delete on the bucket.
   
Why these are the correct answers:

B. Enable versioning on the bucket.

-   [ ] Versioning keeps multiple versions of an object, preventing data loss from overwrites or deletions.
   
D. Enable MFA Delete on the bucket.

-   [ ] MFA Delete requires multi-factor authentication for deletion, preventing accidental deletes.
   
Why are the other answers wrong?

-   [ ] A. Read-only ACL prevents users from modifying and uploading documents.
-   [ ] C. IAM policies control access but do not prevent accidental deletion.
-   [ ] E. Encryption secures data but does not prevent deletion.

Therefore, options B and D are the correct solutions.

</details>
<details>
  <summary>Question 257</summary>

A company is building a solution that will report Amazon EC2 Auto Scaling events across all the applications in an AWS account. The company needs to use a serverless solution to store the EC2 Auto Scaling status data in Amazon S3. The company then will use the data in Amazon S3 to provide near-real-time updates in a dashboard. The solution must not affect the speed of EC2 instance launches. How should the company move the data to Amazon S3 to meet these requirements?

-   [ ] A. Use an Amazon CloudWatch metric stream to send the EC2 Auto Scaling status data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.
-   [ ] B. Launch an Amazon EMR cluster to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.
-   [ ] C. Create an Amazon EventBridge rule to invoke an AWS Lambda function on a schedule. Configure the Lambda function to send the EC2 Auto Scaling status data directly to Amazon S3.
-   [ ] D. Use a bootstrap script during the launch of an EC2 instance to install Amazon Kinesis Agent. Configure Kinesis Agent to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Use an Amazon CloudWatch metric stream to send the EC2 Auto Scaling status data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.
   
Why these are the correct answers:

A. Use an Amazon CloudWatch metric stream to send the EC2 Auto Scaling status data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.

-   [ ] CloudWatch metric streams send data in near-real-time without affecting EC2 instance launches.
-   [ ] Kinesis Data Firehose efficiently delivers data to S3.
-   [ ] This is a serverless solution.
   
Why are the other answers wrong?

-   [ ] B. Launching an EMR cluster is not serverless and adds overhead.
-   [ ] C. Lambda on a schedule is not real-time.
-   [ ] D. Bootstrap scripts add overhead to EC2 launches.

Therefore, Option A is the correct solution.

</details>
<details>
  <summary>Question 258</summary>

A company has an application that places hundreds of .csv files into an Amazon S3 bucket every hour. The files are 1 GB in size. Each time a file is uploaded, the company needs to convert the file to Apache Parquet format and place the output file into an S3 bucket. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Create an AWS Lambda function to download the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Invoke the Lambda function for each S3 PUT event.
-   [ ] B. Create an Apache Spark job to read the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the Spark job.
-   [ ] C. Create an AWS Glue table and an AWS Glue crawler for the S3 bucket where the application places the .csv files. Schedule an AWS Lambda function to periodically use Amazon Athena to query the AWS Glue table, convert the query results into Parquet format, and place the output files into an S3 bucket.
-   [ ] D. Create an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Parquet format and place the output files into an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the ETL job.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Create an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Parquet format and place the output files into an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the ETL job.
   
Why these are the correct answers:

D. Create an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Parquet format and place the output files into an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the ETL job.

-   [ ] AWS Glue ETL jobs are designed for data transformation.
-   [ ] Lambda functions can trigger the ETL job on S3 PUT events.
-   [ ] This solution is efficient and managed.
   
Why are the other answers wrong?

-   [ ] A. Lambda functions may have limitations with large files and complex transformations.
-   [ ] B. Spark jobs require more setup and management.
-   [ ] C. Athena is for querying, not efficient for ETL.

Therefore, Option D is the correct solution.

</details>
<details>
  <summary>Question 259</summary>

A company is implementing new data retention policies for all databases that run on Amazon RDS DB instances. The company must retain daily backups for a minimum period of 2 years. The backups must be consistent and restorable. Which solution should a solutions architect recommend to meet these requirements?

-   [ ] A. Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a daily schedule and an expiration period of 2 years after creation. Assign the RDS DB instances to the backup plan.
-   [ ] B. Configure a backup window for the RDS DB instances for daily snapshots. Assign a snapshot retention policy of 2 years to each RDS DB instance. Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule snapshot deletions.
-   [ ] C. Configure database transaction logs to be automatically backed up to Amazon CloudWatch Logs with an expiration period of 2 years.
-   [ ] D. Configure an AWS Database Migration Service (AWS DMS) replication task. Deploy a replication instance, and configure a change data capture (CDC) task to stream database changes to Amazon S3 as the target. Configure S3 Lifecycle policies to delete the snapshots after 2 years.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a daily schedule and an expiration period of 2 years after creation. Assign the RDS DB instances to the backup plan.
   
Why these are the correct answers:

A. Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a daily schedule and an expiration period of 2 years after creation. Assign the RDS DB instances to the backup plan.

-   [ ] AWS Backup centrally manages backups.
-   [ ] Backup plans automate backup schedules and retention.
-   [ ] This solution meets the requirements for consistent and restorable backups.
   
Why are the other answers wrong?

-   [ ] B. RDS snapshots are point-in-time and not as flexible as AWS Backup. DLM is for EBS volumes, not RDS.
-   [ ] C. CloudWatch Logs are for log data, not database backups.
-   [ ] D. DMS is for database migration, not backups.

Therefore, Option A is the correct solution.

</details>
<details>
  <summary>Question 260</summary>

A company's compliance team needs to move its file shares to AWS. The shares run on a Windows Server SMB file share. A self-managed on-premises Active Directory controls access to the files and folders. The company wants to use Amazon FSx for Windows File Server as part of the solution. The company must ensure that the on-premises Active Directory groups restrict access to the FSx for Windows File Server SMB compliance shares, folders, and files after the move to AWS. The company has created an FSx for Windows File Server file system. Which solution will meet these requirements?

-   [ ] A. Create an Active Directory Connector to connect to the Active Directory. Map the Active Directory groups to IAM groups to restrict access.
-   [ ] B. Assign a tag with a Restrict tag key and a Compliance tag value. Map the Active Directory groups to IAM groups to restrict access.
-   [ ] C. Create an IAM service-linked role that is linked directly to FSx for Windows File Server to restrict access.
-   [ ] D. Join the file system to the Active Directory to restrict access.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Join the file system to the Active Directory to restrict access.
   
Why these are the correct answers:

D. Join the file system to the Active Directory to restrict access.

-   [ ] Joining the file system to the Active Directory preserves existing permissions.
-   [ ] This allows on-premises Active Directory groups to control access.
   
Why are the other answers wrong?

-   [ ] A. AD Connector connects to AD but doesn't directly enforce permissions. Mapping to IAM groups is incorrect.
-   [ ] B. Tags are for metadata, not access control. Mapping to IAM groups is incorrect.
-   [ ] C. IAM service-linked roles are for AWS services to access other AWS services, not for Active Directory permissions.

Therefore, Option D is the correct solution.

</details>



























