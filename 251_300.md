<details>
  <summary>Question 251</summary>

An Amazon EC2 instance is located in a private subnet in a new VPC. This subnet does not have outbound internet access, but the EC2 instance needs the ability to download monthly security updates from an outside vendor. What should a solutions architect do to meet these requirements?

-   [ ] A. Create an internet gateway, and attach it to the VPC. Configure the private subnet route table to use the internet gateway as the default route.
-   [ ] B. Create a NAT gateway, and place it in a public subnet. Configure the private subnet route table to use the NAT gateway as the default route.
-   [ ] C. Create a NAT instance, and place it in the same subnet where the EC2 instance is located. Configure the private subnet route table to use the NAT instance as the default route.
-   [ ] D. Create an internet gateway, and attach it to the VPC. Create a NAT instance, and place it in the same subnet where the EC2 instance is located. Configure the private subnet route table to use the internet gateway as the default route.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Create a NAT gateway, and place it in a public subnet. Configure the private subnet route table to use the NAT gateway as the default route.
   
Why these are the correct answers:

B. Create a NAT gateway, and place it in a public subnet. Configure the private subnet route table to use the NAT gateway as the default route.

-   [ ] A NAT gateway in a public subnet allows instances in private subnets to access the internet.
-   [ ] The private subnet's route table directs outbound traffic to the NAT gateway.
-   [ ] This solution enables the EC2 instance to download updates securely.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] A. An internet gateway allows public subnets to access the internet, but not private subnets.
-   [ ] C. A NAT instance requires more management than a NAT gateway. Placing it in the same subnet is incorrect.
-   [ ] D. Combining an internet gateway and a NAT instance is unnecessary and incorrect.

Therefore, Option B is the correct solution.

</details>
<details>
  <summary>Question 252</summary>

A solutions architect needs to design a system to store client case files. The files are core company assets and are important. The number of files will grow over time. The files must be simultaneously accessible from multiple application servers that run on Amazon EC2 instances. The solution must have built-in redundancy. Which solution meets these requirements?

-   [ ] A. Amazon Elastic File System (Amazon EFS)
-   [ ] B. Amazon Elastic Block Store (Amazon EBS)
-   [ ] C. Amazon S3 Glacier Deep Archive
-   [ ] D. AWS Backup
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Amazon Elastic File System (Amazon EFS)
   
Why these are the correct answers:

A. Amazon Elastic File System (Amazon EFS)

-   [ ] Amazon EFS provides scalable file storage for use with EC2 instances.
-   [ ] It supports concurrent access from multiple EC2 instances.
-   [ ] EFS is designed with built-in redundancy and scalability.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] B. Amazon EBS is block storage and can only be attached to a single EC2 instance at a time.
-   [ ] C. Amazon S3 Glacier Deep Archive is for long-term archival, not concurrent access.
-   [ ] D. AWS Backup is for backup and recovery, not for providing shared file storage.

Therefore, Option A is the correct solution.

</details>

<details>
  <summary>Question 253</summary>

A solutions architect has created two IAM policies: Policy1 and Policy2. Both policies are attached to an IAM group.

![image](https://github.com/user-attachments/assets/33793b35-856c-4f09-be2a-7c4df3846bb2)

A cloud engineer is added as an IAM user to the IAM group. Which action will the cloud engineer be able to perform?

-   [ ] A. Deleting IAM users
-   [ ] B. Deleting directories
-   [ ] C. Deleting Amazon EC2 instances
-   [ ] D. Deleting logs from Amazon CloudWatch Logs
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Deleting Amazon EC2 instances
   
Why these are the correct answers:

C. Deleting Amazon EC2 instances

-   [ ] Policy 1 allows all EC2 actions.
-   [ ] Policy 2 denies "ds:Delete\*" actions.
-   [ ] Deny statements override allow statements, but the cloud engineer is still allowed to delete EC2 instances.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. The engineer can't delete IAM users because it's not in Policy 1.
-   [ ] B. "ds:Delete\*" denies deleting directories.
-   [ ] D. "logs:Get\*" and "logs:Describe\*" do not allow deleting logs.

Therefore, Option C is the correct answer.

</details>

<details>
  <summary>Question 254</summary>

A company is reviewing a recent migration of a three-tier application to a VPC. The security team discovers that the principle of least privilege is not being applied to Amazon EC2 security group ingress and egress rules between the application tiers. What should a solutions architect do to correct this issue?

-   [ ] A. Create security group rules using the instance ID as the source or destination.
-   [ ] B. Create security group rules using the security group ID as the source or destination.
-   [ ] C. Create security group rules using the VPC CIDR blocks as the source or destination.
-   [ ] D. Create security group rules using the subnet CIDR blocks as the source or destination.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Create security group rules using the security group ID as the source or destination.
   
Why these are the correct answers:

B. Create security group rules using the security group ID as the source or destination.

-   [ ] Security groups can reference other security groups, applying least privilege.
-   [ ] This allows traffic only from instances in the specified security group.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] A. Instance IDs are dynamic and not practical for security group rules.
-   [ ] C. VPC CIDR blocks are too broad and violate least privilege.
-   [ ] D. Subnet CIDR blocks are also too broad for tier-level security.

Therefore, Option B is the correct solution.

</details>
<details>
  <summary>Question 255</summary>

A company has an ecommerce checkout workflow that writes an order to a database and calls a service to process the payment. Users are experiencing timeouts during the checkout process. When users resubmit the checkout form, multiple unique orders are created for the same desired transaction. How should a solutions architect refactor this workflow to prevent the creation of multiple orders?

-   [ ] A. Configure the web application to send an order message to Amazon Kinesis Data Firehose. Set the payment service to retrieve the message from Kinesis Data Firehose and process the order.
-   [ ] B. Create a rule in AWS CloudTrail to invoke an AWS Lambda function based on the logged application path request. Use Lambda to query the database, call the payment service, and pass in the order information.
-   [ ] C. Store the order in the database. Send a message that includes the order number to Amazon Simple Notification Service (Amazon SNS). Set the payment service to poll Amazon SNS, retrieve the message, and process the order.
-   [ ] D. Store the order in the database. Send a message that includes the order number to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the payment service to retrieve the message and process the order. Delete the message from the queue.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Store the order in the database. Send a message that includes the order number to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the payment service to retrieve the message and process the order. Delete the message from the queue.
   
Why these are the correct answers:

D. Store the order in the database. Send a message that includes the order number to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the payment service to retrieve the message and process the order. Delete the message from the queue.

-   [ ] SQS FIFO queues ensure messages are processed exactly once and in order.
-   [ ] This prevents duplicate order creation by processing each order only once.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] A. Kinesis Data Firehose is for streaming data to destinations, not for ensuring single processing.
-   [ ] B. CloudTrail is for API call logging, not for workflow management.
-   [ ] C. SNS is for pub/sub, not for ensuring single processing of messages.

Therefore, Option D is the correct solution.

</details>
<details>
  <summary>Question 256</summary>

A solutions architect is implementing a document review application using an Amazon S3 bucket for storage. The solution must prevent accidental deletion of the documents and ensure that all versions of the documents are available. Users must be able to download, modify, and upload documents. Which combination of actions should be taken to meet these requirements? (Choose two.)

-   [ ] A. Enable a read-only bucket ACL.
-   [ ] B. Enable versioning on the bucket.
-   [ ] C. Attach an IAM policy to the bucket.
-   [ ] D. Enable MFA Delete on the bucket.
-   [ ] E. Encrypt the bucket using AWS KMS.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Enable versioning on the bucket.
-   [ ] D. Enable MFA Delete on the bucket.
   
Why these are the correct answers:

B. Enable versioning on the bucket.

-   [ ] Versioning keeps multiple versions of an object, preventing data loss from overwrites or deletions.
   
D. Enable MFA Delete on the bucket.

-   [ ] MFA Delete requires multi-factor authentication for deletion, preventing accidental deletes.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] A. Read-only ACL prevents users from modifying and uploading documents.
-   [ ] C. IAM policies control access but do not prevent accidental deletion.
-   [ ] E. Encryption secures data but does not prevent deletion.

Therefore, options B and D are the correct solutions.

</details>
<details>
  <summary>Question 257</summary>

A company is building a solution that will report Amazon EC2 Auto Scaling events across all the applications in an AWS account. The company needs to use a serverless solution to store the EC2 Auto Scaling status data in Amazon S3. The company then will use the data in Amazon S3 to provide near-real-time updates in a dashboard. The solution must not affect the speed of EC2 instance launches. How should the company move the data to Amazon S3 to meet these requirements?

-   [ ] A. Use an Amazon CloudWatch metric stream to send the EC2 Auto Scaling status data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.
-   [ ] B. Launch an Amazon EMR cluster to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.
-   [ ] C. Create an Amazon EventBridge rule to invoke an AWS Lambda function on a schedule. Configure the Lambda function to send the EC2 Auto Scaling status data directly to Amazon S3.
-   [ ] D. Use a bootstrap script during the launch of an EC2 instance to install Amazon Kinesis Agent. Configure Kinesis Agent to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Use an Amazon CloudWatch metric stream to send the EC2 Auto Scaling status data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.
   
Why these are the correct answers:

A. Use an Amazon CloudWatch metric stream to send the EC2 Auto Scaling status data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.

-   [ ] CloudWatch metric streams send data in near-real-time without affecting EC2 instance launches.
-   [ ] Kinesis Data Firehose efficiently delivers data to S3.
-   [ ] This is a serverless solution.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] B. Launching an EMR cluster is not serverless and adds overhead.
-   [ ] C. Lambda on a schedule is not real-time.
-   [ ] D. Bootstrap scripts add overhead to EC2 launches.

Therefore, Option A is the correct solution.

</details>
<details>
  <summary>Question 258</summary>

A company has an application that places hundreds of .csv files into an Amazon S3 bucket every hour. The files are 1 GB in size. Each time a file is uploaded, the company needs to convert the file to Apache Parquet format and place the output file into an S3 bucket. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Create an AWS Lambda function to download the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Invoke the Lambda function for each S3 PUT event.
-   [ ] B. Create an Apache Spark job to read the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the Spark job.
-   [ ] C. Create an AWS Glue table and an AWS Glue crawler for the S3 bucket where the application places the .csv files. Schedule an AWS Lambda function to periodically use Amazon Athena to query the AWS Glue table, convert the query results into Parquet format, and place the output files into an S3 bucket.
-   [ ] D. Create an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Parquet format and place the output files into an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the ETL job.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Create an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Parquet format and place the output files into an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the ETL job.
   
Why these are the correct answers:

D. Create an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Parquet format and place the output files into an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the ETL job.

-   [ ] AWS Glue ETL jobs are designed for data transformation.
-   [ ] Lambda functions can trigger the ETL job on S3 PUT events.
-   [ ] This solution is efficient and managed.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] A. Lambda functions may have limitations with large files and complex transformations.
-   [ ] B. Spark jobs require more setup and management.
-   [ ] C. Athena is for querying, not efficient for ETL.

Therefore, Option D is the correct solution.

</details>
<details>
  <summary>Question 259</summary>

A company is implementing new data retention policies for all databases that run on Amazon RDS DB instances. The company must retain daily backups for a minimum period of 2 years. The backups must be consistent and restorable. Which solution should a solutions architect recommend to meet these requirements?

-   [ ] A. Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a daily schedule and an expiration period of 2 years after creation. Assign the RDS DB instances to the backup plan.
-   [ ] B. Configure a backup window for the RDS DB instances for daily snapshots. Assign a snapshot retention policy of 2 years to each RDS DB instance. Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule snapshot deletions.
-   [ ] C. Configure database transaction logs to be automatically backed up to Amazon CloudWatch Logs with an expiration period of 2 years.
-   [ ] D. Configure an AWS Database Migration Service (AWS DMS) replication task. Deploy a replication instance, and configure a change data capture (CDC) task to stream database changes to Amazon S3 as the target. Configure S3 Lifecycle policies to delete the snapshots after 2 years.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a daily schedule and an expiration period of 2 years after creation. Assign the RDS DB instances to the backup plan.
   
Why these are the correct answers:

A. Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a daily schedule and an expiration period of 2 years after creation. Assign the RDS DB instances to the backup plan.

-   [ ] AWS Backup centrally manages backups.
-   [ ] Backup plans automate backup schedules and retention.
-   [ ] This solution meets the requirements for consistent and restorable backups.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] B. RDS snapshots are point-in-time and not as flexible as AWS Backup. DLM is for EBS volumes, not RDS.
-   [ ] C. CloudWatch Logs are for log data, not database backups.
-   [ ] D. DMS is for database migration, not backups.

Therefore, Option A is the correct solution.

</details>
<details>
  <summary>Question 260</summary>

A company's compliance team needs to move its file shares to AWS. The shares run on a Windows Server SMB file share. A self-managed on-premises Active Directory controls access to the files and folders. The company wants to use Amazon FSx for Windows File Server as part of the solution. The company must ensure that the on-premises Active Directory groups restrict access to the FSx for Windows File Server SMB compliance shares, folders, and files after the move to AWS. The company has created an FSx for Windows File Server file system. Which solution will meet these requirements?

-   [ ] A. Create an Active Directory Connector to connect to the Active Directory. Map the Active Directory groups to IAM groups to restrict access.
-   [ ] B. Assign a tag with a Restrict tag key and a Compliance tag value. Map the Active Directory groups to IAM groups to restrict access.
-   [ ] C. Create an IAM service-linked role that is linked directly to FSx for Windows File Server to restrict access.
-   [ ] D. Join the file system to the Active Directory to restrict access.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Join the file system to the Active Directory to restrict access.
   
Why these are the correct answers:

D. Join the file system to the Active Directory to restrict access.

-   [ ] Joining the file system to the Active Directory preserves existing permissions.
-   [ ] This allows on-premises Active Directory groups to control access.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] A. AD Connector connects to AD but doesn't directly enforce permissions. Mapping to IAM groups is incorrect.
-   [ ] B. Tags are for metadata, not access control. Mapping to IAM groups is incorrect.
-   [ ] C. IAM service-linked roles are for AWS services to access other AWS services, not for Active Directory permissions.

Therefore, Option D is the correct solution.

</details>

<details>
  <summary>Question 261</summary>

A company recently announced the deployment of its retail website to a global audience. The website runs on multiple Amazon EC2 instances behind an Elastic Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company wants to provide its customers with different versions of content based on the devices that the customers use to access the website. Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)

-   [ ] A. Configure Amazon CloudFront to cache multiple versions of the content.
-   [ ] B. Configure a host header in a Network Load Balancer to forward traffic to different instances.
-   [ ] C. Configure a Lambda@Edge function to send specific objects to users based on the User-Agent header.
-   [ ] D. Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB). Configure the NLB to set up host-based routing to different EC2 instances.
-   [ ] E. Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB). Configure the NLB to set up path-based routing to different EC2 instances.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Configure Amazon CloudFront to cache multiple versions of the content.
-   [ ] C. Configure a Lambda@Edge function to send specific objects to users based on the User-Agent header.
   
Why these are the correct answers:

A. Configure Amazon CloudFront to cache multiple versions of the content.

-   [ ] CloudFront can cache different versions of content.
   
C. Configure a Lambda@Edge function to send specific objects to users based on the User-Agent header.

-   [ ] Lambda@Edge allows customization of content delivery based on headers like User-Agent.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] B. Network Load Balancers do not support host headers.
-   [ ] D. Global Accelerator is for performance, not content versioning. NLBs don't support host-based routing.
-   [ ] E. Global Accelerator is for performance, not content versioning. NLBs don't support path-based routing.

Therefore, options A and C are correct.

</details>
<details>
  <summary>Question 262</summary>

A company plans to use Amazon ElastiCache for its multi-tier web application. A solutions architect creates a Cache VPC for the ElastiCache cluster and an App VPC for the application's Amazon EC2 instances. Both VPCs are in the us-east-1 Region.

The solutions architect must implement a solution to provide the application's EC2 instances with access to the ElastiCache cluster. Which solution will meet these requirements MOST cost-effectively?

-   [ ] A. Create a peering connection between the VPCs. Add a route table entry for the peering connection in both VPCs. Configure an inbound rule for the ElastiCache cluster's security group to allow inbound connection from the application's security group.
-   [ ] B. Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route traffic through the Transit VPC. Configure an inbound rule for the ElastiCache cluster's security group to allow inbound connection from the application's security group.
-   [ ] C. Create a peering connection between the VPCs. Add a route table entry for the peering connection in both VPCs. Configure an inbound rule for the peering connection's security group to allow inbound connection from the application's security group.
-   [ ] D. Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route traffic through the Transit VPC. Configure an inbound rule for the Transit VPC's security group to allow inbound connection from the application's security group.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Create a peering connection between the VPCs. Add a route table entry for the peering connection in both VPCs. Configure an inbound rule for the ElastiCache cluster's security group to allow inbound connection from the application's security group.
   
Why these are the correct answers:

A. Create a peering connection between the VPCs. Add a route table entry for the peering connection in both VPCs. Configure an inbound rule for the ElastiCache cluster's security group to allow inbound connection from the application's security group.

-   [ ] VPC peering is the simplest and most cost-effective way to connect two VPCs.
-   [ ] Route table entries enable traffic flow.
-   [ ] Security groups control access to ElastiCache.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] B. Transit VPC is more complex and expensive for only two VPCs.
-   [ ] C. Security groups of ElastiCache, not peering, control access to ElastiCache.
-   [ ] D. Transit VPC is more complex and expensive than peering.

Therefore, Option A is the correct solution.

</details>
<details>
  <summary>Question 263</summary>

A company is building an application that consists of several microservices. The company has decided to use container technologies to deploy its software on AWS. The company needs a solution that minimizes the amount of ongoing effort for maintenance and scaling. The company cannot manage additional infrastructure. Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)

-   [ ] A. Deploy an Amazon Elastic Container Service (Amazon ECS) cluster.
-   [ ] B. Deploy the Kubernetes control plane on Amazon EC2 instances that span multiple Availability Zones.
-   [ ] C. Deploy an Amazon Elastic Container Service (Amazon ECS) service with an Amazon EC2 launch type. Specify a desired task number level of greater than or equal to 2.
-   [ ] D. Deploy an Amazon Elastic Container Service (Amazon ECS) service with a Fargate launch type. Specify a desired task number level of greater than or equal to 2.
-   [ ] E. Deploy Kubernetes worker nodes on Amazon EC2 instances that span multiple Availability Zones. Create a deployment that specifies two or more replicas for each microservice.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Deploy an Amazon Elastic Container Service (Amazon ECS) cluster.
-   [ ] D. Deploy an Amazon Elastic Container Service (Amazon ECS) service with a Fargate launch type. Specify a desired task number level of greater than or equal to 2.
   
Why these are the correct answers:

A. Deploy an Amazon Elastic Container Service (Amazon ECS) cluster.

-   [ ] ECS is a managed container orchestration service.
   
D. Deploy an Amazon Elastic Container Service (Amazon ECS) service with a Fargate launch type. Specify a desired task number level of greater than or equal to 2.

-   [ ] Fargate removes the need to manage underlying infrastructure.
-   [ ] Specifying at least two tasks ensures high availability.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] B. Managing the Kubernetes control plane adds operational overhead.
-   [ ] C. EC2 launch type requires managing EC2 instances.
-   [ ] E. Managing Kubernetes worker nodes adds operational overhead.

Therefore, options A and D are correct.

</details>
<details>
  <summary>Question 264</summary>

A company has a web application hosted over 10 Amazon EC2 instances with traffic directed by Amazon Route 53. The company occasionally experiences a timeout error when attempting to browse the application. The networking team finds that some DNS queries return IP addresses of unhealthy instances, resulting in the timeout error. What should a solutions architect implement to overcome these timeout errors?

-   [ ] A. Create a Route 53 simple routing policy record for each EC2 instance. Associate a health check with each record.
-   [ ] B. Create a Route 53 failover routing policy record for each EC2 instance. Associate a health check with each record.
-   [ ] C. Create an Amazon CloudFront distribution with EC2 instances as its origin. Associate a health check with the EC2 instances.
-   [ ] D. Create an Application Load Balancer (ALB) with a health check in front of the EC2 instances. Route to the ALB from Route 53.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Create an Application Load Balancer (ALB) with a health check in front of the EC2 instances. Route to the ALB from Route 53.
   
Why these are the correct answers:

D. Create an Application Load Balancer (ALB) with a health check in front of the EC2 instances. Route to the ALB from Route 53.

-   [ ] ALB health checks ensure traffic is routed only to healthy instances.
-   [ ] Route 53 directs traffic to the ALB.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] A. Simple routing does not provide health checks.
-   [ ] B. Failover routing is for disaster recovery, not load balancing.
-   [ ] C. CloudFront is for caching, not load balancing with health checks.

Therefore, Option D is the correct solution.

</details>
<details>
  <summary>Question 265</summary>

A solutions architect needs to design a highly available application consisting of web, application, and database tiers. HTTPS content delivery should be as close to the edge as possible, with the least delivery time. Which solution meets these requirements and is MOST secure?

-   [ ] A. Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in public subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the origin.
-   [ ] B. Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliver HTTPS content using the EC2 instances as the origin.
-   [ ] C. Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the origin.
-   [ ] D. Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in public subnets. Configure Amazon CloudFront to deliver HTTPS content using the EC2 instances as the origin.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the origin.
   
Why these are the correct answers:

C. Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the origin.

-   [ ] CloudFront delivers content close to users (edge locations).
-   [ ] ALB distributes traffic to EC2 instances.
-   [ ] Private subnets secure EC2 instances.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] A. EC2 instances in public subnets are less secure.
-   [ ] B. CloudFront needs a public endpoint like an ALB, not EC2 instances directly. EC2 instances in private subnets can't be directly accessed from the internet.
-   [ ] D. EC2 instances in public subnets are less secure, and CloudFront needs a public endpoint.

Therefore, Option C is the correct solution.

</details>
<details>
  <summary>Question 266</summary>

A company has a popular gaming platform running on AWS. The application is sensitive to latency because latency can impact the user experience and introduce unfair advantages to some players. The application is deployed in every AWS Region. It runs on Amazon EC2 instances that are part of Auto Scaling groups configured behind Application Load Balancers (ALBs). A solutions architect needs to implement a mechanism to monitor the health of the application and redirect traffic to healthy endpoints. Which solution meets these requirements?

-   [ ] A. Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the application listens on, and attach it to a Regional endpoint in each Region. Add the ALB as the endpoint.
-   [ ] B. Create an Amazon CloudFront distribution and specify the ALB as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the traffic.
-   [ ] C. Create an Amazon CloudFront distribution and specify Amazon S3 as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the traffic.
-   [ ] D. Configure an Amazon DynamoDB database to serve as the data store for the application. Create a DynamoDB Accelerator (DAX) cluster to act as the in-memory cache for DynamoDB hosting the application data.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the application listens on, and attach it to a Regional endpoint in each Region. Add the ALB as the endpoint.
   
Why these are the correct answers:

A. Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the application listens on, and attach it to a Regional endpoint in each Region. Add the ALB as the endpoint.

-   [ ] Global Accelerator improves performance by routing traffic to the nearest healthy endpoint.
-   [ ] It monitors application health and redirects traffic away from unhealthy endpoints.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] B. CloudFront is for caching content, not for real-time traffic routing based on health.
-   [ ] C. CloudFront with S3 is for static content, not dynamic application traffic.
-   [ ] D. DynamoDB and DAX are for database performance, not application health and traffic routing.

Therefore, Option A is the correct solution.

</details>
<details>
  <summary>Question 267</summary>

A company has one million users that use its mobile app. The company must analyze the data usage in near-real time. The company also must encrypt the data in near-real time and must store the data in a centralized location in Apache Parquet format for further processing. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data. Invoke an AWS Lambda function to send the data to the Kinesis Data Analytics application.
-   [ ] B. Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon EMR cluster to analyze the data. Invoke an AWS Lambda function to send the data to the EMR cluster.
-   [ ] C. Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon EMR cluster to analyze the data.
-   [ ] D. Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data.
   
Why these are the correct answers:

D. Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data.

-   [ ] Kinesis Data Firehose can deliver data to S3 and convert it to Parquet.
-   [ ] Kinesis Data Analytics can analyze data in near-real time.
-   [ ] This is a managed solution with minimal overhead.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] A. Kinesis Data Streams and Lambda add complexity.
-   [ ] B. EMR clusters add operational overhead.
-   [ ] C. EMR clusters add operational overhead.

Therefore, Option D is the correct solution.

</details>
<details>
  <summary>Question 268</summary>

A gaming company has a web application that displays scores. The application runs on Amazon EC2 instances behind an Application Load Balancer. The application stores data in an Amazon RDS for MySQL database. Users are starting to experience long delays and interruptions that are caused by database read performance. The company wants to improve the user experience while minimizing changes to the application's architecture. What should a solutions architect do to meet these requirements?

-   [ ] A. Use Amazon ElastiCache in front of the database.
-   [ ] B. Use RDS Proxy between the application and the database.
-   [ ] C. Migrate the application from EC2 instances to AWS Lambda.
-   [ ] D. Migrate the database from Amazon RDS for MySQL to Amazon DynamoDB.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Use Amazon ElastiCache in front of the database.
   
Why these are the correct answers:

A. Use Amazon ElastiCache in front of the database.

-   [ ] ElastiCache can cache frequently read data, reducing database load.
-   [ ] This improves read performance with minimal changes to the application.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] B. RDS Proxy is for connection management, not caching.
-   [ ] C. Migrating to Lambda requires significant application changes.
-   [ ] D. Migrating to DynamoDB requires significant application changes.

Therefore, Option A is the correct solution.

</details>
<details>
  <summary>Question 269</summary>

An ecommerce company has noticed performance degradation of its Amazon RDS based web application. The performance degradation is attributed to an increase in the number of read-only SQL queries triggered by business analysts. A solutions architect needs to solve the problem with minimal changes to the existing web application. What should the solutions architect recommend?

-   [ ] A. Export the data to Amazon DynamoDB and have the business analysts run their queries.
-   [ ] B. Load the data into Amazon ElastiCache and have the business analysts run their queries.
-   [ ] C. Create a read replica of the primary database and have the business analysts run their queries.
-   [ ] D. Copy the data into an Amazon Redshift cluster and have the business analysts run their queries.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Create a read replica of the primary database and have the business analysts run their queries.
   
Why these are the correct answers:

C. Create a read replica of the primary database and have the business analysts run their queries.

-   [ ] Read replicas offload read queries from the primary database.
-   [ ] This improves performance with minimal application changes.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] A. DynamoDB is not suitable for complex SQL queries.
-   [ ] B. ElastiCache is for caching, not for running analytical queries.
-   [ ] D. Redshift is for data warehousing and is overkill for simple read queries.

Therefore, Option C is the correct solution.

</details>
<details>
  <summary>Question 270</summary>

A company is using a centralized AWS account to store log data in various Amazon S3 buckets. A solutions architect needs to ensure that the data is encrypted at rest before the data is uploaded to the S3 buckets. The data also must be encrypted in transit. Which solution meets these requirements?

-   [ ] A. Use client-side encryption to encrypt the data that is being uploaded to the S3 buckets.
-   [ ] B. Use server-side encryption to encrypt the data that is being uploaded to the S3 buckets.
-   [ ] C. Create bucket policies that require the use of server-side encryption with S3 managed encryption keys (SSE-S3) for S3 uploads.
-   [ ] D. Enable the security option to encrypt the S3 buckets through the use of a default AWS Key Management Service (AWS KMS) key.
   
</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Use client-side encryption to encrypt the data that is being uploaded to the S3 buckets.
   
Why these are the correct answers:

A. Use client-side encryption to encrypt the data that is being uploaded to the S3 buckets.

-   [ ] Client-side encryption ensures data is encrypted before transit and at rest.
   
<hr> Why are the other answers wrong? <hr>

-   [ ] B. Server-side encryption encrypts data at rest but not before transit.
-   [ ] C. Bucket policies enforce encryption but don't encrypt data before transit.
-   [ ] D. This option does not exist.

Therefore, Option A is the correct solution.

</details>
















