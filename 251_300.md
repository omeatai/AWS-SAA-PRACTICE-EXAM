<details>
  <summary>Question 251</summary>

An Amazon EC2 instance is located in a private subnet in a new VPC. This subnet does not have outbound internet access, but the EC2 instance needs the ability to download monthly security updates from an outside vendor. What should a solutions architect do to meet these requirements?

- [ ] A. Create an internet gateway, and attach it to the VPC. Configure the private subnet route table to use the internet gateway as the default route.
- [ ] B. Create a NAT gateway, and place it in a public subnet. Configure the private subnet route table to use the NAT gateway as the default route.
- [ ] C. Create a NAT instance, and place it in the same subnet where the EC2 instance is located. Configure the private subnet route table to use the NAT instance as the default route.
- [ ] D. Create an internet gateway, and attach it to the VPC. Create a NAT instance, and place it in the same subnet where the EC2 instance is located. Configure the private subnet route table to use the internet gateway as the default route.
   
</details>

<details>
  <summary>Answer</summary>

- [ ] B. Create a NAT gateway, and place it in a public subnet. Configure the private subnet route table to use the NAT gateway as the default route.
   
Why these are the correct answers:

B. Create a NAT gateway, and place it in a public subnet. Configure the private subnet route table to use the NAT gateway as the default route.

- [ ] A NAT gateway in a public subnet allows instances in private subnets to access the internet.
- [ ] The private subnet's route table directs outbound traffic to the NAT gateway.
- [ ] This solution enables the EC2 instance to download updates securely.
   
<hr> Why are the other answers wrong? <hr>

- [ ] A. An internet gateway allows public subnets to access the internet, but not private subnets.
- [ ] C. A NAT instance requires more management than a NAT gateway. Placing it in the same subnet is incorrect.
- [ ] D. Combining an internet gateway and a NAT instance is unnecessary and incorrect.

Therefore, Option B is the correct solution.

</details>
<details>
  <summary>Question 252</summary>

A solutions architect needs to design a system to store client case files. The files are core company assets and are important. The number of files will grow over time. The files must be simultaneously accessible from multiple application servers that run on Amazon EC2 instances. The solution must have built-in redundancy. Which solution meets these requirements?

- [ ] A. Amazon Elastic File System (Amazon EFS)
- [ ] B. Amazon Elastic Block Store (Amazon EBS)
- [ ] C. Amazon S3 Glacier Deep Archive
- [ ] D. AWS Backup
   
</details>

<details>
  <summary>Answer</summary>

- [ ] A. Amazon Elastic File System (Amazon EFS)
   
Why these are the correct answers:

A. Amazon Elastic File System (Amazon EFS)

- [ ] Amazon EFS provides scalable file storage for use with EC2 instances.
- [ ] It supports concurrent access from multiple EC2 instances.
- [ ] EFS is designed with built-in redundancy and scalability.
   
<hr> Why are the other answers wrong? <hr>

- [ ] B. Amazon EBS is block storage and can only be attached to a single EC2 instance at a time.
- [ ] C. Amazon S3 Glacier Deep Archive is for long-term archival, not concurrent access.
- [ ] D. AWS Backup is for backup and recovery, not for providing shared file storage.

Therefore, Option A is the correct solution.

</details>

<details>
  <summary>Question 253</summary>

A solutions architect has created two IAM policies: Policy1 and Policy2. Both policies are attached to an IAM group.

![image](https://github.com/user-attachments/assets/33793b35-856c-4f09-be2a-7c4df3846bb2)

A cloud engineer is added as an IAM user to the IAM group. Which action will the cloud engineer be able to perform?

- [ ] A. Deleting IAM users
- [ ] B. Deleting directories
- [ ] C. Deleting Amazon EC2 instances
- [ ] D. Deleting logs from Amazon CloudWatch Logs
   
</details>

<details>
  <summary>Answer</summary>

- [ ] C. Deleting Amazon EC2 instances
   
Why these are the correct answers:

C. Deleting Amazon EC2 instances

- [ ] Policy 1 allows all EC2 actions.
- [ ] Policy 2 denies "ds:Delete\*" actions.
- [ ] Deny statements override allow statements, but the cloud engineer is still allowed to delete EC2 instances.

<hr> Why are the other answers wrong? <hr>

- [ ] A. The engineer can't delete IAM users because it's not in Policy 1.
- [ ] B. "ds:Delete\*" denies deleting directories.
- [ ] D. "logs:Get\*" and "logs:Describe\*" do not allow deleting logs.

Therefore, Option C is the correct answer.

</details>

<details>
  <summary>Question 254</summary>

A company is reviewing a recent migration of a three-tier application to a VPC. The security team discovers that the principle of least privilege is not being applied to Amazon EC2 security group ingress and egress rules between the application tiers. What should a solutions architect do to correct this issue?

- [ ] A. Create security group rules using the instance ID as the source or destination.
- [ ] B. Create security group rules using the security group ID as the source or destination.
- [ ] C. Create security group rules using the VPC CIDR blocks as the source or destination.
- [ ] D. Create security group rules using the subnet CIDR blocks as the source or destination.
   
</details>

<details>
  <summary>Answer</summary>

- [ ] B. Create security group rules using the security group ID as the source or destination.
   
Why these are the correct answers:

B. Create security group rules using the security group ID as the source or destination.

- [ ] Security groups can reference other security groups, applying least privilege.
- [ ] This allows traffic only from instances in the specified security group.
   
<hr> Why are the other answers wrong? <hr>

- [ ] A. Instance IDs are dynamic and not practical for security group rules.
- [ ] C. VPC CIDR blocks are too broad and violate least privilege.
- [ ] D. Subnet CIDR blocks are also too broad for tier-level security.

Therefore, Option B is the correct solution.

</details>
<details>
  <summary>Question 255</summary>

A company has an ecommerce checkout workflow that writes an order to a database and calls a service to process the payment. Users are experiencing timeouts during the checkout process. When users resubmit the checkout form, multiple unique orders are created for the same desired transaction. How should a solutions architect refactor this workflow to prevent the creation of multiple orders?

- [ ] A. Configure the web application to send an order message to Amazon Kinesis Data Firehose. Set the payment service to retrieve the message from Kinesis Data Firehose and process the order.
- [ ] B. Create a rule in AWS CloudTrail to invoke an AWS Lambda function based on the logged application path request. Use Lambda to query the database, call the payment service, and pass in the order information.
- [ ] C. Store the order in the database. Send a message that includes the order number to Amazon Simple Notification Service (Amazon SNS). Set the payment service to poll Amazon SNS, retrieve the message, and process the order.
- [ ] D. Store the order in the database. Send a message that includes the order number to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the payment service to retrieve the message and process the order. Delete the message from the queue.
   
</details>

<details>
  <summary>Answer</summary>

- [ ] D. Store the order in the database. Send a message that includes the order number to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the payment service to retrieve the message and process the order. Delete the message from the queue.
   
Why these are the correct answers:

D. Store the order in the database. Send a message that includes the order number to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the payment service to retrieve the message and process the order. Delete the message from the queue.

- [ ] SQS FIFO queues ensure messages are processed exactly once and in order.
- [ ] This prevents duplicate order creation by processing each order only once.
   
<hr> Why are the other answers wrong? <hr>

- [ ] A. Kinesis Data Firehose is for streaming data to destinations, not for ensuring single processing.
- [ ] B. CloudTrail is for API call logging, not for workflow management.
- [ ] C. SNS is for pub/sub, not for ensuring single processing of messages.

Therefore, Option D is the correct solution.

</details>
<details>
  <summary>Question 256</summary>

A solutions architect is implementing a document review application using an Amazon S3 bucket for storage. The solution must prevent accidental deletion of the documents and ensure that all versions of the documents are available. Users must be able to download, modify, and upload documents. Which combination of actions should be taken to meet these requirements? (Choose two.)

- [ ] A. Enable a read-only bucket ACL.
- [ ] B. Enable versioning on the bucket.
- [ ] C. Attach an IAM policy to the bucket.
- [ ] D. Enable MFA Delete on the bucket.
- [ ] E. Encrypt the bucket using AWS KMS.
   
</details>

<details>
  <summary>Answer</summary>

- [ ] B. Enable versioning on the bucket.
- [ ] D. Enable MFA Delete on the bucket.
   
Why these are the correct answers:

B. Enable versioning on the bucket.

- [ ] Versioning keeps multiple versions of an object, preventing data loss from overwrites or deletions.
   
D. Enable MFA Delete on the bucket.

- [ ] MFA Delete requires multi-factor authentication for deletion, preventing accidental deletes.
   
<hr> Why are the other answers wrong? <hr>

- [ ] A. Read-only ACL prevents users from modifying and uploading documents.
- [ ] C. IAM policies control access but do not prevent accidental deletion.
- [ ] E. Encryption secures data but does not prevent deletion.

Therefore, options B and D are the correct solutions.

</details>
<details>
  <summary>Question 257</summary>

A company is building a solution that will report Amazon EC2 Auto Scaling events across all the applications in an AWS account. The company needs to use a serverless solution to store the EC2 Auto Scaling status data in Amazon S3. The company then will use the data in Amazon S3 to provide near-real-time updates in a dashboard. The solution must not affect the speed of EC2 instance launches. How should the company move the data to Amazon S3 to meet these requirements?

- [ ] A. Use an Amazon CloudWatch metric stream to send the EC2 Auto Scaling status data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.
- [ ] B. Launch an Amazon EMR cluster to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.
- [ ] C. Create an Amazon EventBridge rule to invoke an AWS Lambda function on a schedule. Configure the Lambda function to send the EC2 Auto Scaling status data directly to Amazon S3.
- [ ] D. Use a bootstrap script during the launch of an EC2 instance to install Amazon Kinesis Agent. Configure Kinesis Agent to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.
   
</details>

<details>
  <summary>Answer</summary>

- [ ] A. Use an Amazon CloudWatch metric stream to send the EC2 Auto Scaling status data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.
   
Why these are the correct answers:

A. Use an Amazon CloudWatch metric stream to send the EC2 Auto Scaling status data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.

- [ ] CloudWatch metric streams send data in near-real-time without affecting EC2 instance launches.
- [ ] Kinesis Data Firehose efficiently delivers data to S3.
- [ ] This is a serverless solution.
   
<hr> Why are the other answers wrong? <hr>

- [ ] B. Launching an EMR cluster is not serverless and adds overhead.
- [ ] C. Lambda on a schedule is not real-time.
- [ ] D. Bootstrap scripts add overhead to EC2 launches.

Therefore, Option A is the correct solution.

</details>
<details>
  <summary>Question 258</summary>

A company has an application that places hundreds of .csv files into an Amazon S3 bucket every hour. The files are 1 GB in size. Each time a file is uploaded, the company needs to convert the file to Apache Parquet format and place the output file into an S3 bucket. Which solution will meet these requirements with the LEAST operational overhead?

- [ ] A. Create an AWS Lambda function to download the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Invoke the Lambda function for each S3 PUT event.
- [ ] B. Create an Apache Spark job to read the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the Spark job.
- [ ] C. Create an AWS Glue table and an AWS Glue crawler for the S3 bucket where the application places the .csv files. Schedule an AWS Lambda function to periodically use Amazon Athena to query the AWS Glue table, convert the query results into Parquet format, and place the output files into an S3 bucket.
- [ ] D. Create an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Parquet format and place the output files into an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the ETL job.
   
</details>

<details>
  <summary>Answer</summary>

- [ ] D. Create an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Parquet format and place the output files into an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the ETL job.
   
Why these are the correct answers:

D. Create an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Parquet format and place the output files into an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the ETL job.

- [ ] AWS Glue ETL jobs are designed for data transformation.
- [ ] Lambda functions can trigger the ETL job on S3 PUT events.
- [ ] This solution is efficient and managed.
   
<hr> Why are the other answers wrong? <hr>

- [ ] A. Lambda functions may have limitations with large files and complex transformations.
- [ ] B. Spark jobs require more setup and management.
- [ ] C. Athena is for querying, not efficient for ETL.

Therefore, Option D is the correct solution.

</details>
<details>
  <summary>Question 259</summary>

A company is implementing new data retention policies for all databases that run on Amazon RDS DB instances. The company must retain daily backups for a minimum period of 2 years. The backups must be consistent and restorable. Which solution should a solutions architect recommend to meet these requirements?

- [ ] A. Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a daily schedule and an expiration period of 2 years after creation. Assign the RDS DB instances to the backup plan.
- [ ] B. Configure a backup window for the RDS DB instances for daily snapshots. Assign a snapshot retention policy of 2 years to each RDS DB instance. Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule snapshot deletions.
- [ ] C. Configure database transaction logs to be automatically backed up to Amazon CloudWatch Logs with an expiration period of 2 years.
- [ ] D. Configure an AWS Database Migration Service (AWS DMS) replication task. Deploy a replication instance, and configure a change data capture (CDC) task to stream database changes to Amazon S3 as the target. Configure S3 Lifecycle policies to delete the snapshots after 2 years.
   
</details>

<details>
  <summary>Answer</summary>

- [ ] A. Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a daily schedule and an expiration period of 2 years after creation. Assign the RDS DB instances to the backup plan.
   
Why these are the correct answers:

A. Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a daily schedule and an expiration period of 2 years after creation. Assign the RDS DB instances to the backup plan.

- [ ] AWS Backup centrally manages backups.
- [ ] Backup plans automate backup schedules and retention.
- [ ] This solution meets the requirements for consistent and restorable backups.
   
<hr> Why are the other answers wrong? <hr>

- [ ] B. RDS snapshots are point-in-time and not as flexible as AWS Backup. DLM is for EBS volumes, not RDS.
- [ ] C. CloudWatch Logs are for log data, not database backups.
- [ ] D. DMS is for database migration, not backups.

Therefore, Option A is the correct solution.

</details>
<details>
  <summary>Question 260</summary>

A company's compliance team needs to move its file shares to AWS. The shares run on a Windows Server SMB file share. A self-managed on-premises Active Directory controls access to the files and folders. The company wants to use Amazon FSx for Windows File Server as part of the solution. The company must ensure that the on-premises Active Directory groups restrict access to the FSx for Windows File Server SMB compliance shares, folders, and files after the move to AWS. The company has created an FSx for Windows File Server file system. Which solution will meet these requirements?

- [ ] A. Create an Active Directory Connector to connect to the Active Directory. Map the Active Directory groups to IAM groups to restrict access.
- [ ] B. Assign a tag with a Restrict tag key and a Compliance tag value. Map the Active Directory groups to IAM groups to restrict access.
- [ ] C. Create an IAM service-linked role that is linked directly to FSx for Windows File Server to restrict access.
- [ ] D. Join the file system to the Active Directory to restrict access.
   
</details>

<details>
  <summary>Answer</summary>

- [ ] D. Join the file system to the Active Directory to restrict access.
   
Why these are the correct answers:

D. Join the file system to the Active Directory to restrict access.

- [ ] Joining the file system to the Active Directory preserves existing permissions.
- [ ] This allows on-premises Active Directory groups to control access.
   
<hr> Why are the other answers wrong? <hr>

- [ ] A. AD Connector connects to AD but doesn't directly enforce permissions. Mapping to IAM groups is incorrect.
- [ ] B. Tags are for metadata, not access control. Mapping to IAM groups is incorrect.
- [ ] C. IAM service-linked roles are for AWS services to access other AWS services, not for Active Directory permissions.

Therefore, Option D is the correct solution.

</details>

<details>
  <summary>Question 261</summary>

A company recently announced the deployment of its retail website to a global audience. The website runs on multiple Amazon EC2 instances behind an Elastic Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company wants to provide its customers with different versions of content based on the devices that the customers use to access the website. Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)

- [ ] A. Configure Amazon CloudFront to cache multiple versions of the content.
- [ ] B. Configure a host header in a Network Load Balancer to forward traffic to different instances.
- [ ] C. Configure a Lambda@Edge function to send specific objects to users based on the User-Agent header.
- [ ] D. Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB). Configure the NLB to set up host-based routing to different EC2 instances.
- [ ] E. Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB). Configure the NLB to set up path-based routing to different EC2 instances.
   
</details>

<details>
  <summary>Answer</summary>

- [ ] A. Configure Amazon CloudFront to cache multiple versions of the content.
- [ ] C. Configure a Lambda@Edge function to send specific objects to users based on the User-Agent header.
   
Why these are the correct answers:

A. Configure Amazon CloudFront to cache multiple versions of the content.

- [ ] CloudFront can cache different versions of content.
   
C. Configure a Lambda@Edge function to send specific objects to users based on the User-Agent header.

- [ ] Lambda@Edge allows customization of content delivery based on headers like User-Agent.
   
<hr> Why are the other answers wrong? <hr>

- [ ] B. Network Load Balancers do not support host headers.
- [ ] D. Global Accelerator is for performance, not content versioning. NLBs don't support host-based routing.
- [ ] E. Global Accelerator is for performance, not content versioning. NLBs don't support path-based routing.

Therefore, options A and C are correct.

</details>
<details>
  <summary>Question 262</summary>

A company plans to use Amazon ElastiCache for its multi-tier web application. A solutions architect creates a Cache VPC for the ElastiCache cluster and an App VPC for the application's Amazon EC2 instances. Both VPCs are in the us-east-1 Region.

The solutions architect must implement a solution to provide the application's EC2 instances with access to the ElastiCache cluster. Which solution will meet these requirements MOST cost-effectively?

- [ ] A. Create a peering connection between the VPCs. Add a route table entry for the peering connection in both VPCs. Configure an inbound rule for the ElastiCache cluster's security group to allow inbound connection from the application's security group.
- [ ] B. Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route traffic through the Transit VPC. Configure an inbound rule for the ElastiCache cluster's security group to allow inbound connection from the application's security group.
- [ ] C. Create a peering connection between the VPCs. Add a route table entry for the peering connection in both VPCs. Configure an inbound rule for the peering connection's security group to allow inbound connection from the application's security group.
- [ ] D. Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route traffic through the Transit VPC. Configure an inbound rule for the Transit VPC's security group to allow inbound connection from the application's security group.
   
</details>

<details>
  <summary>Answer</summary>

- [ ] A. Create a peering connection between the VPCs. Add a route table entry for the peering connection in both VPCs. Configure an inbound rule for the ElastiCache cluster's security group to allow inbound connection from the application's security group.
   
Why these are the correct answers:

A. Create a peering connection between the VPCs. Add a route table entry for the peering connection in both VPCs. Configure an inbound rule for the ElastiCache cluster's security group to allow inbound connection from the application's security group.

- [ ] VPC peering is the simplest and most cost-effective way to connect two VPCs.
- [ ] Route table entries enable traffic flow.
- [ ] Security groups control access to ElastiCache.
   
<hr> Why are the other answers wrong? <hr>

- [ ] B. Transit VPC is more complex and expensive for only two VPCs.
- [ ] C. Security groups of ElastiCache, not peering, control access to ElastiCache.
- [ ] D. Transit VPC is more complex and expensive than peering.

Therefore, Option A is the correct solution.

</details>
<details>
  <summary>Question 263</summary>

A company is building an application that consists of several microservices. The company has decided to use container technologies to deploy its software on AWS. The company needs a solution that minimizes the amount of ongoing effort for maintenance and scaling. The company cannot manage additional infrastructure. Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)

- [ ] A. Deploy an Amazon Elastic Container Service (Amazon ECS) cluster.
- [ ] B. Deploy the Kubernetes control plane on Amazon EC2 instances that span multiple Availability Zones.
- [ ] C. Deploy an Amazon Elastic Container Service (Amazon ECS) service with an Amazon EC2 launch type. Specify a desired task number level of greater than or equal to 2.
- [ ] D. Deploy an Amazon Elastic Container Service (Amazon ECS) service with a Fargate launch type. Specify a desired task number level of greater than or equal to 2.
- [ ] E. Deploy Kubernetes worker nodes on Amazon EC2 instances that span multiple Availability Zones. Create a deployment that specifies two or more replicas for each microservice.
   
</details>

<details>
  <summary>Answer</summary>

- [ ] A. Deploy an Amazon Elastic Container Service (Amazon ECS) cluster.
- [ ] D. Deploy an Amazon Elastic Container Service (Amazon ECS) service with a Fargate launch type. Specify a desired task number level of greater than or equal to 2.
   
Why these are the correct answers:

A. Deploy an Amazon Elastic Container Service (Amazon ECS) cluster.

- [ ] ECS is a managed container orchestration service.
   
D. Deploy an Amazon Elastic Container Service (Amazon ECS) service with a Fargate launch type. Specify a desired task number level of greater than or equal to 2.

- [ ] Fargate removes the need to manage underlying infrastructure.
- [ ] Specifying at least two tasks ensures high availability.
   
<hr> Why are the other answers wrong? <hr>

- [ ] B. Managing the Kubernetes control plane adds operational overhead.
- [ ] C. EC2 launch type requires managing EC2 instances.
- [ ] E. Managing Kubernetes worker nodes adds operational overhead.

Therefore, options A and D are correct.

</details>
<details>
  <summary>Question 264</summary>

A company has a web application hosted over 10 Amazon EC2 instances with traffic directed by Amazon Route 53. The company occasionally experiences a timeout error when attempting to browse the application. The networking team finds that some DNS queries return IP addresses of unhealthy instances, resulting in the timeout error. What should a solutions architect implement to overcome these timeout errors?

- [ ] A. Create a Route 53 simple routing policy record for each EC2 instance. Associate a health check with each record.
- [ ] B. Create a Route 53 failover routing policy record for each EC2 instance. Associate a health check with each record.
- [ ] C. Create an Amazon CloudFront distribution with EC2 instances as its origin. Associate a health check with the EC2 instances.
- [ ] D. Create an Application Load Balancer (ALB) with a health check in front of the EC2 instances. Route to the ALB from Route 53.
   
</details>

<details>
  <summary>Answer</summary>

- [ ] D. Create an Application Load Balancer (ALB) with a health check in front of the EC2 instances. Route to the ALB from Route 53.
   
Why these are the correct answers:

D. Create an Application Load Balancer (ALB) with a health check in front of the EC2 instances. Route to the ALB from Route 53.

- [ ] ALB health checks ensure traffic is routed only to healthy instances.
- [ ] Route 53 directs traffic to the ALB.
   
<hr> Why are the other answers wrong? <hr>

- [ ] A. Simple routing does not provide health checks.
- [ ] B. Failover routing is for disaster recovery, not load balancing.
- [ ] C. CloudFront is for caching, not load balancing with health checks.

Therefore, Option D is the correct solution.

</details>
<details>
  <summary>Question 265</summary>

A solutions architect needs to design a highly available application consisting of web, application, and database tiers. HTTPS content delivery should be as close to the edge as possible, with the least delivery time. Which solution meets these requirements and is MOST secure?

- [ ] A. Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in public subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the origin.
- [ ] B. Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliver HTTPS content using the EC2 instances as the origin.
- [ ] C. Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the origin.
- [ ] D. Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in public subnets. Configure Amazon CloudFront to deliver HTTPS content using the EC2 instances as the origin.
   
</details>

<details>
  <summary>Answer</summary>

- [ ] C. Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the origin.
   
Why these are the correct answers:

C. Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the origin.

- [ ] CloudFront delivers content close to users (edge locations).
- [ ] ALB distributes traffic to EC2 instances.
- [ ] Private subnets secure EC2 instances.
   
<hr> Why are the other answers wrong? <hr>

- [ ] A. EC2 instances in public subnets are less secure.
- [ ] B. CloudFront needs a public endpoint like an ALB, not EC2 instances directly. EC2 instances in private subnets can't be directly accessed from the internet.
- [ ] D. EC2 instances in public subnets are less secure, and CloudFront needs a public endpoint.

Therefore, Option C is the correct solution.

</details>
<details>
  <summary>Question 266</summary>

A company has a popular gaming platform running on AWS. The application is sensitive to latency because latency can impact the user experience and introduce unfair advantages to some players. The application is deployed in every AWS Region. It runs on Amazon EC2 instances that are part of Auto Scaling groups configured behind Application Load Balancers (ALBs). A solutions architect needs to implement a mechanism to monitor the health of the application and redirect traffic to healthy endpoints. Which solution meets these requirements?

- [ ] A. Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the application listens on, and attach it to a Regional endpoint in each Region. Add the ALB as the endpoint.
- [ ] B. Create an Amazon CloudFront distribution and specify the ALB as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the traffic.
- [ ] C. Create an Amazon CloudFront distribution and specify Amazon S3 as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the traffic.
- [ ] D. Configure an Amazon DynamoDB database to serve as the data store for the application. Create a DynamoDB Accelerator (DAX) cluster to act as the in-memory cache for DynamoDB hosting the application data.
   
</details>

<details>
  <summary>Answer</summary>

- [ ] A. Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the application listens on, and attach it to a Regional endpoint in each Region. Add the ALB as the endpoint.
   
Why these are the correct answers:

A. Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the application listens on, and attach it to a Regional endpoint in each Region. Add the ALB as the endpoint.

- [ ] Global Accelerator improves performance by routing traffic to the nearest healthy endpoint.
- [ ] It monitors application health and redirects traffic away from unhealthy endpoints.
   
<hr> Why are the other answers wrong? <hr>

- [ ] B. CloudFront is for caching content, not for real-time traffic routing based on health.
- [ ] C. CloudFront with S3 is for static content, not dynamic application traffic.
- [ ] D. DynamoDB and DAX are for database performance, not application health and traffic routing.

Therefore, Option A is the correct solution.

</details>
<details>
  <summary>Question 267</summary>

A company has one million users that use its mobile app. The company must analyze the data usage in near-real time. The company also must encrypt the data in near-real time and must store the data in a centralized location in Apache Parquet format for further processing. Which solution will meet these requirements with the LEAST operational overhead?

- [ ] A. Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data. Invoke an AWS Lambda function to send the data to the Kinesis Data Analytics application.
- [ ] B. Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon EMR cluster to analyze the data. Invoke an AWS Lambda function to send the data to the EMR cluster.
- [ ] C. Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon EMR cluster to analyze the data.
- [ ] D. Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data.
   
</details>

<details>
  <summary>Answer</summary>

- [ ] D. Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data.
   
Why these are the correct answers:

D. Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data.

- [ ] Kinesis Data Firehose can deliver data to S3 and convert it to Parquet.
- [ ] Kinesis Data Analytics can analyze data in near-real time.
- [ ] This is a managed solution with minimal overhead.
   
<hr> Why are the other answers wrong? <hr>

- [ ] A. Kinesis Data Streams and Lambda add complexity.
- [ ] B. EMR clusters add operational overhead.
- [ ] C. EMR clusters add operational overhead.

Therefore, Option D is the correct solution.

</details>
<details>
  <summary>Question 268</summary>

A gaming company has a web application that displays scores. The application runs on Amazon EC2 instances behind an Application Load Balancer. The application stores data in an Amazon RDS for MySQL database. Users are starting to experience long delays and interruptions that are caused by database read performance. The company wants to improve the user experience while minimizing changes to the application's architecture. What should a solutions architect do to meet these requirements?

- [ ] A. Use Amazon ElastiCache in front of the database.
- [ ] B. Use RDS Proxy between the application and the database.
- [ ] C. Migrate the application from EC2 instances to AWS Lambda.
- [ ] D. Migrate the database from Amazon RDS for MySQL to Amazon DynamoDB.
   
</details>

<details>
  <summary>Answer</summary>

- [ ] A. Use Amazon ElastiCache in front of the database.
   
Why these are the correct answers:

A. Use Amazon ElastiCache in front of the database.

- [ ] ElastiCache can cache frequently read data, reducing database load.
- [ ] This improves read performance with minimal changes to the application.
   
<hr> Why are the other answers wrong? <hr>

- [ ] B. RDS Proxy is for connection management, not caching.
- [ ] C. Migrating to Lambda requires significant application changes.
- [ ] D. Migrating to DynamoDB requires significant application changes.

Therefore, Option A is the correct solution.

</details>
<details>
  <summary>Question 269</summary>

An ecommerce company has noticed performance degradation of its Amazon RDS based web application. The performance degradation is attributed to an increase in the number of read-only SQL queries triggered by business analysts. A solutions architect needs to solve the problem with minimal changes to the existing web application. What should the solutions architect recommend?

- [ ] A. Export the data to Amazon DynamoDB and have the business analysts run their queries.
- [ ] B. Load the data into Amazon ElastiCache and have the business analysts run their queries.
- [ ] C. Create a read replica of the primary database and have the business analysts run their queries.
- [ ] D. Copy the data into an Amazon Redshift cluster and have the business analysts run their queries.
   
</details>

<details>
  <summary>Answer</summary>

- [ ] C. Create a read replica of the primary database and have the business analysts run their queries.
   
Why these are the correct answers:

C. Create a read replica of the primary database and have the business analysts run their queries.

- [ ] Read replicas offload read queries from the primary database.
- [ ] This improves performance with minimal application changes.
   
<hr> Why are the other answers wrong? <hr>

- [ ] A. DynamoDB is not suitable for complex SQL queries.
- [ ] B. ElastiCache is for caching, not for running analytical queries.
- [ ] D. Redshift is for data warehousing and is overkill for simple read queries.

Therefore, Option C is the correct solution.

</details>

<details>
  <summary>Question 270</summary>

A company is using a centralized AWS account to store log data in various Amazon S3 buckets. A solutions architect needs to ensure that the data is encrypted at rest before the data is uploaded to the S3 buckets. The data also must be encrypted in transit.

Which solution meets these requirements?

- [ ] A. Use client-side encryption to encrypt the data that is being uploaded to the S3 buckets.
- [ ] B. Use server-side encryption to encrypt the data that is being uploaded to the S3 buckets.
- [ ] C. Create bucket policies that require the use of server-side encryption with S3 managed encryption keys (SSE-S3) for S3 uploads.
- [ ] D. Enable the security option to encrypt the S3 buckets through the use of a default AWS Key Management Service (AWS KMS) key.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Use client-side encryption to encrypt the data that is being uploaded to the S3 buckets.

Why these are the correct answers:

A. Use client-side encryption to encrypt the data that is being uploaded to the S3 buckets.

- [ ] Client-side encryption ensures that data is encrypted before it is sent to S3, thus meeting both at-rest and in-transit encryption requirements.
- [ ] The data is encrypted on the client before it leaves the client's environment, providing end-to-end encryption control.

<hr> Why are the other answers wrong? <hr>

- [ ] B. Server-side encryption only encrypts the data at rest in the S3 bucket, not during transit from the client to S3.
- [ ] C. Bucket policies enforcing server-side encryption also only address encryption at rest, not in transit.
- [ ] D. Enabling bucket encryption with a KMS key is a form of server-side encryption and does not encrypt the data before it is transmitted to S3.

Therefore, client-side encryption is necessary to fulfill both encryption requirements.
</details>

<details>
  <summary>Question 271</summary>

A solutions architect observes that a nightly batch processing job is automatically scaled up for 1 hour before the desired Amazon EC2 capacity is reached. The peak capacity is the 'same every night and the batch jobs always start at 1 AM. The solutions architect needs to find a cost-effective solution that will allow for the desired EC2 capacity to be reached quickly and allow the Auto Scaling group to scale down after the batch jobs are complete.

What should the solutions architect do to meet these requirements?

- [ ] A. Increase the minimum capacity for the Auto Scaling group.
- [ ] B. Increase the maximum capacity for the Auto Scaling group.
- [ ] C. Configure scheduled scaling to scale up to the desired compute level.
- [ ] D. Change the scaling policy to add more EC2 instances during each scaling operation.

</details>

<details>
  <summary>Answer</summary>

- [ ] C. Configure scheduled scaling to scale up to the desired compute level.

Why these are the correct answers:

C. Configure scheduled scaling to scale up to the desired compute level.

- [ ] Scheduled scaling is the most appropriate solution because the peak capacity is consistent and the start time is known.
- [ ] It allows the Auto Scaling group to reach the desired capacity precisely at the required time (1 AM) and can be configured to scale down afterward, optimizing costs.

<hr> Why are the other answers wrong? <hr>

- [ ] A. Increasing the minimum capacity keeps those instances running continuously, which is not cost-effective since the extra capacity is only needed for a short period each night.
- [ ] B. Increasing the maximum capacity only allows Auto Scaling to scale up to that limit if triggered by a metric, but it doesn't ensure the scaling occurs at the specific time needed.
- [ ] D. Changing the scaling policy affects how Auto Scaling reacts to metrics, not when it scales. It doesn't provide the precise timing control required.

Therefore, scheduled scaling is the best choice for predictable, time-based scaling requirements.
</details>

<details>
  <summary>Question 272</summary>

A company serves a dynamic website from a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB). The website needs to support multiple languages to serve customers around the world. The website's architecture is running in the us-west-1 Region and is exhibiting high request latency for users that are located in other parts of the world. The website needs to serve requests quickly and efficiently regardless of a user's location. However, the company does not want to recreate the existing architecture across multiple Regions.

What should a solutions architect do to meet these requirements?

- [ ] A. Replace the existing architecture with a website that is served from an Amazon S3 bucket. Configure an Amazon CloudFront distribution with the S3 bucket as the origin. Set the cache behavior settings to cache based on the Accept-Language request header.
- [ ] B. Configure an Amazon CloudFront distribution with the ALB as the origin. Set the cache behavior settings to cache based on the Accept-Language request header.
- [ ] C. Create an Amazon API Gateway API that is integrated with the ALB. Configure the API to use the HTTP integration type. Set up an API Gateway stage to enable the API cache based on the Accept-Language request header.
- [ ] D. Launch an EC2 instance in each additional Region and configure NGINX to act as a cache server for that Region. Put all the EC2 instances and the ALB behind an Amazon Route 53 record set with a geolocation routing policy.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Configure an Amazon CloudFront distribution with the ALB as the origin. Set the cache behavior settings to cache based on the Accept-Language request header.

Why these are the correct answers:

B. Configure an Amazon CloudFront distribution with the ALB as the origin. Set the cache behavior settings to cache based on the Accept-Language request header.

- [ ] Amazon CloudFront can cache content close to users globally, reducing latency.
- [ ] Configuring CloudFront to cache based on the Accept-Language header allows it to serve different language versions of the site.
- [ ] This approach avoids the complexity and cost of replicating the entire architecture in multiple Regions.

<hr> Why are the other answers wrong? <hr>

- [ ] A. Serving a dynamic website entirely from S3 is generally not feasible, and this option doesn't leverage the existing ALB and EC2 infrastructure.
- [ ] C. API Gateway is not designed for caching full web pages. While it can cache API responses, it's not the right tool for this scenario.
- [ ] D. Launching EC2 instances in each Region is complex and costly. It also involves managing infrastructure in multiple locations, which the company wants to avoid.

Therefore, using CloudFront with language-based caching is the most efficient and cost-effective solution.
</details>

<details>
  <summary>Question 273</summary>

A rapidly growing ecommerce company is running its workloads in a single AWS Region. A solutions architect must create a disaster recovery (DR) strategy that includes a different AWS Region. The company wants its database to be up to date in the DR Region with the least possible latency. The remaining infrastructure in the DR Region needs to run at reduced capacity and must be able to scale up if necessary.

Which solution will meet these requirements with the LOWEST recovery time objective (RTO)?

- [ ] A. Use an Amazon Aurora global database with a pilot light deployment.
- [ ] B. Use an Amazon Aurora global database with a warm standby deployment.
- [ ] C. Use an Amazon RDS Multi-AZ DB instance with a pilot light deployment.
- [ ] D. Use an Amazon RDS Multi-AZ DB instance with a warm standby deployment.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Use an Amazon Aurora global database with a warm standby deployment.

Why these are the correct answers:

B. Use an Amazon Aurora global database with a warm standby deployment.

- [ ]   An Aurora global database provides the lowest RTO because it maintains a fully operational standby database in the DR Region.
- [ ]   A warm standby setup minimizes data loss and recovery time, as the database is already running and synchronized.

<hr> Why are the other answers wrong? <hr>

- [ ]   A. A pilot light deployment requires starting up infrastructure in the DR Region, increasing the RTO.
- [ ]   C and D. RDS Multi-AZ primarily focuses on high availability within a single Region. While it improves resilience, it doesn't offer the same low RTO as an Aurora global database for cross-region DR.

Therefore, an Aurora global database with a warm standby is the optimal solution for minimizing RTO in a cross-region DR scenario.
</details>

<details>
  <summary>Question 274</summary>

A company runs an application on Amazon EC2 instances. The company needs to implement a disaster recovery (DR) solution for the application. The DR solution needs to have a recovery time objective (RTO) of less than 4 hours. The DR solution also needs to use the fewest possible AWS resources during normal operations.

Which solution will meet these requirements in the MOST operationally efficient way?

- [ ] A. Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS Lambda and custom scripts.
- [ ] B. Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS CloudFormation.
- [ ] C. Launch EC2 instances in a secondary AWS Region. Keep the EC2 instances in the secondary Region active at all times.
- [ ] D. Launch EC2 instances in a secondary Availability Zone. Keep the EC2 instances in the secondary Availability Zone active at all times.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS CloudFormation.

Why these are the correct answers:

B. Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS CloudFormation.

- [ ]   This approach minimizes resource usage during normal operations since the infrastructure in the DR Region is only provisioned when needed.
- [ ]   CloudFormation enables the automation of infrastructure deployment, which is more operationally efficient than using Lambda and custom scripts.
- [ ]   Using AMIs allows for quick recovery of EC2 instances in the DR Region, helping to meet the RTO.

<hr> Why are the other answers wrong? <hr>

- [ ]   A. While using Lambda and custom scripts can automate deployment, it is less efficient and harder to maintain than using CloudFormation.
- [ ]   C. Launching and maintaining active EC2 instances in a secondary Region consumes more resources and increases costs during normal operations.
- [ ]   D. Using a secondary Availability Zone, not a secondary Region, does not constitute a full DR solution and does not protect against regional outages.

Therefore, using AMIs with CloudFormation provides the most operationally efficient DR solution that meets the requirements.
</details>

<details>
  <summary>Question 275</summary>

A company runs an internal browser-based application. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales up to 20 instances during work hours, but scales down to 2 instances overnight. Staff are complaining that the application is very slow when the day begins, although it runs well by mid-morning.

How should the scaling be changed to address the staff complaints and keep costs to a minimum?

- [ ] A. Implement a scheduled action that sets the desired capacity to 20 shortly before the office opens.
- [ ] B. Implement a step scaling action triggered at a lower CPU threshold, and decrease the cooldown period.
- [ ] C. Implement a target tracking action triggered at a lower CPU threshold, and decrease the cooldown period.
- [ ] D. Implement a scheduled action that sets the minimum and maximum capacity to 20 shortly before the office opens.

</details>

<details>
  <summary>Answer</summary>

- [ ] C. Implement a target tracking action triggered at a lower CPU threshold, and decrease the cooldown period.

Why these are the correct answers:

C. Implement a target tracking action triggered at a lower CPU threshold, and decrease the cooldown period.

- [ ]   Target tracking scaling adjusts the number of instances to maintain a specific metric (e.g., CPU utilization) at a desired level, providing a balance between responsiveness and cost.
- [ ]   Setting a lower CPU threshold ensures that the Auto Scaling group starts adding instances earlier, addressing the slow performance at the beginning of the day.
- [ ]   Decreasing the cooldown period allows the Auto Scaling group to respond more quickly to changes in demand.

<hr> Why are the other answers wrong? <hr>

- [ ]   A. Scheduled actions are based on time, not actual load. They might not be responsive enough to handle variations in user activity. Setting the desired capacity alone does not guarantee a smooth scaling process.
- [ ]   B. Step scaling adjusts capacity in fixed increments, which can lead to over- [ ] or under-provisioning. It is less flexible than target tracking.
- [ ]   D. Setting the minimum and maximum capacity to 20 would eliminate scaling and result in high costs overnight, which contradicts the requirement to minimize costs.

Therefore, target tracking with an appropriate threshold and cooldown period is the most suitable solution.
</details>

<details>
  <summary>Question 276</summary>

A company has a multi-tier application deployed on several Amazon EC2 instances in an Auto Scaling group. An Amazon RDS for Oracle instance is the application' s data layer that uses Oracle-specific PL/SQL functions. Traffic to the application has been steadily increasing. This is causing the EC2 instances to become overloaded and the RDS instance to run out of storage. The Auto Scaling group does not have any scaling metrics and defines the minimum healthy instance count only. The company predicts that traffic will continue to increase at a steady but unpredictable rate before leveling off.

What should a solutions architect do to ensure the system can automatically scale for the increased traffic? (Choose two.)

- [ ] A. Configure storage Auto Scaling on the RDS for Oracle instance.
- [ ] B. Migrate the database to Amazon Aurora to use Auto Scaling storage.
- [ ] C. Configure an alarm on the RDS for Oracle instance for low free storage space.
- [ ] D. Configure the Auto Scaling group to use the average CPU as the scaling metric.
- [ ] E. Configure the Auto Scaling group to use the average free memory as the scaling metric.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Configure storage Auto Scaling on the RDS for Oracle instance.
- [ ] D. Configure the Auto Scaling group to use the average CPU as the scaling metric.

Why these are the correct answers:

A. Configure storage Auto Scaling on the RDS for Oracle instance.

- [ ]   Storage Auto Scaling for RDS allows the database to automatically increase storage capacity in response to growth, preventing it from running out of space.

D. Configure the Auto Scaling group to use the average CPU as the scaling metric.

- [ ]   Using CPU utilization as a scaling metric ensures that the Auto Scaling group adds or removes EC2 instances based on the application's processing load, maintaining performance.

<hr> Why are the other answers wrong? <hr>

- [ ]   B. Migrating to Aurora is a significant change and is not necessary to address scaling. It is also more complex than simply enabling storage Auto Scaling.
- [ ]   C. Configuring an alarm only notifies about low storage but does not automatically scale it.
- [ ]   E. While memory can be a factor, CPU is generally a more direct indicator of application load for scaling web applications.

Therefore, enabling storage Auto Scaling for RDS and using CPU as the scaling metric are the most appropriate actions.
</details>

<details>
  <summary>Question 277</summary>

A company provides an online service for posting video content and transcoding it for use by any mobile platform. The application architecture uses Amazon Elastic File System (Amazon EFS) Standard to collect and store the videos so that multiple Amazon EC2 Linux instances can access the video content for processing. As the popularity of the service has grown over time, the storage costs have become too expensive.

Which storage solution is MOST cost-effective?

- [ ] A. Use AWS Storage Gateway for files to store and process the video content.
- [ ] B. Use AWS Storage Gateway for volumes to store and process the video content.
- [ ] C. Use Amazon EFS for storing the video content. Once processing is complete, transfer the files to Amazon Elastic Block Store (Amazon EBS).
- [ ] D. Use Amazon S3 for storing the video content. Move the files temporarily over to an Amazon Elastic Block Store (Amazon EBS) volume attached to the server for processing.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Use Amazon S3 for storing the video content. Move the files temporarily over to an Amazon Elastic Block Store (Amazon EBS) volume attached to the server for processing.

Why these are the correct answers:

D. Use Amazon S3 for storing the video content. Move the files temporarily over to an Amazon Elastic Block Store (Amazon EBS) volume attached to the server for processing.

- [ ]   Amazon S3 is significantly cheaper for storing large amounts of video data compared to Amazon EFS.
- [ ]   Using EBS for temporary processing allows for the necessary performance while keeping long-term storage costs low.

<hr> Why are the other answers wrong? <hr>

- [ ]   A and B. AWS Storage Gateway is used to integrate on-premises storage with AWS and is not cost-effective for this cloud-native use case.
- [ ]   C. While transferring files from EFS to EBS can reduce costs compared to using EFS exclusively, it does not provide the same cost savings as using S3 for storage.

Therefore, using S3 for storage and EBS for processing is the most cost-effective solution.
</details>

<details>
  <summary>Question 278</summary>

A company wants to create an application to store employee data in a hierarchical structured relationship. The company needs a minimum-latency response to high-traffic queries for the employee data and must protect any sensitive data. The company also needs to receive monthly email messages if any financial information is present in the employee data.

Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)

- [ ] A. Use Amazon Redshift to store the employee data in hierarchies. Unload the data to Amazon S3 every month.
- [ ] B. Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon S3 every month.
- [ ] C. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly events to AWS Lambda.
- [ ] D. Use Amazon Athena to analyze the employee data in Amazon S3. Integrate Athena with Amazon QuickSight to publish analysis dashboards and share the dashboards with users.
- [ ] E. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly notifications through an Amazon Simple Notification Service (Amazon SNS) subscription.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon S3 every month.
- [ ] E. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly notifications through an Amazon Simple Notification Service (Amazon SNS) subscription.

Why these are the correct answers:

B. Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon S3 every month.

- [ ]   DynamoDB provides low-latency, high-throughput access, which meets the requirement for minimum-latency response to high-traffic queries.
- [ ]   While DynamoDB is not inherently designed for hierarchical data, it can be modeled using techniques like adjacency lists or materialized paths.
- [ ]   Exporting data to S3 supports long-term storage and analysis.

E. Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly notifications through an Amazon Simple Notification Service (Amazon SNS) subscription.

- [ ]   Amazon Macie can identify sensitive data, such as financial information.
- [ ]   Integrating Macie with EventBridge and SNS enables automated notifications when sensitive data is detected.

<hr> Why are the other answers wrong? <hr>

- [ ]   A. Amazon Redshift is a data warehouse service, not optimized for low-latency, high-traffic queries. It's better suited for analytics.
- [ ]   C. While Macie can integrate with Lambda, sending events to Lambda for monthly analysis is less direct than using SNS for notifications. Lambda is better for processing, not just forwarding notifications.
- [ ]   D. Athena is for querying data in S3, not for storing and providing low-latency access to frequently queried data. QuickSight is for visualization, not data storage or sensitive data detection.

Therefore, DynamoDB for data storage and Macie with SNS for sensitive data detection are the most appropriate choices.
</details>

<details>
  <summary>Question 279</summary>

A company has an application that is backed by an Amazon DynamoDB table. The company's compliance requirements specify that database backups must be taken every month, must be available for 6 months, and must be retained for 7 years.

Which solution will meet these requirements?

- [ ] A. Create an AWS Backup plan to back up the DynamoDB table on the first day of each month. Specify a lifecycle policy that transitions the backup to cold storage after 6 months. Set the retention period for each backup to 7 years.
- [ ] B. Create a DynamoDB on-demand backup of the DynamoDB table on the first day of each month. Transition the backup to Amazon S3 Glacier Flexible Retrieval after 6 months. Create an S3 Lifecycle policy to delete backups that are older than 7 years.
- [ ] C. Use the AWS SDK to develop a script that creates an on-demand backup of the DynamoDB table. Set up an Amazon EventBridge rule that runs the script on the first day of each month. Create a second script that will run on the second day of each month to transition DynamoDB backups that are older than 6 months to cold storage and to delete backups that are older than 7 years.
- [ ] D. Use the AWS CLI to create an on-demand backup of the DynamoDB table. Set up an Amazon EventBridge rule that runs the command on the first day of each month with a cron expression. Specify in the command to transition the backups to cold storage after 6 months and to delete the backups after 7 years.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Create an AWS Backup plan to back up the DynamoDB table on the first day of each month. Specify a lifecycle policy that transitions the backup to cold storage after 6 months. Set the retention period for each backup to 7 years.

Why these are the correct answers:

A. Create an AWS Backup plan to back up the DynamoDB table on the first day of each month. Specify a lifecycle policy that transitions the backup to cold storage after 6 months. Set the retention period for each backup to 7 years.

- [ ]   AWS Backup is designed to centralize and automate backups across AWS services, including DynamoDB.
- [ ]   It allows setting backup schedules and retention policies, including transitioning backups to cold storage.
- [ ]   This solution meets all the requirements within a single service, simplifying management.

<hr> Why are the other answers wrong? <hr>

- [ ]   B. DynamoDB backups cannot be directly transitioned to S3 Glacier. This option involves managing backups and lifecycle policies across multiple services, which is more complex.
- [ ]   C. Using custom scripts and EventBridge rules adds complexity and maintenance overhead compared to using AWS Backup. It also requires managing separate scripts for backup and lifecycle management.
- [ ]   D. Similar to Option C, using the AWS CLI and EventBridge with cron expressions increases complexity. It also lacks the integrated lifecycle management of AWS Backup.

Therefore, AWS Backup provides the most straightforward and efficient solution for meeting the backup and retention requirements.
</details>

<details>
  <summary>Question 280</summary>

A company is using Amazon CloudFront with its website. The company has enabled logging on the CloudFront distribution, and logs are saved in one of the company's Amazon S3 buckets. The company needs to perform advanced analyses on the logs and build visualizations.

What should a solutions architect do to meet these requirements?

- [ ] A. Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with AWS Glue.
- [ ] B. Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight.
- [ ] C. Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize the results with AWS Glue.
- [ ] D. Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight.

Why these are the correct answers:

B. Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight.

- [ ]   Amazon Athena allows querying data directly in S3 using SQL, which is ideal for analyzing CloudFront logs.
- [ ]   Amazon QuickSight is a business intelligence service that can create visualizations and dashboards from the query results.

<hr> Why are the other answers wrong? <hr>

- [ ]   A. AWS Glue is primarily an ETL (Extract, Transform, Load) service, not a visualization tool. While it can process data, it's not the best choice for creating dashboards.
- [ ]   C and D. Amazon DynamoDB is a NoSQL database and not suitable for running SQL queries on CloudFront logs stored in S3. It is designed for different use cases.

Therefore, Athena for analysis and QuickSight for visualization provide the most appropriate solution.
</details>

<details>
  <summary>Question 281</summary>

A company runs a fleet of web servers using an Amazon RDS for PostgreSQL DB instance. After a routine compliance check, the company sets a standard that requires a recovery point objective (RPO) of less than 1 second for all its production databases.

Which solution meets these requirements?

- [ ] A. Enable a Multi-AZ deployment for the DB instance.
- [ ] B. Enable auto scaling for the DB instance in one Availability Zone.
- [ ] C. Configure the DB instance in one Availability Zone, and create multiple read replicas in a separate Availability Zone.
- [ ] D. Configure the DB instance in one Availability Zone, and configure AWS Database Migration Service (AWS DMS) change data capture (CDC) tasks.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Enable a Multi-AZ deployment for the DB instance.

Why these are the correct answers:

A. Enable a Multi-AZ deployment for the DB instance.

- [ ]   Multi-AZ deployments in Amazon RDS provide a synchronous replication to a standby instance in another Availability Zone.
- [ ]   This minimizes data loss in the event of a failure, supporting a very low RPO (close to zero).

<hr> Why are the other answers wrong? <hr>

- [ ]   B. Auto Scaling for RDS adjusts compute capacity, not data replication, and does not help with RPO.
- [ ]   C. Read replicas are for scaling read operations and involve asynchronous replication, which cannot meet a sub-second RPO.
- [ ]   D. DMS CDC is also for asynchronous replication and migration, not for providing the near-zero RPO required.

Therefore, Multi-AZ deployments are essential for meeting a very strict RPO requirement.
</details>

<details>
  <summary>Question 282</summary>

A company runs a web application that is deployed on Amazon EC2 instances in the private subnet of a VPC. An Application Load Balancer (ALB) that extends across the public subnets directs web traffic to the EC2 instances. The company wants to implement new security measures to restrict inbound traffic from the ALB to the EC2 instances while preventing access from any other source inside or outside the private subnet of the EC2 instances.

Which solution will meet these requirements?

- [ ] A. Configure a route in a route table to direct traffic from the internet to the private IP addresses of the EC2 instances.
- [ ] B. Configure the security group for the EC2 instances to only allow traffic that comes from the security group for the ALB.
- [ ] C. Move the EC2 instances into the public subnet. Give the EC2 instances a set of Elastic IP addresses.
- [ ] D. Configure the security group for the ALB to allow any TCP traffic on any port.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Configure the security group for the EC2 instances to only allow traffic that comes from the security group for the ALB.

Why these are the correct answers:

B. Configure the security group for the EC2 instances to only allow traffic that comes from the security group for the ALB.

- [ ]   Security groups act as a virtual firewall for EC2 instances.
- [ ]   By allowing traffic only from the ALB's security group, you ensure that only the ALB can access the EC2 instances, and no other traffic is permitted.

<hr> Why are the other answers wrong? <hr>

- [ ]   A. Modifying route tables to allow direct internet traffic to private subnets is a security risk and defeats the purpose of having private subnets.
- [ ]   C. Moving EC2 instances to public subnets exposes them to the internet, which is insecure. Elastic IP addresses are for public IP addresses.
- [ ]   D. Configuring the ALB's security group to allow all traffic is overly permissive and does not restrict access to the EC2 instances.

Therefore, using security groups to restrict traffic is the most secure and appropriate solution.
</details>

<details>
  <summary>Question 283</summary>

A research company runs experiments that are powered by a simulation application and a visualization application. The simulation application runs on Linux and outputs intermediate data to an NFS share every 5 minutes. The visualization application is a Windows desktop application that displays the simulation output and requires an SMB file system. The company maintains two synchronized file systems. This strategy is causing data duplication and inefficient resource usage. The company needs to migrate the applications to AWS without making code changes to either application.

Which solution will meet these requirements?

- [ ] A. Migrate both applications to AWS Lambda. Create an Amazon S3 bucket to exchange data between the applications.
- [ ] B. Migrate both applications to Amazon Elastic Container Service (Amazon ECS). Configure Amazon FSx File Gateway for storage.
- [ ] C. Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon Simple Queue Service (Amazon SQS) to exchange data between the applications.
- [ ] D. Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon FSx for NetApp ONTAP for storage.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon FSx for NetApp ONTAP for storage.

Why these are the correct answers:

D. Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon FSx for NetApp ONTAP for storage.

- [ ]   Migrating the applications to EC2 instances allows them to run without code changes, preserving their existing functionality.
- [ ]   Amazon FSx for NetApp ONTAP supports both NFS and SMB, providing a single file system that both applications can use.
- [ ]   This eliminates data duplication and inefficient resource usage.

<hr> Why are the other answers wrong? <hr>

- [ ]   A. Lambda is not suitable for applications that require persistent file storage and continuous processing like these.
- [ ]   B. ECS is for containerized applications, and while it can use file storage, it doesn't directly address the NFS/SMB compatibility requirement without code changes.
- [ ]   C. SQS is a message queuing service, not a file system. It cannot replace the need for shared file storage.

Therefore, using EC2 instances and FSx for NetApp ONTAP is the most appropriate solution.
</details>

<details>
  <summary>Question 284</summary>

As part of budget planning, management wants a report of AWS billed items listed by user. The data will be used to create department budgets. A solutions architect needs to determine the most efficient way to obtain this report information.

Which solution meets these requirements?

- [ ] A. Run a query with Amazon Athena to generate the report.
- [ ] B. Create a report in Cost Explorer and download the report.
- [ ] C. Access the bill details from the billing dashboard and download the bill.
- [ ] D. Modify a cost budget in AWS Budgets to alert with Amazon Simple Email Service (Amazon SES).

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Create a report in Cost Explorer and download the report.

Why these are the correct answers:

B. Create a report in Cost Explorer and download the report.

- [ ]   AWS Cost Explorer is a tool that enables you to visualize, understand, and manage your AWS costs and usage over time.
- [ ]   It allows filtering and grouping costs by various dimensions, including users (if you have cost allocation tags set up), and can generate detailed reports.

<hr> Why are the other answers wrong? <hr>

- [ ]   A. While Athena can query cost and usage data, it requires more setup and SQL knowledge compared to using Cost Explorer's built-in reporting features.
- [ ]   C. Downloading the bill from the billing dashboard provides overall costs but does not offer the same level of granularity for reporting costs by user.
- [ ]   D. AWS Budgets is for setting cost budgets and receiving alerts, not for generating detailed cost reports.

Therefore, Cost Explorer is the most efficient and user-friendly way to obtain the required report.
</details>

<details>
  <summary>Question 285</summary>

A company has a static website that is hosted on Amazon S3. The company wants to add a contact form to its webpage. The contact form will have dynamic server-side components for users to input their name, email address, phone number, and user message. The company anticipates that there will be fewer than 100 site visits each month.

Which solution will meet these requirements MOST cost-effectively?

- [ ] A. Host a dynamic contact form page in Amazon Elastic Container Service (Amazon ECS). Set up Amazon Simple Email Service (Amazon SES) to connect to any third-party email provider.
- [ ] B. Create an Amazon API Gateway endpoint with an AWS Lambda backend that makes a call to Amazon Simple Email Service (Amazon SES).
- [ ] C. Convert the static webpage to dynamic by deploying Amazon Lightsail. Use client-side scripting to build the contact form. Integrate the form with Amazon WorkMail.
- [ ] D. Create a t2.micro Amazon EC2 instance. Deploy a LAMP (Linux, Apache, MySQL, PHP/Perl/Python) stack to host the webpage. Use client-side scripting to build the contact form. Integrate the form with Amazon WorkMail.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Create an Amazon API Gateway endpoint with an AWS Lambda backend that makes a call to Amazon Simple Email Service (Amazon SES).

Why these are the correct answers:

B. Create an Amazon API Gateway endpoint with an AWS Lambda backend that makes a call to Amazon Simple Email Service (Amazon SES).

- [ ]   API Gateway and Lambda are serverless and cost-effective for low traffic, as you only pay for usage.
- [ ]   Lambda can process form data and use SES to send emails, providing the required dynamic functionality.

<hr> Why are the other answers wrong? <hr>

- [ ]   A. ECS is for containerized applications and is too resource-intensive and costly for a simple contact form.
- [ ]   C. Lightsail provides virtual private servers and is more expensive than serverless options for low traffic. Client-side scripting cannot handle server-side processing for sending emails.
- [ ]   D. An EC2 instance requires constant running and incurs costs even when idle, making it inefficient for low traffic. A LAMP stack is also complex for this simple requirement.

Therefore, API Gateway and Lambda offer the most cost-effective solution.
</details>

<details>
  <summary>Question 286</summary>

A company has a static website that is hosted on Amazon CloudFront in front of Amazon S3. The static website uses a database backend. The company notices that the website does not reflect updates that have been made in the website's Git repository. The company checks the continuous integration and continuous delivery $(Cl/CD)$ pipeline between the Git repository and Amazon S3. The company verifies that the webhooks are configured properly and that the CI/CD pipeline is sending messages that indicate successful deployments. A solutions architect needs to implement a solution that displays the updates on the website.

Which solution will meet these requirements?

- [ ] A. Add an Application Load Balancer.
- [ ] B. Add Amazon ElastiCache for Redis or Memcached to the database layer of the web application.
- [ ] C. Invalidate the CloudFront cache.
- [ ] D. Use AWS Certificate Manager (ACM) to validate the website's SSL certificate.

</details>

<details>
  <summary>Answer</summary>

- [ ] C. Invalidate the CloudFront cache.

Why these are the correct answers:

C. Invalidate the CloudFront cache.

- [ ]   CloudFront caches content to reduce latency and improve performance.
- [ ]   When updates are made, CloudFront may continue to serve the old content from its cache.
- [ ]   Invalidating the cache forces CloudFront to retrieve the latest content from the origin (S3), ensuring that users see the updates.

<hr> Why are the other answers wrong? <hr>

- [ ]   A. An Application Load Balancer is used to distribute traffic to EC2 instances, not to refresh cached content in CloudFront.
- [ ]   B. ElastiCache is a caching service for databases, not for updating static website content in CloudFront.
- [ ]   D. AWS Certificate Manager is for managing SSL certificates, not for content updates.

Therefore, invalidating the CloudFront cache is the correct solution to display website updates.
</details>

<details>
  <summary>Question 287</summary>

A company wants to migrate a Windows-based application from on premises to the AWS Cloud. The application has three tiers: an application tier, a business tier, and a database tier with Microsoft SQL Server. The company wants to use specific features of SQL Server such as native backups and Data Quality Services. The company also needs to share files for processing between the tiers.

How should a solutions architect design the architecture to meet these requirements?

- [ ] A. Host all three tiers on Amazon EC2 instances. Use Amazon FSx File Gateway for file sharing between the tiers.
- [ ] B. Host all three tiers on Amazon EC2 instances. Use Amazon FSx for Windows File Server for file sharing between the tiers.
- [ ] C. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use Amazon Elastic File System (Amazon EFS) for file sharing between the tiers.
- [ ] D. Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use a Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volume for file sharing between the tiers.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Host all three tiers on Amazon EC2 instances. Use Amazon FSx for Windows File Server for file sharing between the tiers.

Why these are the correct answers:

B. Host all three tiers on Amazon EC2 instances. Use Amazon FSx for Windows File Server for file sharing between the tiers.

- [ ]   Hosting all tiers on EC2 instances allows the use of specific SQL Server features like native backups and Data Quality Services.
- [ ]   Amazon FSx for Windows File Server provides a fully managed Windows file server in the cloud, compatible with the application's file-sharing needs.

<hr> Why are the other answers wrong? <hr>

- [ ]   A. FSx File Gateway is for integrating on-premises file shares with AWS, not for sharing files between tiers within AWS.
- [ ]   C. Amazon RDS for SQL Server is a managed database service, which may not support all SQL Server features. EFS is a Linux file system and not ideal for Windows-based applications.
- [ ]   D. While EBS volumes can be used for storage, they are block storage and not designed for file sharing between multiple EC2 instances like FSx for Windows File Server.

Therefore, hosting all tiers on EC2 and using FSx for Windows File Server is the most suitable solution.
</details>

<details>
  <summary>Question 288</summary>

A company is migrating a Linux-based web server group to AWS. The web servers must access files in a shared file store for some content. The company must not make any changes to the application.

What should a solutions architect do to meet these requirements?

- [ ] A. Create an Amazon S3 Standard bucket with access to the web servers.
- [ ] B. Configure an Amazon CloudFront distribution with an Amazon S3 bucket as the origin.
- [ ] C. Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on all web servers.
- [ ] D. Configure a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume to all web servers.

</details>

<details>
  <summary>Answer</summary>

- [ ] C. Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on all web servers.

Why these are the correct answers:

C. Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on all web servers.

- [ ]   Amazon EFS provides a shared file system that can be mounted by multiple EC2 instances.
- [ ]   It supports Linux file permissions and semantics, allowing the application to access files without modification.

<hr> Why are the other answers wrong? <hr>

- [ ]   A. Amazon S3 is object storage, not a file system, and requires application changes to access files.
- [ ]   B. CloudFront is a content delivery network (CDN) and not suitable for shared file storage.
- [ ]   D. EBS volumes are block storage and can only be attached to a single EC2 instance at a time, requiring application changes to share files.

Therefore, EFS is the appropriate solution for shared file storage without application changes.
</details>

<details>
  <summary>Question 289</summary>

A company has an AWS Lambda function that needs read access to an Amazon S3 bucket that is located in the same AWS account.

Which solution will meet these requirements in the MOST secure manner?

- [ ] A. Apply an S3 bucket policy that grants read access to the S3 bucket.
- [ ] B. Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to the S3 bucket.
- [ ] C. Embed an access key and a secret key in the Lambda function's code to grant the required IAM permissions for read access to the S3 bucket.
- [ ] D. Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to all S3 buckets in the account.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to the S3 bucket.

Why these are the correct answers:

B. Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to the S3 bucket.

- [ ]   IAM roles provide secure permissions to AWS resources.
- [ ]   Granting the Lambda function an IAM role with specific permissions to the S3 bucket follows the principle of least privilege.

<hr> Why are the other answers wrong? <hr>

- [ ]   A. Bucket policies grant access to anyone who can invoke the Lambda function, which is less secure.
- [ ]   C. Embedding access keys and secret keys in code is highly insecure and a major security risk.
- [ ]   D. Granting access to all S3 buckets violates the principle of least privilege.

Therefore, using an IAM role with specific permissions is the most secure method.
</details>

<details>
  <summary>Question 290</summary>

A company hosts a web application on multiple Amazon EC2 instances. The EC2 instances are in an Auto Scaling group that scales in response to user demand. The company wants to optimize cost savings without making a long-term commitment.

Which EC2 instance purchasing option should a solutions architect recommend to meet these requirements?

- [ ] A. Dedicated Instances only
- [ ] B. On-Demand Instances only
- [ ] C. A mix of On-Demand Instances and Spot Instances
- [ ] D. A mix of On-Demand Instances and Reserved Instances

</details>

<details>
  <summary>Answer</summary>

- [ ] C. A mix of On-Demand Instances and Spot Instances

Why these are the correct answers:

C. A mix of On-Demand Instances and Spot Instances

- [ ]   On-Demand Instances provide flexibility and are suitable for ensuring that the application is always available.
- [ ]   Spot Instances offer significant cost savings but can be interrupted, making them suitable for non-critical or fault-tolerant workloads within the Auto Scaling group.
- [ ]   This combination optimizes cost while maintaining availability.

<hr> Why are the other answers wrong? <hr>

- [ ]   A. Dedicated Instances are the most expensive and do not offer cost savings.
- [ ]   B. On-Demand Instances alone do not provide the cost optimization that Spot Instances offer.
- [ ]   D. Reserved Instances require a long-term commitment, which the company wants to avoid.

Therefore, a mix of On-Demand and Spot Instances is the best option for cost savings without long-term commitment.
</details>

<details>
  <summary>Question 291</summary>

A media company uses Amazon CloudFront for its publicly available streaming video content. The company wants to secure the video content that is hosted in Amazon S3 by controlling who has access. Some of the company's users are using a custom HTTP client that does not support cookies. Some of the company's users are unable to change the hardcoded URLs that they are using for access.

Which services or methods will meet these requirements with the LEAST impact to the users? (Choose two.)

- [ ] A. Signed cookies
- [ ] B. Signed URLs
- [ ] C. AWS AppSync
- [ ] D. JSON Web Token (JWT)
- [ ] E. AWS Secrets Manager

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Signed cookies
- [ ] B. Signed URLs

Why these are the correct answers:

A. Signed cookies

- [ ]   Signed cookies allow control over access to multiple restricted files, which is useful for streaming video content.
- [ ]   They can be used with custom HTTP clients that support cookies.

B. Signed URLs

- [ ]   Signed URLs provide access to individual files for a limited time.
- [ ]   They can be used even if users cannot change hardcoded URLs, as the URLs can be pre-signed.

<hr> Why are the other answers wrong? <hr>

- [ ]   C. AWS AppSync is a GraphQL service for real-time data, not designed for controlling access to S3 content.
- [ ]   D. JWTs are for authentication and authorization within applications, not directly for controlling access to S3 URLs.
- [ ]   E. AWS Secrets Manager is for managing secrets like passwords, not for controlling content access.

Therefore, Signed Cookies and Signed URLs are the most suitable options.
</details>

<details>
  <summary>Question 292</summary>

A company is preparing a new data platform that will ingest real-time streaming data from multiple sources. The company needs to transform the data before writing the data to Amazon S3. The company needs the ability to use SQL to query the transformed data.

Which solutions will meet these requirements? (Choose two.)

- [ ] A. Use Amazon Kinesis Data Streams to stream the data. Use Amazon Kinesis Data Analytics to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.
- [ ] B. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use AWS Glue to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.
- [ ] C. Use AWS Database Migration Service (AWS DMS) to ingest the data. Use Amazon EMR to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.
- [ ] D. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use Amazon Kinesis Data Analytics to transform the data and to write the data to Amazon S3. Use the Amazon RDS query editor to query the transformed data from Amazon S3.
- [ ] E. Use Amazon Kinesis Data Streams to stream the data. Use AWS Glue to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use the Amazon RDS query editor to query the transformed data from Amazon S3.

</details>

<details>
  <summary>Answer</summary>

- [ ] A. Use Amazon Kinesis Data Streams to stream the data. Use Amazon Kinesis Data Analytics to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.
- [ ] B. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use AWS Glue to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.

Why these are the correct answers:

A. Use Amazon Kinesis Data Streams to stream the data. Use Amazon Kinesis Data Analytics to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.

- [ ]   Kinesis Data Streams captures real-time data.
- [ ]   Kinesis Data Analytics transforms the data using SQL.
- [ ]   Kinesis Data Firehose delivers the transformed data to S3.
- [ ]   Athena allows querying the data in S3 with SQL.

B. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use AWS Glue to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.

- [ ]   MSK is an alternative for streaming data.
- [ ]   AWS Glue can perform ETL (Extract, Transform, Load) operations to transform and write data to S3.
- [ ]   Athena again is used to query the data in S3.

<hr> Why are the other answers wrong? <hr>

- [ ]   C. DMS is for database migration, not for real-time streaming. EMR is for big data processing, which is more complex than needed for simple transformations.
- [ ]   D and E. RDS query editor is for querying relational databases, not data in S3.

Therefore, options A and B provide viable solutions for streaming, transforming, storing, and querying data.
</details>

<details>
  <summary>Question 293</summary>

A company has an on-premises volume backup solution that has reached its end of life. The company wants to use AWS as part of a new backup solution and wants to maintain local access to all the data while it is backed up on AWS. The company wants to ensure that the data backed up on AWS is automatically and securely transferred.

Which solution meets these requirements?

- [ ] A. Use AWS Snowball to migrate data out of the on-premises solution to Amazon S3. Configure on-premises systems to mount the Snowball S3 endpoint to provide local access to the data.
- [ ] B. Use AWS Snowball Edge to migrate data out of the on-premises solution to Amazon S3. Use the Snowball Edge file interface to provide on-premises systems with local access to the data.
- [ ] C. Use AWS Storage Gateway and configure a cached volume gateway. Run the Storage Gateway software appliance on premises and configure a percentage of data to cache locally. Mount the gateway storage volumes to provide local access to the data.
- [ ] D. Use AWS Storage Gateway and configure a stored volume gateway. Run the Storage Gateway software appliance on premises and map the gateway storage volumes to on-premises storage. Mount the gateway storage volumes to provide local access to the data.

</details>

<details>
  <summary>Answer</summary>

- [ ] D. Use AWS Storage Gateway and configure a stored volume gateway. Run the Storage Gateway software appliance on premises and map the gateway storage volumes to on-premises storage. Mount the gateway storage volumes to provide local access to the data.

Why these are the correct answers:

D. Use AWS Storage Gateway and configure a stored volume gateway. Run the Storage Gateway software appliance on premises and map the gateway storage volumes to on-premises storage. Mount the gateway storage volumes to provide local access to the data.

- [ ]   A stored volume gateway retains all data locally and asynchronously backs it up to AWS, providing local access and offsite storage.
- [ ]   Storage Gateway automates the secure transfer of data to AWS.

<hr> Why are the other answers wrong? <hr>

- [ ]   A and B. Snowball and Snowball Edge are for large-scale data migration, not for ongoing backups with local access. They do not provide continuous local access.
- [ ]   C. A cached volume gateway caches frequently accessed data locally but does not store all data locally.

Therefore, a stored volume gateway is the appropriate solution for local access and AWS backups.
</details>

<details>
  <summary>Question 294</summary>

An application that is hosted on Amazon EC2 instances needs to access an Amazon S3 bucket. Traffic must not traverse the internet.

How should a solutions architect configure access to meet these requirements?

- [ ] A. Create a private hosted zone by using Amazon Route 53.
- [ ] B. Set up a gateway VPC endpoint for Amazon S3 in the VPC.
- [ ] C. Configure the EC2 instances to use a NAT gateway to access the S3 bucket.
- [ ] D. Establish an AWS Site-to-Site VPN connection between the VPC and the S3 bucket.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Set up a gateway VPC endpoint for Amazon S3 in the VPC.

Why these are the correct answers:

B. Set up a gateway VPC endpoint for Amazon S3 in the VPC.

- [ ]   A gateway VPC endpoint enables private connections to S3 from within the VPC without using the internet.
- [ ]   This ensures that traffic remains within the AWS network.

<hr> Why are the other answers wrong? <hr>

- [ ]   A. Route 53 private hosted zones are for DNS resolution within a VPC, not for routing traffic to S3 without using the internet.
- [ ]   C. A NAT gateway is used to allow instances in a private subnet to connect to the internet.
- [ ]   D. A Site-to-Site VPN connects an on-premises network to a VPC, not for private access to S3 within the same VPC.

Therefore, a gateway VPC endpoint is the correct solution for private S3 access.
</details>

<details>
  <summary>Question 295</summary>

An ecommerce company stores terabytes of customer data in the AWS Cloud. The data contains personally identifiable information (PII). The company wants to use the data in three applications. Only one of the applications needs to process the PII. The Pll must be removed before the other two applications process the data.

Which solution will meet these requirements with the LEAST operational overhead?

- [ ] A. Store the data in an Amazon DynamoDB table. Create a proxy application layer to intercept and process the data that each application requests.
- [ ] B. Store the data in an Amazon S3 bucket. Process and transform the data by using S3 Object Lambda before returning the data to the requesting application.
- [ ] C. Process the data and store the transformed data in three separate Amazon S3 buckets so that each application has its own custom dataset. Point each application to its respective S3 bucket.
- [ ] D. Process the data and store the transformed data in three separate Amazon DynamoDB tables so that each application has its own custom dataset. Point each application to its respective DynamoDB table.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Store the data in an Amazon S3 bucket. Process and transform the data by using S3 Object Lambda before returning the data to the requesting application.

Why these are the correct answers:

B. Store the data in an Amazon S3 bucket. Process and transform the data by using S3 Object Lambda before returning the data to the requesting application.

- [ ]   S3 Object Lambda allows you to add your own code to S3 to process data as it is retrieved.
- [ ]   This minimizes operational overhead by processing the PII removal in S3, rather than managing separate processing layers or data copies.

<hr> Why are the other answers wrong? <hr>

- [ ]   A. A proxy application layer adds complexity and overhead, as it requires managing and scaling an additional application.
- [ ]   C and D. Storing transformed data in separate S3 buckets or DynamoDB tables increases storage costs and management overhead.

Therefore, S3 Object Lambda provides the most efficient solution with the least overhead.
</details>

<details>
  <summary>Question 296</summary>

A development team has launched a new application that is hosted on Amazon EC2 instances inside a development VPC. A solutions architect needs to create a new VPC in the same account. The new VPC will be peered with the development VPC. The VPC CIDR block for the development VPC is $192.168.0.0/24$. The solutions architect needs to create a CIDR block for the new VPC. The CIDR block must be valid for a VPC peering connection to the development VPC. What is the SMALLEST CIDR block that meets these requirements?

- [ ] A. $10.0.1.0/32$
- [ ] B. $192.168.0.0/24$
- [ ] C. $192.168.1.0/32$
- [ ] D. $10.0.1.0/24$

</details>

<details>
  <summary>Answer</summary>

- [ ] D. $10.0.1.0/24$

Why these are the correct answers:

D. $10.0.1.0/24$

- [ ]   VPC peering requires that the CIDR blocks of the peered VPCs do not overlap.
- [ ]   $10.0.1.0/24$ is a valid, non-overlapping CIDR block.
- [ ]   /24 provides a reasonable number of IP addresses.

<hr> Why are the other answers wrong? <hr>

- [ ]   A and C. /32 CIDR blocks are too small, providing only 1 IP address, which is insufficient for a VPC.
- [ ]   B. $192.168.0.0/24$ overlaps with the existing VPC's CIDR block, which is not allowed for peering.

Therefore, $10.0.1.0/24$ is the smallest valid CIDR block.
</details>

<details>
  <summary>Question 297</summary>

A company deploys an application on five Amazon EC2 instances. An Application Load Balancer (ALB) distributes traffic to the instances by using a target group. The average CPU usage on each of the instances is below 10% most of the time, with occasional surges to 65%. A solutions architect needs to implement a solution to automate the scalability of the application. The solution must optimize the cost of the architecture and must ensure that the application has enough CPU resources when surges occur. Which solution will meet these requirements?

- [ ] A. Create an Amazon CloudWatch alarm that enters the ALARM state when the CPUUtilization metric is less than 20%. Create an AWS Lambda function that the CloudWatch alarm invokes to terminate one of the EC2 instances in the ALB target group.
- [ ] B. Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set a target tracking scaling policy that is based on the ASGAverageCPUUtilization metric. Set the minimum instances to 2, the desired capacity to 3, the maximum instances to 6, and the target value to 50%. Add the EC2 instances to the Auto Scaling group.
- [ ] C. Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set the minimum instances to 2, the desired capacity to 3, and the maximum instances to 6. Add the EC2 instances to the Auto Scaling group.
- [ ] D. Create two Amazon CloudWatch alarms. Configure the first CloudWatch alarm to enter the ALARM state when the average CPUUtilization metric is below 20%. Configure the second CloudWatch alarm to enter the ALARM state when the average CPUUtilization matric is above 50%. Configure the alarms to publish to an Amazon Simple Notification Service (Amazon SNS) topic to send an email message. After receiving the message, log in to decrease or increase the number of EC2 instances that are running.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set a target tracking scaling policy that is based on the ASGAverageCPUUtilization metric. Set the minimum instances to 2, the desired capacity to 3, the maximum instances to 6, and the target value to 50%. Add the EC2 instances to the Auto Scaling group.

Why these are the correct answers:

B. Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set a target tracking scaling policy that is based on the ASGAverageCPUUtilization metric. Set the minimum instances to 2, the desired capacity to 3, the maximum instances to 6, and the target value to 50%. Add the EC2 instances to the Auto Scaling group.

- [ ]   Auto Scaling groups automatically adjust the number of EC2 instances based on demand.
- [ ]   Target tracking scaling policies maintain a specific metric (CPU utilization) at a desired level.
- [ ]   This ensures that the application has enough resources during surges while optimizing costs during low usage.

<hr> Why are the other answers wrong? <hr>

- [ ]   A. Using CloudWatch alarms and Lambda to terminate instances is complex and risky. It can lead to application instability if not implemented carefully.
- [ ]   C. Creating an Auto Scaling group without a scaling policy does not automate scaling. It only provides basic management.
- [ ]   D. Manual scaling based on SNS notifications is inefficient and does not provide automatic, real-time scaling.

Therefore, using an Auto Scaling group with target tracking is the most efficient and cost-effective solution.
</details>

<details>
  <summary>Question 298</summary>

A company is running a critical business application on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances run in an Auto Scaling group and access an Amazon RDS DB instance. The design did not pass an operational review because the EC2 instances and the DB instance are all located in a single Availability Zone. A solutions architect must update the design to use a second Availability Zone. Which solution will make the application highly available?

- [ ] A. Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance with connections to each network.
- [ ] B. Provision two subnets that extend across both Availability Zones. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance with connections to each network.
- [ ] C. Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment.
- [ ] D. Provision a subnet that extends across both Availability Zones. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment.

</details>

<details>
  <summary>Answer</summary>

- [ ] C. Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment.

Why these are the correct answers:

C. Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment.

- [ ]   Subnets within each AZ provide network segmentation.
- [ ]   Auto Scaling across AZs ensures EC2 instances are available even if one AZ fails.
- [ ]   RDS Multi-AZ provides a standby DB instance in another AZ for failover.

<hr> Why are the other answers wrong? <hr>

- [ ]   A and B. Configuring the DB instance with "connections to each network" is vague and not the standard way to achieve high availability for RDS.
- [ ]   D. A subnet cannot span multiple Availability Zones.

Therefore, the correct solution involves separate subnets per AZ, Auto Scaling across AZs, and RDS Multi-AZ.
</details>

<details>
  <summary>Question 299</summary>

A research laboratory needs to process approximately 8 TB of data. The laboratory requires sub-millisecond latencies and a minimum throughput of 6 GBps for the storage subsystem. Hundreds of Amazon EC2 instances that run Amazon Linux will distribute and process the data. Which solution will meet the performance requirements?

- [ ] A. Create an Amazon FSx for NetApp ONTAP file system. Sat each volume' tiering policy to ALL. Import the raw data into the file system. Mount the fila system on the EC2 instances.
- [ ] B. Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses persistent SSD storage. Select the option to import data from and export data to Amazon S3. Mount the file system on the EC2 instances.
- [ ] C. Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses persistent HDD storage. Select the option to import data from and export data to Amazon S3. Mount the file system on the EC2 instances.
- [ ] D. Create an Amazon FSx for NetApp ONTAP file system. Set each volume's tiering policy to NONE. Import the raw data into the file system. Mount the file system on the EC2 instances.

</details>

<details>
  <summary>Answer</summary>

- [ ] B. Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses persistent SSD storage. Select the option to import data from and export data to Amazon S3. Mount the file system on the EC2 instances.

Why these are the correct answers:

B. Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses persistent SSD storage. Select the option to import data from and export data to Amazon S3. Mount the file system on the EC2 instances.

- [ ]   Amazon S3 provides scalable and durable storage for the raw data.
- [ ]   Amazon FSx for Lustre with persistent SSD storage is designed for high-performance computing and can provide the required sub-millisecond latencies and high throughput.
- [ ]   Importing/exporting data between S3 and Lustre allows for efficient data management.

<hr> Why are the other answers wrong? <hr>

- [ ]   A and D. FSx for NetApp ONTAP is a general-purpose file system and does not offer the same level of high performance as FSx for Lustre for this workload.
- [ ]   C. FSx for Lustre with HDD storage does not meet the sub-millisecond latency requirement.

Therefore, using S3 for storage and FSx for Lustre with SSDs for processing is the appropriate solution.
</details>

<details>
  <summary>Question 300</summary>

A company needs to migrate a legacy application from an on-premises data center to the AWS Cloud because of hardware capacity constraints. The application runs 24 hours a day, 7 days a week. The application's database storage continues to grow over time. What should a solutions architect do to meet these requirements MOST cost-effectively?

- [ ] A. Migrate the application layer to Amazon EC2 Spot Instances. Migrate the data storage layer to Amazon S3.
- [ ] B. Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon RDS On-Demand Instances.
- [ ] C. Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon Aurora Reserved Instances.
- [ ] D. Migrate the application layer to Amazon EC2 On-Demand Instances. Migrate the data storage layer to Amazon RDS Reserved Instances.
</details>

<details>
  <summary>Answer</summary>

- [ ] C. Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon Aurora Reserved Instances.

Why these are the correct answers:

C. Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon Aurora Reserved Instances.

- [ ]   EC2 Reserved Instances provide cost savings for long-running applications with predictable usage.
- [ ]   Amazon Aurora Reserved Instances offer cost benefits for databases with consistent usage patterns.
- [ ]   This combination is cost-effective for applications running 24/7 with growing storage needs.

Why are the other answers wrong?

- [ ]   A. Spot Instances are cost-effective but are not suitable for applications requiring continuous availability. Amazon S3 is object storage, not suitable for the application's database storage.
- [ ]   B. RDS On-Demand Instances do not provide the cost benefits of Reserved Instances for long-term usage.
- [ ]   D. EC2 On-Demand Instances are more expensive than Reserved Instances for applications with consistent, long-term needs. RDS Reserved Instances are suitable for the database layer, but On-Demand Instances for the application layer are not cost-effective.

Therefore, Option C is the most cost-effective solution.
</details>






