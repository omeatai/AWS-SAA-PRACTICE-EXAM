# AWS-SAA-PRACTICE-EXAM Questions 601-610

<details>
  <summary>Question 601</summary>

A company runs its critical database on an Amazon RDS for PostgreSQL DB instance. The company wants to migrate to Amazon Aurora PostgreSQL with minimal downtime and data loss. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Create a DB snapshot of the RDS for PostgreSQL DB instance to populate a new Aurora PostgreSQL DB cluster.
-   [ ] B. Create an Aurora read replica of the RDS for PostgreSQL DB instance. Promote the Aurora read replicate to a new Aurora PostgreSQL DB cluster.
-   [ ] C. Use data import from Amazon S3 to migrate the database to an Aurora PostgreSQL DB cluster.
-   [ ] D. Use the pg_dump utility to back up the RDS for PostgreSQL database. Restore the backup to a new Aurora PostgreSQL DB cluster.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Create an Aurora read replica of the RDS for PostgreSQL DB instance. Promote the Aurora read replicate to a new Aurora PostgreSQL DB cluster.

Why this is the correct answer:

B. Create an Aurora read replica of the RDS for PostgreSQL DB instance. Promote the Aurora read replicate to a new Aurora PostgreSQL DB cluster.

-   [ ]   Creating an Aurora read replica of the RDS for PostgreSQL DB instance is a method that minimizes downtime because the replica can be promoted to become the primary database.
-   [ ]   This process reduces data loss as the replica is kept relatively in sync with the source database.
-   [ ]   Promoting the read replica to a new Aurora PostgreSQL DB cluster is a managed process, reducing operational overhead.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Creating a DB snapshot and restoring it involves more downtime than promoting a read replica.
-   [ ]   C. Using data import from Amazon S3 adds complexity and potential downtime.
-   [ ]   D. Using the pg_dump utility and restoring also involves downtime for the backup and restore process.

Therefore, Option B is the most suitable solution for minimal downtime and data loss with the least operational overhead.
</details>
<details>
  <summary>Question 602</summary>

A company's infrastructure consists of hundreds of Amazon EC2 instances that use Amazon Elastic Block Store (Amazon EBS) storage. A solutions architect must ensure that every EC2 instance can be recovered after a disaster. What should the solutions architect do to meet this requirement with the LEAST amount of effort?

-   [ ] A. Take a snapshot of the EBS storage that is attached to each EC2 instance. Create an AWS CloudFormation template to launch new EC2 instances from the EBS storage.
-   [ ] B. Take a snapshot of the EBS storage that is attached to each EC2 instance. Use AWS Elastic Beanstalk to set the environment based on the EC2 template and attach the EBS storage.
-   [ ] C. Use AWS Backup to set up a backup plan for the entire group of EC2 instances. Use the AWS Backup API or the AWS CLI to speed up the restore process for multiple EC2 instances.
-   [ ] D. Create an AWS Lambda function to take a snapshot of the EBS storage that is attached to each EC2 instance and copy the Amazon Machine Images (AMIs). Create another Lambda function to perform the restores with the copied AMIs and attach the EBS storage.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Use AWS Backup to set up a backup plan for the entire group of EC2 instances. Use the AWS Backup API or the AWS CLI to speed up the restore process for multiple EC2 instances.

Why this is the correct answer:

C. Use AWS Backup to set up a backup plan for the entire group of EC2 instances. Use the AWS Backup API or the AWS CLI to speed up the restore process for multiple EC2 instances.

-   [ ]   AWS Backup centralizes backup management, making it easier to create and manage backup plans for EC2 instances and their EBS volumes.
-   [ ]   It automates the backup process, reducing the effort required to ensure every EC2 instance can be recovered.
-   [ ]   Using the AWS Backup API or CLI allows for efficient and automated restoration of multiple instances.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A and B. Manually taking snapshots and using CloudFormation or Elastic Beanstalk involves more manual steps and effort compared to using AWS Backup.
-   [ ]   D. Creating Lambda functions to automate snapshots and restores adds complexity and overhead.

Therefore, Option C is the most efficient solution for ensuring EC2 instance recovery with the least amount of effort.
</details>
<details>
  <summary>Question 603</summary>

A company recently migrated to the AWS Cloud. The company wants a serverless solution for large-scale parallel on-demand processing of a semistructured dataset. The data consists of logs, media files, sales transactions, and loT sensor data that is stored in Amazon S3. The company wants the solution to process thousands of items in the dataset in parallel. Which solution will meet these requirements with the MOST operational efficiency?

-   [ ] A. Use the AWS Step Functions Map state in Inline mode to process the data in parallel.
-   [ ] B. Use the AWS Step Functions Map state in Distributed mode to process the data in parallel.
-   [ ] C. Use AWS Glue to process the data in parallel.
-   [ ] D. Use several AWS Lambda functions to process the data in parallel.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Use the AWS Step Functions Map state in Distributed mode to process the data in parallel.

Why this is the correct answer:

B. Use the AWS Step Functions Map state in Distributed mode to process the data in parallel.

-   [ ]   The AWS Step Functions Map state in Distributed mode is designed for large-scale parallel processing of data.
-   [ ]   It can handle thousands of items in a dataset and distribute the processing across multiple workflows, providing high operational efficiency for serverless parallel processing.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. The Inline mode of the Map state is suitable for smaller datasets and has limitations on parallelism.
-   [ ]   C. AWS Glue is an ETL service, not designed for on-demand parallel processing of large datasets.
-   [ ]   D. Using several Lambda functions directly would require managing the orchestration and parallelism, increasing operational overhead.

Therefore, Option B is the most efficient solution for large-scale parallel processing.
</details>
<details>
  <summary>Question 604</summary>

A company will migrate 10 PB of data to Amazon S3 in 6 weeks. The current data center has a 500 Mbps uplink to the internet. Other on-premises applications share the uplink. The company can use 80% of the internet bandwidth for this one-time migration task. Which solution will meet these requirements?

-   [ ] A. Configure AWS DataSync to migrate the data to Amazon S3 and to automatically verify the data.
-   [ ] B. Use rsync to transfer the data directly to Amazon S3.
-   [ ] C. Use the AWS CLI and multiple copy processes to send the data directly to Amazon S3.
-   [ ] D. Order multiple AWS Snowball devices. Copy the data to the devices. Send the devices to AWS to copy the data to Amazon S3.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Order multiple AWS Snowball devices. Copy the data to the devices. Send the devices to AWS to copy the data to Amazon S3.

Why this is the correct answer:

D. Order multiple AWS Snowball devices. Copy the data to the devices. Send the devices to AWS to copy the data to Amazon S3.

-   [ ]   AWS Snowball devices are designed for large-scale data transfers into and out of AWS.
-   [ ]   Transferring 10 PB of data over a 500 Mbps connection in 6 weeks is not feasible due to bandwidth limitations.
-   [ ]   Snowball provides a physical transfer mechanism that bypasses internet bandwidth constraints.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A, B, and C. Using AWS DataSync, rsync, or the AWS CLI is not practical for transferring 10 PB of data over a limited bandwidth connection within the given timeframe.

Therefore, Option D is the only viable solution to meet the migration requirements.
</details>
<details>
  <summary>Question 605</summary>

A company has several on-premises Internet Small Computer Systems Interface (ISCSI) network storage servers. The company wants to reduce the number of these servers by moving to the AWS Cloud. A solutions architect must provide low-latency access to frequently used data and reduce the dependency on on-premises servers with a minimal number of infrastructure changes. Which solution will meet these requirements?

-   [ ] A. Deploy an Amazon S3 File Gateway.
-   [ ] B. Deploy Amazon Elastic Block Store (Amazon EBS) storage with backups to Amazon S3.
-   [ ] C. Deploy an AWS Storage Gateway volume gateway that is configured with stored volumes.
-   [ ] D. Deploy an AWS Storage Gateway volume gateway that is configured with cached volumes.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Deploy an AWS Storage Gateway volume gateway that is configured with cached volumes.

Why this is the correct answer:

D. Deploy an AWS Storage Gateway volume gateway that is configured with cached volumes.

-   [ ]   AWS Storage Gateway with cached volumes stores frequently accessed data locally for low-latency access.
-   [ ]   It also asynchronously backs up data to AWS, reducing dependency on on-premises servers.
-   [ ]   This solution minimizes infrastructure changes as it integrates with existing iSCSI interfaces.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Amazon S3 File Gateway is for file-based access, not block-based access like iSCSI.
-   [ ]   B. EBS storage is for direct attachment to EC2 instances, not for replacing on-premises iSCSI servers.
-   [ ]   C. Stored volumes store all data locally, which does not reduce dependency on on-premises servers.

Therefore, Option D is the most suitable solution for low-latency access and reducing on-premises dependency.
</details>
<details>
  <summary>Question 606</summary>

A solutions architect is designing an application that will allow business users to upload objects to Amazon S3. The solution needs to maximize object durability. Objects also must be readily available at any time and for any length of time. Users will access objects frequently within the first 30 days after the objects are uploaded, but users are much less likely to access objects that are older than 30 days. Which solution meets these requirements MOST cost-effectively?

-   [ ] A. Store all the objects in S3 Standard with an S3 Lifecycle rule to transition the objects to S3 Glacier after 30 days.
-   [ ] B. Store all the objects in S3 Standard with an S3 Lifecycle rule to transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.
-   [ ] C. Store all the objects in S3 Standard with an S3 Lifecycle rule to transition the objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.
-   [ ] D. Store all the objects in S3 Intelligent-Tiering with an S3 Lifecycle rule to transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Store all the objects in S3 Standard with an S3 Lifecycle rule to transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.

Why this is the correct answer:

B. Store all the objects in S3 Standard with an S3 Lifecycle rule to transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.

-   [ ]   S3 Standard provides high durability and availability for frequently accessed objects.
-   [ ]   S3 Lifecycle rules automate the transition of objects to a different storage class.
-   [ ]   S3 Standard-IA is cost-effective for data that is accessed less frequently while maintaining quick retrieval times.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. S3 Glacier is for long-term archival and has retrieval times that are not suitable for readily available objects.
-   [ ]   C. S3 One Zone-IA is less durable as it stores data in a single Availability Zone.
-   [ ]   D. S3 Intelligent-Tiering automatically optimizes storage costs but adds complexity without a clear benefit over using lifecycle rules for known access patterns.

Therefore, Option B is the most cost-effective solution for durability and availability requirements.
</details>
<details>
  <summary>Question 607</summary>

A company has migrated a two-tier application from its on-premises data center to the AWS Cloud. The data tier is a Multi-AZ deployment of Amazon RDS for Oracle with 12 TB of General Purpose SSD Amazon Elastic Block Store (Amazon EBS) storage. The application is designed to process and store documents in the database as binary large objects (blobs) with an average document size of 6 MB. The database size has grown over time, reducing the performance and increasing the cost of storage. The company must improve the database performance and needs a solution that is highly available and resilient. Which solution will meet these requirements MOST cost-effectively?

-   [ ] A. Reduce the RDS DB instance size. Increase the storage capacity to 24 TiB. Change the storage type to Magnetic.
-   [ ] B. Increase the RDS DB instance size. Increase the storage capacity to 24 TiB. Change the storage type to Provisioned IOPS.
-   [ ] C. Create an Amazon S3 bucket. Update the application to store documents in the S3 bucket. Store the object metadata in the existing database.
-   [ ] D. Create an Amazon DynamoDB table. Update the application to use DynamoDB. Use AWS Database Migration Service (AWS DMS) to migrate data from the Oracle database to DynamoDB.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Create an Amazon S3 bucket. Update the application to store documents in the S3 bucket. Store the object metadata in the existing database.

Why this is the correct answer:

C. Create an Amazon S3 bucket. Update the application to store documents in the S3 bucket. Store the object metadata in the existing database.

-   [ ]   Storing blobs in S3 offloads the storage burden from the RDS database, improving performance and reducing costs.
-   [ ]   Keeping metadata in the database maintains data integrity and relationships.
-   [ ]   S3 is highly available and resilient, meeting the application's requirements.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A and B. Changing the RDS instance size or storage type addresses performance but does not solve the storage cost issue.
-   [ ]   D. Migrating to DynamoDB is complex and not cost-effective for this scenario.

Therefore, Option C is the most suitable and cost-effective solution for improving performance and managing storage.
</details>
<details>
  <summary>Question 608</summary>

A company has an application that serves clients that are deployed in more than 20.000 retail storefront locations around the world. The application consists of backend web services that are exposed over HTTPS on port 443. The application is hosted on Amazon EC2 instances behind an Application Load Balancer (ALB). The retail locations communicate with the web application over the public internet. The company allows each retail location to register the IP address that the retail location has been allocated by its local ISP. The company's security team recommends to increase the security of the application endpoint by restricting access to only the IP addresses registered by the retail locations. What should a solutions architect do to meet these requirements?

-   [ ] A. Associate an AWS WAF web ACL with the ALB. Use IP rule sets on the ALB to filter traffic. Update the IP addresses in the rule to include the registered IP addresses.
-   [ ] B. Deploy AWS Firewall Manager to manage the ALConfigure firewall rules to restrict traffic to the ALModify the firewall rules to include the registered IP addresses.
-   [ ] C. Store the IP addresses in an Amazon DynamoDB table. Configure an AWS Lambda authorization function on the ALB to validate that incoming requests are from the registered IP addresses.
-   [ ] D. Configure the network ACL on the subnet that contains the public interface of the ALB. Update the ingress rules on the network ACL with entries for each of the registered IP addresses.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Associate an AWS WAF web ACL with the ALB. Use IP rule sets on the ALB to filter traffic. Update the IP addresses in the rule to include the registered IP addresses.

Why this is the correct answer:

A. Associate an AWS WAF web ACL with the ALB. Use IP rule sets on the ALB to filter traffic. Update the IP addresses in the rule to include the registered IP addresses.

-   [ ]   AWS WAF is designed to protect web applications from common web exploits.
-   [ ]   WAF allows you to create IP rule sets to filter traffic based on source IP addresses.
-   [ ]   Associating WAF with the ALB provides a scalable and flexible way to restrict access to the registered IP addresses.

<hr> Why are the other answers wrong? <hr>

-   [ ]   B. AWS Firewall Manager is for central management of firewalls across multiple accounts and is not necessary for this scenario.
-   [ ]   C. Using a Lambda authorizer adds complexity and latency compared to WAF.
-   [ ]   D. Network ACLs operate at the subnet level and are not as granular or efficient for filtering HTTP/HTTPS traffic as WAF.

Therefore, Option A is the most suitable solution for securing the application endpoint.
</details>
<details>
  <summary>Question 609</summary>

A company is building a data analysis platform on AWS by using AWS Lake Formation. The platform will ingest data from different sources such as Amazon S3 and Amazon RDS. The company needs a secure solution to prevent access to portions of the data that contain sensitive information. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Create an IAM role that includes permissions to access Lake Formation tables.
-   [ ] B. Create data filters to implement row-level security and cell-level security.
-   [ ] C. Create an AWS Lambda function that removes sensitive information before Lake Formation ingests the data.
-   [ ] D. Create an AWS Lambda function that periodically queries and removes sensitive information from Lake Formation tables.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Create data filters to implement row-level security and cell-level security.

Why this is the correct answer:

B. Create data filters to implement row-level security and cell-level security.

-   [ ]   AWS Lake Formation provides data filters to control access to data at the row and cell level.
-   [ ]   This is a built-in feature of Lake Formation and requires the least operational overhead.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. IAM roles control access to Lake Formation tables but do not provide granular control at the row or cell level.
-   [ ]   C and D. Using Lambda functions to remove or filter sensitive information adds complexity and operational overhead.

Therefore, Option B is the most efficient solution for securing data in Lake Formation.
</details>

<details>
  <summary>Question 610</summary>

A company deploys Amazon EC2 instances that run in a VPC. The EC2 instances load source data into Amazon S3 buckets so that the data can be processed in the future. According to compliance laws, the data must not be transmitted over the public internet. Servers in the company's on-premises data center will consume the output from an application that runs on the EC2 instances. Which solution will meet these requirements?

-   [ ] A. Deploy an interface VPC endpoint for Amazon EC2. Create an AWS Site-to-Site VPN connection between the company and the VPC.
-   [ ] B. Deploy a gateway VPC endpoint for Amazon S3. Set up an AWS Direct Connect connection between the on-premises network and the VPC.
-   [ ] C. Set up an AWS Transit Gateway connection from the VPC to the S3 buckets. Create an AWS Site-to-Site VPN connection between the company and the VPC.
-   [ ] D. Set up proxy EC2 instances that have routes to NAT gateways. Configure the proxy EC2 instances to fetch S3 data and feed the application instances.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Deploy a gateway VPC endpoint for Amazon S3. Set up an AWS Direct Connect connection between the on-premises network and the VPC.

Why this is the correct answer:

B. Deploy a gateway VPC endpoint for Amazon S3. Set up an AWS Direct Connect connection between the on-premises network and the VPC.

-   [ ]   A gateway VPC endpoint for S3 allows EC2 instances in the VPC to access S3 without traversing the internet.
-   [ ]   AWS Direct Connect provides a dedicated private network connection between the on-premises data center and the VPC, ensuring data does not go over the public internet.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Interface VPC endpoints are for services like API Gateway and Kinesis, not S3. A VPN does not prevent data from traversing the internet.
-   [ ]   C. Transit Gateway is for connecting multiple VPCs and on-premises networks, but it doesn't inherently prevent internet traffic.
-   [ ]   D. Proxy servers and NAT gateways are used for internet access, which is what the requirement prohibits.

Therefore, Option B is the correct solution to ensure data is not transmitted over the public internet.
</details>

# AWS-SAA-PRACTICE-EXAM Questions 611-620

<details>
  <summary>Question 611</summary>

A company has an application with a REST-based interface that allows data to be received in near-real time from a third-party vendor. Once received, the application processes and stores the data for further analysis. The application is running on Amazon EC2 instances. The third-party vendor has received many 503 Service Unavailable Errors when sending data to the application. When the data volume spikes, the compute capacity reaches its maximum limit and the application is unable to process all requests. Which design should a solutions architect recommend to provide a more scalable solution?

-   [ ] A. Use Amazon Kinesis Data Streams to ingest the data. Process the data using AWS Lambda functions.
-   [ ] B. Use Amazon API Gateway on top of the existing application. Create a usage plan with a quota limit for the third-party vendor.
-   [ ] C. Use Amazon Simple Notification Service (Amazon SNS) to ingest the data. Put the EC2 instances in an Auto Scaling group behind an Application Load Balancer.
-   [ ] D. Repackage the application as a container. Deploy the application using Amazon Elastic Container Service (Amazon ECS) using the EC2 launch type with an Auto Scaling group.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Use Amazon Kinesis Data Streams to ingest the data. Process the data using AWS Lambda functions.

Why this is the correct answer:

A. Use Amazon Kinesis Data Streams to ingest the data. Process the data using AWS Lambda functions.

-   [ ]   Amazon Kinesis Data Streams can handle high-volume, real-time data ingestion.
-   [ ]   AWS Lambda functions can process the data in a scalable and serverless manner, automatically adjusting to the data volume.
-   [ ]   This combination provides a highly scalable solution to prevent 503 errors during data spikes.

<hr> Why are the other answers wrong? <hr>

-   [ ]   B. API Gateway with usage plans can control traffic but does not inherently scale the application's processing capacity.
-   [ ]   C. SNS is for pub/sub messaging and does not provide scalable data ingestion and processing like Kinesis and Lambda.
-   [ ]   D. Containerizing the application can improve scalability but may not be as efficient as Lambda for handling unpredictable data spikes.

Therefore, Option A is the most suitable solution for a scalable, real-time data ingestion and processing system.
</details>
<details>
  <summary>Question 612</summary>

A company has an application that runs on Amazon EC2 instances in a private subnet. The application needs to process sensitive information from an Amazon S3 bucket. The application must not use the internet to connect to the S3 bucket. Which solution will meet these requirements?

-   [ ] A. Configure an internet gateway. Update the S3 bucket policy to allow access from the internet gateway. Update the application to use the new internet gateway.
-   [ ] B. Configure a VPN connection. Update the S3 bucket policy to allow access from the VPN connection. Update the application to use the new VPN connection.
-   [ ] C. Configure a NAT gateway. Update the S3 bucket policy to allow access from the NAT gateway. Update the application to use the new NAT gateway.
-   [ ] D. Configure a VPC endpoint. Update the S3 bucket policy to allow access from the VPC endpoint. Update the application to use the new VPC endpoint.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Configure a VPC endpoint. Update the S3 bucket policy to allow access from the VPC endpoint. Update the application to use the new VPC endpoint.

Why this is the correct answer:

D. Configure a VPC endpoint. Update the S3 bucket policy to allow access from the VPC endpoint. Update the application to use the new VPC endpoint.

-   [ ]   VPC endpoints for S3 enable private connectivity between EC2 instances and S3 within the VPC.
-   [ ]   This ensures that the application can access S3 without using the internet.
-   [ ]   Updating the S3 bucket policy and application to use the VPC endpoint completes the setup.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. An internet gateway allows internet access, which is prohibited by the requirement.
-   [ ]   B. A VPN connection is for connecting to on-premises networks, not for private access to S3.
-   [ ]   C. A NAT gateway is used for instances in a private subnet to access the internet, not for private S3 access.

Therefore, Option D is the only solution that meets the requirement of not using the internet.
</details>
<details>
  <summary>Question 613</summary>

A company uses Amazon Elastic Kubernetes Service (Amazon EKS) to run a container application. The EKS cluster stores sensitive information in the Kubernetes secrets object. The company wants to ensure that the information is encrypted. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Use the container application to encrypt the information by using AWS Key Management Service (AWS KMS).
-   [ ] B. Enable secrets encryption in the EKS cluster by using AWS Key Management Service (AWS KMS).
-   [ ] C. Implement an AWS Lambda function to encrypt the information by using AWS Key Management Service (AWS KMS).
-   [ ] D. Use AWS Systems Manager Parameter Store to encrypt the information by using AWS Key Management Service (AWS KMS).

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Enable secrets encryption in the EKS cluster by using AWS Key Management Service (AWS KMS).

Why this is the correct answer:

B. Enable secrets encryption in the EKS cluster by using AWS Key Management Service (AWS KMS).

-   [ ]   EKS supports encrypting Kubernetes secrets at rest using AWS KMS.
-   [ ]   This is a built-in feature that requires minimal configuration and management, reducing operational overhead.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Encrypting within the container application requires changes to the application code and increases complexity.
-   [ ]   C. Using a Lambda function to encrypt secrets adds complexity and operational overhead.
-   [ ]   D. Parameter Store is a separate service and adds complexity compared to EKS-native secret encryption.

Therefore, Option B is the most efficient solution for encrypting Kubernetes secrets.
</details>
<details>
  <summary>Question 614</summary>

A company is designing a new multi-tier web application that consists of the following components:

-   [ ]   Web and application servers that run on Amazon EC2 instances as part of Auto Scaling groups
-   [ ]   An Amazon RDS DB instance for data storage

A solutions architect needs to limit access to the application servers so that only the web servers can access them. Which solution will meet these requirements?

-   [ ] A. Deploy AWS PrivateLink in front of the application servers. Configure the network ACL to allow only the web servers to access the application servers.
-   [ ] B. Deploy a VPC endpoint in front of the application servers. Configure the security group to allow only the web servers to access the application servers.
-   [ ] C. Deploy a Network Load Balancer with a target group that contains the application servers' Auto Scaling group. Configure the network ACL to allow only the web servers to access the application servers.
-   [ ] D. Deploy an Application Load Balancer with a target group that contains the application servers' Auto Scaling group. Configure the security group to allow only the web servers to access the application servers.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Deploy an Application Load Balancer with a target group that contains the application servers' Auto Scaling group. Configure the security group to allow only the web servers to access the application servers.

Why this is the correct answer:

D. Deploy an Application Load Balancer with a target group that contains the application servers' Auto Scaling group. Configure the security group to allow only the web servers to access the application servers.

-   [ ]   Application Load Balancers (ALBs) are suitable for HTTP traffic and can distribute requests to application servers.
-   [ ]   Security groups act as a virtual firewall for EC2 instances, allowing you to control inbound and outbound traffic.
-   [ ]   By configuring the security group of the application servers to only allow traffic from the web servers, you enforce the required access control.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. AWS PrivateLink is used for private connectivity to AWS services, not for internal application traffic. Network ACLs operate at the subnet level and are not as granular as security groups.
-   [ ]   B. VPC endpoints are for connecting to AWS services, not for internal application traffic.
-   [ ]   C. Network Load Balancers operate at the transport layer (TCP/UDP) and are not as suitable for controlling HTTP traffic as ALBs.

Therefore, Option D is the most appropriate solution for limiting access to application servers.
</details>
<details>
  <summary>Question 615</summary>

A company runs a critical, customer-facing application on Amazon Elastic Kubernetes Service (Amazon EKS). The application has a microservices architecture. The company needs to implement a solution that collects, aggregates, and summarizes metrics and logs from the application in a centralized location. Which solution meets these requirements?

-   [ ] A. Run the Amazon CloudWatch agent in the existing EKS cluster. View the metrics and logs in the CloudWatch console.
-   [ ] B. Run AWS App Mesh in the existing EKS cluster. View the metrics and logs in the App Mesh console.
-   [ ] C. Configure AWS CloudTrail to capture data events. Query CloudTrail by using Amazon OpenSearch Service.
-   [ ] D. Configure Amazon CloudWatch Container Insights in the existing EKS cluster. View the metrics and logs in the CloudWatch console.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Configure Amazon CloudWatch Container Insights in the existing EKS cluster. View the metrics and logs in the CloudWatch console.

Why this is the correct answer:

D. Configure Amazon CloudWatch Container Insights in the existing EKS cluster. View the metrics and logs in the CloudWatch console.

-   [ ]   Amazon CloudWatch Container Insights is specifically designed to collect, aggregate, and summarize metrics and logs from containerized applications, including those running on EKS.
-   [ ]   It provides a centralized view of performance and operational data in the CloudWatch console.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. The CloudWatch agent can collect logs and metrics but does not provide the same level of aggregation and summarization as Container Insights.
-   [ ]   B. AWS App Mesh is a service mesh that provides observability but is not designed for centralized log and metric collection.
-   [ ]   C. CloudTrail captures API calls but is not suitable for collecting application-level metrics and logs.

Therefore, Option D is the most appropriate solution for centralized monitoring of EKS applications.
</details>
<details>
  <summary>Question 616</summary>

A company has deployed its newest product on AWS. The product runs in an Auto Scaling group behind a Network Load Balancer. The company stores the product's objects in an Amazon S3 bucket. The company recently experienced malicious attacks against its systems. The company needs a solution that continuously monitors for malicious activity in the AWS account, workloads, and access patterns to the S3 bucket. The solution must also report suspicious activity and display the information on a dashboard. Which solution will meet these requirements?

-   [ ] A. Configure Amazon Macie to monitor and report findings to AWS Config.
-   [ ] B. Configure Amazon Inspector to monitor and report findings to AWS CloudTrail.
-   [ ] C. Configure Amazon GuardDuty to monitor and report findings to AWS Security Hub.
-   [ ] D. Configure AWS Config to monitor and report findings to Amazon EventBridge.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Configure Amazon GuardDuty to monitor and report findings to AWS Security Hub.

Why this is the correct answer:

C. Configure Amazon GuardDuty to monitor and report findings to AWS Security Hub.

-   [ ]   Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and delivers security findings.
-   [ ]   It analyzes various data sources, including CloudTrail logs, VPC Flow Logs, and DNS logs, to detect threats.
-   [ ]   AWS Security Hub provides a centralized view of security alerts and findings from various AWS security services, including GuardDuty, along with a dashboard.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Amazon Macie is for discovering and protecting sensitive data, not for general threat detection.
-   [ ]   B. Amazon Inspector is for automated security assessments of applications, not for real-time threat monitoring.
-   [ ]   D. AWS Config monitors resource configurations but does not detect malicious activity.

Therefore, Option C is the most suitable solution for continuous threat monitoring and reporting.
</details>
<details>
  <summary>Question 617</summary>

A company wants to migrate an on-premises data center to AWS. The data center hosts a storage server that stores data in an NFS-based file system. The storage server holds 200 GB of data. The company needs to migrate the data without interruption to existing services. Multiple resources in AWS must be able to access the data by using the NFS protocol. Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)

-   [ ] A. Create an Amazon FSx for Lustre file system.
-   [ ] B. Create an Amazon Elastic File System (Amazon EFS) file system.
-   [ ] C. Create an Amazon S3 bucket to receive the data.
-   [ ] D. Manually use an operating system copy command to push the data into the AWS destination.
-   [ ] E. Install an AWS DataSync agent in the on-premises data center. Use a DataSync task between the on-premises location and AWS.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Create an Amazon Elastic File System (Amazon EFS) file system.
-   [ ] E. Install an AWS DataSync agent in the on-premises data center. Use a DataSync task between the on-premises location and AWS.

Why these are the correct answers:

B. Create an Amazon Elastic File System (Amazon EFS) file system.

-   [ ]   Amazon EFS provides scalable file storage that can be accessed by multiple EC2 instances concurrently using the NFS protocol.
-   [ ]   This meets the requirement for multiple resources in AWS to access the data via NFS.

E. Install an AWS DataSync agent in the on-premises data center. Use a DataSync task between the on-premises location and AWS.

-   [ ]   AWS DataSync is designed to transfer data between on-premises storage and AWS storage services.
-   [ ]   It can efficiently transfer data without interrupting existing services and is suitable for migrating data from an NFS file system to EFS.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Amazon FSx for Lustre is optimized for high-performance computing and is more expensive than EFS for simple file sharing.
-   [ ]   C. Amazon S3 is object storage and does not support the NFS protocol.
-   [ ]   D. Manually copying data using operating system commands is inefficient and can cause interruptions.

Therefore, Options B and E provide the most cost-effective and efficient solution for migrating the NFS data to AWS.
</details>

<details>
  <summary>Question 618</summary>

A company wants to use Amazon FSx for Windows File Server for its Amazon EC2 instances that have an SMB file share mounted as a volume in the us-east-1 Region. The company has a recovery point objective (RPO) of 5 minutes for planned system maintenance or unplanned service disruptions. The company needs to replicate the file system to the us-west-2 Region. The replicated data must not be deleted by any user for 5 years. Which solution will meet these requirements?

-   [ ] A. Create an FSx for Windows File Server file system in us-east-1 that has a Single-AZ 2 deployment type. Use AWS Backup to create a daily backup plan that includes a backup rule that copies the backup to us-west-2. Configure AWS Backup Vault Lock in compliance mode for a target vault in us-west-2. Configure a minimum duration of 5 years.
-   [ ] B. Create an FSx for Windows File Server file system in us-east-1 that has a Multi-AZ deployment type. Use AWS Backup to create a daily backup plan that includes a backup rule that copies the backup to us-west-2. Configure AWS Backup Vault Lock in governance mode for a target vault in us-west-2. Configure a minimum duration of 5 years.
-   [ ] C. Create an FSx for Windows File Server file system in us-east-1 that has a Multi-AZ deployment type. Use AWS Backup to create a daily backup plan that includes a backup rule that copies the backup to us-west-2. Configure AWS Backup Vault Lock in compliance mode for a target vault in us-west-2. Configure a minimum duration of 5 years.
-   [ ] D. Create an FSx for Windows File Server file system in us-east-1 that has a Single-AZ 2 deployment type. Use AWS Backup to create a daily backup plan that includes a backup rule that copies the backup to us-west-2. Configure AWS Backup Vault Lock in governance mode for a target vault in us-west-2. Configure a minimum duration of 5 years.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Create an FSx for Windows File Server file system in us-east-1 that has a Multi-AZ deployment type. Use AWS Backup to create a daily backup plan that includes a backup rule that copies the backup to us-west-2. Configure AWS Backup Vault Lock in compliance mode for a target vault in us-west-2. Configure a minimum duration of 5 years.

Why this is the correct answer:

C. Create an FSx for Windows File Server file system in us-east-1 that has a Multi-AZ deployment type. Use AWS Backup to create a daily backup plan that includes a backup rule that copies the backup to us-west-2. Configure AWS Backup Vault Lock in compliance mode for a target vault in us-west-2. Configure a minimum duration of 5 years.

-   [ ]   Multi-AZ deployment provides high availability and meets the RPO for planned and unplanned disruptions.
-   [ ]   AWS Backup allows for automated backups and cross-region copying for disaster recovery.
-   [ ]   Backup Vault Lock in compliance mode prevents deletion of backups, meeting the 5-year retention requirement.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A and D. Single-AZ deployments do not provide sufficient availability for production workloads.
-   [ ]   B and D. Governance mode in Backup Vault Lock allows privileged users to delete backups, which does not meet the requirement to prevent deletion for 5 years.

Therefore, Option C is the only solution that meets all the requirements.
</details>
<details>
  <summary>Question 619</summary>

A solutions architect is designing a security solution for a company that wants to provide developers with individual AWS accounts through AWS Organizations, while also maintaining standard security controls. Because the individual developers will have AWS account root user-level access to their own accounts, the solutions architect wants to ensure that the mandatory AWS CloudTrail configuration that is applied to new developer accounts is not modified. Which action meets these requirements?

-   [ ] A. Create an IAM policy that prohibits changes to CloudTrail. and attach it to the root user.
-   [ ] B. Create a new trail in CloudTrail from within the developer accounts with the organization trails option enabled.
-   [ ] C. Create a service control policy (SCP) that prohibits changes to CloudTrail, and attach it the developer accounts.
-   [ ] D. Create a service-linked role for CloudTrail with a policy condition that allows changes only from an Amazon Resource Name (ARN) in the management account.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Create a service control policy (SCP) that prohibits changes to CloudTrail, and attach it the developer accounts.

Why this is the correct answer:

C. Create a service control policy (SCP) that prohibits changes to CloudTrail, and attach it the developer accounts.

-   [ ]   Service control policies (SCPs) are used in AWS Organizations to centrally manage permissions for accounts in the organization.
-   [ ]   An SCP can prevent even the root user in individual accounts from modifying CloudTrail configurations, ensuring that logging is not disabled or altered.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. IAM policies do not apply to the root user, so this will not prevent the root user from modifying CloudTrail.
-   [ ]   B. Creating a new trail from within developer accounts does not prevent them from modifying or deleting it.
-   [ ]   D. Service-linked roles do not prevent the root user from modifying CloudTrail.

Therefore, Option C is the most suitable solution to enforce CloudTrail configuration.
</details>
<details>
  <summary>Question 620</summary>

A company is planning to deploy a business-critical application in the AWS Cloud. The application requires durable storage with consistent, low-latency performance. Which type of storage should a solutions architect recommend to meet these requirements?

-   [ ] A. Instance store volume
-   [ ] B. Amazon ElastiCache for Memcached cluster
-   [ ] C. Provisioned IOPS SSD Amazon Elastic Block Store (Amazon EBS) volume
-   [ ] D. Throughput Optimized HDD Amazon Elastic Block Store (Amazon EBS) volume

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Provisioned IOPS SSD Amazon Elastic Block Store (Amazon EBS) volume

Why this is the correct answer:

C. Provisioned IOPS SSD Amazon Elastic Block Store (Amazon EBS) volume

-   [ ]   Provisioned IOPS SSD EBS volumes are designed for applications that require consistent and low-latency performance.
-   [ ]   They offer durable block storage with predictable I/O performance.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Instance store volumes are ephemeral and do not provide data durability.
-   [ ]   B. Amazon ElastiCache is an in-memory caching service, not durable block storage.
-   [ ]   D. Throughput Optimized HDD EBS volumes are for high throughput, not low-latency performance.

Therefore, Option C is the most suitable for durable, low-latency storage.
</details>

# AWS-SAA-PRACTICE-EXAM Questions 621-630

<details>
  <summary>Question 621</summary>

An online photo-sharing company stores its photos in an Amazon S3 bucket that exists in the us-west-1 Region. The company needs to store a copy of all new photos in the us-east-1 Region. Which solution will meet this requirement with the LEAST operational effort?

-   [ ] A. Create a second S3 bucket in us-east-1. Use S3 Cross-Region Replication to copy photos from the existing S3 bucket to the second S3 bucket.
-   [ ] B. Create a cross-origin resource sharing (CORS) configuration of the existing S3 bucket. Specify us-east-1 in the CORS rule's AllowedOrigin element.
-   [ ] C. Create a second S3 bucket in us-east-1 across multiple Availability Zones. Create an S3 Lifecycle rule to save photos into the second S3 bucket.
-   [ ] D. Create a second S3 bucket in us-east-1. Configure S3 event notifications on object creation and update events to invoke an AWS Lambda function to copy photos from the existing S3 bucket to the second S3 bucket.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Create a second S3 bucket in us-east-1. Use S3 Cross-Region Replication to copy photos from the existing S3 bucket to the second S3 bucket.

Why this is the correct answer:

A. Create a second S3 bucket in us-east-1. Use S3 Cross-Region Replication to copy photos from the existing S3 bucket to the second S3 bucket.

-   [ ]   S3 Cross-Region Replication (CRR) automatically replicates objects between S3 buckets in different AWS Regions.
-   [ ]   CRR is designed for asynchronous, automatic copying of objects and requires minimal configuration, providing the least operational effort.

<hr> Why are the other answers wrong? <hr>

-   [ ]   B. CORS is for enabling cross-origin requests in web browsers, not for replicating data between S3 buckets.
-   [ ]   C. S3 Lifecycle rules move objects within the same region, not across regions.
-   [ ]   D. Using Lambda functions for copying objects adds complexity and operational overhead compared to using CRR.

Therefore, Option A is the most efficient solution for replicating photos across regions.
</details>
<details>
  <summary>Question 622</summary>

A company is creating a new web application for its subscribers. The application will consist of a static single page and a persistent database layer. The application will have millions of users for 4 hours in the morning, but the application will have only a few thousand users during the rest of the day. The company's data architects have requested the ability to rapidly evolve their schema. Which solutions will meet these requirements and provide the MOST scalability? (Choose two.)

-   [ ] A. Deploy Amazon DynamoDB as the database solution. Provision on-demand capacity.
-   [ ] B. Deploy Amazon Aurora as the database solution. Choose the serverless DB engine mode.
-   [ ] C. Deploy Amazon DynamoDB as the database solution. Ensure that DynamoDB auto scaling is enabled.
-   [ ] D. Deploy the static content into an Amazon S3 bucket. Provision an Amazon CloudFront distribution with the S3 bucket as the origin.
-   [ ] E. Deploy the web servers for static content across a fleet of Amazon EC2 instances in Auto Scaling groups. Configure the instances to periodically refresh the content from an Amazon Elastic File System (Amazon EFS) volume.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Deploy Amazon DynamoDB as the database solution. Provision on-demand capacity.
-   [ ] D. Deploy the static content into an Amazon S3 bucket. Provision an Amazon CloudFront distribution with the S3 bucket as the origin.

Why these are the correct answers:

A. Deploy Amazon DynamoDB as the database solution. Provision on-demand capacity.

-   [ ]   Amazon DynamoDB is a NoSQL database that can scale to handle millions of requests.
-   [ ]   On-demand capacity mode allows DynamoDB to automatically scale in response to traffic, making it suitable for applications with variable usage.
-   [ ]   DynamoDB's flexible schema supports rapid evolution.

D. Deploy the static content into an Amazon S3 bucket. Provision an Amazon CloudFront distribution with the S3 bucket as the origin.

-   [ ]   Amazon S3 is a scalable and cost-effective storage for static content.
-   [ ]   Amazon CloudFront is a CDN that can handle millions of users by caching content at edge locations.
-   [ ]   This combination provides high scalability for serving static web content.

<hr> Why are the other answers wrong? <hr>

-   [ ]   B. Aurora Serverless v2 can scale, but it is a relational database and might not be the best fit for rapid schema evolution.
-   [ ]   C. DynamoDB auto scaling is a good option, but on-demand capacity is simpler for unpredictable workloads.
-   [ ]   E. Using EC2 instances and EFS for static content is less scalable and more complex than using S3 and CloudFront.

Therefore, Options A and D provide the best scalability and meet the requirements.
</details>
<details>
  <summary>Question 623</summary>

A company uses Amazon API Gateway to manage its REST APIs that third-party service providers access. The company must protect the REST APIs from SQL injection and cross-site scripting attacks. What is the MOST operationally efficient solution that meets these requirements?

-   [ ] A. Configure AWS Shield.
-   [ ] B. Configure AWS WAF.
-   [ ] C. Set up API Gateway with an Amazon CloudFront distribution. Configure AWS Shield in CloudFront.
-   [ ] D. Set up API Gateway with an Amazon CloudFront distribution. Configure AWS WAF in CloudFront.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Configure AWS WAF.

Why this is the correct answer:

B. Configure AWS WAF.

-   [ ]   AWS WAF is a web application firewall that protects web applications from common web exploits.
-   [ ]   It can filter malicious web traffic and is designed to prevent SQL injection and cross-site scripting attacks.
-   [ ]   WAF integrates directly with API Gateway, providing an operationally efficient solution.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. AWS Shield protects against DDoS attacks, not web application exploits like SQL injection.
-   [ ]   C and D. CloudFront can be used with WAF, but it adds complexity and is not necessary for protecting API Gateway from web exploits.

Therefore, Option B is the most operationally efficient solution for protecting the APIs.
</details>
<details>
  <summary>Question 624</summary>

A company wants to provide users with access to AWS resources. The company has 1,500 users and manages their access to on-premises resources through Active Directory user groups on the corporate network. However, the company does not want users to have to maintain another identity to access the resources. A solutions architect must manage user access to the AWS resources while preserving access to the on-premises resources. What should the solutions architect do to meet these requirements?

-   [ ] A. Create an IAM user for each user in the company. Attach the appropriate policies to each user.
-   [ ] B. Use Amazon Cognito with an Active Directory user pool. Create roles with the appropriate policies attached.
-   [ ] C. Define cross-account roles with the appropriate policies attached. Map the roles to the Active Directory groups.
-   [ ] D. Configure Security Assertion Markup Language (SAML) 2 0-based federation. Create roles with the appropriate policies attached Map the roles to the Active Directory groups.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Configure Security Assertion Markup Language (SAML) 2 0-based federation. Create roles with the appropriate policies attached Map the roles to the Active Directory groups.

Why this is the correct answer:

D. Configure Security Assertion Markup Language (SAML) 2 0-based federation. Create roles with the appropriate policies attached Map the roles to the Active Directory groups.

-   [ ]   SAML 2.0-based federation allows users to use their existing Active Directory credentials to access AWS resources.
-   [ ]   This eliminates the need for users to maintain separate AWS credentials.
-   [ ]   Roles in AWS can be mapped to Active Directory groups, simplifying access management.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Creating IAM users for each user is complex and requires managing separate credentials.
-   [ ]   B. Cognito is for managing user identities, but it does not directly integrate with Active Directory groups for authentication.
-   [ ]   C. Cross-account roles are for granting access between AWS accounts, not for integrating with Active Directory.

Therefore, Option D is the most suitable solution for federated access management.
</details>
<details>
  <summary>Question 625</summary>

A company is hosting a website behind multiple Application Load Balancers. The company has different distribution rights for its content around the world. A solutions architect needs to ensure that users are served the correct content without violating distribution rights. Which configuration should the solutions architect choose to meet these requirements?

-   [ ] A. Configure Amazon CloudFront with AWS WAF.
-   [ ] B. Configure Application Load Balancers with AWS WAF
-   [ ] C. Configure Amazon Route 53 with a geolocation policy
-   [ ] D. Configure Amazon Route 53 with a geoproximity routing policy

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Configure Amazon Route 53 with a geolocation policy

Why this is the correct answer:

C. Configure Amazon Route 53 with a geolocation policy

-   [ ]   Amazon Route 53 geolocation routing allows you to route traffic based on the geographic location of users.
-   [ ]   This enables serving different content to users based on their location, respecting distribution rights.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A and B. AWS WAF protects against web exploits but does not provide geographic routing capabilities.
-   [ ]   D. Geoproximity routing is based on the distance between users and resources, not on defined geographic boundaries for distribution rights.

Therefore, Option C is the most appropriate solution for serving content based on geographic location.
</details>
<details>
  <summary>Question 626</summary>

A company stores its data on premises. The amount of data is growing beyond the company's available capacity. The company wants to migrate its data from the on-premises location to an Amazon S3 bucket. The company needs a solution that will automatically validate the integrity of the data after the transfer. Which solution will meet these requirements?

-   [ ] A. Order an AWS Snowball Edge device. Configure the Snowball Edge device to perform the online data transfer to an S3 bucket
-   [ ] B. Deploy an AWS DataSync agent on premises. Configure the DataSync agent to perform the online data transfer to an S3 bucket.
-   [ ] C. Create an Amazon S3 File Gateway on premises Configure the S3 File Gateway to perform the online data transfer to an S3 bucket
-   [ ] D. Configure an accelerator in Amazon S3 Transfer Acceleration on premises. Configure the accelerator to perform the online data transfer to an S3 bucket.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Deploy an AWS DataSync agent on premises. Configure the DataSync agent to perform the online data transfer to an S3 bucket.

Why this is the correct answer:

B. Deploy an AWS DataSync agent on premises. Configure the DataSync agent to perform the online data transfer to an S3 bucket.

-   [ ]   AWS DataSync is designed to efficiently and securely transfer data between on-premises storage and Amazon S3.
-   [ ]   DataSync includes built-in data integrity validation features to ensure data is transferred correctly.

<hr> Why are the other answers wrong? <hr>

-   [ ]   A. Snowball Edge is for large-scale data transfers, not for online data transfer with integrity validation.
-   [ ]   C. S3 File Gateway is for integrating on-premises applications with S3 using file protocols, not for data migration with integrity validation.
-   [ ]   D. S3 Transfer Acceleration speeds up data transfers but does not provide built-in data integrity validation.

Therefore, Option B is the most suitable solution for online data transfer with automatic integrity validation.
</details>

<details>
 <summary>Question 627</summary>

A company wants to migrate two DNS servers to AWS. The servers host a total of approximately 200 zones and receive 1 million requests each day on average. The company wants to maximize availability while minimizing the operational overhead that is related to the management of the two servers. What should a solutions architect recommend to meet these requirements?

- [ ] A. Create 200 new hosted zones in the Amazon Route 53 console Import zone files.
- [ ] B. Launch a single large Amazon EC2 instance Import zone tiles. Configure Amazon CloudWatch alarms and notifications to alert the company about any downtime.
- [ ] C. Migrate the servers to AWS by using AWS Server Migration Service (AWS SMS). Configure Amazon CloudWatch alarms and notifications to alert the company about any downtime.
- [ ] D. Launch an Amazon EC2 instance in an Auto Scaling group across two Availability Zones. Import zone files. Set the desired capacity to 1 and the maximum capacity to 3 for the Auto Scaling group. Configure scaling alarms to scale based on CPU utilization.

</details>

<details>
 <summary>Answer</summary>

- [ ] A. Create 200 new hosted zones in the Amazon Route 53 console Import zone files.

Why these are the correct answers:

A. Create 200 new hosted zones in the Amazon Route 53 console Import zone files.

- [ ] Amazon Route 53 is a highly available and scalable DNS web service.
- [ ] Creating hosted zones in Route 53 and importing zone files simplifies DNS management and reduces operational overhead.
- [ ] This solution allows AWS to handle the DNS infrastructure, providing high availability and scalability.

<hr> Why are the other answers wrong? <hr>

- [ ] B. Launching a single large Amazon EC2 instance increases management overhead and introduces a single point of failure, reducing availability.
- [ ] C. Migrating servers using AWS SMS involves managing EC2 instances, which does not minimize operational overhead compared to using Route 53.
- [ ] D. Using Auto Scaling with EC2 instances for DNS servers is more complex to manage than using Route 53 and does not provide the same level of scalability and availability with less overhead.

Therefore, Option A is the most suitable solution for maximizing availability and minimizing operational overhead.
</details>

<details>
 <summary>Question 628</summary>

A global company runs its applications in multiple AWS accounts in AWS Organizations. The company's applications use multipart uploads to upload data to multiple Amazon S3 buckets across AWS Regions. The company wants to report on incomplete multipart uploads for cost compliance purposes. Which solution will meet these requirements with the LEAST operational overhead?

- [ ] A. Configure AWS Config with a rule to report the incomplete multipart upload object count.
- [ ] B. Create a service control policy (SCP) to report the incomplete multipart upload object count.
- [ ] C. Configure S3 Storage Lens to report the incomplete multipart upload object count.
- [ ] D. Create an S3 Multi-Region Access Point to report the incomplete multipart upload object count.

</details>

<details>
 <summary>Answer</summary>

- [ ] C. Configure S3 Storage Lens to report the incomplete multipart upload object count.

Why these are the correct answers:

C. Configure S3 Storage Lens to report the incomplete multipart upload object count.

- [ ] S3 Storage Lens provides organization-wide visibility into object storage usage and activity, including metrics on incomplete multipart uploads.
- [ ] It offers a centralized dashboard and can automate reporting, minimizing operational overhead.
- [ ] This solution is designed for cost optimization and compliance monitoring of S3 storage.

<hr> Why are the other answers wrong? <hr>

- [ ] A. AWS Config can track configuration changes but is not designed for detailed storage metrics like incomplete multipart uploads, increasing complexity.
- [ ] B. Service Control Policies (SCPs) manage permissions, not storage reporting, and would not directly provide the necessary metrics.
- [ ] D. S3 Multi-Region Access Points simplify data access across regions but do not offer built-in reporting on incomplete multipart uploads.

Therefore, Option C is the most operationally efficient solution for reporting on incomplete multipart uploads.
</details>
<details>
 <summary>Question 629</summary>

A company runs a production database on Amazon RDS for MySQL. The company wants to upgrade the database version for security compliance reasons. Because the database contains critical data, the company wants a quick solution to upgrade and test functionality without losing any data. Which solution will meet these requirements with the LEAST operational overhead?

- [ ] A. Create an RDS manual snapshot. Upgrade to the new version of Amazon RDS for MySQL.
- [ ] B. Use native backup and restore. Restore the data to the upgraded new version of Amazon RDS for MySQL.
- [ ] C. Use AWS Database Migration Service (AWS DMS) to replicate the data to the upgraded new version of Amazon RDS for MySQL.
- [ ] D. Use Amazon RDS Blue/Green Deployments to deploy and test production changes.

</details>

<details>
 <summary>Answer</summary>

- [ ] D. Use Amazon RDS Blue/Green Deployments to deploy and test production changes.

Why these are the correct answers:

D. Use Amazon RDS Blue/Green Deployments to deploy and test production changes.

- [ ] Amazon RDS Blue/Green Deployments create a separate, identical environment for database upgrades.
- [ ] It allows for testing the upgraded environment before switching traffic, minimizing downtime and data loss.
- [ ] This method simplifies the upgrade process and reduces operational overhead.

<hr> Why are the other answers wrong? <hr>

- [ ] A. Creating a manual snapshot and then upgrading involves downtime during the upgrade, and testing is less streamlined.
- [ ] B. Native backup and restore requires manual steps and can result in significant downtime.
- [ ] C. AWS DMS is designed for migrating databases, not for simple version upgrades, and adds complexity.

Therefore, Option D is the most efficient solution for upgrading the database with minimal downtime and operational overhead.
</details>
<details>
 <summary>Question 630</summary>

A solutions architect is creating a data processing job that runs once daily and can take up to 2 hours to complete. If the job is interrupted, it has to restart from the beginning. How should the solutions architect address this issue in the MOST cost-effective manner?

- [ ] A. Create a script that runs locally on an Amazon EC2 Reserved Instance that is triggered by a cron job.
- [ ] B. Create an AWS Lambda function triggered by an Amazon EventBridge scheduled event.
- [ ] C. Use an Amazon Elastic Container Service (Amazon ECS) Fargate task triggered by an Amazon EventBridge scheduled event.
- [ ] D. Use an Amazon Elastic Container Service (Amazon ECS) task running on Amazon EC2 triggered by an Amazon EventBridge scheduled event.

</details>

<details>
 <summary>Answer</summary>

- [ ] C. Use an Amazon Elastic Container Service (Amazon ECS) Fargate task triggered by an Amazon EventBridge scheduled event.

Why these are the correct answers:

C. Use an Amazon Elastic Container Service (Amazon ECS) Fargate task triggered by an Amazon EventBridge scheduled event.

- [ ] Amazon ECS Fargate allows running containers without managing the underlying EC2 instances, reducing operational overhead.
- [ ] EventBridge scheduled events can trigger the Fargate task daily.
- [ ] Fargate is cost-effective for containerized workloads with varying durations.

<hr> Why are the other answers wrong? <hr>

- [ ] A. EC2 Reserved Instances are cost-effective for long-running, predictable workloads, but less so for daily jobs, and require instance management.
- [ ] B. Lambda functions have execution time limits, making them unsuitable for jobs that run for up to 2 hours.
- [ ] D. ECS tasks on EC2 require managing EC2 instances, which adds operational overhead and cost.

Therefore, Option C provides the most cost-effective and efficient solution for running the daily data processing job.
</details>

# AWS-SAA-PRACTICE-EXAM Questions 631-640

<details>
 <summary>Question 631</summary>

A social media company wants to store its database of user profiles, relationships, and interactions in the AWS Cloud. The company needs an application to monitor any changes in the database. The application needs to analyze the relationships between the data entities and to provide recommendations to users. Which solution will meet these requirements with the LEAST operational overhead?

- [ ] A. Use Amazon Neptune to store the information. Use Amazon Kinesis Data Streams to process changes in the database.
- [ ] B. Use Amazon Neptune to store the information. Use Neptune Streams to process changes in the database.
- [ ] C. Use Amazon Quantum Ledger Database (Amazon QLDB) to store the information. Use Amazon Kinesis Data Streams to process changes in the database.
- [ ] D. Use Amazon Quantum Ledger Database (Amazon QLDB) to store the information. Use Neptune Streams to process changes in the database.

</details>

<details>
 <summary>Answer</summary>

- [ ] B. Use Amazon Neptune to store the information. Use Neptune Streams to process changes in the database.

Why these are the correct answers:

B. Use Amazon Neptune to store the information. Use Neptune Streams to process changes in the database.

- [ ] Amazon Neptune is designed for storing and querying graph data, making it suitable for analyzing relationships in social media data.
- [ ] Neptune Streams allows for real-time tracking of changes to the graph, which can be used to monitor database changes and provide recommendations.
- [ ] This combination provides a managed graph database and change tracking, minimizing operational overhead.

<hr> Why are the other answers wrong? <hr>

- [ ] A. While Amazon Neptune is suitable for storing the data, using Kinesis Data Streams adds complexity for processing database changes compared to using Neptune Streams.
- [ ] C and D. Amazon QLDB is a ledger database for transactional data, not optimized for analyzing relationships like Neptune.

Therefore, Option B is the most efficient solution for storing social media data, monitoring changes, and analyzing relationships with the least operational overhead.
</details>
<details>
 <summary>Question 632</summary>

A company is creating a new application that will store a large amount of data. The data will be analyzed hourly and will be modified by several Amazon EC2 Linux instances that are deployed across multiple Availability Zones. The needed amount of storage space will continue to grow for the next 6 months. Which storage solution should a solutions architect recommend to meet these requirements?

- [ ] A. Store the data in Amazon S3 Glacier. Update the S3 Glacier vault policy to allow access to the application instances.
- [ ] B. Store the data in an Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume on the application instances.
- [ ] C. Store the data in an Amazon Elastic File System (Amazon EFS) file system. Mount the file system on the application instances.
- [ ] D. Store the data in an Amazon Elastic Block Store (Amazon EBS) Provisioned IOPS volume shared between the application instances.

</details>

<details>
 <summary>Answer</summary>

- [ ] C. Store the data in an Amazon Elastic File System (Amazon EFS) file system. Mount the file system on the application instances.

Why these are the correct answers:

C. Store the data in an Amazon Elastic File System (Amazon EFS) file system. Mount the file system on the application instances.

- [ ] Amazon EFS provides scalable file storage that can be accessed by multiple EC2 instances concurrently across Availability Zones.
- [ ] EFS automatically grows as data is added, meeting the requirement for increasing storage space.
- [ ] It is suitable for shared access and large amounts of data, making it ideal for this scenario.

<hr> Why are the other answers wrong? <hr>

- [ ] A. Amazon S3 Glacier is for archiving data, not for frequently accessed and modified data.
- [ ] B. Amazon EBS volumes are block storage and can only be attached to a single EC2 instance at a time, making them unsuitable for shared access across multiple instances.
- [ ] D. Provisioned IOPS EBS volumes do not provide shared access across multiple instances in different Availability Zones.

Therefore, Option C is the most appropriate storage solution for shared access, scalability, and concurrent modifications.
</details>
<details>
 <summary>Question 633</summary>

A company manages an application that stores data on an Amazon RDS for PostgreSQL Multi-AZ DB instance. Increases in traffic are causing performance problems. The company determines that database queries are the primary reason for the slow performance. What should a solutions architect do to improve the application's performance?

- [ ] A. Serve read traffic from the Multi-AZ standby replica.
- [ ] B. Configure the DB instance to use Transfer Acceleration.
- [ ] C. Create a read replica from the source DB instance. Serve read traffic from the read replica.
- [ ] D. Use Amazon Kinesis Data Firehose between the application and Amazon RDS to increase the concurrency of database requests.

</details>

<details>
 <summary>Answer</summary>

- [ ] C. Create a read replica from the source DB instance. Serve read traffic from the read replica.

Why these are the correct answers:

C. Create a read replica from the source DB instance. Serve read traffic from the read replica.

- [ ] Read replicas in Amazon RDS allow you to offload read traffic from the primary database instance.
- [ ] This improves performance by distributing the database load, as read operations are handled by the replica.
- [ ] It is a common strategy to scale read-heavy workloads in RDS.

<hr> Why are the other answers wrong? <hr>

- [ ] A. The Multi-AZ standby replica is for failover, not for serving read traffic.
- [ ] B. Transfer Acceleration is for S3, not RDS.
- [ ] D. Kinesis Data Firehose is for streaming data, not for improving RDS query performance.

Therefore, Option C is the most effective solution to improve performance by scaling read operations.
</details>
<details>
 <summary>Question 634</summary>

A company collects 10 GB of telemetry data daily from various machines. The company stores the data in an Amazon S3 bucket in a source data account. The company has hired several consulting agencies to use this data for analysis. Each agency needs read access to the data for its analysts. The company must share the data from the source data account by choosing a solution that maximizes security and operational efficiency. Which solution will meet these requirements?

- [ ] A. Configure S3 global tables to replicate data for each agency.
- [ ] B. Make the S3 bucket public for a limited time. Inform only the agencies.
- [ ] C. Configure cross-account access for the S3 bucket to the accounts that the agencies own.
- [ ] D. Set up an IAM user for each analyst in the source data account. Grant each user access to the S3 bucket.

</details>

<details>
 <summary>Answer</summary>

- [ ] C. Configure cross-account access for the S3 bucket to the accounts that the agencies own.

Why these are the correct answers:

C. Configure cross-account access for the S3 bucket to the accounts that the agencies own.

- [ ] Cross-account access allows you to grant specific permissions to other AWS accounts to access your S3 bucket.
- [ ] This is a secure and efficient way to share data without needing to manage individual credentials for each analyst.
- [ ] It provides centralized control and simplifies access management.

<hr> Why are the other answers wrong? <hr>

- [ ] A. S3 Global Tables are for multi-region replication, not for sharing data with external accounts.
- [ ] B. Making the bucket public is insecure and not recommended for sharing sensitive data.
- [ ] D. Creating IAM users for each analyst is operationally inefficient and harder to manage.

Therefore, Option C is the most secure and efficient solution for sharing the S3 data with multiple agencies.
</details>
<details>
 <summary>Question 635</summary>

A company uses Amazon FSx for NetApp ONTAP in its primary AWS Region for CIFS and NFS file shares. Applications that run on Amazon EC2 instances access the file shares. The company needs a storage disaster recovery (DR) solution in a secondary Region. The data that is replicated in the secondary Region needs to be accessed by using the same protocols as the primary Region. Which solution will meet these requirements with the LEAST operational overhead?

- [ ] A. Create an AWS Lambda function to copy the data to an Amazon S3 bucket. Replicate the S3 bucket to the secondary Region.
- [ ] B. Create a backup of the FSx for ONTAP volumes by using AWS Backup. Copy the volumes to the secondary Region. Create a new FSx for ONTAP instance from the backup.
- [ ] C. Create an FSx for ONTAP instance in the secondary Region. Use NetApp SnapMirror to replicate data from the primary Region to the secondary Region.
- [ ] D. Create an Amazon Elastic File System (Amazon EFS) volume. Migrate the current data to the volume. Replicate the volume to the secondary Region.

</details>

<details>
 <summary>Answer</summary>

- [ ] C. Create an FSx for ONTAP instance in the secondary Region. Use NetApp SnapMirror to replicate data from the primary Region to the secondary Region.

Why these are the correct answers:

C. Create an FSx for ONTAP instance in the secondary Region. Use NetApp SnapMirror to replicate data from the primary Region to the secondary Region.

- [ ] Amazon FSx for NetApp ONTAP supports NetApp SnapMirror for efficient replication of data between FSx for ONTAP file systems.
- [ ] This solution provides a native and efficient way to replicate data for DR, maintaining protocol compatibility.
- [ ] It minimizes operational overhead by using built-in replication tools.

<hr> Why are the other answers wrong? <hr>

- [ ] A. Using Lambda and S3 adds complexity and does not preserve the original file protocols (CIFS and NFS).
- [ ] B. Backing up and restoring volumes involves more manual steps and operational overhead.
- [ ] D. Amazon EFS does not support CIFS and requires migrating data, which increases complexity.

Therefore, Option C is the most efficient solution for setting up a DR site for FSx for ONTAP with minimal overhead.
</details>

<details>
 <summary>Question 636</summary>

A development team is creating an event-based application that uses AWS Lambda functions. Events will be generated when files are added to an Amazon S3 bucket. The development team currently has Amazon Simple Notification Service (Amazon SNS) configured as the event target from Amazon S3. What should a solutions architect do to process the events from Amazon S3 in a scalable way?

- [ ] A. Create an SNS subscription that processes the event in Amazon Elastic Container Service (Amazon ECS) before the event runs in Lambda.
- [ ] B. Create an SNS subscription that processes the event in Amazon Elastic Kubernetes Service (Amazon EKS) before the event runs in Lambda
- [ ] C. Create an SNS subscription that sends the event to Amazon Simple Queue Service (Amazon SQS). Configure the SOS queue to trigger a Lambda function.
- [ ] D. Create an SNS subscription that sends the event to AWS Server Migration Service (AWS SMS). Configure the Lambda function to poll from the SMS event.

</details>

<details>
 <summary>Answer</summary>

-   [ ] C. Create an SNS subscription that sends the event to Amazon Simple Queue Service (Amazon SQS). Configure the SOS queue to trigger a Lambda function.

Why these are the correct answers:

C. Create an SNS subscription that sends the event to Amazon Simple Queue Service (Amazon SQS). Configure the SOS queue to trigger a Lambda function.

-   [ ] Using SQS to decouple SNS from Lambda provides a scalable and reliable way to handle events.
-   [ ] SQS can buffer events, preventing Lambda from being overwhelmed during spikes.
-   [ ] This approach allows for asynchronous processing and better error handling.

<hr> Why are the other answers wrong? <hr>

-   [ ] A and B. Processing events in ECS or EKS before Lambda adds unnecessary complexity and cost.
-   [ ] D. AWS SMS is for migrating servers, not for event-driven processing.

Therefore, Option C is the most scalable and efficient solution for processing events from S3.
</details>
<details>
 <summary>Question 637</summary>

A solutions architect is designing a new service behind Amazon API Gateway. The request patterns for the service will be unpredictable and can change suddenly from 0 requests to over 500 per second. The total size of the data that needs to be persisted in a backend database is currently less than 1 GB with unpredictable future growth. Data can be queried using simple key-value requests. Which combination of AWS services would meet these requirements? (Choose two.)

-   [ ] A. AWS Fargate
-   [ ] B. AWS Lambda
-   [ ] C. Amazon DynamoDB
-   [ ] D. Amazon EC2 Auto Scaling
-   [ ] E. MySQL-compatible Amazon Aurora

</details>

<details>
 <summary>Answer</summary>

-   [ ] B. AWS Lambda
-   [ ] C. Amazon DynamoDB

Why these are the correct answers:

B. AWS Lambda

-   [ ] Lambda scales automatically in response to request volume, making it suitable for unpredictable traffic.
-   [ ] It is serverless, reducing operational overhead.

C. Amazon DynamoDB

-   [ ] DynamoDB is a NoSQL database that can handle large amounts of data and scale automatically.
-   [ ] It supports key-value lookups with high performance.

<hr> Why are the other answers wrong? <hr>

-   [ ] A and D. Fargate and EC2 Auto Scaling are for containerized applications and require more management than Lambda.
-   [ ] E. Aurora is a relational database, less suitable for key-value lookups and more complex to scale than DynamoDB.

Therefore, Lambda and DynamoDB are the most suitable services for handling unpredictable traffic and scalable key-value data storage.
</details>
<details>
 <summary>Question 638</summary>

A company collects and shares research data with the company's employees all over the world. The company wants to collect and store the data in an Amazon S3 bucket and process the data in the AWS Cloud. The company will share the data with the company's employees. The company needs a secure solution in the AWS Cloud that minimizes operational overhead. Which solution will meet these requirements?

-   [ ] A. Use an AWS Lambda function to create an S3 presigned URL. Instruct employees to use the URL.
-   [ ] B. Create an IAM user for each employee. Create an IAM policy for each employee to allow S3 access. Instruct employees to use the AWS Management Console.
-   [ ] C. Create an S3 File Gateway. Create a share for uploading and a share for downloading. Allow employees to mount shares on their local computers to use S3 File Gateway.
-   [ ] D. Configure AWS Transfer Family SFTP endpoints. Select the custom identity provider options. Use AWS Secrets Manager to manage the user credentials Instruct employees to use Transfer Family.

</details>

<details>
 <summary>Answer</summary>

-   [ ] A. Use an AWS Lambda function to create an S3 presigned URL. Instruct employees to use the URL.

Why these are the correct answers:

A. Use an AWS Lambda function to create an S3 presigned URL. Instruct employees to use the URL.

-   [ ] Presigned URLs provide secure, temporary access to S3 objects without requiring IAM credentials.
-   [ ] Lambda can automate the creation of these URLs, minimizing operational overhead.
-   [ ] This method is efficient and secure for sharing data with employees.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. Creating IAM users for each employee is complex and inefficient.
-   [ ] C. S3 File Gateway is for on-premises access to S3, not for sharing data with remote employees.
-   [ ] D. AWS Transfer Family is for secure file transfers, not ideal for simple data sharing within the company.

Therefore, Option A is the most secure and operationally efficient solution.
</details>
<details>
 <summary>Question 639</summary>

A company is building a new furniture inventory application. The company has deployed the application on a fleet of Amazon EC2 instances across multiple Availability Zones. The EC2 instances run behind an Application Load Balancer (ALB) in their VPC. A solutions architect has observed that incoming traffic seems to favor one EC2 instance, resulting in latency for some requests. What should the solutions architect do to resolve this issue?

-   [ ] A. Disable session affinity (sticky sessions) on the ALB
-   [ ] B. Replace the ALB with a Network Load Balancer
-   [ ] C. Increase the number of EC2 instances in each Availability Zone
-   [ ] D. Adjust the frequency of the health checks on the ALB's target group

</details>

<details>
 <summary>Answer</summary>

-   [ ] A. Disable session affinity (sticky sessions) on the ALB

Why these are the correct answers:

A. Disable session affinity (sticky sessions) on the ALB

-   [ ] Session affinity (sticky sessions) can cause traffic to be unevenly distributed if some users have longer sessions.
-   [ ] Disabling it allows the ALB to distribute traffic more evenly based on load.
-   [ ] This ensures that traffic is distributed to all instances, reducing latency.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. Replacing the ALB with an NLB is unnecessary; the ALB can handle HTTP traffic efficiently.
-   [ ] C. Increasing the number of EC2 instances might help but does not address the root cause of uneven distribution.
-   [ ] D. Adjusting health checks does not solve the traffic distribution problem.

Therefore, Option A is the most appropriate solution to ensure even traffic distribution.
</details>
<details>
 <summary>Question 640</summary>

A company has an application workflow that uses an AWS Lambda function to download and decrypt files from Amazon S3. These files are encrypted using AWS Key Management Service (AWS KMS) keys. A solutions architect needs to design a solution that will ensure the required permissions are set correctly. Which combination of actions accomplish this? (Choose two.)

-   [ ] A. Attach the kms:decrypt permission to the Lambda function's resource policy
-   [ ] B. Grant the decrypt permission for the Lambda IAM role in the KMS key's policy
-   [ ] C. Grant the decrypt permission for the Lambda resource policy in the KMS key's policy.
-   [ ] D. Create a new IAM policy with the kms:decrypt permission and attach the policy to the Lambda function.
-   [ ] E. Create a new IAM role with the kms:decrypt permission and attach the execution role to the Lambda function.

</details>

<details>
 <summary>Answer</summary>

-   [ ] B. Grant the decrypt permission for the Lambda IAM role in the KMS key's policy
-   [ ] E. Create a new IAM role with the kms:decrypt permission and attach the execution role to the Lambda function.

Why these are the correct answers:

B. Grant the decrypt permission for the Lambda IAM role in the KMS key's policy

-   [ ] The KMS key policy must allow the Lambda function's IAM role to use the key for decryption.
-   [ ] This gives the Lambda function permission to decrypt the files.

E. Create a new IAM role with the kms:decrypt permission and attach the execution role to the Lambda function.

-   [ ] Lambda functions assume an IAM role that grants them permissions to access AWS resources.
-   [ ] This role needs the `kms:decrypt` permission to allow the function to decrypt files.

<hr> Why are the other answers wrong? <hr>

-   [ ] A and C. Resource policies are not the primary way to grant KMS permissions to Lambda functions.
-   [ ] D. Creating and attaching an IAM policy to the Lambda function is correct, but Option E is more precise about using an IAM *role*.

Therefore, Options B and E are the correct combination to ensure the Lambda function can decrypt files from S3 using KMS.
</details>

# AWS-SAA-PRACTICE-EXAM Questions 641-650

<details>
 <summary>Question 641</summary>

A company wants to monitor its AWS costs for financial review. The cloud operations team is designing an architecture in the AWS Organizations management account to query AWS Cost and Usage Reports for all member accounts. The team must run this query once a month and provide a detailed analysis of the bill. Which solution is the MOST scalable and cost-effective way to meet these requirements?

-   [ ] A. Enable Cost and Usage Reports in the management account. Deliver reports to Amazon Kinesis. Use Amazon EMR for analysis.
-   [ ] B. Enable Cost and Usage Reports in the management account. Deliver the reports to Amazon S3 Use Amazon Athena for analysis.
-   [ ] C. Enable Cost and Usage Reports for member accounts. Deliver the reports to Amazon S3 Use Amazon Redshift for analysis.
-   [ ] D. Enable Cost and Usage Reports for member accounts. Deliver the reports to Amazon Kinesis. Use Amazon QuickSight tor analysis.

</details>

<details>
 <summary>Answer</summary>

-   [ ] B. Enable Cost and Usage Reports in the management account. Deliver the reports to Amazon S3 Use Amazon Athena for analysis.

Why these are the correct answers:

B. Enable Cost and Usage Reports in the management account. Deliver the reports to Amazon S3 Use Amazon Athena for analysis.

-   [ ] AWS Cost and Usage Reports provide detailed billing data.
-   [ ] Storing them in S3 is cost-effective.
-   [ ] Amazon Athena allows querying the data with SQL, which is scalable and cost-effective for monthly analysis.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Amazon Kinesis and EMR are more complex and expensive for simple monthly analysis.
-   [ ] C. Redshift is a powerful data warehouse but is overkill for monthly reporting and less cost-effective.
-   [ ] D. Kinesis and QuickSight are not as efficient for detailed, ad-hoc analysis of Cost and Usage Reports.

Therefore, Option B is the most scalable and cost-effective solution.
</details>
<details>
 <summary>Question 642</summary>

A company wants to run a gaming application on Amazon EC2 instances that are part of an Auto Scaling group in the AWS Cloud. The application will transmit data by using UDP packets. The company wants to ensure that the application can scale out and in as traffic increases and decreases. What should a solutions architect do to meet these requirements?

-   [ ] A. Attach a Network Load Balancer to the Auto Scaling group.
-   [ ] B. Attach an Application Load Balancer to the Auto Scaling group.
-   [ ] C. Deploy an Amazon Route 53 record set with a weighted policy to route traffic appropriately.
-   [ ] D. Deploy a NAT instance that is configured with port forwarding to the EC2 instances in the Auto Scaling group.

</details>

<details>
 <summary>Answer</summary>

-   [ ] A. Attach a Network Load Balancer to the Auto Scaling group.

Why these are the correct answers:

A. Attach a Network Load Balancer to the Auto Scaling group.

-   [ ] Network Load Balancers (NLBs) are designed to handle UDP traffic and provide high performance.
-   [ ] NLBs can scale with the Auto Scaling group, distributing UDP traffic efficiently.
-   [ ] This ensures the application can handle traffic fluctuations.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. Application Load Balancers (ALBs) only support HTTP and HTTPS, not UDP.
-   [ ] C. Route 53 with a weighted policy is for DNS-based routing, not for load balancing application traffic.
-   [ ] D. NAT instances are for enabling instances in a private subnet to connect to the internet, not for load balancing.

Therefore, Option A is the correct solution for load balancing UDP traffic.
</details>
<details>
 <summary>Question 643</summary>

A company runs several websites on AWS for its different brands. Each website generates tens of gigabytes of web traffic logs each day. A solutions architect needs to design a scalable solution to give the company's developers the ability to analyze traffic patterns across all the company's websites. This analysis by the developers will occur on demand once a week over the course of several months. The solution must support queries with standard SQL. Which solution will meet these requirements MOST cost-effectively?

-   [ ] A. Store the logs in Amazon S3. Use Amazon Athena tor analysis.
-   [ ] B. Store the logs in Amazon RDS. Use a database client for analysis.
-   [ ] C. Store the logs in Amazon OpenSearch Service. Use OpenSearch Service for analysis.
-   [ ] D. Store the logs in an Amazon EMR cluster Use a supported open-source framework for SQL-based analysis.

</details>

<details>
 <summary>Answer</summary>

-   [ ] A. Store the logs in Amazon S3. Use Amazon Athena tor analysis.

Why these are the correct answers:

A. Store the logs in Amazon S3. Use Amazon Athena tor analysis.

-   [ ] Amazon S3 provides cost-effective storage for large volumes of log data.
-   [ ] Amazon Athena allows querying S3 data using standard SQL, which meets the requirements.
-   [ ] This combination is scalable and cost-effective for on-demand analysis.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. Amazon RDS is not designed for storing large volumes of log data and is more expensive.
-   [ ] C. Amazon OpenSearch Service is suitable for log analysis but is more expensive than Athena for infrequent queries.
-   [ ] D. Amazon EMR is powerful but is more complex and expensive for simple SQL queries.

Therefore, Option A is the most cost-effective solution for scalable log analysis.
</details>
<details>
 <summary>Question 644</summary>

An international company has a subdomain for each country that the company operates in. The subdomains are formatted as example.com, country1.example.com, and country2.example.com. The company's workloads are behind an Application Load Balancer. The company wants to encrypt the website data that is in transit. Which combination of steps will meet these requirements? (Choose two.)

-   [ ] A. Use the AWS Certificate Manager (ACM) console to request a public certificate for the apex top domain example com and a wildcard certificate for *.example.com.
-   [ ] B. Use the AWS Certificate Manager (ACM) console to request a private certificate for the apex top domain example.com and a wildcard certificate for *.example.com.
-   [ ] C. Use the AWS Certificate Manager (ACM) console to request a public and private certificate for the apex top domain example.com.
-   [ ] D. Validate domain ownership by email address. Switch to DNS validation by adding the required DNS records to the DNS provider.
-   [ ] E. Validate domain ownership for the domain by adding the required DNS records to the DNS provider.

</details>

<details>
 <summary>Answer</summary>

-   [ ] A. Use the AWS Certificate Manager (ACM) console to request a public certificate for the apex top domain example com and a wildcard certificate for *.example.com.
-   [ ] E. Validate domain ownership for the domain by adding the required DNS records to the DNS provider.

Why these are the correct answers:

A. Use the AWS Certificate Manager (ACM) console to request a public certificate for the apex top domain example com and a wildcard certificate for *.example.com.

-   [ ] A public certificate is required for encrypting traffic over the internet.
-   [ ] A wildcard certificate (*.example.com) covers all subdomains.

E. Validate domain ownership for the domain by adding the required DNS records to the DNS provider.

-   [ ] DNS validation is a recommended method to prove domain ownership for ACM certificates.
-   [ ] It allows ACM to automatically renew certificates.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. Private certificates are for internal use, not for public websites.
-   [ ] C. Requesting both public and private certificates for the apex domain is unnecessary.
-   [ ] D. Email validation is less reliable and does not support automatic renewal like DNS validation.

Therefore, Options A and E are the correct steps for encrypting website data for multiple subdomains.
</details>
<details>
 <summary>Question 645</summary>

A company is required to use cryptographic keys in its on-premises key manager. The key manager is outside of the AWS Cloud because of regulatory and compliance requirements. The company wants to manage encryption and decryption by using cryptographic keys that are retained outside of the AWS Cloud and that support a variety of external key managers from different vendors. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Use AWS CloudHSM key store backed by a CloudHSM cluster.
-   [ ] B. Use an AWS Key Management Service (AWS KMS) external key store backed by an external key manager.
-   [ ] C. Use the default AWS Key Management Service (AWS KMS) managed key store.
-   [ ] D. Use a custom key store backed by an AWS CloudHSM cluster.

</details>

<details>
 <summary>Answer</summary>

-   [ ] B. Use an AWS Key Management Service (AWS KMS) external key store backed by an external key manager.

Why these are the correct answers:

B. Use an AWS Key Management Service (AWS KMS) external key store backed by an external key manager.

-   [ ] KMS external key stores allow you to use cryptographic keys stored in your own external key manager.
-   [ ] This meets the requirement to keep keys outside the AWS Cloud.
-   [ ] It provides a managed service with less operational overhead.

<hr> Why are the other answers wrong? <hr>

-   [ ] A and D. CloudHSM involves managing dedicated hardware security modules, which increases operational overhead.
-   [ ] C. The default KMS managed key store does not allow using external key managers.

Therefore, Option B is the most suitable solution for using external keys with the least operational overhead.
</details>
<details>
 <summary>Question 646</summary>

A solutions architect needs to host a high performance computing (HPC) workload in the AWS Cloud. The workload will run on hundreds of Amazon EC2 instances and will require parallel access to a shared file system to enable distributed processing of large datasets. Datasets will be accessed across multiple instances simultaneously. The workload requires access latency within 1 ms. After processing has completed, engineers will need access to the dataset for manual postprocessing. Which solution will meet these requirements?

-   [ ] A. Use Amazon Elastic File System (Amazon EFS) as a shared file system. Access the dataset from Amazon EFS.
-   [ ] B. Mount an Amazon S3 bucket to serve as the shared file system. Perform postprocessing directly from the S3 bucket.
-   [ ] C. Use Amazon FSx for Lustre as a shared file system. Link the file system to an Amazon S3 bucket for postprocessing.
-   [ ] D. Configure AWS Resource Access Manager to share an Amazon S3 bucket so that it can be mounted to all instances for processing and postprocessing.

</details>

<details>
 <summary>Answer</summary>

-   [ ] C. Use Amazon FSx for Lustre as a shared file system. Link the file system to an Amazon S3 bucket for postprocessing.

Why these are the correct answers:

C. Use Amazon FSx for Lustre as a shared file system. Link the file system to an Amazon S3 bucket for postprocessing.

-   [ ] Amazon FSx for Lustre is designed for high-performance computing and provides the required low latency.
-   [ ] It supports parallel access and is suitable for large datasets.
-   [ ] Linking it to S3 allows for cost-effective storage for postprocessing.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Amazon EFS does not provide the sub-millisecond latency required for HPC workloads.
-   [ ] B. S3 is object storage and does not provide the file system interface or low latency needed for HPC.
-   [ ] D. Resource Access Manager is for sharing AWS resources, not for providing a high-performance file system.

Therefore, Option C is the most suitable solution for HPC workloads requiring low latency and parallel access.
</details>
<details>
 <summary>Question 647</summary>

A gaming company is building an application with Voice over IP capabilities. The application will serve traffic to users across the world. The application needs to be highly available with an automated failover across AWS Regions. The company wants to minimize the latency of users without relying on IP address caching on user devices. What should a solutions architect do to meet these requirements?

-   [ ] A. Use AWS Global Accelerator with health checks.
-   [ ] B. Use Amazon Route 53 with a geolocation routing policy.
-   [ ] C. Create an Amazon CloudFront distribution that includes multiple origins.
-   [ ] D. Create an Application Load Balancer that uses path-based routing.

</details>

<details>
 <summary>Answer</summary>

-   [ ] A. Use AWS Global Accelerator with health checks.

Why these are the correct answers:

A. Use AWS Global Accelerator with health checks.

-   [ ] AWS Global Accelerator minimizes latency by using the AWS global network.
-   [ ] It provides static IP addresses for fast connections and supports automatic failover across AWS Regions.
-   [ ] Health checks ensure high availability.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. Route 53 geolocation routing routes traffic based on user location but does not inherently minimize latency like Global Accelerator.
-   [ ] C. CloudFront is for caching content, not for optimizing real-time traffic like VoIP.
-   [ ] D. Application Load Balancers distribute traffic within a region, not globally.

Therefore, Option A is the most suitable solution for minimizing latency and ensuring high availability for a global VoIP application.
</details>
<details>
 <summary>Question 648</summary>

A weather forecasting company needs to process hundreds of gigabytes of data with sub-millisecond latency. The company has a high performance computing (HPC) environment in its data center and wants to expand its forecasting capabilities. A solutions architect must identify a highly available cloud storage solution that can handle large amounts of sustained throughput. Files that are stored in the solution should be accessible to thousands of compute instances that will simultaneously access and process the entire dataset. What should the solutions architect do to meet these requirements?

-   [ ] A. Use Amazon FSx for Lustre scratch file systems.
-   [ ] B. Use Amazon FSx for Lustre persistent file systems.
-   [ ] C. Use Amazon Elastic File System (Amazon EFS) with Bursting Throughput mode.
-   [ ] D. Use Amazon Elastic File System (Amazon EFS) with Provisioned Throughput mode.

</details>

<details>
 <summary>Answer</summary>

-   [ ] B. Use Amazon FSx for Lustre persistent file systems.

Why these are the correct answers:

B. Use Amazon FSx for Lustre persistent file systems.

-   [ ] Amazon FSx for Lustre is designed for high-performance computing and provides sub-millisecond latency.
-   [ ] Persistent file systems are suitable for workloads that require data to be available for longer durations.
-   [ ] It can handle the throughput and concurrent access from thousands of instances.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. Scratch file systems are for temporary storage, not for persistent data needed for analysis.
-   [ ] C and D. Amazon EFS does not provide the low latency and high throughput required for HPC workloads.

Therefore, Option B is the most suitable storage solution for the HPC environment.
</details>
<details>
 <summary>Question 649</summary>

An ecommerce company runs a PostgreSQL database on premises. The database stores data by using high IOPS Amazon Elastic Block Store (Amazon EBS) block storage. The daily peak 1/0 transactions per second do not exceed 15,000 IOPS. The company wants to migrate the database to Amazon RDS for PostgreSQL and provision disk IOPS performance independent of disk storage capacity. Which solution will meet these requirements MOST cost-effectively?

-   [ ] A. Configure the General Purpose SSD (gp2) EBS volume storage type and provision 15,000 IOPS.
-   [ ] B. Configure the Provisioned IOPS SSD (io1) EBS volume storage type and provision 15,000 IOPS.
-   [ ] C. Configure the General Purpose SSD (gp3) EBS volume storage type and provision 15,000 IOPS.
-   [ ] D. Configure the EBS magnetic volume type to achieve maximum IOPS.

</details>

<details>
 <summary>Answer</summary>

-   [ ] C. Configure the General Purpose SSD (gp3) EBS volume storage type and provision 15,000 IOPS.

Why these are the correct answers:

C. Configure the General Purpose SSD (gp3) EBS volume storage type and provision 15,000 IOPS.

-   [ ] gp3 volumes allow you to provision IOPS independently of storage capacity, which meets the requirement.
-   [ ] gp3 provides a balance of performance and cost-effectiveness for this IOPS range.

<hr> Why are the other answers wrong? <hr>

-   [ ] A. gp2 volumes tie IOPS to storage capacity, which is less flexible and potentially more expensive.
-   [ ] B. io1 volumes are for the highest performance and are more expensive than gp3 for this IOPS range.
-   [ ] D. Magnetic volumes are not suitable for high IOPS workloads.

Therefore, Option C is the most cost-effective solution for provisioning the required IOPS.
</details>

<details>
    <summary>Question 650</summary>

A company wants to migrate its on-premises Microsoft SQL Server Enterprise edition database to AWS. The company's online application uses the database to process transactions. The data analysis team uses the same production database to run reports for analytical processing. The company wants to reduce operational overhead by moving to managed services wherever possible. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Migrate to Amazon RDS for Microsoft SOL Server.
    Use read replicas for reporting purposes
-   [ ] B. Migrate to Microsoft SQL Server on Amazon EC2.
    Use Always On read replicas for reporting purposes
-   [ ] C. Migrate to Amazon DynamoDB.
    Use DynamoDB on-demand replicas for reporting purposes
-   [ ] D. Migrate to Amazon Aurora MySQL.
    Use Aurora read replicas for reporting purposes

</details>

<details>
    <summary>Answer</summary>

-   [ ] A. Migrate to Amazon RDS for Microsoft SOL Server.
    Use read replicas for reporting purposes

Why these are the correct answers:

A. Migrate to Amazon RDS for Microsoft SOL Server. Use read replicas for reporting purposes

-   [ ] Amazon RDS is a managed database service that reduces operational overhead.
-   [ ] RDS for Microsoft SQL Server allows you to continue using SQL Server.
-   [ ] Read replicas in RDS can offload reporting workloads from the primary database.

<hr> Why are the other answers wrong? <hr>

-   [ ] B. Microsoft SQL Server on Amazon EC2 requires managing the server infrastructure, which increases operational overhead.
-   [ ] C. Amazon DynamoDB is a NoSQL database, and migrating a SQL Server database to DynamoDB would require significant application changes.
-   [ ] D. Amazon Aurora MySQL is a MySQL-compatible database, and migrating a SQL Server database to Aurora would also require significant application changes.

Therefore, Option A is the most suitable solution as it provides a managed service with minimal changes to the application.
</details>











































