# AWS-SAA-PRACTICE-EXAM Questions 701-710

<details>
    <summary>Question 701</summary>

A city has deployed a web application running on Amazon EC2 instances behind an Application Load Balancer (ALB). The application's users have reported sporadic performance, which appears to be related to DDoS attacks originating from random IP addresses. The city needs a solution that requires minimal configuration changes and provides an audit trail for the DDoS sources. Which solution meets these requirements?

-   [ ] A. Enable an AWS WAF web ACL on the ALB, and configure rules to block traffic from unknown sources.
-   [ ] B. Subscribe to Amazon Inspector.
    Engage the AWS DDoS Response Team (DRT) to integrate mitigating controls into the service.
-   [ ] C. Subscribe to AWS Shield Advanced.
    Engage the AWS DDoS Response Team (DRT) to integrate mitigating controls into the service.
-   [ ] D. Create an Amazon CloudFront distribution for the application, and set the ALB as the origin.
    Enable an AWS WAF web ACL on the distribution, and configure rules to block traffic from unknown sources

</details>

<details>
    <summary>Answer</summary>

-   [ ] C. Subscribe to AWS Shield Advanced.
    Engage the AWS DDoS Response Team (DRT) to integrate mitigating controls into the service.

Why these are the correct answers:

C. Subscribe to AWS Shield Advanced. Engage the AWS DDoS Response Team (DRT) to integrate mitigating controls into the service.

-   [ ] AWS Shield Advanced provides enhanced DDoS protection.
-   [ ] It includes the DDoS Response Team (DRT) for expert support.
-   [ ] Shield Advanced provides detailed attack reporting.
-   [ ] This solution minimizes configuration changes.

Why are the other answers wrong?

-   [ ] A. AWS WAF can block traffic but does not provide the same level of automated DDoS protection or DRT support as Shield Advanced.
-   [ ] B. Amazon Inspector is for vulnerability management, not DDoS protection.
-   [ ] D. CloudFront can help with some DDoS attacks, but Shield Advanced offers more comprehensive protection and DRT support.

Therefore, Option C is the most suitable solution for DDoS protection and audit trails.
</details>
<details>
    <summary>Question 702</summary>

A company copies 200 TB of data from a recent ocean survey onto AWS Snowball Edge Storage Optimized devices. The company has a high performance computing (HPC) cluster that is hosted on AWS to look for oil and gas deposits. A solutions architect must provide the cluster with consistent sub-millisecond latency and high-throughput access to the data on the Snowball Edge Storage Optimized devices. The company is sending the devices back to AWS. Which solution will meet these requirements?

-   [ ] A. Create an Amazon S3 bucket.
    Import the data into the S3 bucket.
    Configure an AWS Storage Gateway file gateway to use the S3 bucket.
    Access the file gateway from the HPC cluster instances.
-   [ ] B. Create an Amazon S3 bucket.
    Import the data into the S3 bucket.
    Configure an Amazon FSx for Lustre file system, and integrate it with the S3 bucket.
    Access the FSx for Lustre file system from the HPC cluster instances.
-   [ ] C. Create an Amazon S3 bucket and an Amazon Elastic File System (Amazon EFS) file system.
    Import the data into the S3 bucket.
    Copy the data from the S3 bucket to the EFS file system.
    Access the EFS file system from the HPC cluster instances.
-   [ ] D. Create an Amazon FSx for Lustre file system.
    Import the data directly into the FSx for Lustre file system.
    Access the FSx for Lustre file system from the HPC cluster instances.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Create an Amazon S3 bucket.
    Import the data into the S3 bucket.
    Configure an Amazon FSx for Lustre file system, and integrate it with the S3 bucket.
    Access the FSx for Lustre file system from the HPC cluster instances.

Why these are the correct answers:

B. Create an Amazon S3 bucket. Import the data into the S3 bucket. Configure an Amazon FSx for Lustre file system, and integrate it with the S3 bucket. Access the FSx for Lustre file system from the HPC cluster instances.

-   [ ] Amazon FSx for Lustre provides high-performance file storage with sub-millisecond latency.
-   [ ] Integrating FSx for Lustre with S3 allows for efficient access to the data.
-   [ ] This solution is suitable for HPC workloads.

Why are the other answers wrong?

-   [ ] A. Storage Gateway is not designed for HPC workloads requiring sub-millisecond latency.
-   [ ] C. Amazon EFS does not provide the required low latency for HPC.
-   [ ] D. Importing data directly into FSx for Lustre from Snowball Edge is not a standard or efficient workflow.

Therefore, Option B is the most appropriate solution for HPC data access.
</details>
<details>
    <summary>Question 703</summary>

A company has NFS servers in an on-premises data center that need to periodically back up small amounts of data to Amazon S3. Which solution meets these requirements and is MOST cost-effective?

-   [ ] A. Set up AWS Glue to copy the data from the on-premises servers to Amazon S3.
-   [ ] B. Set up an AWS DataSync agent on the on-premises servers, and sync the data to Amazon S3.
-   [ ] C. Set up an SFTP sync using AWS Transfer for SFTP to sync data from on premises to Amazon S3.
-   [ ] D. Set up an AWS Direct Connect connection between the on-premises data center and a VPC, and copy the data to Amazon S3.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Set up an AWS DataSync agent on the on-premises servers, and sync the data to Amazon S3.

Why these are the correct answers:

B. Set up an AWS DataSync agent on the on-premises servers, and sync the data to Amazon S3.

-   [ ] AWS DataSync is designed for efficient and automated data transfer between on-premises storage and Amazon S3.
-   [ ] It is cost-effective for small amounts of data.
-   [ ] DataSync handles the complexities of network optimization and data integrity.

Why are the other answers wrong?

-   [ ] A. AWS Glue is for ETL (Extract, Transform, Load) operations, not simple data backup.
-   [ ] C. AWS Transfer for SFTP is for secure file transfers, not optimized for backup.
-   [ ] D. AWS Direct Connect is expensive and unnecessary for small, periodic backups.

Therefore, Option B is the most cost-effective solution for backing up data to S3.
</details>
<details>
    <summary>Question 704</summary>

An online video game company must maintain ultra-low latency for its game servers. The game servers run on Amazon EC2 instances. The company needs a solution that can handle millions of UDP internet traffic requests each second. Which solution will meet these requirements MOST cost-effectively?

-   [ ] A. Configure an Application Load Balancer with the required protocol and ports for the internet traffic.
    Specify the EC2 instances as the targets.
-   [ ] B. Configure a Gateway Load Balancer for the internet traffic.
    Specify the EC2 instances as the targets.
-   [ ] C. Configure a Network Load Balancer with the required protocol and ports for the internet traffic.
    Specify the EC2 instances as the targets.
-   [ ] D. Launch an identical set of game servers on EC2 instances in separate AWS Regions.
    Route internet traffic to both sets of EC2 instances.

</details>

<details>
    <summary>Answer</summary>

-   [ ] C. Configure a Network Load Balancer with the required protocol and ports for the internet traffic.
    Specify the EC2 instances as the targets.

Why these are the correct answers:

C. Configure a Network Load Balancer with the required protocol and ports for the internet traffic. Specify the EC2 instances as the targets.

-   [ ] Network Load Balancers (NLBs) are designed to handle high-throughput, low-latency traffic.
-   [ ] NLBs support UDP traffic.
-   [ ] They are cost-effective for handling millions of requests per second.

Why are the other answers wrong?

-   [ ] A. Application Load Balancers (ALBs) do not support UDP.
-   [ ] B. Gateway Load Balancers are for third-party virtual appliances.
-   [ ] D. Launching game servers in multiple Regions is more complex and expensive.

Therefore, Option C is the most cost-effective solution for handling UDP traffic with low latency.
</details>
<details>
    <summary>Question 705</summary>

A company runs a three-tier application in a VPC. The database tier uses an Amazon RDS for MySQL DB instance. The company plans to migrate the RDS for MySQL DB instance to an Amazon Aurora PostgreSQL DB cluster. The company needs a solution that replicates the data changes that happen during the migration to the new database. Which combination of steps will meet these requirements? (Choose two.)

-   [ ] A. Use AWS Database Migration Service (AWS DMS) Schema Conversion to transform the database objects.
-   [ ] B. Use AWS Database Migration Service (AWS DMS) Schema Conversion to create an Aurora PostgreSQL read replica on the RDS for MySQL DB instance.
-   [ ] C. Configure an Aurora MySQL read replica for the RDS for MySQL DB instance.
-   [ ] D. Define an AWS Database Migration Service (AWS DMS) task with change data capture (CDC) to migrate the data.
-   [ ] E. Promote the Aurora PostgreSQL read replica to a standalone Aurora PostgreSQL DB cluster when the replica lag is zero.

</details>

<details>
    <summary>Answer</summary>

-   [ ] A. Use AWS Database Migration Service (AWS DMS) Schema Conversion to transform the database objects.
-   [ ] D. Define an AWS Database Migration Service (AWS DMS) task with change data capture (CDC) to migrate the data.

Why these are the correct answers:

A. Use AWS Database Migration Service (AWS DMS) Schema Conversion to transform the database objects.

-   [ ] AWS DMS Schema Conversion helps in converting the database schema from MySQL to PostgreSQL.
-   [ ] This is necessary because the schemas might not be directly compatible.

D. Define an AWS Database Migration Service (AWS DMS) task with change data capture (CDC) to migrate the data.

-   [ ] DMS with CDC captures changes in the source database and applies them to the target database.
-   [ ] This ensures data replication during the migration.

Why are the other answers wrong?

-   [ ] B. DMS Schema Conversion does not create Aurora read replicas.
-   [ ] C. Aurora MySQL read replicas are not compatible with RDS for MySQL.
-   [ ] E. Promoting an Aurora PostgreSQL read replica is not part of a standard migration process from RDS for MySQL.

Therefore, Options A and D are the correct steps for migrating the database.
</details>
<details>
    <summary>Question 706</summary>

A company hosts a database that runs on an Amazon RDS instance that is deployed to multiple Availability Zones. The company periodically runs a script against the database to report new entries that are added to the database. The script that runs against the database negatively affects the performance of a critical application. The company needs to improve application performance with minimal costs. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Add functionality to the script to identify the instance that has the fewest active connections.
    Configure the script to read from that instance to report the total new entries.
-   [ ] B. Create a read replica of the database.
    Configure the script to query only the read replica to report the total new entries.
-   [ ] C. Instruct the development team to manually export the new entries for the day in the database at the end of each day.
-   [ ] D. Use Amazon ElastiCache to cache the common queries that the script runs against the database.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Create a read replica of the database.
    Configure the script to query only the read replica to report the total new entries.

Why these are the correct answers:

B. Create a read replica of the database. Configure the script to query only the read replica to report the total new entries.

-   [ ] Read replicas offload read traffic from the primary database.
-   [ ] This improves performance for the critical application.
-   [ ] It is a cost-effective solution with minimal operational overhead.

Why are the other answers wrong?

-   [ ] A. Modifying the script adds complexity and does not guarantee performance improvement.
-   [ ] C. Manual exports add operational overhead.
-   [ ] D. ElastiCache is for caching application data, not for offloading reporting queries.

Therefore, Option B is the most suitable solution for improving application performance.
</details>
<details>
    <summary>Question 707</summary>

A company uses an Application Load Balancer (ALB) to present its application to the internet. The company finds abnormal traffic access patterns across the application. A solutions architect needs to improve visibility into the infrastructure to help the company understand these abnormalities better. What is the MOST operationally efficient solution that meets these requirements?

-   [ ] A. Create a table in Amazon Athena for AWS CloudTrail logs.
    Create a query for the relevant information.
-   [ ] B. Enable ALB access logging to Amazon S3.
    Create a table in Amazon Athena, and query the logs.
-   [ ] C. Enable ALB access logging to Amazon S3.
    Open each file in a text editor, and search each line for the relevant information.
-   [ ] D. Use Amazon EMR on a dedicated Amazon EC2 instance to directly query the ALB to acquire traffic access log information.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Enable ALB access logging to Amazon S3.
    Create a table in Amazon Athena, and query the logs.

Why these are the correct answers:

B. Enable ALB access logging to Amazon S3. Create a table in Amazon Athena, and query the logs.

-   [ ] ALB access logs provide detailed information about requests.
-   [ ] S3 is a cost-effective storage for logs.
-   [ ] Amazon Athena allows querying logs with SQL, which is efficient.

Why are the other answers wrong?

-   [ ] A. CloudTrail logs API calls, not application traffic.
-   [ ] C. Manually searching logs is inefficient.
-   [ ] D. EMR is overkill for simple log analysis.

Therefore, Option B is the most operationally efficient solution for analyzing traffic patterns.
</details>
<details>
    <summary>Question 708</summary>

A company wants to use NAT gateways in its AWS environment. The company's Amazon EC2 instances in private subnets must be able to connect to the public internet through the NAT gateways. Which solution will meet these requirements?

-   [ ] A. Create public NAT gateways in the same private subnets as the EC2 instances.
-   [ ] B. Create private NAT gateways in the same private subnets as the EC2 instances.
-   [ ] C. Create public NAT gateways in public subnets in the same VPCs as the EC2 instances.
-   [ ] D. Create private NAT gateways in public subnets in the same VPCs as the EC2 instances.

</details>

<details>
    <summary>Answer</summary>

-   [ ] C. Create public NAT gateways in public subnets in the same VPCs as the EC2 instances.

Why these are the correct answers:

C. Create public NAT gateways in public subnets in the same VPCs as the EC2 instances.

-   [ ] NAT gateways in public subnets allow instances in private subnets to access the internet.
-   [ ] Public NAT gateways have a public IP address.

Why are the other answers wrong?

-   [ ] A and B. NAT gateways cannot be created in private subnets.
-   [ ] D. Private NAT gateways do not exist.

Therefore, Option C is the correct solution for enabling internet access for private instances.
</details>
<details>
    <summary>Question 709</summary>

A company has an organization in AWS Organizations. The company runs Amazon EC2 instances across four AWS accounts in the root organizational unit (OU). There are three nonproduction accounts and one production account. The company wants to prohibit users from launching EC2 instances of a certain size in the nonproduction accounts. The company has created a service control policy (SCP) to deny access to launch instances that use the prohibited types. Which solutions to deploy the SCP will meet these requirements? (Choose two.)

-   [ ] A. Attach the SCP to the root OU for the organization.
-   [ ] B. Attach the SCP to the three nonproduction Organizations member accounts.
-   [ ] C. Attach the SCP to the Organizations management account.
-   [ ] D. Create an OU for the production account.
    Attach the SCP to the OU.
    Move the production member account into the new OU.
-   [ ] E. Create an OU for the required accounts.
    Attach the SCP to the OU.
    Move the nonproduction member accounts into the new OU.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Attach the SCP to the three nonproduction Organizations member accounts.
-   [ ] E. Create an OU for the required accounts.
    Attach the SCP to the OU.
    Move the nonproduction member accounts into the new OU.

Why these are the correct answers:

B. Attach the SCP to the three nonproduction Organizations member accounts.

-   [ ] Attaching the SCP directly to the nonproduction accounts applies the restriction to those accounts.

E. Create an OU for the required accounts. Attach the SCP to the OU. Move the nonproduction member accounts into the new OU.

-   [ ] Creating an OU for nonproduction accounts and attaching the SCP to that OU simplifies management.
-   [ ] Moving accounts into the OU ensures the policy is applied.

Why are the other answers wrong?

-   [ ] A. Attaching to the root OU applies the SCP to all accounts, including production.
-   [ ] C and D. These options do not effectively target the nonproduction accounts.

Therefore, Options B and E are the correct solutions for applying the SCP.
</details>
<details>
    <summary>Question 710</summary>

A company's website hosted on Amazon EC2 instances processes classified data stored in Amazon S3. Due to security concerns, the company requires a private and secure connection between its EC2 resources and Amazon S3. Which solution meets these requirements?

-   [ ] A. Set up S3 bucket policies to allow access from a VPC endpoint.
-   [ ] B. Set up an IAM policy to grant read-write access to the S3 bucket.
-   [ ] C. Set up a NAT gateway to access resources outside the private subnet.
-   [ ] D. Set up an access key ID and a secret access key to access the S3 bucket.

</details>

<details>
    <summary>Answer</summary>

-   [ ] A. Set up S3 bucket policies to allow access from a VPC endpoint.

Why these are the correct answers:

A. Set up S3 bucket policies to allow access from a VPC endpoint.

-   [ ] VPC endpoints enable private connectivity to S3 without traversing the internet.
-   [ ] S3 bucket policies control access to the bucket.
-   [ ] This solution ensures a private and secure connection.

Why are the other answers wrong?

-   [ ] B. IAM policies control permissions but do not ensure private connectivity.
-   [ ] C. NAT gateways are for instances in private subnets to access the internet.
-   [ ] D. Access keys are for API access and do not provide private connectivity.

Therefore, Option A is the correct solution for secure S3 access.
</details>

# AWS-SAA-PRACTICE-EXAM Questions 711-720

<details>
    <summary>Question 711</summary>

An ecommerce company runs its application on AWS. The application uses an Amazon Aurora PostgreSQL cluster in Multi-AZ mode for the underlying database. During a recent promotional campaign, the application experienced heavy read load and write load. Users experienced timeout issues when they attempted to access the application. A solutions architect needs to make the application architecture more scalable and highly available. Which solution will meet these requirements with the LEAST downtime?

-   [ ] A. Create an Amazon EventBridge rule that has the Aurora cluster as a source.
    Create an AWS Lambda function to log the state change events of the Aurora cluster.
    Add the Lambda function as a target for the EventBridge rule.
    Add additional reader nodes to fail over to.
-   [ ] B. Modify the Aurora cluster and activate the zero-downtime restart (ZDR) feature.
    Use Database Activity Streams on the cluster to track the cluster status.
-   [ ] C. Add additional reader instances to the Aurora cluster.
    Create an Amazon RDS Proxy target group for the Aurora cluster.
-   [ ] D. Create an Amazon ElastiCache for Redis cache.
    Replicate data from the Aurora cluster to Redis by using AWS Database Migration Service (AWS DMS) with a write-around approach.

</details>

<details>
    <summary>Answer</summary>

-   [ ] C. Add additional reader instances to the Aurora cluster.
    Create an Amazon RDS Proxy target group for the Aurora cluster.

Why these are the correct answers:

C. Add additional reader instances to the Aurora cluster. Create an Amazon RDS Proxy target group for the Aurora cluster.

-   [ ] Adding reader instances scales read capacity.
-   [ ] RDS Proxy manages database connections, improving efficiency and reducing downtime during scaling.
-   [ ] This combination addresses both read and write load with minimal downtime.

Why are the other answers wrong?

-   [ ] A. EventBridge and Lambda are for event-driven architectures, not scaling database read/write capacity.
-   [ ] B. Zero-downtime restart (ZDR) minimizes downtime for database engine version upgrades, not for scaling.
-   [ ] D. ElastiCache is for caching, and DMS with write-around adds complexity.

Therefore, Option C is the most suitable solution for scaling the database with minimal downtime.
</details>
<details>
    <summary>Question 712</summary>

A company is designing a web application on AWS. The application will use a VPN connection between the company's existing data centers and the company's VPCs. The company uses Amazon Route 53 as its DNS service. The application must use private DNS records to communicate with the on-premises services from a VPC. Which solution will meet these requirements in the MOST secure manner?

-   [ ] A. Create a Route 53 Resolver outbound endpoint.
    Create a resolver rule.
    Associate the resolver rule with the VPC.
-   [ ] B. Create a Route 53 Resolver inbound endpoint.
    Create a resolver rule.
    Associate the resolver rule with the VPC.
-   [ ] C. Create a Route 53 private hosted zone.
    Associate the private hosted zone with the VPC.
-   [ ] D. Create a Route 53 public hosted zone.
    Create a record for each service to allow service communication

</details>

<details>
    <summary>Answer</summary>

-   [ ] A. Create a Route 53 Resolver outbound endpoint.
    Create a resolver rule.
    Associate the resolver rule with the VPC.

Why these are the correct answers:

A. Create a Route 53 Resolver outbound endpoint. Create a resolver rule. Associate the resolver rule with the VPC.

-   [ ] Route 53 Resolver outbound endpoints allow your VPC to query your on-premises DNS servers.
-   [ ] Resolver rules specify how DNS queries are forwarded.
-   [ ] This is secure because it uses the existing VPN connection and allows for controlled DNS resolution.

Why are the other answers wrong?

-   [ ] B. Inbound endpoints allow on-premises DNS servers to query AWS, not the other way around.
-   [ ] C. Private hosted zones are for DNS within the VPC, not for querying on-premises DNS.
-   [ ] D. Public hosted zones are visible on the internet and are not suitable for private on-premises communication.

Therefore, Option A is the most secure solution for private DNS resolution.
</details>
<details>
    <summary>Question 713</summary>

A company is running a photo hosting service in the us-east-1 Region. The service enables users across multiple countries to upload and view photos. Some photos are heavily viewed for months, and others are viewed for less than a week. The application allows uploads of up to 20 MB for each photo. The service uses the photo metadata to determine which photos to display to each user. Which solution provides the appropriate user access MOST cost-effectively?

-   [ ] A. Store the photos in Amazon DynamoDB.
    Turn on DynamoDB Accelerator (DAX) to cache frequently viewed items.
-   [ ] B. Store the photos in the Amazon S3 Intelligent-Tiering storage class.
    Store the photo metadata and its S3 location in DynamoDB.
-   [ ] C. Store the photos in the Amazon S3 Standard storage class.
    Set up an S3 Lifecycle policy to move photos older than 30 days to the S3 Standard-Infrequent Access (S3 Standard-IA) storage class.
    Use the object tags to keep track of metadata.
-   [ ] D. Store the photos in the Amazon S3 Glacier storage class.
    Set up an S3 Lifecycle policy to move photos older than 30 days to the S3 Glacier Deep Archive storage class.
    Store the photo metadata and its S3 location in Amazon OpenSearch Service.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Store the photos in the Amazon S3 Intelligent-Tiering storage class.
    Store the photo metadata and its S3 location in DynamoDB.

Why these are the correct answers:

B. Store the photos in the Amazon S3 Intelligent-Tiering storage class. Store the photo metadata and its S3 location in DynamoDB.

-   [ ] S3 Intelligent-Tiering automatically optimizes storage costs by moving data between frequent and infrequent access tiers.
-   [ ] DynamoDB is efficient for storing and retrieving metadata.
-   [ ] This combination is cost-effective and provides appropriate access.

Why are the other answers wrong?

-   [ ] A. DynamoDB is not designed for storing large binary files like photos.
-   [ ] C. S3 Standard with Lifecycle policies is less cost-effective than Intelligent-Tiering for varying access patterns.
-   [ ] D. Glacier is for archiving, not for frequently accessed photos, and OpenSearch Service is expensive for metadata storage.

Therefore, Option B is the most cost-effective solution for storing and accessing photos.
</details>
<details>
    <summary>Question 714</summary>

A company runs a highly available web application on Amazon EC2 instances behind an Application Load Balancer. The company uses Amazon CloudWatch metrics. As the traffic to the web application increases, some EC2 instances become overloaded with many outstanding requests. The CloudWatch metrics show that the number of requests processed and the time to receive the responses from some EC2 instances are both higher compared to other EC2 instances. The company does not want new requests to be forwarded to the EC2 instances that are already overloaded. Which solution will meet these requirements?

-   [ ] A. Use the round robin routing algorithm based on the RequestCountPerTarget and ActiveConnectionCount CloudWatch metrics.
-   [ ] B. Use the least outstanding requests algorithm based on the RequestCount Per Target and ActiveConnectionCount CloudWatch metrics.
-   [ ] C. Use the round robin routing algorithm based on the RequestCount and TargetResponse Time CloudWatch metrics.
-   [ ] D. Use the least outstanding requests algorithm based on the RequestCount and TargetResponse Time CloudWatch metrics.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Use the least outstanding requests algorithm based on the RequestCount Per Target and ActiveConnectionCount CloudWatch metrics.

Why these are the correct answers:

B. Use the least outstanding requests algorithm based on the RequestCount Per Target and ActiveConnectionCount CloudWatch metrics.

-   [ ] The least outstanding requests algorithm distributes traffic to instances with fewer active requests.
-   [ ] RequestCountPerTarget and ActiveConnectionCount metrics help determine instance load.
-   [ ] This prevents overloading instances and ensures even distribution.

Why are the other answers wrong?

-   [ ] A and C. Round robin distributes traffic evenly without considering instance load.
-   [ ] D. TargetResponseTime is important but less directly indicative of current load than ActiveConnectionCount.

Therefore, Option B is the correct solution for preventing overloaded instances.
</details>
<details>
    <summary>Question 715</summary>

A company uses Amazon EC2, AWS Fargate, and AWS Lambda to run multiple workloads in the company's AWS account. The company wants to fully make use of its Compute Savings Plans. The company wants to receive notification when coverage of the Compute Savings Plans drops. Which solution will meet these requirements with the MOST operational efficiency?

-   [ ] A. Create a daily budget for the Savings Plans by using AWS Budgets.
    Configure the budget with a coverage threshold to send notifications to the appropriate email message recipients.
-   [ ] B. Create a Lambda function that runs a coverage report against the Savings Plans.
    Use Amazon Simple Email Service (Amazon SES) to email the report to the appropriate email message recipients.
-   [ ] C. Create an AWS Budgets report for the Savings Plans budget.
    Set the frequency to daily.
-   [ ] D. Create a Savings Plans alert subscription.
    Enable all notification options.
    Enter an email address to receive notifications.

</details>

<details>
    <summary>Answer</summary>

-   [ ] A. Create a daily budget for the Savings Plans by using AWS Budgets.
    Configure the budget with a coverage threshold to send notifications to the appropriate email message recipients.

Why these are the correct answers:

A. Create a daily budget for the Savings Plans by using AWS Budgets. Configure the budget with a coverage threshold to send notifications to the appropriate email message recipients.

-   [ ] AWS Budgets can track Savings Plans coverage.
-   [ ] Budgets can send notifications when coverage falls below a threshold.
-   [ ] This is a managed service, minimizing operational overhead.

Why are the other answers wrong?

-   [ ] B. Lambda and SES require custom code and management.
-   [ ] C. Budgets reports do not provide automated notifications based on coverage thresholds.
-   [ ] D. Savings Plans do not have built-in alert subscriptions for coverage.

Therefore, Option A is the most efficient solution for monitoring Savings Plans coverage.
</details>
<details>
    <summary>Question 716</summary>

A company runs a real-time data ingestion solution on AWS. The solution consists of the most recent version of Amazon Managed Streaming for Apache Kafka (Amazon MSK). The solution is deployed in a VPC in private subnets across three Availability Zones. A solutions architect needs to redesign the data ingestion solution to be publicly available over the internet. The data in transit must also be encrypted. Which solution will meet these requirements with the MOST operational efficiency?

-   [ ] A. Configure public subnets in the existing VPC.
    Deploy an MSK cluster in the public subnets.
    Update the MSK cluster security settings to enable mutual TLS authentication.
-   [ ] B. Create a new VPC that has public subnets.
    Deploy an MSK cluster in the public subnets.
    Update the MSK cluster security settings to enable mutual TLS authentication.
-   [ ] C. Deploy an Application Load Balancer (ALB) that uses private subnets.
    Configure an ALB security group inbound rule to allow inbound traffic from the VPC CIDR block for HTTPS protocol.
-   [ ] D. Deploy a Network Load Balancer (NLB) that uses private subnets.
    Configure an NLB listener for HTTPS communication over the internet.

</details>

<details>
    <summary>Answer</summary>

-   [ ] A. Configure public subnets in the existing VPC.
    Deploy an MSK cluster in the public subnets.
    Update the MSK cluster security settings to enable mutual TLS authentication.

Why these are the correct answers:

A. Configure public subnets in the existing VPC. Deploy an MSK cluster in the public subnets. Update the MSK cluster security settings to enable mutual TLS authentication.

-   [ ] Deploying the MSK cluster in public subnets allows it to be accessible over the internet.
-   [ ] Mutual TLS authentication provides encryption and authentication.
-   [ ] Using the existing VPC minimizes changes and is operationally efficient.

Why are the other answers wrong?

-   [ ] B. Creating a new VPC adds unnecessary complexity.
-   [ ] C. ALB is not designed for handling Kafka traffic.
-   [ ] D. NLB is also not suitable for handling Kafka traffic.

Therefore, Option A is the most efficient solution for making the MSK cluster publicly available.
</details>
<details>
    <summary>Question 717</summary>

A company wants to migrate an on-premises legacy application to AWS. The application ingests customer order files from an on-premises enterprise resource planning (ERP) system. The application then uploads the files to an SFTP server. The application uses a scheduled job that checks for order files every hour. The company already has an AWS account that has connectivity to the on-premises network. The new application on AWS must support integration with the existing ERP system. The new application must be secure and resilient and must use the SFTP protocol to process orders from the ERP system immediately. Which solution will meet these requirements?

-   [ ] A. Create an AWS Transfer Family SFTP internet-facing server in two Availability Zones.
    Use Amazon S3 storage.
    Create an AWS Lambda function to process order files.
    Use S3 Event Notifications to send s3:ObjectCreated:* events to the Lambda function.
-   [ ] B. Create an AWS Transfer Family SFTP internet-facing server in one Availability Zone.
    Use Amazon Elastic File System (Amazon EFS) storage.
    Create an AWS Lambda function to process order files.
    Use a Transfer Family managed workflow to invoke the Lambda function.
-   [ ] C. Create an AWS Transfer Family SFTP internal server in two Availability Zones.
    Use Amazon Elastic File System (Amazon EFS) storage.
    Create an AWS Step Functions state machine to process order files.
    Use Amazon EventBridge Scheduler to invoke the state machine to periodically check Amazon EFS for order files.
-   [ ] D. Create an AWS Transfer Family SFTP internal server in two Availability Zones.
    Use Amazon S3 storage.
    Create an AWS Lambda function to process order files.
    Use a Transfer Family managed workflow to invoke the Lambda function.

</details>

<details>
    <summary>Answer</summary>

-   [ ] D. Create an AWS Transfer Family SFTP internal server in two Availability Zones.
    Use Amazon S3 storage.
    Create an AWS Lambda function to process order files.
    Use a Transfer Family managed workflow to invoke the Lambda function.

Why these are the correct answers:

D. Create an AWS Transfer Family SFTP internal server in two Availability Zones. Use Amazon S3 storage. Create an AWS Lambda function to process order files. Use a Transfer Family managed workflow to invoke the Lambda function.

-   [ ] AWS Transfer Family provides managed SFTP servers.
-   [ ] Internal servers are secure within the AWS network.
-   [ ] Multi-AZ ensures resilience.
-   [ ] Lambda processes files.
-   [ ] Transfer Family workflows automate file processing.

Why are the other answers wrong?

-   [ ] A. Internet-facing servers are less secure. S3 Event Notifications are not suitable for immediate processing.
-   [ ] B. Single AZ is not resilient. EFS is not necessary for this use case.
-   [ ] C. Step Functions are for orchestration, not file processing. EventBridge Scheduler is for periodic checks, not immediate processing.

Therefore, Option D is the correct solution for secure, resilient, and immediate SFTP processing.
</details>
<details>
    <summary>Question 718</summary>

A company's applications use Apache Hadoop and Apache Spark to process data on premises. The existing infrastructure is not scalable and is complex to manage. A solutions architect must design a scalable solution that reduces operational complexity. The solution must keep the data processing on premises. Which solution will meet these requirements?

-   [ ] A. Use AWS Site-to-Site VPN to access the on-premises Hadoop Distributed File System (HDFS) data and application.
    Use an Amazon EMR cluster to process the data.
-   [ ] B. Use AWS DataSync to connect to the on-premises Hadoop Distributed File System (HDFS) cluster.
    Create an Amazon EMR cluster to process the data.
-   [ ] C. Migrate the Apache Hadoop application and the Apache Spark application to Amazon EMR clusters on AWS Outposts.
    Use the EMR clusters to process the data.
-   [ ] D. Use an AWS Snowball device to migrate the data to an Amazon S3 bucket.
    Create an Amazon EMR cluster to process the data.

</details>

<details>
    <summary>Answer</summary>

-   [ ] C. Migrate the Apache Hadoop application and the Apache Spark application to Amazon EMR clusters on AWS Outposts.
    Use the EMR clusters to process the data.

Why these are the correct answers:

C. Migrate the Apache Hadoop application and the Apache Spark application to Amazon EMR clusters on AWS Outposts. Use the EMR clusters to process the data.

-   [ ] AWS Outposts brings AWS infrastructure and services to on-premises locations.
-   [ ] EMR clusters on Outposts provide scalable and managed Hadoop and Spark.
-   [ ] This solution keeps data processing on premises while reducing operational complexity.

Why are the other answers wrong?

-   [ ] A and B. Using EMR in AWS and accessing on-premises data adds network complexity and latency.
-   [ ] D. Snowball is for data migration, not ongoing processing. S3 is not suitable for running Hadoop/Spark.

Therefore, Option C is the correct solution for scalable on-premises data processing.
</details>
<details>
    <summary>Question 719</summary>

A company is migrating a large amount of data from on-premises storage to AWS. Windows, Mac, and Linux based Amazon EC2 instances in the same AWS Region will access the data by using SMB and NFS storage protocols. The company will access a portion of the data routinely. The company will access the remaining data infrequently. The company needs to design a solution to host the data. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Create an Amazon Elastic File System (Amazon EFS) volume that uses EFS Intelligent-Tiering.
    Use AWS DataSync to migrate the data to the EFS volume.
-   [ ] B. Create an Amazon FSx for ONTAP instance.
    Create an FSx for ONTAP file system with a root volume that uses the auto tiering policy.
    Migrate the data to the FSx for ONTAP volume.
-   [ ] C. Create an Amazon S3 bucket that uses S3 Intelligent-Tiering.
    Migrate the data to the S3 bucket by using an AWS Storage Gateway Amazon S3 File Gateway.
-   [ ] D. Create an Amazon FSx for OpenZFS file system.
    Migrate the data to the new volume.

</details>

<details>
    <summary>Answer</summary>

-   [ ] B. Create an Amazon FSx for ONTAP instance.
    Create an FSx for ONTAP file system with a root volume that uses the auto tiering policy.
    Migrate the data to the FSx for ONTAP volume.

Why these are the correct answers:

B. Create an Amazon FSx for ONTAP instance. Create an FSx for ONTAP file system with a root volume that uses the auto tiering policy. Migrate the data to the FSx for ONTAP volume.

-   [ ] FSx for ONTAP supports both SMB and NFS.
-   [ ] Auto-tiering in FSx for ONTAP automatically moves data between performance and capacity-optimized storage tiers.
-   [ ] This solution minimizes operational overhead by automating tiering.

Why are the other answers wrong?

-   [ ] A. EFS does not support SMB.
-   [ ] C. S3 File Gateway is for hybrid cloud storage, not for providing native SMB/NFS access to EC2 instances.
-   [ ] D. FSx for OpenZFS does not have built-in auto-tiering.

Therefore, Option B is the most efficient solution for multi-protocol access and data tiering.
</details>
<details>
    <summary>Question 720</summary>

A manufacturing company runs its report generation application on AWS. The application generates each report in about 20 minutes. The application is built as a monolith that runs on a single Amazon EC2 instance. The application requires frequent updates to its tightly coupled modules. The application becomes complex to maintain as the company adds new features. Each time the company patches a software module, the application experiences downtime. Report generation must restart from the beginning after any interruptions. The company wants to redesign the application so that the application can be flexible, scalable, and gradually improved. The company wants to minimize application downtime. Which solution will meet these requirements?

-   [ ] A. Run the application on AWS Lambda as a single function with maximum provisioned concurrency.
-   [ ] B. Run the application on Amazon EC2 Spot Instances as microservices with a Spot Fleet default allocation strategy.
-   [ ] C. Run the application on Amazon Elastic Container Service (Amazon ECS) as microservices with service auto scaling.
-   [ ] D. Run the application on AWS Elastic Beanstalk as a single application environment with an all-at-once deployment strategy.

</details>

<details>
    <summary>Answer</summary>

-   [ ] C. Run the application on Amazon Elastic Container Service (Amazon ECS) as microservices with service auto scaling.

Why these are the correct answers:

C. Run the application on Amazon Elastic Container Service (Amazon ECS) as microservices with service auto scaling.

-   [ ] ECS allows running the application as microservices, improving flexibility and scalability.
-   [ ] Service auto scaling ensures the application can handle varying workloads.
-   [ ] Microservices architecture reduces downtime during updates.

Why are the other answers wrong?

-   [ ] A. Lambda has execution time limits and is not suitable for long-running reports.
-   [ ] B. Spot Instances can be interrupted, causing report restarts.
-   [ ] D. Elastic Beanstalk with an all-at-once deployment strategy causes downtime during updates.

Therefore, Option C is the most suitable solution for a flexible, scalable, and highly available application.
</details>

# AWS-SAA-PRACTICE-EXAM Questions 721-730

<details>
  <summary>Question 721</summary>

A company wants to rearchitect a large-scale web application to a serverless microservices architecture. The application uses Amazon EC2 instances and is written in Python. The company selected one component of the web application to test as a microservice. The component supports hundreds of requests each second. The company wants to create and test the microservice on an AWS solution that supports Python. The solution must also scale automatically and require minimal infrastructure and minimal operational support.

Which solution will meet these requirements?

-   [ ] A. Use a Spot Fleet with auto scaling of EC2 instances that run the most recent Amazon Linux operating system.
-   [ ] B. Use an AWS Elastic Beanstalk web server environment that has high availability configured.
-   [ ] C. Use Amazon Elastic Kubernetes Service (Amazon EKS). Launch Auto Scaling groups of self-managed EC2 instances.
-   [ ] D. Use an AWS Lambda function that runs custom developed code.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Use an AWS Lambda function that runs custom developed code.

Why these are the correct answers:

D.  Use an AWS Lambda function that runs custom developed code.

-   [ ] AWS Lambda supports Python and allows you to run code without provisioning or managing servers.
-   [ ] Lambda functions scale automatically in response to demand, and they require minimal operational overhead.

Why are the other answers wrong?

-   [ ] A. Using a Spot Fleet with EC2 instances involves managing EC2 instances, including patching, scaling configurations, and infrastructure maintenance, which does not meet the requirement for minimal operational support.
-   [ ] B. AWS Elastic Beanstalk simplifies deployment and management but still requires some level of infrastructure management and does not offer the same level of automatic scaling and minimal operation as Lambda.
-   [ ] C. Amazon EKS involves significant operational overhead for managing the Kubernetes control plane and worker nodes. It also requires more infrastructure management than Lambda.

Therefore, Option D is the most suitable solution for creating and testing the microservice with Python, automatic scaling, and minimal operational support.
</details>
<details>
  <summary>Question 722</summary>

A company has an AWS Direct Connect connection from its on-premises location to an AWS account. The AWS account has 30 different VPCs in the same AWS Region. The VPCs use private virtual interfaces (VIFs). Each VPC has a CIDR block that does not overlap with other networks under the company's control. The company wants to centrally manage the networking architecture while still allowing each VPC to communicate with all other VPCs and on-premises networks. Which solution will meet these requirements with the LEAST amount of operational overhead?

-   [ ] A. Create a transit gateway, and associate the Direct Connect connection with a new transit VIF. Turn on the transit gateway's route propagation feature.
-   [ ] B. Create a Direct Connect gateway. Recreate the private VIFs to use the new gateway. Associate each VPC by creating new virtual private gateways.
-   [ ] C. Create a transit VPConnect the Direct Connect connection to the transit VPCreate a peering connection between all other VPCs in the Region. Update the route tables.
-   [ ] D. Create AWS Site-to-Site VPN connections from on premises to each VPC. Ensure that both VPN tunnels are UP for each connection. Turn on the route propagation feature.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Create a transit gateway, and associate the Direct Connect connection with a new transit VIF. Turn on the transit gateway's route propagation feature.

Why these are the correct answers:

A.  Create a transit gateway, and associate the Direct Connect connection with a new transit VIF. Turn on the transit gateway's route propagation feature.

-   [ ] AWS Transit Gateway simplifies network management by providing a central hub that connects VPCs and on-premises networks.
-   [ ] It reduces the complexity of managing multiple connections and route tables.
-   [ ] Using route propagation automates route distribution, minimizing operational overhead.

Why are the other answers wrong?

-   [ ] B. Creating a Direct Connect gateway and recreating private VIFs is more complex and requires significant reconfiguration, increasing operational overhead.
-   [ ] C. Creating a transit VP and peering connections between all VPCs adds complexity and requires managing many peering connections and route tables.
-   [ ] D. Creating individual Site-to-Site VPN connections to each VPC is the most complex solution, involving managing and maintaining numerous VPN connections and tunnels.

Therefore, Option A provides the most efficient and least operationally intensive solution for connecting multiple VPCs and on-premises networks.
</details>
<details>
  <summary>Question 723</summary>

A company has applications that run on Amazon EC2 instances. The EC2 instances connect to Amazon RDS databases by using an IAM role that has associated policies. The company wants to use AWS Systems Manager to patch the EC2 instances without disrupting the running applications. Which solution will meet these requirements?

-   [ ] A. Create a new IAM role. Attach the AmazonSSMManagedInstanceCore policy to the new IAM role. Attach the new IAM role to the EC2 instances and the existing IAM role.
-   [ ] B. Create an IAM user. Attach the AmazonSSMManagedInstanceCore policy to the IAM user. Configure Systems Manager to use the IAM user to manage the EC2 instances.
-   [ ] C. Enable Default Host Configuration Management in Systems Manager to manage the EC2 instances.
-   [ ] D. Remove the existing policies from the existing IAM role. Add the AmazonSSMManagedInstanceCore policy to the existing IAM role.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Enable Default Host Configuration Management in Systems Manager to manage the EC2 instances.

Why these are the correct answers:

C.  Enable Default Host Configuration Management in Systems Manager to manage the EC2 instances.

-   [ ] Default Host Configuration Management in Systems Manager simplifies the setup and management of EC2 instances for Systems Manager, minimizing operational overhead.
-   [ ] It allows Systems Manager to manage the instances without requiring specific IAM role modifications.

Why are the other answers wrong?

-   [ ] A. Creating and attaching new IAM roles or modifying existing ones adds complexity and is not necessary for using Systems Manager for patching.
-   [ ] B. Creating an IAM user for Systems Manager is not the recommended approach. Systems Manager is designed to work with IAM roles attached to instances.
-   [ ] D. Removing existing policies from the IAM role could disrupt the applications that rely on those policies to access RDS databases.

Therefore, Option C is the most straightforward and least disruptive solution for using Systems Manager to patch EC2 instances.
</details>
<details>
  <summary>Question 724</summary>

A company runs container applications by using Amazon Elastic Kubernetes Service (Amazon EKS) and the Kubernetes Horizontal Pod Autoscaler. The workload is not consistent throughout the day. A solutions architect notices that the number of nodes does not automatically scale out when the existing nodes have reached maximum capacity in the cluster, which causes performance issues. Which solution will resolve this issue with the LEAST administrative overhead?

-   [ ] A. Scale out the nodes by tracking the memory usage.
-   [ ] B. Use the Kubernetes Cluster Autoscaler to manage the number of nodes in the cluster.
-   [ ] C. Use an AWS Lambda function to resize the EKS cluster automatically.
-   [ ] D. Use an Amazon EC2 Auto Scaling group to distribute the workload.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Use the Kubernetes Cluster Autoscaler to manage the number of nodes in the cluster.

Why these are the correct answers:

B.  Use the Kubernetes Cluster Autoscaler to manage the number of nodes in the cluster.

-   [ ] The Kubernetes Cluster Autoscaler is specifically designed to automatically adjust the size of the EKS cluster based on the demands of the workloads.
-   [ ] It integrates seamlessly with EKS and requires minimal administrative overhead to set up and maintain.

Why are the other answers wrong?

-   [ ] A. Scaling nodes by tracking memory usage alone might not be sufficient, as CPU or other resources could also be a bottleneck.
-   [ ] C. Using an AWS Lambda function to resize the EKS cluster adds complexity and requires custom code and maintenance, increasing administrative overhead.
-   [ ] D. Using an Amazon EC2 Auto Scaling group directly does not integrate with Kubernetes in the same way as the Cluster Autoscaler, and it would require additional configuration to coordinate with the Kubernetes scheduler.

Therefore, Option B is the most efficient and least burdensome way to ensure that the EKS cluster scales appropriately.
</details>
<details>
  <summary>Question 725</summary>

A company maintains about 300 TB in Amazon S3 Standard storage month after month. The S3 objects are each typically around 50 GB in size and are frequently replaced with multipart uploads by their global application. The number and size of S3 objects remain constant, but the company's S3 storage costs are increasing each month. How should a solutions architect reduce costs in this situation?

-   [ ] A. Switch from multipart uploads to Amazon S3 Transfer Acceleration.
-   [ ] B. Enable an S3 Lifecycle policy that deletes incomplete multipart uploads.
-   [ ] C. Configure S3 inventory to prevent objects from being archived too quickly.
-   [ ] D. Configure Amazon CloudFront to reduce the number of objects stored in Amazon S3.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Enable an S3 Lifecycle policy that deletes incomplete multipart uploads.

Why these are the correct answers:

B.  Enable an S3 Lifecycle policy that deletes incomplete multipart uploads.

-   [ ] Multipart uploads that are not completed can consume storage space, leading to increased costs.
-   [ ] An S3 Lifecycle policy can automatically delete these incomplete uploads, helping to reduce storage costs.

Why are the other answers wrong?

-   [ ] A. Switching to S3 Transfer Acceleration is designed to speed up uploads but does not directly address the issue of increasing storage costs due to incomplete uploads.
-   [ ] C. Configuring S3 inventory provides insights into storage but does not directly reduce costs.
-   [ ] D. Configuring Amazon CloudFront is for content delivery and caching, not for managing S3 storage costs related to incomplete uploads.

Therefore, Option B is the most effective solution to reduce costs by cleaning up incomplete multipart uploads.
</details>
<details>
  <summary>Question 726</summary>

A company has deployed a multiplayer game for mobile devices. The game requires live location tracking of players based on latitude and longitude. The data store for the game must support rapid updates and retrieval of locations. The game uses an Amazon RDS for PostgreSQL DB instance with read replicas to store the location data. During peak usage periods, the database is unable to maintain the performance that is needed for reading and writing updates. The game's user base is increasing rapidly.

What should a solutions architect do to improve the performance of the data tier?

-   [ ] A. Take a snapshot of the existing DB instance. Restore the snapshot with Multi-AZ enabled.
-   [ ] B. Migrate from Amazon RDS to Amazon OpenSearch Service with OpenSearch Dashboards.
-   [ ] C. Deploy Amazon DynamoDB Accelerator (DAX) in front of the existing DB instance. Modify the game to use DAX.
-   [ ] D. Deploy an Amazon ElastiCache for Redis cluster in front of the existing DB instance. Modify the game to use Redis.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Deploy an Amazon ElastiCache for Redis cluster in front of the existing DB instance. Modify the game to use Redis.

Why these are the correct answers:

D.  Deploy an Amazon ElastiCache for Redis cluster in front of the existing DB instance. Modify the game to use Redis.

-   [ ] Amazon ElastiCache for Redis is an in-memory data store that can significantly improve read and write performance by caching frequently accessed data.
-   [ ] It is well-suited for real-time applications that require low-latency data access.

Why are the other answers wrong?

-   [ ] A. Taking a snapshot and restoring with Multi-AZ enabled improves availability but does not address the performance issues caused by high read/write loads.
-   [ ] B. Migrating to Amazon OpenSearch Service is more suitable for search and analytics, not for rapid updates and retrieval of location data.
-   [ ] C. Deploying Amazon DynamoDB Accelerator (DAX) is specific to DynamoDB and not suitable for an RDS PostgreSQL instance.

Therefore, Option D is the most appropriate solution to enhance the performance of the data tier for the multiplayer game.
</details>
<details>
  <summary>Question 727</summary>

A company stores critical data in Amazon DynamoDB tables in the company's AWS account. An IT administrator accidentally deleted a DynamoDB table. The deletion caused a significant loss of data and disrupted the company's operations. The company wants to prevent this type of disruption in the future. Which solution will meet this requirement with the LEAST operational overhead?

-   [ ] A. Configure a trail in AWS CloudTrail. Create an Amazon EventBridge rule for delete actions. Create an AWS Lambda function to automatically restore deleted DynamoDB tables.
-   [ ] B. Create a backup and restore plan for the DynamoDB tables. Recover the DynamoDB tables manually.
-   [ ] C. Configure deletion protection on the DynamoDB tables.
-   [ ] D. Enable point-in-time recovery on the DynamoDB tables.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Configure deletion protection on the DynamoDB tables.

Why these are the correct answers:

C.  Configure deletion protection on the DynamoDB tables.

-   [ ] Deletion protection prevents accidental deletion of DynamoDB tables, providing a straightforward way to avoid data loss with minimal operational overhead.

Why are the other answers wrong?

-   [ ] A. Configuring CloudTrail, EventBridge rules, and Lambda functions involves significant setup and maintenance, increasing operational overhead.
-   [ ] B. Creating a backup and restore plan requires manual intervention to recover tables, which is not ideal for minimizing operational overhead.
-   [ ] D. Enabling point-in-time recovery allows for recovery to a specific point in time but does not prevent accidental deletion in the first place.

Therefore, Option C is the most efficient and least operationally intensive solution to prevent accidental deletion of DynamoDB tables.
</details>
<details>
  <summary>Question 728</summary>

A company has an on-premises data center that is running out of storage capacity. The company wants to migrate its storage infrastructure to AWS while minimizing bandwidth costs. The solution must allow for immediate retrieval of data at no additional cost.

How can these requirements be met?

-   [ ] A. Deploy Amazon S3 Glacier Vault and enable expedited retrieval. Enable provisioned retrieval capacity for the workload.
-   [ ] B. Deploy AWS Storage Gateway using cached volumes. Use Storage Gateway to store data in Amazon S3 while retaining copies of frequently accessed data subsets locally.
-   [ ] C. Deploy AWS Storage Gateway using stored volumes to store data locally. Use Storage Gateway to asynchronously back up point-in-time snapshots of the data to Amazon S3.
-   [ ] D. Deploy AWS Direct Connect to connect with the on-premises data center. Configure AWS Storage Gateway to store data locally. Use Storage Gateway to asynchronously back up point-in-time snapshots of the data to Amazon S3.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Deploy AWS Storage Gateway using cached volumes. Use Storage Gateway to store data in Amazon S3 while retaining copies of frequently accessed data subsets locally.

Why these are the correct answers:

B.  Deploy AWS Storage Gateway using cached volumes. Use Storage Gateway to store data in Amazon S3 while retaining copies of frequently accessed data subsets locally.

-   [ ] AWS Storage Gateway with cached volumes allows you to store primary data in Amazon S3 while keeping a cache of frequently accessed data on premises for low-latency access.
-   [ ] This minimizes bandwidth costs by reducing the amount of data that needs to be transferred and provides immediate retrieval of frequently used data.

Why are the other answers wrong?

-   [ ] A. Amazon S3 Glacier is designed for archiving, and even with expedited retrieval, it is not suitable for immediate retrieval of data.
-   [ ] C. AWS Storage Gateway with stored volumes stores all data on premises and only backs up to S3, which does not meet the requirement of migrating storage infrastructure to AWS.
-   [ ] D. Deploying AWS Direct Connect can reduce bandwidth costs but does not, by itself, provide a solution for immediate data retrieval.

Therefore, Option B is the most suitable solution to migrate storage to AWS, minimize bandwidth costs, and allow for immediate data retrieval.
</details>

<details>
  <summary>Question 729</summary>

A company runs a three-tier web application in a VPC across multiple Availability Zones. Amazon EC2 instances run in an Auto Scaling group for the application tier. The company needs to make an automated scaling plan that will analyze each resource's daily and weekly historical workload trends. The configuration must scale resources appropriately according to both the forecast and live changes in utilization. Which scaling strategy should a solutions architect recommend to meet these requirements?

-   [ ] A. Implement dynamic scaling with step scaling based on average CPU utilization from the EC2 instances.
-   [ ] B. Enable predictive scaling to forecast and scale. Configure dynamic scaling with target tracking
-   [ ] C. Create an automated scheduled scaling action based on the traffic patterns of the web application.
-   [ ] D. Set up a simple scaling policy. Increase the cooldown period based on the EC2 instance startup time.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Enable predictive scaling to forecast and scale. Configure dynamic scaling with target tracking

Why these are the correct answers:

B.  Enable predictive scaling to forecast and scale. Configure dynamic scaling with target tracking

-   [ ] Predictive scaling uses machine learning to analyze historical workload trends and forecast future traffic.
-   [ ] It automatically adjusts the number of EC2 instances in the Auto Scaling group in advance of anticipated traffic changes.
-   [ ] Dynamic scaling with target tracking responds to live changes in resource utilization, ensuring that the application scales appropriately in real-time.
-   [ ] Combining predictive and dynamic scaling provides a comprehensive solution that handles both anticipated and immediate changes in demand.

Why are the other answers wrong?

-   [ ] A. Dynamic scaling with step scaling responds to changes in CPU utilization but does not anticipate future changes based on historical trends.
-   [ ] C. Scheduled scaling actions scale resources at specific times but do not adapt to unexpected or rapidly changing traffic patterns.
-   [ ] D. A simple scaling policy with a cooldown period does not provide the sophisticated scaling that combines historical analysis with real-time response.

Therefore, Option B is the most suitable strategy to meet the requirements for automated scaling based on both forecast and live changes.
</details>
<details>
  <summary>Question 730</summary>

A package delivery company has an application that uses Amazon EC2 instances and an Amazon Aurora MySQL DB cluster. As the application becomes more popular, EC2 instance usage increases only slightly. DB cluster usage increases at a much faster rate.

The company adds a read replica, which reduces the DB cluster usage for a short period of time. However, the load continues to increase. The operations that cause the increase in DB cluster usage are all repeated read statements that are related to delivery details. The company needs to alleviate the effect of repeated reads on the DB cluster. Which solution will meet these requirements MOST cost-effectively?

-   [ ] A. Implement an Amazon ElastiCache for Redis cluster between the application and the DB cluster.
-   [ ] B. Add an additional read replica to the DB cluster.
-   [ ] C. Configure Aurora Auto Scaling for the Aurora read replicas.
-   [ ] D. Modify the DB cluster to have multiple writer instances.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Implement an Amazon ElastiCache for Redis cluster between the application and the DB cluster.

Why these are the correct answers:

A.  Implement an Amazon ElastiCache for Redis cluster between the application and the DB cluster.

-   [ ] Amazon ElastiCache for Redis is an in-memory caching service that can store frequently accessed data, such as delivery details, reducing the load on the DB cluster.
-   [ ] It is a cost-effective way to improve performance for read-heavy workloads by minimizing the number of requests that reach the database.

Why are the other answers wrong?

-   [ ] B. Adding another read replica may provide some relief but might not be as cost-effective as caching, especially if the read load continues to increase significantly.
-   [ ] C. Configuring Aurora Auto Scaling for read replicas can help with scaling but is more focused on handling increased load through the database layer rather than reducing it.
-   [ ] D. Modifying the DB cluster to have multiple writer instances is not relevant since the issue is with repeated read statements, not write operations.

Therefore, Option A is the most cost-effective solution to alleviate the effect of repeated reads on the DB cluster.
</details>

# AWS-SAA-PRACTICE-EXAM Questions 731-740

<details>
  <summary>Question 731</summary>

A company has an application that uses an Amazon DynamoDB table for storage. A solutions architect discovers that many requests to the table are not returning the latest data. The company's users have not reported any other issues with database performance. Latency is in an acceptable range. Which design change should the solutions architect recommend?

-   [ ] A. Add read replicas to the table.
-   [ ] B. Use a global secondary index (GSI).
-   [ ] C. Request strongly consistent reads for the table.
-   [ ] D. Request eventually consistent reads for the table.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Request strongly consistent reads for the table.

Why these are the correct answers:

C.  Request strongly consistent reads for the table.

-   [ ] DynamoDB provides both eventually consistent and strongly consistent reads.
-   [ ] Strongly consistent reads always return the most recent updated data, which solves the issue of not getting the latest data.

Why are the other answers wrong?

-   [ ] A. Adding read replicas is not applicable to DynamoDB. DynamoDB does not use read replicas in the same way as relational databases.
-   [ ] B. Using a global secondary index (GSI) is for optimizing queries based on attributes other than the primary key, not for ensuring read consistency.
-   [ ] D. Requesting eventually consistent reads is the default behavior and the reason why the application is not returning the latest data.

Therefore, Option C is the correct solution to ensure that the application always retrieves the most up-to-date data from the DynamoDB table.
</details>
<details>
  <summary>Question 732</summary>

A company has deployed its application on Amazon EC2 instances with an Amazon RDS database. The company used the principle of least privilege to configure the database access credentials. The company's security team wants to protect the application and the database from SQL injection and other web-based attacks. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Use security groups and network ACLs to secure the database and application servers.
-   [ ] B. Use AWS WAF to protect the application. Use RDS parameter groups to configure the security settings.
-   [ ] C. Use AWS Network Firewall to protect the application and the database.
-   [ ] D. Use different database accounts in the application code for different functions. Avoid granting excessive privileges to the database users.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Use AWS WAF to protect the application. Use RDS parameter groups to configure the security settings.

Why these are the correct answers:

B.  Use AWS WAF to protect the application. Use RDS parameter groups to configure the security settings.

-   [ ] AWS WAF is designed to protect web applications from common web exploits like SQL injection.
-   [ ] RDS parameter groups allow you to configure database settings, including security-related parameters.
-   [ ] This combination provides a strong security posture with relatively low operational overhead.

Why are the other answers wrong?

-   [ ] A. Security groups and network ACLs provide network-level security but do not offer specific protection against SQL injection.
-   [ ] C. AWS Network Firewall is a powerful network firewall service but may introduce more operational overhead than AWS WAF for web application protection.
-   [ ] D. Using different database accounts and adhering to least privilege is a good practice but does not, by itself, prevent SQL injection attacks.

Therefore, Option B is the most suitable solution to protect against web-based attacks with the least operational overhead.
</details>
<details>
  <summary>Question 733</summary>

An ecommerce company runs applications in AWS accounts that are part of an organization in AWS Organizations. The applications run on Amazon Aurora PostgreSQL databases across all the accounts. The company needs to prevent malicious activity and must identify abnormal failed and incomplete login attempts to the databases. Which solution will meet these requirements in the MOST operationally efficient way?

-   [ ] A. Attach service control policies (SCPs) to the root of the organization to identity the failed login attempts.
-   [ ] B. Enable the Amazon RDS Protection feature in Amazon GuardDuty for the member accounts of the organization.
-   [ ] C. Publish the Aurora general logs to a log group in Amazon CloudWatch Logs. Export the log data to a central Amazon S3 bucket.
-   [ ] D. Publish all the Aurora PostgreSQL database events in AWS CloudTrail to a central Amazon S3 bucket.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Enable the Amazon RDS Protection feature in Amazon GuardDuty for the member accounts of the organization.

Why these are the correct answers:

B.  Enable the Amazon RDS Protection feature in Amazon GuardDuty for the member accounts of the organization.

-   [ ] Amazon GuardDuty's RDS Protection feature is specifically designed to detect threats to RDS databases, including malicious login attempts.
-   [ ] It provides automated threat detection with minimal operational overhead.

Why are the other answers wrong?

-   [ ] A. Service control policies (SCPs) are used to manage permissions across AWS Organizations accounts but do not provide detailed monitoring of database login attempts.
-   [ ] C. Publishing logs to CloudWatch Logs and exporting them to S3 requires setting up and managing logging and export processes, increasing operational overhead.
-   [ ] D. Publishing database events to CloudTrail captures API calls but is not as efficient for monitoring database-specific login attempts as GuardDuty's RDS Protection.

Therefore, Option B is the most operationally efficient solution for monitoring and protecting Aurora PostgreSQL databases across multiple accounts.
</details>
<details>
  <summary>Question 734</summary>

A company has an AWS Direct Connect connection from its corporate data center to its VPC in the us-east-1 Region. The company recently acquired a corporation that has several VPCs and a Direct Connect connection between its on-premises data center and the eu-west-2 Region. The CIDR blocks for the VPCs of the company and the corporation do not overlap. The company requires connectivity between two Regions and the data centers. The company needs a solution that is scalable while reducing operational overhead. What should a solutions architect do to meet these requirements?

-   [ ] A. Set up inter-Region VPC peering between the VPC in us-east-1 and the VPCs in eu-west-2.
-   [ ] B. Create private virtual interfaces from the Direct Connect connection in us-east-1 to the VPCs in eu-west-2.
-   [ ] C. Establish VPN appliances in a fully meshed VPN network hosted by Amazon EC2. Use AWS VPN CloudHub to send and receive data between the data centers and each VPC.
-   [ ] D. Connect the existing Direct Connect connection to a Direct Connect gateway. Route traffic from the virtual private gateways of the VPCs in each Region to the Direct Connect gateway.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Connect the existing Direct Connect connection to a Direct Connect gateway. Route traffic from the virtual private gateways of the VPCs in each Region to the Direct Connect gateway.

Why these are the correct answers:

D.  Connect the existing Direct Connect connection to a Direct Connect gateway. Route traffic from the virtual private gateways of the VPCs in each Region to the Direct Connect gateway.

-   [ ] A Direct Connect gateway allows you to connect a Direct Connect connection to multiple VPCs across different AWS Regions.
-   [ ] This simplifies the network architecture and reduces operational overhead.
-   [ ] It provides a scalable solution for connecting multiple VPCs and on-premises locations.

Why are the other answers wrong?

-   [ ] A. Inter-Region VPC peering can connect VPCs across Regions but does not directly integrate with Direct Connect for on-premises connectivity.
-   [ ] B. Creating private virtual interfaces from one Direct Connect connection to VPCs in another Region is not a supported or efficient architecture.
-   [ ] C. Establishing VPN appliances in a fully meshed network is complex and adds operational overhead compared to using a Direct Connect gateway.

Therefore, Option D is the most scalable and operationally efficient solution for connecting the company's and the acquired corporation's networks.
</details>
<details>
  <summary>Question 735</summary>

A company is developing a mobile game that streams score updates to a backend processor and then posts results on a leaderboard. A solutions architect needs to design a solution that can handle large traffic spikes, process the mobile game updates in order of receipt, and store the processed updates in a highly available database. The company also wants to minimize the management overhead required to maintain the solution. What should the solutions architect do to meet these requirements?

-   [ ] A. Push score updates to Amazon Kinesis Data Streams. Process the updates in Kinesis Data Streams with AWS Lambda. Store the processed updates in Amazon DynamoDB.
-   [ ] B. Push score updates to Amazon Kinesis Data Streams. Process the updates with a fleet of Amazon EC2 instances set up for Auto Scaling. Store the processed updates in Amazon Redshift.
-   [ ] C. Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe an AWS Lambda function to the SNS topic to process the updates. Store the processed updates in a SQL database running on Amazon EC2.
-   [ ] D. Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue. Use a fleet of Amazon EC2 instances with Auto Scaling to process the updates in the SQS queue. Store the processed updates in an Amazon RDS Multi-AZ DB instance.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Push score updates to Amazon Kinesis Data Streams. Process the updates in Kinesis Data Streams with AWS Lambda. Store the processed updates in Amazon DynamoDB.

Why these are the correct answers:

A.  Push score updates to Amazon Kinesis Data Streams. Process the updates in Kinesis Data Streams with AWS Lambda. Store the processed updates in Amazon DynamoDB.

-   [ ] Amazon Kinesis Data Streams can handle large traffic spikes and maintain the order of data records.
-   [ ] AWS Lambda allows for serverless processing of the updates, minimizing management overhead.
-   [ ] Amazon DynamoDB is a highly available NoSQL database that can store the processed updates.

Why are the other answers wrong?

-   [ ] B. Using Amazon EC2 instances with Auto Scaling increases management overhead compared to using Lambda. Amazon Redshift is designed for data warehousing and analytics, not for real-time processing of game updates.
-   [ ] C. Amazon SNS is a publish/subscribe service and does not guarantee message ordering. Storing data in a SQL database on Amazon EC2 increases management overhead.
-   [ ] D. Amazon SQS does not guarantee message ordering, which is a requirement for processing updates in order of receipt. Using EC2 instances also increases management overhead.

Therefore, Option A is the most suitable solution for handling traffic spikes, processing updates in order, and minimizing management overhead.
</details>
<details>
  <summary>Question 736</summary>

A company has multiple AWS accounts with applications deployed in the us-west-2 Region. Application logs are stored within Amazon S3 buckets in each account. The company wants to build a centralized log analysis solution that uses a single S3 bucket. Logs must not leave us-west-2, and the company wants to incur minimal operational overhead. Which solution meets these requirements and is MOST cost-effective?

-   [ ] A. Create an S3 Lifecycle policy that copies the objects from one of the application S3 buckets to the centralized S3 bucket.
-   [ ] B. Use S3 Same-Region Replication to replicate logs from the S3 buckets to another S3 bucket in us-west-2. Use this S3 bucket for log analysis.
-   [ ] C. Write a script that uses the PutObject API operation every day to copy the entire contents of the buckets to another S3 bucket in us-west-2. Use this S3 bucket for log analysis.
-   [ ] D. Write AWS Lambda functions in these accounts that are triggered every time logs are delivered to the S3 buckets (s3:ObjectCreated:\* event). Copy the logs to another S3 bucket in us-west-2. Use this S3 bucket for log analysis.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Use S3 Same-Region Replication to replicate logs from the S3 buckets to another S3 bucket in us-west-2. Use this S3 bucket for log analysis.

Why these are the correct answers:

B.  Use S3 Same-Region Replication to replicate logs from the S3 buckets to another S3 bucket in us-west-2. Use this S3 bucket for log analysis.

-   [ ] S3 Same-Region Replication is designed to replicate objects between S3 buckets within the same AWS Region.
-   [ ] It is an efficient and cost-effective way to centralize logs without incurring the overhead of custom solutions.
-   [ ] Replication ensures that logs remain within the us-west-2 Region.

Why are the other answers wrong?

-   [ ] A. S3 Lifecycle policies are primarily for managing object lifecycle within a single bucket, not for replicating objects to another bucket.
-   [ ] C. Writing a script to copy objects is more complex and adds operational overhead compared to using S3 Replication.
-   [ ] D. Using Lambda functions to copy logs introduces additional complexity and cost, as it requires writing, deploying, and managing Lambda functions.

Therefore, Option B is the most cost-effective and operationally efficient solution for centralizing logs within the same AWS Region.
</details>
<details>
  <summary>Question 737</summary>

A company has an application that delivers on-demand training videos to students around the world. The application also allows authorized content developers to upload videos. The data is stored in an Amazon S3 bucket in the us-east-2 Region. The company has created an S3 bucket in the eu-west-2 Region and an S3 bucket in the ap-southeast-1 Region. The company wants to replicate the data to the new S3 buckets. The company needs to minimize latency for developers who upload videos and students who stream videos near eu-west-2 and ap-southeast-1. Which combination of steps will meet these requirements with the FEWEST changes to the application? (Choose two.)

-   [ ] A. Configure one-way replication from the us-east-2 S3 bucket to the eu-west-2 S3 bucket. Configure one-way replication from the us-east-2 S3 bucket to the ap-southeast-1 S3 bucket.
-   [ ] B. Configure one-way replication from the us-east-2 S3 bucket to the eu-west-2 S3 bucket. Configure one-way replication from the eu-west-2 S3 bucket to the ap-southeast-1 S3 bucket.
-   [ ] C. Configure two-way (bidirectional) replication among the S3 buckets that are in all three Regions.
-   [ ] D. Create an S3 Multi-Region Access Point. Modify the application to use the Amazon Resource Name (ARN) of the Multi-Region Access Point for video streaming. Do not modify the application for video uploads.
-   [ ] E. Create an S3 Multi-Region Access Point. Modify the application to use the Amazon Resource Name (ARN) of the Multi-Region Access Point for video streaming and uploads.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Configure two-way (bidirectional) replication among the S3 buckets that are in all three Regions.
-   [ ] E. Create an S3 Multi-Region Access Point. Modify the application to use the Amazon Resource Name (ARN) of the Multi-Region Access Point for video streaming and uploads.

Why these are the correct answers:

C.  Configure two-way (bidirectional) replication among the S3 buckets that are in all three Regions.

-   [ ] Bidirectional replication ensures that uploads and updates in any Region are reflected in the other Regions, minimizing latency for both uploads and streaming.

E.  Create an S3 Multi-Region Access Point. Modify the application to use the Amazon Resource Name (ARN) of the Multi-Region Access Point for video streaming and uploads.

-   [ ] S3 Multi-Region Access Points simplify data access across multiple S3 buckets in different Regions using a single endpoint.
-   [ ] Modifying the application to use the Multi-Region Access Point ARN for both uploads and streaming optimizes latency and reduces changes to the application.

Why are the other answers wrong?

-   [ ] A and B. One-way replication does not address the requirement for low latency for both uploads and streaming in all Regions. It only replicates data in one direction.
-   [ ] D. Not modifying the application for uploads means that uploads would still go to a single Region, which does not minimize latency for developers in other Regions.

Therefore, the combination of bidirectional replication and using S3 Multi-Region Access Points is the most suitable solution to minimize latency for both uploads and streaming with the fewest application changes.
</details>

<details>
  <summary>Question 737</summary>

A company has an application that delivers on-demand training videos to students around the world. The application also allows authorized content developers to upload videos. The data is stored in an Amazon S3 bucket in the us-east-2 Region. The company has created an S3 bucket in the eu-west-2 Region and an S3 bucket in the ap-southeast-1 Region. The company wants to replicate the data to the new S3 buckets. The company needs to minimize latency for developers who upload videos and students who stream videos near eu-west-2 and ap-southeast-1. Which combination of steps will meet these requirements with the FEWEST changes to the application? (Choose two.)

-   [ ] A. Configure one-way replication from the us-east-2 S3 bucket to the eu-west-2 S3 bucket. Configure one-way replication from the us-east-2 S3 bucket to the ap-southeast-1 S3 bucket.
-   [ ] B. Configure one-way replication from the us-east-2 S3 bucket to the eu-west-2 S3 bucket. Configure one-way replication from the eu-west-2 S3 bucket to the ap-southeast-1 S3 bucket.
-   [ ] C. Configure two-way (bidirectional) replication among the S3 buckets that are in all three Regions.
-   [ ] D. Create an S3 Multi-Region Access Point. Modify the application to use the Amazon Resource Name (ARN) of the Multi-Region Access Point for video streaming. Do not modify the application for video uploads.
-   [ ] E. Create an S3 Multi-Region Access Point. Modify the application to use the Amazon Resource Name (ARN) of the Multi-Region Access Point for video streaming and uploads.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Configure two-way (bidirectional) replication among the S3 buckets that are in all three Regions.
-   [ ] E. Create an S3 Multi-Region Access Point. Modify the application to use the Amazon Resource Name (ARN) of the Multi-Region Access Point for video streaming and uploads.

Why these are the correct answers:

C.  Configure two-way (bidirectional) replication among the S3 buckets that are in all three Regions.

-   [ ] Bidirectional replication ensures that uploads and updates in any Region are reflected in the other Regions, minimizing latency for both uploads and streaming.

E.  Create an S3 Multi-Region Access Point. Modify the application to use the Amazon Resource Name (ARN) of the Multi-Region Access Point for video streaming and uploads.

-   [ ] S3 Multi-Region Access Points simplify data access across multiple S3 buckets in different Regions using a single endpoint.
-   [ ] Modifying the application to use the Multi-Region Access Point ARN for both uploads and streaming optimizes latency and reduces changes to the application.

Why are the other answers wrong?

-   [ ] A and B. One-way replication does not address the requirement for low latency for both uploads and streaming in all Regions. It only replicates data in one direction.
-   [ ] D. Not modifying the application for uploads means that uploads would still go to a single Region, which does not minimize latency for developers in other Regions.

Therefore, the combination of bidirectional replication and using S3 Multi-Region Access Points is the most suitable solution to minimize latency for both uploads and streaming with the fewest application changes.
</details>
<details>
  <summary>Question 738</summary>

A company has a new mobile app. Anywhere in the world, users can see local news on topics they choose. Users also can post photos and videos from inside the app. Users access content often in the first minutes after the content is posted. New content quickly replaces older content, and then the older content disappears. The local nature of the news means that users consume 90% of the content within the AWS Region where it is uploaded. Which solution will optimize the user experience by providing the LOWEST latency for content uploads?

-   [ ] A. Upload and store content in Amazon S3. Use Amazon CloudFront for the uploads.
-   [ ] B. Upload and store content in Amazon S3. Use S3 Transfer Acceleration for the uploads.
-   [ ] C. Upload content to Amazon EC2 instances in the Region that is closest to the user. Copy the data to Amazon S3.
-   [ ] D. Upload and store content in Amazon S3 in the Region that is closest to the user. Use multiple distributions of Amazon CloudFront.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Upload and store content in Amazon S3. Use S3 Transfer Acceleration for the uploads.

Why these are the correct answers:

B.  Upload and store content in Amazon S3. Use S3 Transfer Acceleration for the uploads.

-   [ ] S3 Transfer Acceleration is designed to speed up transfers of files over long distances by using Amazon's CloudFront network.
-   [ ] It optimizes the path to the S3 bucket, reducing latency for uploads from anywhere in the world.

Why are the other answers wrong?

-   [ ] A. Amazon CloudFront is optimized for content delivery (downloads) rather than uploads.
-   [ ] C. Uploading to EC2 instances first and then copying to S3 adds complexity and latency.
-   [ ] D. Using multiple CloudFront distributions does not improve upload latency; CloudFront is primarily for caching and delivering content.

Therefore, Option B is the most effective solution for minimizing upload latency for a global mobile app.
</details>

<details>
  <summary>Question 739</summary>

A company is building a new application that uses serverless architecture. The architecture will consist of an Amazon API Gateway REST API and AWS Lambda functions to manage incoming requests. Topic 1

The company wants to add a service that can send messages received from the API Gateway REST API to multiple target Lambda functions for processing. The service must offer message filtering that gives the target Lambda functions the ability to receive only the messages the functions need. Which solution will meet these requirements with the LEAST operational overhead?

-   [ ] A. Send the requests from the API Gateway REST API to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe Amazon Simple Queue Service (Amazon SQS) queues to the SNS topic. Configure the target Lambda functions to poll the different SQS queues.
-   [ ] B. Send the requests from the API Gateway REST API to Amazon EventBridge. Configure EventBridge to invoke the target Lambda functions.
-   [ ] C. Send the requests from the API Gateway REST API to Amazon Managed Streaming for Apache Kafka (Amazon MSK). Configure Amazon MSK to publish the messages to the target Lambda functions.
-   [ ] D. Send the requests from the API Gateway REST API to multiple Amazon Simple Queue Service (Amazon SQS) queues. Configure the target Lambda functions to poll the different SQS queues.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Send the requests from the API Gateway REST API to Amazon EventBridge. Configure EventBridge to invoke the target Lambda functions.

Why these are the correct answers:

B.  Send the requests from the API Gateway REST API to Amazon EventBridge. Configure EventBridge to invoke the target Lambda functions.

-   [ ] Amazon EventBridge allows you to route events from API Gateway to multiple Lambda functions.
-   [ ] EventBridge provides built-in filtering capabilities, enabling each Lambda function to receive only relevant messages.
-   [ ] This approach minimizes operational overhead as EventBridge is a serverless service that automates event routing and filtering.

Why are the other answers wrong?

-   [ ] A. Using Amazon SNS with SQS queues adds complexity and operational overhead, as it requires managing both SNS topics and SQS queues, and setting up polling mechanisms for Lambda functions.
-   [ ] C. Amazon MSK is a managed Kafka service, which is designed for streaming data, not for simple message routing to Lambda functions. It introduces significant operational overhead.
-   [ ] D. Sending requests to multiple SQS queues requires configuring and managing multiple queues and setting up polling for each Lambda function, increasing operational overhead.

Therefore, Option B is the most efficient and least operationally intensive solution for routing and filtering messages to multiple Lambda functions.
</details>
<details>
  <summary>Question 740</summary>

A company migrated millions of archival files to Amazon S3. A solutions architect needs to implement a solution that will encrypt all the archival data by using a customer-provided key. The solution must encrypt existing unencrypted objects and future objects.

Which solution will meet these requirements?

-   [ ] A. Create a list of unencrypted objects by filtering an Amazon S3 Inventory report. Configure an S3 Batch Operations job to encrypt the objects from the list with a server-side encryption with a customer-provided key (SSE-C). Configure the S3 default encryption feature to use a server-side encryption with a customer-provided key (SSE-C).
-   [ ] B. Use S3 Storage Lens metrics to identify unencrypted S3 buckets. Configure the S3 default encryption feature to use a server-side encryption with AWS KMS keys (SSE-KMS).
-   [ ] C. Create a list of unencrypted objects by filtering the AWS usage report for Amazon S3. Configure an AWS Batch job to encrypt the objects from the list with a server-side encryption with AWS KMS keys (SSE-KMS). Configure the S3 default encryption feature to use a server-side encryption with AWS KMS keys (SSE-KMS).
-   [ ] D. Create a list of unencrypted objects by filtering the AWS usage report for Amazon S3. Configure the S3 default encryption feature to use a server-side encryption with a customer-provided key (SSE-C).

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Create a list of unencrypted objects by filtering an Amazon S3 Inventory report. Configure an S3 Batch Operations job to encrypt the objects from the list with a server-side encryption with a customer-provided key (SSE-C). Configure the S3 default encryption feature to use a server-side encryption with a customer-provided key (SSE-C).

Why these are the correct answers:

A.  Create a list of unencrypted objects by filtering an Amazon S3 Inventory report. Configure an S3 Batch Operations job to encrypt the objects from the list with a server-side encryption with a customer-provided key (SSE-C). Configure the S3 default encryption feature to use a server-side encryption with a customer-provided key (SSE-C).

-   [ ] S3 Inventory reports provide a list of objects and their metadata, which can be used to identify unencrypted objects.
-   [ ] S3 Batch Operations can be used to encrypt existing objects using SSE-C with a customer-provided key.
-   [ ] Configuring S3 default encryption with SSE-C ensures that all future objects are also encrypted with the customer-provided key.

Why are the other answers wrong?

-   [ ] B. S3 Storage Lens metrics provide insights into storage usage at the bucket level but do not provide a list of unencrypted objects. SSE-KMS uses AWS-managed keys, not customer-provided keys.
-   [ ] C and D. AWS usage reports do not provide a list of unencrypted objects. AWS Batch is not designed for directly manipulating S3 objects like S3 Batch Operations.

Therefore, Option A is the correct solution to encrypt both existing and future objects using a customer-provided key.
</details>

# AWS-SAA-PRACTICE-EXAM Questions 741-750

<details>
  <summary>Question 741</summary>

The DNS provider that hosts a company's domain name records is experiencing outages that cause service disruption for a website running on AWS. The company needs to migrate to a more resilient managed DNS service and wants the service to run on AWS. What should a solutions architect do to rapidly migrate the DNS hosting service?

-   [ ] A. Create an Amazon Route 53 public hosted zone for the domain name. Import the zone file containing the domain records hosted by the previous provider.
-   [ ] B. Create an Amazon Route 53 private hosted zone for the domain name. Import the zone file containing the domain records hosted by the previous provider.
-   [ ] C. Create a Simple AD directory in AWS. Enable zone transfer between the DNS provider and AWS Directory Service for Microsoft Active Directory for the domain records.
-   [ ] D. Create an Amazon Route 53 Resolver inbound endpoint in the VPC. Specify the IP addresses that the provider's DNS will forward DNS queries to. Configure the provider's DNS to forward DNS queries for the domain to the IP addresses that are specified in the inbound endpoint.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Create an Amazon Route 53 public hosted zone for the domain name. Import the zone file containing the domain records hosted by the previous provider.

Why these are the correct answers:

A.  Create an Amazon Route 53 public hosted zone for the domain name. Import the zone file containing the domain records hosted by the previous provider.

-   [ ] Amazon Route 53 is a highly available and scalable DNS service.
-   [ ] Creating a public hosted zone and importing the zone file is the quickest way to migrate DNS records.

Why are the other answers wrong?

-   [ ] B. A private hosted zone is used for DNS resolution within a VPC, not for public internet resolution.
-   [ ] C. Simple AD is a managed directory service, not a DNS service. Zone transfers are not the primary method for migrating public DNS records.
-   [ ] D. Route 53 Resolver inbound endpoints are for resolving private DNS records from on-premises networks, not for migrating public DNS.

Therefore, Option A is the most efficient and appropriate solution for rapidly migrating public DNS hosting to AWS.
</details>
<details>
  <summary>Question 742</summary>

A company is building an application on AWS that connects to an Amazon RDS database. The company wants to manage the application configuration and to securely store and retrieve credentials for the database and other services. Which solution will meet these requirements with the LEAST administrative overhead?

-   [ ] A. Use AWS AppConfig to store and manage the application configuration. Use AWS Secrets Manager to store and retrieve the credentials.
-   [ ] B. Use AWS Lambda to store and manage the application configuration. Use AWS Systems Manager Parameter Store to store and retrieve the credentials.
-   [ ] C. Use an encrypted application configuration file. Store the file in Amazon S3 for the application configuration. Create another S3 file to store and retrieve the credentials.
-   [ ] D. Use AWS AppConfig to store and manage the application configuration. Use Amazon RDS to store and retrieve the credentials.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Use AWS AppConfig to store and manage the application configuration. Use AWS Secrets Manager to store and retrieve the credentials.

Why these are the correct answers:

A.  Use AWS AppConfig to store and manage the application configuration. Use AWS Secrets Manager to store and retrieve the credentials.

-   [ ] AWS AppConfig is designed for managing application configuration and allows for dynamic updates.
-   [ ] AWS Secrets Manager is designed for securely storing and retrieving credentials.
-   [ ] This combination provides managed services that reduce administrative overhead.

Why are the other answers wrong?

-   [ ] B. Using AWS Lambda to store configuration adds complexity and overhead. Parameter Store is suitable for secrets but AppConfig is better for configuration.
-   [ ] C. Using encrypted files in Amazon S3 requires manual management of encryption, storage, and retrieval, increasing overhead.
-   [ ] D. Storing credentials in Amazon RDS is not a secure or recommended practice.

Therefore, Option A is the most suitable solution for managing configuration and credentials with the least administrative overhead.
</details>
<details>
  <summary>Question 743</summary>

To meet security requirements, a company needs to encrypt all of its application data in transit while communicating with an Amazon RDS MySQL DB instance. A recent security audit revealed that encryption at rest is enabled using AWS Key Management Service (AWS KMS), but data in transit is not enabled. What should a solutions architect do to satisfy the security requirements?

-   [ ] A. Enable IAM database authentication on the database.
-   [ ] B. Provide self-signed certificates. Use the certificates in all connections to the RDS instance.
-   [ ] C. Take a snapshot of the RDS instance. Restore the snapshot to a new instance with encryption enabled.
-   [ ] D. Download AWS-provided root certificates. Provide the certificates in all connections to the RDS instance.

</details>

<details>
  <summary>Answer</summary>

-   [ ] D. Download AWS-provided root certificates. Provide the certificates in all connections to the RDS instance.

Why these are the correct answers:

D.  Download AWS-provided root certificates. Provide the certificates in all connections to the RDS instance.

-   [ ] AWS provides root certificates that can be used to encrypt connections to RDS instances.
-   [ ] Using these certificates ensures that data in transit is encrypted.

Why are the other answers wrong?

-   [ ] A. IAM database authentication is for authenticating users with IAM roles, not for encrypting data in transit.
-   [ ] B. Self-signed certificates are not recommended for production environments due to security concerns.
-   [ ] C. Taking a snapshot and restoring does not address encryption in transit.

Therefore, Option D is the correct solution to encrypt data in transit to the RDS MySQL DB instance.
</details>
<details>
  <summary>Question 744</summary>

A company is designing a new web service that will run on Amazon EC2 instances behind an Elastic Load Balancing (ELB) load balancer. However, many of the web service clients can only reach IP addresses authorized on their firewalls. What should a solutions architect recommend to meet the clients' needs?

-   [ ] A. A Network Load Balancer with an associated Elastic IP address.
-   [ ] B. An Application Load Balancer with an associated Elastic IP address.
-   [ ] C. An A record in an Amazon Route 53 hosted zone pointing to an Elastic IP address.
-   [ ] D. An EC2 instance with a public IP address running as a proxy in front of the load balancer.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. A Network Load Balancer with an associated Elastic IP address.

Why these are the correct answers:

A.  A Network Load Balancer with an associated Elastic IP address.

-   [ ] Network Load Balancers (NLBs) support assigning Elastic IP addresses, providing static IP addresses that clients can authorize on their firewalls.

Why are the other answers wrong?

-   [ ] B. Application Load Balancers (ALBs) do not support assigning Elastic IP addresses.
-   [ ] C. An A record in Route 53 points a domain name to an IP address but does not ensure the load balancer itself has a static IP.
-   [ ] D. Using an EC2 instance as a proxy adds complexity and is not as scalable or highly available as using an NLB.

Therefore, Option A is the correct solution to provide static IP addresses for the web service clients.
</details>
<details>
  <summary>Question 745</summary>

A company has established a new AWS account. The account is newly provisioned and no changes have been made to the default settings. The company is concerned about the security of the AWS account root user. What should be done to secure the root user?

-   [ ] A. Create IAM users for daily administrative tasks. Disable the root user.
-   [ ] B. Create IAM users for daily administrative tasks. Enable multi-factor authentication on the root user.
-   [ ] C. Generate an access key for the root user. Use the access key for daily administration tasks instead of the AWS Management Console.
-   [ ] D. Provide the root user credentials to the most senior solutions architect. Have the solutions architect use the root user for daily administration tasks.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Create IAM users for daily administrative tasks. Enable multi-factor authentication on the root user.

Why these are the correct answers:

B.  Create IAM users for daily administrative tasks. Enable multi-factor authentication on the root user.

-   [ ] Creating IAM users follows the principle of least privilege.
-   [ ] Enabling multi-factor authentication (MFA) adds an extra layer of security to the root user account.

Why are the other answers wrong?

-   [ ] A. Disabling the root user is not possible or recommended, as it is required for certain account-level tasks.
-   [ ] C. Generating an access key for the root user is not recommended for daily tasks, as it poses a security risk if the key is compromised.
-   [ ] D. Sharing root user credentials is a security risk and a bad practice.

Therefore, Option B is the correct approach to secure the root user while allowing for administrative tasks.
</details>
<details>
  <summary>Question 746</summary>

A company is deploying an application that processes streaming data in near-real time. The company plans to use Amazon EC2 instances for the workload. The network architecture must be configurable to provide the lowest possible latency between nodes. Which combination of network solutions will meet these requirements? (Choose two.)

-   [ ] A. Enable and configure enhanced networking on each EC2 instance.
-   [ ] B. Group the EC2 instances in separate accounts.
-   [ ] C. Run the EC2 instances in a cluster placement group.
-   [ ] D. Attach multiple elastic network interfaces to each EC2 instance.
-   [ ] E. Use Amazon Elastic Block Store (Amazon EBS) optimized instance types.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Enable and configure enhanced networking on each EC2 instance.
-   [ ] C. Run the EC2 instances in a cluster placement group.

Why these are the correct answers:

A.  Enable and configure enhanced networking on each EC2 instance.

-   [ ] Enhanced networking enables higher bandwidth, higher packet per second (PPS) performance, and lower latency between EC2 instances.

C.  Run the EC2 instances in a cluster placement group.

-   [ ] Cluster placement groups reduce latency and improve performance for applications that require tight coupling and high communication speed between instances.

Why are the other answers wrong?

-   [ ] B. Grouping EC2 instances in separate accounts does not impact network latency.
-   [ ] D. Attaching multiple elastic network interfaces does not necessarily reduce latency for inter-node communication in the same way as enhanced networking and placement groups.
-   [ ] E. EBS-optimized instance types improve throughput to EBS volumes but do not directly reduce latency between EC2 instances.

Therefore, the combination of enhanced networking and cluster placement groups is the most suitable solution for minimizing latency in this scenario.
</details>
<details>
  <summary>Question 747</summary>

A financial services company wants to shut down two data centers and migrate more than 100 TB of data to AWS. The data has an intricate directory structure with millions of small files stored in deep hierarchies of subfolders. Most of the data is unstructured, and the company's file storage consists of SMB-based storage types from multiple vendors. The company does not want to change its applications to access the data after migration. What should a solutions architect do to meet these requirements with the LEAST operational overhead?

-   [ ] A. Use AWS Direct Connect to migrate the data to Amazon S3.
-   [ ] B. Use AWS DataSync to migrate the data to Amazon FSx for Lustre.
-   [ ] C. Use AWS DataSync to migrate the data to Amazon FSx for Windows File Server.
-   [ ] D. Use AWS Direct Connect to migrate the data on-premises file storage to an AWS Storage Gateway volume gateway.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Use AWS DataSync to migrate the data to Amazon FSx for Windows File Server.

Why these are the correct answers:

C.  Use AWS DataSync to migrate the data to Amazon FSx for Windows File Server.

-   [ ] AWS DataSync simplifies and accelerates data migration.
-   [ ] Amazon FSx for Windows File Server provides a fully managed Windows file server in AWS, supporting SMB and preserving file system metadata and permissions.
-   [ ] This allows applications to access the data without changes.

Why are the other answers wrong?

-   [ ] A. Amazon S3 is object storage, not file storage, and requires application changes.
-   [ ] B. Amazon FSx for Lustre is designed for high-performance computing and is not suitable for general-purpose file storage or SMB.
-   [ ] D. AWS Storage Gateway requires changes to application access patterns and does not fully migrate the file storage to AWS.

Therefore, Option C is the most suitable solution for migrating the data with minimal application changes and operational overhead.
</details>
<details>
  <summary>Question 748</summary>

A company uses an organization in AWS Organizations to manage AWS accounts that contain applications. The company sets up a dedicated monitoring member account in the organization. The company wants to query and visualize observability data across the accounts by using Amazon CloudWatch. Which solution will meet these requirements?

-   [ ] A. Enable CloudWatch cross-account observability for the monitoring account. Deploy an AWS CloudFormation template provided by the monitoring account in each AWS account to share the data with the monitoring account.
-   [ ] B. Set up service control policies (SCPs) to provide access to CloudWatch in the monitoring account under the Organizations root organizational unit (OU).
-   [ ] C. Configure a new IAM user in the monitoring account. In each AWS account, configure an IAM policy to have access to query and visualize the CloudWatch data in the account. Attach the new IAM policy to the new IAM user.
-   [ ] D. Create a new IAM user in the monitoring account. Create cross-account IAM policies in each AWS account. Attach the IAM policies to the new IAM user.

</details>

<details>
  <summary>Answer</summary>

-   [ ] A. Enable CloudWatch cross-account observability for the monitoring account. Deploy an AWS CloudFormation template provided by the monitoring account in each AWS account to share the data with the monitoring account.

Why these are the correct answers:

A.  Enable CloudWatch cross-account observability for the monitoring account. Deploy an AWS CloudFormation template provided by the monitoring account in each AWS account to share the data with the monitoring account.

-   [ ] CloudWatch cross-account observability allows you to view and analyze metrics, logs, and traces from multiple AWS accounts in a single monitoring account.
-   [ ] Using CloudFormation templates simplifies the process of configuring the necessary permissions in each account.

Why are the other answers wrong?

-   [ ] B. SCPs control permissions at the AWS Organizations level but do not provide the necessary granularity for CloudWatch cross-account access.
-   [ ] C and D. Creating IAM users and policies in each account is more complex and introduces more operational overhead than using CloudWatch cross-account observability.

Therefore, Option A is the most efficient and recommended solution for centralized monitoring with CloudWatch across multiple AWS accounts.
</details>
<details>
  <summary>Question 749</summary>

A company's website is used to sell products to the public. The site runs on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). There is also an Amazon CloudFront distribution, and AWS WAF is being used to protect against SQL injection attacks. The ALB is the origin for the CloudFront distribution. A recent review of security logs revealed an external malicious IP that needs to be blocked from accessing the website. What should a solutions architect do to protect the application?

-   [ ] A. Modify the network ACL on the CloudFront distribution to add a deny rule for the malicious IP address.
-   [ ] B. Modify the configuration of AWS WAF to add an IP match condition to block the malicious IP address.
-   [ ] C. Modify the network ACL for the EC2 instances in the target groups behind the ALB to deny the malicious IP address.
-   [ ] D. Modify the security groups for the EC2 instances in the target groups behind the ALB to deny the malicious IP address.

</details>

<details>
  <summary>Answer</summary>

-   [ ] B. Modify the configuration of AWS WAF to add an IP match condition to block the malicious IP address.

Why these are the correct answers:

B.  Modify the configuration of AWS WAF to add an IP match condition to block the malicious IP address.

-   [ ] AWS WAF is designed to protect web applications from various attacks, including blocking specific IP addresses.
-   [ ] It integrates with CloudFront and ALB, making it the appropriate place to block malicious IPs.

Why are the other answers wrong?

-   [ ] A. Network ACLs operate at the subnet level and are not as granular or efficient for blocking specific IPs for web traffic as WAF.
-   [ ] C and D. Modifying network ACLs or security groups on the EC2 instances is less efficient and harder to manage than using WAF, especially when CloudFront is in use.

Therefore, Option B is the most suitable and efficient solution for blocking a malicious IP address in this web application architecture.
</details>

<details>
  <summary>Question 750</summary>

A company sets up an organization in AWS Organizations that contains 10 AWS accounts. A solutions architect must design a solution to provide access to the accounts for several thousand employees. The company has an existing identity provider (IdP). The company wants to use the existing IdP for authentication to AWS. Which solution will meet these requirements?

-   [ ] A. Create IAM users for the employees in the required AWS accounts. Connect IAM users to the existing IdP. Configure federated authentication for the IAM users.
-   [ ] B. Set up AWS account root users with user email addresses and passwords that are synchronized from the existing IdP.
-   [ ] C. Configure AWS IAM Identity Center (AWS Single Sign-On). Connect IAM Identity Center to the existing IdP. Provision users and groups from the existing IdP.
-   [ ] D. Use AWS Resource Access Manager (AWS RAM) to share access to the AWS accounts with the users in the existing IdP.

</details>

<details>
  <summary>Answer</summary>

-   [ ] C. Configure AWS IAM Identity Center (AWS Single Sign-On). Connect IAM Identity Center to the existing IdP. Provision users and groups from the existing IdP.

Why these are the correct answers:

C.  Configure AWS IAM Identity Center (AWS Single Sign-On). Connect IAM Identity Center to the existing IdP. Provision users and groups from the existing IdP.

-   [ ] AWS IAM Identity Center (successor to AWS Single Sign-On) allows you to connect an existing IdP to AWS, providing centralized access management for multiple AWS accounts.
-   [ ] It supports provisioning users and groups, simplifying the management of access for a large number of employees.

Why are the other answers wrong?

-   [ ] A. Creating IAM users in each AWS account and configuring federated authentication would be complex and time-consuming for thousands of employees across 10 accounts.
-   [ ] B. Using AWS account root users is not a secure or recommended practice for providing employee access.
-   [ ] D. AWS Resource Access Manager (AWS RAM) is used for sharing AWS resources, not for managing user authentication to AWS accounts.

Therefore, Option C is the most suitable solution for providing access to multiple AWS accounts for a large number of employees using an existing IdP.
</details>














